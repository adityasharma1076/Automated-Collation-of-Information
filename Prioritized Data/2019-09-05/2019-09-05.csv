,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,ACM,Companies Use Cyber Ranges to Practice Security Skills,ACM,2019-09-03,-,https://www.wsj.com/articles/companies-use-cyber-ranges-to-practice-security-skills-11567530531?mod=itp_wsj&ru=yahoo,"       Companies Use Cyber Ranges to Practice Security Skills     The Wall Street JournalAdam JanofskySeptember 3, 2019   U.S. companies and universities are developing cybersecurity training facilities that model real-world networks and scenarios to educate staff and evaluate theories about cyberdefense and response strategies. MasterCard's Ron Green said these cyber ranges allow companies to observe and assess staff reactions to such real-world situations as malware infections and data breaches. In addition to a cyber range in St. Louis, MasterCard built a mobile range—basically a server rack within an armored box—to conduct tests and exercises nationwide. University-based cyber ranges offer small companies training and testing capabilities that are otherwise unaffordable to them, with most ranges running in the cloud in conjunction with remote tools. Green said MasterCard's range aims to support both defensive and offensive drills, including assault by antitank rounds, without potentially harming systems at a host range.               *May Require Paid Registration  ",Computer Science,0.1763284512472013
1,MIT News,MIT’s fleet of autonomous boats can now shapeshift,Research,2019-08-29,-,http://news.mit.edu/2019/roboats-autonomous-connect-assemble-0829,"  MIT’s fleet of robotic boats has been updated with new capabilities to “shapeshift,” by autonomously disconnecting and reassembling into a variety of configurations, to form floating structures in Amsterdam’s many canals. The autonomous boats — rectangular hulls equipped with sensors, thrusters, microcontrollers, GPS modules, cameras, and other hardware — are being developed as part of the ongoing “Roboat” project between MIT and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute). The project is led by MIT professors Carlo Ratti, Daniela Rus, Dennis Frenchman, and Andrew Whittle. In the future, Amsterdam wants the roboats to cruise its 165 winding canals, transporting goods and people, collecting trash, or self-assembling into “pop-up” platforms — such as bridges and stages — to help relieve congestion on the city’s busy streets. In 2016, MIT researchers tested a roboat prototype that could move forward, backward, and laterally along a preprogrammed path in the canals. Last year, researchers designed low-cost, 3-D-printed, one-quarter scale versions of the boats, which were more efficient and agile, and came equipped with advanced trajectory-tracking algorithms. In June, they created an autonomous latching mechanism that let the boats target and clasp onto each other, and keep trying if they fail. In a new paper presented at the last week’s IEEE International Symposium on Multi-Robot and Multi-Agent Systems, the researchers describe an algorithm that enables the roboats to smoothly reshape themselves as efficiently as possible. The algorithm handles all the planning and tracking that enables groups of roboat units to unlatch from one another in one set configuration, travel a collision-free path, and reattach to their appropriate spot on the new set configuration. In demonstrations in an MIT pool and in computer simulations, groups of linked roboat units rearranged themselves from straight lines or squares into other configurations, such as rectangles and “L” shapes. The experimental transformations only took a few minutes. More complex shapeshifts may take longer, depending on the number of moving units — which could be dozens — and differences between the two shapes.  “We’ve enabled the roboats to now make and break connections with other roboats, with hopes of moving activities on the streets of Amsterdam to the water,” says Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “A set of boats can come together to form linear shapes as pop-up bridges, if we need to send materials or people from one side of a canal to the other. Or, we can create pop-up wider platforms for flower or food markets.” Joining Rus on the paper are: Ratti, director of MIT’s Senseable City Lab, and, also from the lab, first author Banti Gheneti, Ryan Kelly, and Drew Meyers, all researchers; postdoc Shinkyu Park; and research fellow Pietro Leoni. Collision-free trajectories For their work, the researchers had to tackle challenges with autonomous planning, tracking, and connecting groups of roboat units. Giving each unit unique capabilities to, for instance, locate each other, agree on how to break apart and reform, and then move around freely, would require complex communication and control techniques that could make movement inefficient and slow. To enable smoother operations, the researchers developed two types of units: coordinators and workers. One or more workers connect to one coordinator to form a single entity, called a “connected-vessel platform” (CVP). All coordinator and worker units have four propellers, a wireless-enabled microcontroller, and several automated latching mechanisms and sensing systems that enable them to link together. Coordinators, however, also come equipped with GPS for navigation, and an inertial measurement unit (IMU), which computes localization, pose, and velocity. Workers only have actuators that help the CVP steer along a path. Each coordinator is aware of and can wirelessly communicate with all connected workers. Structures comprise multiple CVPs, and individual CVPs can latch onto one another to form a larger entity. During shapeshifting, all connected CVPs in a structure compare the geometric differences between its initial shape and new shape. Then, each CVP determines if it stays in the same spot and if it needs to move. Each moving CVP is then assigned a time to disassemble and a new position in the new shape. Each CVP uses a custom trajectory-planning technique to compute a way to reach its target position without interruption, while optimizing the route for speed. To do so, each CVP precomputes all collision-free regions around the moving CVP as it rotates and moves away from a stationary one. After precomputing those collision-free regions, the CVP then finds the shortest trajectory to its final destination, which still keeps it from hitting the stationary unit. Notably, optimization techniques are used to make the whole trajectory-planning process very efficient, with the precomputation taking little more than 100 milliseconds to find and refine safe paths. Using data from the GPS and IMU, the coordinator then estimates its pose and velocity at its center of mass, and wirelessly controls all the propellers of each unit and moves into the target location. In their experiments, the researchers tested three-unit CVPs, consisting of one coordinator and two workers, in several different shapeshifting scenarios. Each scenario involved one CVP unlatching from the initial shape and moving and relatching to a target spot around a second CVP. Three CVPs, for instance, rearranged themselves from a connected straight line — where they were latched together at their sides — into a straight line connected at front and back, as well as an “L.” In computer simulations, up to 12 roboat units rearranged themselves from, say, a rectangle into a square or from a solid square into a Z-like shape.  Scaling up Experiments were conducted on quarter-sized roboat units, which measure about 1 meter long and half a meter wide. But the researchers believe their trajectory-planning algorithm will scale well in controlling full-sized units, which will measure about 4 meters long and 2 meters wide. The researchers hope to use the roboats to form into a dynamic “bridge” across a 60-meter canal between the NEMO Science Museum in Amsterdam’s city center and an area that’s under development. Called RoundAround, the idea is to employ roboats to sail in a continuous circle across the canal, picking up and dropping off passengers at docks and stopping or rerouting when they detect anything in the way. Currently, walking around that waterway takes about 10 minutes, but the bridge can cut that time to around two minutes. This is still an explorative concept. “This will be the world’s first bridge comprised of a fleet of autonomous boats,” Ratti says. “A regular bridge would be super expensive, because you have boats going through, so you’d need to have a mechanical bridge that opens up or a very high bridge. But we can connect two sides of canal [by using] autonomous boats that become dynamic, responsive architecture that float on the water.” To reach that goal, the researchers are further developing the roboats to ensure they can safely hold people, and are robust to all weather conditions, such as heavy rain. They’re also making sure the roboats can effectively connect to the sides of the canals, which can vary greatly in structure and design. ",Computer Science,0.17426779937633452
2,ACM,Developing Embedded Systems Faster,ACM,2019-09-02,-,https://www.fraunhofer.de/en/press/research-news/2019/september/developing-embedded-systems-faster.html,"    Developing Embedded Systems Faster     Fraunhofer-GesellschaftSeptember 2, 2019   Researchers in a consortium of eight partners from six EU countries, including the Fraunhofer Institute for Optronics, System Technologies and Image Exploitation IOSB (Fraunhofer IOSB) in Germany, have created the TULIPP platform, which makes it possible develop energy-efficient embedded image processing systems more quickly and less expensively, with a much shorter time to market. The platform consists of design guidelines, a configurable hardware platform, and a real-time-capable operating system that supports multicore processors, as well as a programming tool chain. Said Fraunhofer IOSB researcher Igor Tchouchenkov, ""The toolchain makes it possible to individually display and optimize energy consumption for each code function.""                 ",Computer Science,0.16777657143926092
3,ACM,AI System Passed an Eighth-Grade Science Test,ACM,2019-09-04,-,https://www.nytimes.com/2019/09/04/technology/artificial-intelligence-aristo-passed-test.html,"       AI System Passed an Eighth-Grade Science Test     The New York TimesCade MetzSeptember 4, 2019   The Allen Institute for Artificial Intelligence introduced an artificial intelligence (AI) system that successfully passed an eighth-grade multiple-choice science test, correctly answering over 90% of the questions, as well as scoring more than 80% on a 12th-grade test. The Aristo system's milestone suggests understanding the language and logic that high school students are expected to possess is no longer outside AI's capabilities. Aristo took standard exams written for students in New York schools, with questions including pictures and diagrams removed; some questions required simple information retrieval, while others required logical thinking. Aristo was built atop Google's Bert, a language-model system that learned via guessing missing words in sentences. By feeding Bert a broad spectrum of question and answers, the researchers enabled it to answer similar questions by itself.               *May Require Paid Registration  ",Computer Science,0.16417571247182583
4,MIT News,MIT engineers build advanced microprocessor out of carbon nanotubes,Electronics and Technology,2019-08-28,-,http://news.mit.edu/2019/carbon-nanotubes-microprocessor-0828,"  After years of tackling numerous design and manufacturing challenges, MIT researchers have built a modern microprocessor from carbon nanotube transistors, which are widely seen as a faster, greener alternative to their traditional silicon counterparts. The microprocessor, described today in the journal Nature, can be built using traditional silicon-chip fabrication processes, representing a major step toward making carbon nanotube microprocessors more practical. Silicon transistors — critical microprocessor components that switch between 1 and 0 bits to carry out computations — have carried the computer industry for decades. As predicted by Moore’s Law, industry has been able to shrink down and cram more transistors onto chips every couple of years to help carry out increasingly complex computations. But experts now foresee a time when silicon transistors will stop shrinking, and become increasingly inefficient. Making carbon nanotube field-effect transistors (CNFET) has become a major goal for building next-generation computers. Research indicates CNFETs have properties that promise around 10 times the energy efficiency and far greater speeds compared to silicon. But when fabricated at scale, the transistors often come with many defects that affect performance, so they remain impractical. The MIT researchers have invented new techniques to dramatically limit defects and enable full functional control in fabricating CNFETs, using processes in traditional silicon chip foundries. They demonstrated a 16-bit microprocessor with more than 14,000 CNFETs that performs the same tasks as commercial microprocessors. The Nature paper describes the microprocessor design and includes more than 70 pages detailing the manufacturing methodology. The microprocessor is based on the RISC-V open-source chip architecture that has a set of instructions that a microprocessor can execute. The researchers’ microprocessor was able to execute the full set of instructions accurately. It also executed a modified version of the classic “Hello, World!” program, printing out, “Hello, World! I am RV16XNano, made from CNTs.” “This is by far the most advanced chip made from any emerging nanotechnology that is promising for high-performance and energy-efficient computing,” says co-author Max M. Shulaker, the Emanuel E Landsman Career Development Assistant Professor of Electrical Engineering and Computer Science (EECS) and a member of the Microsystems Technology Laboratories. “There are limits to silicon. If we want to continue to have gains in computing, carbon nanotubes represent one of the most promising ways to overcome those limits. [The paper] completely re-invents how we build chips with carbon nanotubes.” Joining Shulaker on the paper are: first author and postdoc Gage Hills, graduate students Christian Lau, Andrew Wright, Mindy D. Bishop, Tathagata Srimani, Pritpal Kanhaiya, Rebecca Ho, and Aya Amer, all of EECS; Arvind, the Johnson Professor of Computer Science and Engineering and a researcher in the Computer Science and Artificial Intelligence Laboratory; Anantha Chandrakasan, the dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science; and Samuel Fuller, Yosi Stein, and Denis Murphy, all of Analog Devices. Fighting the “bane” of CNFETs The microprocessor builds on a previous iteration designed by Shulaker and other researchers six years ago that had only 178 CNFETs and ran on a single bit of data. Since then, Shulaker and his MIT colleagues have tackled three specific challenges in producing the devices: material defects, manufacturing defects, and functional issues. Hills did the bulk of the microprocessor design, while Lau handled most of the manufacturing. For years, the defects intrinsic to carbon nanotubes have been a “bane of the field,” Shulaker says. Ideally, CNFETs need semiconducting properties to switch their conductivity on an off, corresponding to the bits 1 and 0. But unavoidably, a small portion of carbon nanotubes will be metallic, and will slow or stop the transistor from switching. To be robust to those failures, advanced circuits will need carbon nanotubes at around 99.999999 percent purity, which is virtually impossible to produce today.   The researchers came up with a technique called DREAM (an acronym for “designing resiliency against metallic CNTs”), which positions metallic CNFETs in a way that they won’t disrupt computing. In doing so, they relaxed that stringent purity requirement by around four orders of magnitude — or 10,000 times — meaning they only need carbon nanotubes at about 99.99 percent purity, which is currently possible. Designing circuits basically requires a library of different logic gates attached to transistors that can be combined to, say, create adders and multipliers — like combining letters in the alphabet to create words. The researchers realized that the metallic carbon nanotubes impacted different pairings of these gates differently. A single metallic carbon nanotube in gate A, for instance, may break the connection between A and B. But several metallic carbon nanotubes in gates B may not impact any of its connections. In chip design, there are many ways to implement code onto a circuit. The researchers ran simulations to find all the different gate combinations that would be robust and wouldn’t be robust to any metallic carbon nanotubes. They then customized a chip-design program to automatically learn the combinations least likely to be affected by metallic carbon nanotubes. When designing a new chip, the program will only utilize the robust combinations and ignore the vulnerable combinations. “The ‘DREAM’ pun is very much intended, because it’s the dream solution,” Shulaker says. “This allows us to buy carbon nanotubes off the shelf, drop them onto a wafer, and just build our circuit like normal, without doing anything else special.” Exfoliating and tuning CNFET fabrication starts with depositing carbon nanotubes in a solution onto a wafer with predesigned transistor architectures. However, some carbon nanotubes inevitably stick randomly together to form big bundles — like strands of spaghetti formed into little balls — that form big particle contamination on the chip.   To cleanse that contamination, the researchers created RINSE (for “removal of incubated nanotubes through selective exfoliation”). The wafer gets pretreated with an agent that promotes carbon nanotube adhesion. Then, the wafer is coated with a certain polymer and dipped in a special solvent. That washes away the polymer, which only carries away the big bundles, while the single carbon nanotubes remain stuck to the wafer. The technique leads to about a 250-times reduction in particle density on the chip compared to similar methods. Lastly, the researchers tackled common functional issues with CNFETs. Binary computing requires two types of transistors: “N” types, which turn on with a 1 bit and off with a 0 bit, and “P” types, which do the opposite. Traditionally, making the two types out of carbon nanotubes has been challenging, often yielding transistors that vary in performance. For this solution, the researchers developed a technique called MIXED (for “metal interface engineering crossed with electrostatic doping”), which precisely tunes transistors for function and optimization. In this technique, they attach certain metals to each transistor — platinum or titanium — which allows them to fix that transistor as P or N. Then, they coat the CNFETs in an oxide compound through atomic-layer deposition, which allows them to tune the transistors’ characteristics for specific applications. Servers, for instance, often require transistors that act very fast but use up energy and power. Wearables and medical implants, on the other hand, may use slower, low-power transistors.   The main goal is to get the chips out into the real world. To that end, the researchers have now started implementing their manufacturing techniques into a silicon chip foundry through a program by Defense Advanced Research Projects Agency, which supported the research. Although no one can say when chips made entirely from carbon nanotubes will hit the shelves, Shulaker says it could be fewer than five years. “We think it’s no longer a question of if, but when,” he says. The work was also supported by Analog Devices, the National Science Foundation, and the Air Force Research Laboratory. ",Electronics and Technology,0.16397905147888495
5,MIT News,Ultrathin 3-D-printed films convert energy of one form into another,Electronics and Technology,2019-08-28,-,http://news.mit.edu/2019/3-d-printed-piezoelectric-films-0828,"  MIT researchers have developed a simple, low-cost method to 3-D print ultrathin films with high-performing “piezoelectric” properties, which could be used for components in flexible electronics or highly sensitive biosensors. Piezoelectric materials produce a voltage in response to physical strain, and they respond to a voltage by physically deforming. They’re commonly used for transducers, which convert energy of one form into another. Robotic actuators, for instance, use piezoelectric materials to move joints and parts in response to an electrical signal. And various sensors use the materials to convert changes in pressure, temperature, force, and other physical stimuli, into a measurable electrical signal. Researchers have been trying for years to develop piezoelectric ultrathin films that can be used as energy harvesters, sensitive pressure sensors for touch screens, and other components in flexible electronics. The films could also be used as tiny biosensors that are sensitive enough to detect the presence of molecules that are biomarkers for certain diseases and conditions. The material of choice for those applications is often a type of ceramic with a crystal structure that resonates at high frequencies due to its extreme thinness. (Higher frequencies basically translate to faster speeds and higher sensitivity.) But, with traditional fabrication techniques, creating ceramic ultrathin films is a complex and expensive process. In a paper recently published in the journal Applied Materials and Interfaces, the MIT researchers describe a way to 3-D print ceramic transducers about 100 nanometers thin by adapting an additive manufacturing technique for the process that builds objects layer by layer, at room temperature. The films can be printed in flexible substrates with no loss in performance, and can resonate at around 5 gigahertz, which is high enough for high-performance biosensors. “Making transducing components is at the heart of the technological revolution,” says Luis Fernando Velásquez-García, a researcher in the Microsystems Technology Laboratories (MTL) in the Department of Electrical Engineering and Computer Science. “Until now, it’s been thought 3-D-printed transducing materials will have poor performances. But we’ve developed an additive fabrication method for piezoelectric transducers at room temperature, and the materials oscillate at gigahertz-level frequencies, which is orders of magnitude higher than anything previously fabricated through 3-D printing.” Joining Velásquez-García on the paper is first author Brenda García-Farrera of MTL and the Monterrey Institute of Technology and Higher Education in Mexico. Electrospraying nanoparticles Ceramic piezoelectric thin films, made of aluminum nitride or zinc oxide, can be fabricated through physical vapor deposition and chemical vapor deposition. But those processes must be completed in sterile clean rooms, under high temperature and high vacuum conditions. That can be a time-consuming, expensive process. There are lower-cost 3-D-printed piezoelectric thin films available. But those are fabricated with polymers, which must be “poled”— meaning they must be given piezoelectric properties after they’re printed. Moreover, those materials usually end up tens of microns thick and thus can’t be made into ultrathin films capable of high-frequency actuation. The researchers’ system adapts an additive fabrication technique, called near-field electrohydrodynamic deposition (NFEHD), which uses high electric fields to eject a liquid jet through a nozzle to print an ultrathin film. Until now, the technique has not been used to print films with piezoelectric properties. The researchers’ liquid feedstock — raw material used in 3-D printing — contains zinc oxide nanoparticles mixed with some inert solvents, which forms into a piezoelectric material when printed onto a substrate and dried. The feedstock is fed through a hollow needle in a 3-D printer. As it prints, the researchers apply a specific bias voltage to the tip of the needle and control the flow rate, causing the meniscus — the curve seen at the top of a liquid — to form into a cone shape that ejects a fine jet from its tip. The jet is naturally inclined to break into droplets. But when the researchers bring the tip of the needle close to the substrate — about a millimeter — the jet doesn’t break apart. That process prints long, narrow lines on a substrate. They then overlap the lines and dry them at about 76 degrees Fahrenheit, hanging upside down. Printing the film precisely that way creates an ultrathin film of crystal structure with piezoelectric properties that resonates at about 5 gigahertz. “If anything of that process is missing, it doesn’t work,” Velásquez-García says. Using microscopy techniques, the team was able to prove that the films have a much stronger piezoelectric response — meaning the measurable signal it emits — than films made through traditional bulk fabrication methods. Those methods don’t really control the film’s piezoelectric axis direction, which determines the material’s response. “That was a little surprising,” Velásquez-García says. “In those bulk materials, they may have inefficiencies in the structure that affect performance. But when you can manipulate materials at the nanoscale, you get a stronger piezoelectric response.” “This very nice body of work demonstrates the feasibility of preparing functional piezoelectric films using 3-D printing techniques,” says Mark Allen, a professor specializing in microfabrication, nanotechnology, and microelectromechanical systems at the University of Pennsylvania. “Exploitation of this fabrication technique can lead to complex, three-dimensional, and low temperature fabrication of piezoelectric structures. I expect we will see new classes of microscale sensors, actuators, and resonators enabled by this exciting fabrication technology."" Low-cost sensors Because the piezoelectric ultrathin films are 3-D printed and resonate at very high frequencies, they can be leveraged to fabricate low-cost, highly sensitive sensors. The researchers are currently working with colleagues in Monterrey Tec as part of a collaborative program in nanoscience and nanotechnology, to make piezoelectric biosensors to detect biomarkers for certain diseases and conditions. A resonating circuit is integrated into these biosensors, which makes the piezoelectric ultrathin film oscillate at a specific frequency, and the piezoelectric material can be functionalized to attract certain molecule biomarkers to its surface. When the molecules stick to the surface, it causes the piezoelectric material to slightly shift the frequency oscillations of the circuit. That small frequency shift can be measured and correlated to a certain amount of the molecule that piles up on its surface. The researchers are also developing a sensor to measure the decay of electrodes in fuel cells. That would function similarly to the biosensor, but the shifts in frequency would correlate to the degradation of a certain alloy in the electrodes. “We’re making sensors that can diagnose the health of fuel cells, to see if they need to be replaced,” Velásquez-García says. “If you assess the health of these systems in real time, you can make decisions about when to replace them, before something serious happens.” ",Electronics and Technology,0.16298054378912016
6,ACM,Construction Robots Learn to Excavate by Mimicking Humans,ACM,2019-09-03,-,https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans,"       Construction Robots Learn to Excavate by Mimicking Humans     IEEE SpectrumLynne Peskoe-YangSeptember 3, 2019   Companies are developing excavator robots that emulate humans, with the startup SE4's software enabling a machine to stack blocks under the guidance of an operator who uses a virtual reality headset and handheld controls to demonstrate the task to the robot. Remote construction requires developers to communicate instructions to robots on-site, in real time, while managing constant environmental feedback. SE4 envisions construction robots on Mars unbound from distance-related communication lags, by teaching them to group micro-movements into logical units and make simple relational judgments so they can receive a full set of instruction modules at once, and execute them in sequence.                 ",Computer Science,0.15747436255880373
7,Science Daily,AI Learns to Model Our Universe,Computers & Math,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828100554.htm,"   Researchers seek to understand our Universe by making model predictions to match observations. Historically, they have been able to model simple or highly simplified physical systems, jokingly dubbed the ""spherical cows,"" with pencils and paper. Later, the arrival of computers enabled them to model complex phenomena with numerical simulations. For example, researchers have programmed supercomputers to simulate the motion of billions of particles through billions of years of cosmic time, a procedure known as the N-body simulations, in order to study how the Universe evolved to what we observe today. ""Now with machine learning, we have developed the first neural network model of the Universe, and demonstrated there's a third route to making predictions, one that combines the merits of both analytic calculation and numerical simulation,"" said Yin Li, a Postdoctoral Researcher at the Kavli Institute for the Physics and Mathematics of the Universe, University of Tokyo, and jointly the University of California, Berkeley. A comparison of the accuracy of two models of the Universe. The new deep learning model (left), dubbed D3M, is much more accurate than an existing analytic method (right) called 2LPT. The colors represent the error in displacement at each point relative to the numerical simulation, which is accurate but much slower than the deep learning model. At the beginning of our Universe, things were extremely uniform. As time went by, the denser parts grew denser and sparser parts became sparser due to gravity, eventually forming a foam-like structure known as the ""cosmic web."" To study this structure formation process, researchers have tried many methods, including analytic calculations and numerical simulations. Analytic methods are fast, but fail to produce accurate results for large density fluctuations. On the other hand, numerical (N-body) methods simulate structure formation accurately, but tracking gazillions of particles is costly, even on supercomputers. Thus, to model the Universe, scientists often face the accuracy versus efficiency trade-off. However, the explosive growth of observational data in quality and quantity calls for methods that excel in both accuracy and efficiency. To tackle this challenge, a team of researchers from the US, Canada, and Japan, including Li, set their sights on machine learning, a cutting-edge approach to detecting patterns and making predictions. Just as machine learning can transform a young man's portrait into his older self, Li and colleagues asked whether it can also predict how universes evolve based on their early snapshots. They trained a convolutional neural network with simulation data of trillions of cubic light years in volume, and built a deep learning model that was able to mimic the structure formation process. The new model is not only many times more accurate than the analytic methods, but is also much more efficient than the numerical simulations used for its training. ""It has the strengths of both previous [analytic calculation and numerical simulation] methods,"" said Li. Li says the power of AI emulation will scale up in the future. N-body simulations are already heavily optimized, and as a first attempt, his team's AI model still has large room for improvement. Also, more complicated phenomena incur a larger cost on simulation, but not likely so on emulation. Li and his colleagues expect a bigger performance gain from their AI emulator when they move on to including other effects, such as hydrodynamics, into the simulations. ""It won't be long before we can uncover the initial conditions of and the physics encoded in our Universe along this path,"" he said. ",Computer Science,0.15643679265582772
8,ACM,Spreading Light Over Quantum Computers,ACM,2019-09-02,-,https://liu.se/en/news-item/de-sprider-ljus-over-kvantdatorn,"    Spreading Light Over Quantum Computers     Linkoping UniversityMonica Westman SvenseliusSeptember 2, 2019   Researchers at Linkoping University in Sweden have demonstrated actual quantum-computing functioning, and simulated quantum computer properties in a classical system. Linkoping's Jan-Ake Larsson and Niklas Johansson showed that, unlike classical computers, quantum computers have two degrees of freedom for each bit. The researchers' Quantum Simulation Logic tool incorporated these two degrees of freedom, which involve computation bits and phase bits that respectively convey data on function result and function structure. Several quantum algorithms that were studied operated as rapidly in the simulation as they would in a quantum computer. Larsson said, ""The higher speed in quantum computers comes from their ability to store, process, and retrieve information in one additional information-carrying degree of freedom. This enables us to better understand how quantum computers work.""                 ",Electronics and Technology,0.15611109567332704
9,ACM,French Researcher Hacks into Moscow's New E-Voting System,ACM,2019-08-28,-,https://www.france24.com/en/20190828-french-researcher-hacks-into-moscow-s-new-e-voting-system,"    French Researcher Hacks into Moscow's New E-Voting System     Agence France-PresseAugust 28, 2019   A French cryptographer has exposed a security flaw in an electronic voting system to be used in this month's municipal elections in Moscow. The flaw could give hackers access to voters' choices. Pierrick Gaudry used a standard computer and free software to access the source code being published daily as part of a public test that started in late July. Gaudry needed just 20 minutes to break the encryption code that is meant to protect voters' identities and choices. The online voting system requires passport information, home addresses, and other sensitive data, and uses a text message verification. ""In the worst-case scenario, the votes of all the voters using this system would be revealed to anyone as soon as they cast their vote,"" Gaudry wrote. Moscow authorities have said since Gaudry’s discovery was made public, the encryption code has been made more complex, and will be divided into seven parts kept separate until voting ends.                 ",Computer Science,0.15487920103640984
10,IEEE,AI at the Speed of Light,Electronics and Technology,2019-08-29,-,https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light,"      Neural networks shine for solving tough problems such as facial and voice recognition, but conventional electronic versions are limited in speed and hungry for power. In theory, optics could beat digital electronic computers in the matrix calculations used in neural networks. However, optics had been limited by their inability to do some complex calculations that had required electronics. Now new experiments show that all-optical neural networks can tackle those problems.  The key attraction of neural networks is their massive interconnections among processors, comparable to the complex interconnections among neurons in the brain. This lets them perform many operations simultaneously, like the human brain does when looking at faces or listening to speech, making them more efficient for facial and voice recognition than traditional electronic computers that execute one instruction at a time. Today  electronic neural networks have reached eight million neurons, but their future use in artificial intelligence may be limited by their high power usage and limited parallelism in connections. Optical connections through lenses are inherently parallel. The lens in your eye simultaneously focuses light from across your field of view onto the retina in the back of your eye, where an array of light-detecting nerve cells detects the light. Each cell then relays the signal it receives to neurons in the brain that process the visual signals to show us an image. Glass lenses process optical signals by focusing light, which performs a complex mathematical operation called a Fourier transform that preserves the information in the original scene but rearranges is completely. One use of Fourier transforms is converting time variations in signal intensity into a plot of the frequencies present in the signal. The military used this trick in the 1950s to convert raw radar return signals recorded by an aircraft in flight into a three-dimensional image of the landscape viewed by the plane. Today that conversion is done electronically, but the vacuum-tube computers of the 1950s were not up to the task. Development of neural networks for artificial intelligence started with electronics, but their AI applications have been limited by their slow processing and need for extensive computing resources. Some researchers have developed hybrid neural networks, in which optics perform simple linear operations, but electronics perform more complex nonlinear calculations. Now two groups have demonstrated simple all-optical neural networks that do all processing with light. In May, Wolfram Pernice of the Institute of Physics at the University of Münster in Germany and colleagues reported testing an all-optical ""neuron"" in which signals change target materials between liquid and solid states, an effect that has been used for optical data storage. They demonstrated nonlinear processing, and produced output pulses like those from organic neurons. They then produced an integrated photonic circuit that incorporated four optical neurons operating at different wavelengths, each of which connected to 15 optical synapses. The photonic circuit contained more than 140 components and could recognize simple optical patterns. The group wrote that their device is scalable, and that the technology promises ""access to the high speed and high bandwidth inherent to optical systems, thus enabling the direct processing of optical telecommunication and visual data.”  Now a group at the Hong Kong University of Science and Technology reports in Optica that they have made an all-optical neural network based on a different process, electromagnetically induced transparency, in which incident light affects how atoms shift between quantum-mechanical energy levels. The process is nonlinear and can be triggered by very weak light signals, says Shengwang Du, a physics professor and coauthor of the paper. In their demonstration, they illuminated rubidium-85 atoms cooled by lasers to about 10 microKelvin (10 microdegrees above absolute zero). Although the technique may seem unusually complex, Du said the system was the most accessible one in the lab that could produce the desired effects. ""As a pure quantum atomic system [it] is ideal for this proof-of-principle experiment,"" he says. Next, they plan to scale up the demonstration using a hot atomic vapor center, which is less expensive, does not require time-consuming preparation of cold atoms, and can be integrated with photonic chips. Du says the major challenges are reducing cost of the nonlinear processing medium and increasing the scale of the all-optical neural network for more complex tasks. ""Their demonstration seems valid,"" says Volker Sorger, an electrical engineer at George Washington University in Washington who was not involved in either demonstration. He says the all-optical approach is attractive because it offers very high parallelism, but the update rate is limited to about 100 hertz because of the liquid crystals used in their test, and he is not completely convinced their approach can be scaled error-free.    Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Computer Science,0.1531173856591807
11,IEEE,A Carbon Nanotube Microprocessor Mature Enough to Say Hello,Electronics and Technology,2019-08-28,-,https://spectrum.ieee.org/nanoclast/semiconductors/processors/modern-microprocessor-built-using-carbon-nanotubes,"      Engineers at MIT and Analog Devices have created the first fully-programmable 16-bit carbon nanotube microprocessor. It’s the most complex integration of carbon nanotube-based CMOS logic so far, with nearly 15,000 transistors, and it was done using technologies that have already been proven to work in a commercial chip-manufacturing facility. The processor, called RV16X-NANO, is a milestone in the development of beyond-silicon technologies, its inventors say.  Unlike silicon transistors, nanotube devices can easily be made in multiple layers with dense 3D interconnections. The Defense Advanced Research Projects Agency is hoping this 3D aspect will lead to commercial carbon nanotube (CNT) chips with the performance of today’s cutting-edge silicon but without the high design and manufacturing cost. Some of the same researchers created a modest one-bit, 178-transistor processor back in 2013. In contrast, the new one, which is based on the open source RISC-V instruction set, is capable of working with 16-bit data and 32-bit instructions. Naturally, the team, led by MIT assistant professor Max Shulaker, tested the chip by running a version of the obligatory “Hello, World!” program. They reported the achievement this week in Nature. “Ten years ago, we hoped this was possible,” says Shulaker. “Now we know it is possible… and we know it can be done in commercial facilities.” Shulaker’s team, along with engineers at Analog Devices and, later, Skywater Technology Foundry, developed three commercially-viable techniques to create the RV16X-NANO. Two dealt with stubborn issues of carbon nanotube purity and uniformity, and the third allowed for the creation of both n-type and p-type transistors to form complementary logic circuits. 1. When making CNT transistors, the nanotubes are first put into a solution and spread across a silicon wafer. Most of the nanotubes lie uniformly on the silicon, but every once in a while, they ball up into bundles of a thousand or more. These bundles can’t form transistors. When building small-scale test circuits, this was no big deal, Shulaker explains, because even if they killed one circuit, another would work. But for a large-scale integration like for the RV16X-NANO these nanotube-pile-ups would be common enough to mess up the whole processor. RINSE, a solution one of Shulaker’s students, Christian Lau, arrived at, relies on the fact that individual nanotubes are stuck to the substrate by Van Der Waals forces more strongly than bundles are. By first coating the nanotube-covered substrate with a photo resist and then carefully washing it away—under just the right conditions—the process selectively removes the bundles but leaves the individual CNTs. 2. While RINSE dealt with one carbon-nanotube impurity, another purity problem nearly crashed the whole project. CNTs have always come in two basic flavors, metallic and semiconducting. Having some metallic nanotubes in a CNT-based logic gate means the circuit will waste power and produce a noisy signal. But how many metallic nanotubes is too many when you’re trying to build a full-scale processor? “It’s a very basic question,” says Shulaker. And to his surprise, it hadn’t been answered. The answer his team came up with was “pretty depressing.” The best today’s commercial processes could produce is 99.99 percent semiconducting nanotubes and 0.01 percent metallic. But what’s needed is 99.999999 percent purity—impossibly far out of reach. “We thought, if we can’t process our way out of this… then somehow we had to design our way around it,” says Shulaker. The team found that, by far, the main driver for the needed purity was not the power issue but the noise. Amongst the many logic circuits they’d made, they found a pattern that suggested some combinations were much more susceptible to the noise problem than others. “So the solution at that point was simple: We’ll just design circuits with the good combinations of logic gates and avoid using the bad combinations.” DREAM, the set of design rules post-doctoral researcher Gage Hill came up with, allows large-scale integration using carbon nanotubes you can purchase off-the-shelf. 3. The third big breakthrough, called MIXED, allowed for the creation of the two types of transistors needed for CMOS logic, the kind in use in all-kinds of processors for decades. For that you need both electron-conducting (NMOS) and hole conducting (PMOS) transistors. Previous attempts at nanotube processors, such as the one-bit system Shulaker built as a graduate student, used only PMOS. In silicon, the distinction is achieved by doping the transistor’s channel region with different atoms to effectively add electrons to the silicon crystal lattice or steal some. But such “substitutional doping” doesn’t work for carbon nanotubes. “It’s difficult to swap out an atom without destroying the properties of the nanotube,” says Shulaker. So instead they turned to “electrostatic doping.” Here, a dielectric oxide is engineered to add or subtract electrons from the nanotube. Using a common semiconductor manufacturing technology called atomic layer deposition, the team was able to deposit dielectrics, such as halfnium dioxide, one atomic layer at a time. By manipulating the exact composition of the layer, say to have slightly fewer oxygens or a bit more, the oxide “wants to either donate electrons to the nanotube or steal from the nanotube,” explains Shulaker. Between careful selection of the metal electrodes involved and the ALD process, the researchers were able to reliably build PMOS and NMOS devices together. Crucially, MIXED is a low-temperature process, so the transistors can be built on top of other layers of circuitry without damaging them. In fact, the transistors in RV16X-NANO were built in between a layer of interconnects that provide power to the transistors and another layer that connects the transistors into logic gates and larger systems. Engineers are interested in such “buried power line” schemes in order to free up space that would allow for better-performing or smaller systems. But they are more difficult to achieve in silicon, in part because of high processing temperatures.  Monthly newsletter about how new materials, designs, and processes drive the chip industry.  IEEE Spectrum’s nanotechnology blog, featuring news and analysis about the development, applications, and future of science and technology at the nanoscale. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Electronics and Technology,0.14832517876684145
12,MIT News,Legatum Center announces 2019-20 fellowship class,Computer Science,2019-08-23,-,http://news.mit.edu/2019/mit-legatum-center-announces-new-fellows-0823,"  The Legatum Center for Development and Entrepreneurship at MIT announced its fellowship class for the 2019-20 academic year. These 23 student-entrepreneurs are developing innovative business solutions for emerging markets across the globe, including 10 in Africa, six in Latin America, and seven in Asia. Solutions range from portable devices that protect newborns from hypothermia in India, to enhanced preservation of perishable products being shipped internationally from Colombia, to technology solutions that improve delivery of humanitarian aid for refugees in Kenya. The Legatum Center operates on the belief that entrepreneurial innovators and their market-driven solutions are critical to advancing a more inclusive, global prosperity. The center offers a range of programs for students, but its fellowship is reserved for those most committed to building and scaling ventures in the developing world. Besides tuition, travel, and prototyping support, fellows receive access to mentors and advisors, a targeted for-credit curriculum, and the peer support of an incubator-like community. Fellowships within the Legatum Center are supported by the Mastercard Foundation, as well as the Legatum Group and the Jacobs Foundation. Since its founding in 2007, the Legatum Center has supported 272 fellows, many of whom continue to lead and grow impactful ventures across the globe, while others have gone on to support the entrepreneurial ecosystem in their roles as investors, corporate/non-governmental organization executives, academics, and policymakers. “As always, our fellows represent the next generation of impact-driven entrepreneurial leaders, and we can’t wait to begin working with them,” says Megan Mitchell, director of fellowship and student programs. “And, of course, every cohort is unique. This year, many of our fellows are developing innovative solutions for education and financial services, but we also have ventures in health care, agriculture, media, consumer goods, energy, transportation, and more.” The 2019-20 fellows within the Legatum Center are: Larissa Bezerra Abreu is an MBA student in the MIT Sloan School of Management. Abreu’s business, mxnMEDIA, is an above-the-line media platform which enables Brazilian companies of all sizes to get verifiable mass reach without significant advertising budgets. Nafees Ahmed is an MBA student in MIT Sloan. Ahmed’s business, Usawa Investments, is a digital platform that cultivates the Pakistani startup ecosystem by connecting investors to entrepreneurs. Michael Joseph Bautista is an MBA student in MIT Sloan. Bautista’s venture, TOCA, seeks to provide job opportunities in the Philippines while supplying technology firms with cheaper machine-learning data. Chinh Bui is a master’s student in engineering and management within the Integrated Design and Management program. Bui’s venture, Learn-In-Context, leverages automation and artificial intelligence to revolutionize the way English is taught and learned in emerging markets like Vietnam. Fatima Diallo is a master’s and MBA student in the interdisciplinary Leaders for Global Operations program within MIT Sloan and the School of Engineering. Diallo’s venture, Cadi, seeks to enhance the student learning experience in Guinea by providing primary school teachers access to curated curricula. Efewongbe Gboneme is an MBA student in MIT Sloan. Gboneme’s venture, Sky High, is focused on reducing unemployment and underemployment in Nigeria by providing career exploration opportunities for students in secondary schools. Sahil Joshi is an MBA student in MIT Sloan. Joshi’s venture RiskBoard, based in Mexico, uses machine learning to help multinational companies operate more sustainably. Nithin Kantareddy is a PhD student in the Department of Mechnical Engineering. Kantareddy’s venture, Digilitics, helps factories in India reduce their monthly electricity bills and provides hospitals insights to better schedule their operations. Hugo Lopez Velarde Martinez is an MBA student in MIT Sloan. Martinez’s venture DUO (cofounded with Fellow Luis Torres) is a challenger bank building the next generation of financial services, beginning with a management dashboard and corporate card, for small and medium businesses in Mexico. Sergio Medina is an Executive MBA student in MIT Sloan. Medina’s venture, RISE, is deploying technology solutions to accelerate humanitarian aid for refugee children globally, starting in Kenya, with a particular focus on education, gender parity, and food security. Anatole Menon-Johansson is an MBA student and Sloan Fellow in MIT Sloan. Menon-Johansson’s venture SXT, based in South Africa, is an anonymous and cost-effective way to inform sexual partners of their infection risk and digitally curate their journey to effective testing. David Miranda is a PhD candidate in medical engineering within the Harvard-MIT Health Science and Technology Program. Miranda's venture, Floricola, aims to improve the quality of perishable products shipped from Colombia during long-range transport. Michel Mosse is an MBA student in MIT Sloan. Mosse’s venture, Inlara, is an online marketplace for individuals and enterprises in Argentina to find the most convenient coaching experience to unlock their potential. Mercy Ndambuki is an MBA student in MIT Sloan. Ndambuki’s venture, Mbavu, aims to help upskill local Kenyan talent in private mid-sized companies. Quadri Oguntade is an MBA student in MIT Sloan. Oguntade’s venture, Bright Future, seeks to provide an alternative light source for Nigerian students who lack access to electricity. Joshua Reed-Diawuoh is an MBA student in MIT Sloan. Reed-Diawuoh’s venture, GhanaMade Cashew Company, will focus on making and selling premium Ghanaian food and beverage products for specialty markets. Sebie Salim is an Executive MBA student in MIT Sloan. Salim’s Kenya-based fintech venture, Tenakata, seeks to help small businesses keep better records and increase their borrowing power in order to grow their business. Pulkit Shamshery is an MBA student in MIT Sloan. Shamshery’s venture, Illumina Africa, aims to use solar mini-grids to provide access to water, cold storage, and electricity in underdeveloped communities in Kenya. Sumit Sharma is an MBA student in MIT Sloan. Sharma’s India-based venture, i^4, aims to develop a portable incubator that will minimize the more than two million neonatal deaths that occur annually due to hypothermia. Rodrick Tan is an MBA student in MIT Sloan. Tan’s venture, Sakay, empowers lower- and middle-class Filipinos without a car to get around Metro Manila through information on their phones. Yih Lin Teh is an MBA student in MIT Sloan. Teh’s venture, CapSphere, is Malaysia’s first asset-based financing peer-to-peer lending platform. Luis Torres is an MBA student in MIT Sloan. Torres’ venture, DUO (cofounded with Fellow Hugo Lopez Velarde Martinez), is a challenger bank building the next generation of financial services, beginning with a management dashboard and corporate card, for small and medium businesses in Mexico. Ezinne Uzo-Okoro is a master’s student in the Media Lab. Uzo-Okoro’s Nigeria-based venture, Terraformers, aims to grow fresh food everywhere. ",Computer Science,0.14754149377728712
13,Science Daily,New Insulation Technique Paves the Way for More Powerful and Smaller Chips,Matter & Energy,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904100759.htm,"   Computer chips are getting increasingly smaller. That's not new: Gordon Moore, one of the founders of chip manufacturer Intel, already predicted it in 1965. Moore's law states that the number of transistors in a chip, or integrated circuit, doubles about every two years. This prognosis was later adjusted to 18 months, but the theory still stands. Chips are getting smaller and their processing power is increasing. Nowadays, a chip can have over a billion transistors. But this continued reduction in size also brings with it a number of obstacles. The switches and wires are packed together so tightly that they generate more resistance. This, in turn, causes the chip to consume more energy to send signals. To have a well-functioning chip, you need an insulating substance that separates the wires from each other, and ensures that the electrical signals are not disrupted. However, that's not an easy thing to achieve at the nanoscale level. Nanoporous crystals A study led by KU Leuven professor Rob Ameloot (Department of Microbial and Molecular systems) shows that a new technique might provide the solution. ""We're using metal-organic frameworks (MOFs) as the insulating substance. These are materials that consist of metal ions and organic molecules. Together, they form a crystal that is porous yet sturdy."" For the first time, a research team at KU Leuven and imec managed to apply the MOF insulation to electronic material. An industrial method called chemical vapour deposition was used for this, says postdoctoral researcher Mikhail Krishtab (Department of Microbial and Molecular systems). ""First, we place an oxide film on the surface. Then, we let it react with vapour of the organic material. This reaction causes the material to expand, forming the nanoporous crystals."" ""The main advantage of this method is that it's bottom-up,"" says Krishtab. ""We first deposit an oxide film, which then swells up to a very porous MOF material. You can compare it to a soufflé; that puffs up in the oven and becomes very light. The MOF material forms a porous structure that fills all the gaps between the conductors. That's how we know the insulation is complete and homogeneous. With other, top-down methods, there's always still the risk of small gaps in the insulation."" Powerful and energy efficient Professor Ameloot's research group has received an ERC Proof of Concept grant to further develop the technique, in collaboration with Silvia Armini from imec's team working on advanced dielectric materials for nanochips. ""At imec, we have the expertise to develop wafer-based solutions, scaling technologies from lab to fab and paving the way to realising a manufacturable solution for the microelectronics industry."" ""We've shown that the MOF material has the right properties,"" Ameloot continues. ""Now, we just have to refine the finishing. The surface of the crystals is still irregular at the moment. We have to smoothen this to integrate the material in a chip."" Once the technique has been perfected, it can be used to create powerful, small chips that consume less energy. Ameloot: ""Various AI applications require a lot of processing power. Think of self-driving cars and smart cities. Technology companies are constantly looking for new solutions that are both quick and energy efficient. Our research can be a valuable contribution to a new generation of chips."" ",Electronics and Technology,0.14748911267444922
14,Science Daily,A New Alphabet to Write and Read Quantum Messages With Very Fast Particles,Computers & Math,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903114223.htm,"   Let us imagine the following situation: Anna and Bill want to communicate exchanging a message by using a property of a quantum particle, say the spin of an electron, which is an intrinsic form of particle's rotation. Bill needs Anna's message as quickly as possible, so Anna has to send the electron at maximum speed, very close to the speed of light. Given that Anna has the electron in her laboratory localized, one of the fundamentals of quantum physics, the Heisenberg uncertainty principle, forbids the velocity of the electron to be defined with arbitrary precision. When the electron travels extremely fast, that means, relativistically, the interplay between special relativity and quantum physics causes the spin and the velocity of the electron to get entangled. Due to this correlation, which is stronger than what is classically possible, Bill is not able to read out the spin with the standard method. Can Anna and Bill improve their communication strategy? A group of researchers led by Časlav Brukner at the University of Vienna and the Institute for Quantum Optics and Quantum Information (IQOQI-Vienna) of the Austrian Academy of Sciences have introduced a novel alternative to the standard alphabet used by Anna and Bill. Their technique guarantees that the message, written by Anna and read by Bill, can be decoded unambiguously even when the particle behaves according to both quantum mechanics, because of Heisenberg's uncertainty principle, and special relativity, due to its very high velocity. The novel method as presented in the journal Physical Review Letters delivers a new definition of the spin of quantum particles that move very fast. Thus it modifies both the way Anna writes the message and the way Bill reads it. Key to this technique is a ""translation"" of the way the message would be written and read between the standard alphabet, used when the electron is at rest, and the new alphabet, used when the electron travels very fast. ""These results are indicative that this translation procedure could open up to new applications in relativistic quantum information,"" says Flaminia Giacomini, the lead author of the paper. For instance, this technique could be helpful in satellite-based quantum communication, where a particle carrying a message has to travel quickly between two far-away points. ",Electronics and Technology,0.1446447674411615
15,ACM,Researchers Use AI to Plot Green Route to Nylon,ACM,2019-08-26,-,https://engineering.nyu.edu/news/researchers-use-ai-plot-green-route-nylon,"    Researchers Use AI to Plot Green Route to Nylon     NYU Tandon School of EngineeringAugust 26, 2019   New York University Tandon School of Engineering researchers used artificial intelligence (AI) to greatly improve the efficiency of organic electrosynthesis. The researchers slightly changed how electrical current is delivered to catalytic electrodes and then applied AI to further optimize the reaction; this resulted in a 30% improvement in the production of adiponitrile (ADN)—the main precursor to nylon 6,6. Rather than use the standard electrosynthetic process for ADN, in which a direct electrical current is constantly delivered to the electrocatalytic site, the researchers opted for a process in which electricity is delivered to the site in pulses. The researchers supplied an artificial neural network with data from 16 different experimental cases of pulse times. Said NYU researcher Miguel Modestino, ""This innovative, integrated approach led to an unprecedented 30% improvement in ADN production and a 325% increase in the ratio of ADN to PN (propionitrile), mostly due a large decrease in production of the latter.”                 ",Computer Science,0.1366613102933054
16,Science Daily,Electronic Glove Offers 'Humanlike' Features for Prosthetic Hand Users,Computers & Math,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904081320.htm,"   An electronic glove, or e-glove, developed by Purdue University researchers can be worn over a prosthetic hand to provide humanlike softness, warmth, appearance and sensory perception, such as the ability to sense pressure, temperature and hydration. The technology is published in the Aug. 30 edition of NPG Asia Materials. While a conventional prosthetic hand helps restore mobility, the new e-glove advances the technology by offering the realistic human hand-like features in daily activities and life roles, with the potential to improve their mental health and wellbeing by helping them more naturally integrate into social contexts. The e-glove uses thin, flexible electronic sensors and miniaturized silicon-based circuit chips on the commercially available nitrile glove. The e-glove is connected to a specially designed wristwatch, allowing for real-time display of sensory data and remote transmission to the user for post-data processing. Chi Hwan Lee, an assistant professor in Purdue's College of Engineering, in collaboration with other researchers at Purdue, the University of Georgia and the University of Texas, worked on the development of the e-glove technology. ""We developed a novel concept of the soft-packaged, sensor-instrumented e-glove built on a commercial nitrile glove, allowing it to seamlessly fit on arbitrary hand shapes,"" Lee said. ""The e-glove is configured with a stretchable form of multimodal sensors to collect various information such as pressure, temperature, humidity and electrophysiological biosignals, while simultaneously providing realistic human hand-like softness, appearance and even warmth."" Lee and his team hope that the appearance and capabilities of the e-glove will improve the well-being of prosthetic hand users by allowing them to feel more comfortable in social contexts. The glove is available in different skin tone colors, has lifelike fingerprints and artificial fingernails. ""The prospective end user could be any prosthetic hand users who have felt uncomfortable wearing current prosthetic hands, especially in many social contexts,"" Lee said. The fabrication process of the e-glove is cost-effective and manufacturable in high volume, making it an affordable option for users unlike other emerging technologies with mind, voice and muscle control embedded within the prosthetic at a high cost. Additionally, these emerging technologies do not provide the humanlike features that the e-glove provides. Lee and Min Ku Kim, an engineering doctoral student at Purdue and a co-author on the paper, have worked to patent the technology with the Purdue Research Foundation Office of Technology Commercialization. The team is seeking partners to collaborate in clinical trials or experts in the prosthetics field to validate the use of the e-glove and to continue optimizing the design of the glove. A video about the technology is available at https://youtu.be/lF1VYzKagNo. ",Computer Science,0.1357258602868018
17,Science Daily,AI Learns the Language of Chemistry to Predict How to Make Medicines,Computers & Math,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903111250.htm,"   A central challenge in drug discovery and materials science is finding ways to make complicated organic molecules by chemically joining together simpler building blocks. The problem is that those building blocks often react in unexpected ways. ""Making molecules is often described as an art realised with trial-and-error experimentation because our understanding of chemical reactivity is far from complete,"" said Dr Alpha Lee from Cambridge's Cavendish Laboratory, who led the studies. ""Machine learning algorithms can have a better understanding of chemistry because they distil patterns of reactivity from millions of published chemical reactions, something that a chemist cannot do."" The algorithm developed by Lee and his group uses tools in pattern recognition to recognise how chemical groups in molecules react, by training the model on millions of reactions published in patents. The researchers looked at chemical reaction prediction as a machine translation problem. The reacting molecules are considered as one 'language,' while the product is considered as a different language. The model then uses the patterns in the text to learn how to 'translate' between the two languages. Using this approach, the model achieves 90% accuracy in predicting the correct product of unseen chemical reactions, whereas the accuracy of trained human chemists is around 80%. The researchers say that the model is accurate enough to detect errors in the data and correctly predict a plethora of difficult reactions. The model also knows what it doesn't know. It produces an uncertainty score, which eliminates incorrect predictions with 89% accuracy. As experiments are time-consuming, accurate prediction is crucial to avoid pursuing expensive experimental pathways that eventually end in failure. In the second study, Lee and his group, collaborating with the biopharmaceutical company Pfizer, demonstrated the practical potential of the method in drug discovery. The researchers showed that when trained on published chemistry research, the model can make accurate predictions of reactions based on lab notebooks, showing that the model has learned the rules of chemistry and can apply it to drug discovery settings. The team also showed that the model can predict sequences of reactions that would lead to a desired product. They applied this methodology to diverse drug-like molecules, showing that the steps that it predicts are chemically reasonable. This technology can significantly reduce the time of preclinical drug discovery because it provides medicinal chemists with a blueprint of where to begin. ""Our platform is like a GPS for chemistry,"" said Lee, who is also a Research Fellow at St Catharine's College. ""It informs chemists whether a reaction is a go or a no-go, and how to navigate reaction routes to make a new molecule."" The Cambridge researchers are currently using this reaction prediction technology to develop a complete platform that bridges the design-make-test cycle in drug discovery and materials discovery: predicting promising bioactive molecules, ways to make those complex organic molecules, and selecting the experiments that are the most informative. The researchers are now working on extracting chemical insights from the model, attempting to understand what it has learned that humans have not. ""We can potentially make a lot of progress in chemistry if we learn what kinds of patterns the model is looking at to make a prediction,"" said Peter Bolgar, a PhD student in synthetic organic chemistry involved in both studies. ""The model and human chemists together would become extremely powerful in designing experiments, more than each would be without the other."" The research was supported by the Winton Programme for the Physics of Sustainability and the Herchel Smith Fund. ",Computer Science,0.13309796559586898
18,MIT News,"David H. Koch, prominent supporter of cancer research at MIT, dies at 79",Computer Science,2019-08-23,-,http://news.mit.edu/2019/david-koch-prominent-supporter-cancer-research-mit-dies-79-0823,"  David H. Koch ’62, SM ’63, one of the most important benefactors in MIT’s modern history, has died. He was 79 years old. Koch’s willingness to back significant initiatives at the Institute was exemplified by his foundational gift establishing the David H. Koch Institute for Integrative Cancer Research, a pioneering facility that brings research scientists and engineers together to advance the frontiers of cancer medicine. The Koch Institute has become a centerpiece of MIT’s pursuit of biomedical innovation and the useful application of knowledge to global health. Koch had wide-ranging interests concerning the life of the Institute, however, and in addition to cancer research, he supported many other causes and activities at MIT, including chemical engineering, childcare for employees, and athletics. At any given moment around MIT, beneficiaries of Koch’s gifts included faculty with endowed professorships, students with fellowships he supported — and toddlers in the childcare center he helped found. “David Koch had a brilliant instinct for opportunities where the lever of his philanthropy could make a transformative difference,” says MIT President L. Rafael Reif. “As one example, his gift to launch the Koch Institute dramatically advanced a new strategy in which engineers and scientists push the frontiers of cancer research by working side by side. At the same time, he saw that the David H. Koch Childcare Center could play an indispensable role in helping young faculty, staff, postdocs, and graduate students manage the balance of family and career. We are grateful for his longstanding devotion to the Institute. Very few graduates have left such a broad and indelible mark on the life of MIT.” The Koch Institute, dedicated in 2011, was backed by a $100 million gift Koch made to MIT in October 2007, allowing for a new state-of-the-art facility at MIT and an innovative, interdisciplinary approach to the fight against cancer. The Koch Institute houses a wide array of world-leading scientists: Five current and former faculty have been awarded the Nobel Prize, and nine current and former faculty have been awarded the National Medals of Science or Technology and Innovation. All told, Koch has given MIT $134 million to support cancer research and facilities. “From my very first days as MIT’s president, David Koch became a friend, collaborator, supporter, and enthusiast,” says President Emerita Susan Hockfield, who led MIT from 2004 to 2012. “He already had a long history of generosity to MIT, but his commitment to accelerating progress against cancer gave particular force to MIT’s efforts to reimagine our own cancer research. David was one of this nation’s most generous donors to cancer research, and his engagement with many of the leading cancer research centers gave him an amazingly sophisticated understanding of the frontier of cancer biology and therapy.” The Koch Institute emphasizes five main areas of research: the development of nanotechnology-based cancer treatments; new devices for cancer detection and monitoring; research about the molecular and cellular processes of metastasis; the advancement of personalized medicine, by studying cancer pathways and resistance to drugs; and research about how the immune system can fight cancer. “This is a new approach to cancer research with the potential to uncover breakthroughs in therapies and diagnostics,” Koch said in 2007. “Conquering cancer will require multidisciplined initiatives and MIT is positioned to enable that collaboration. As a cancer survivor, I feel especially fortunate to be able to help advance this effort.” President Emerita Hockfield, whose tenure included the period when David H. Koch made his initial gift funding the Koch Institute, as well as its opening, lauded Koch’s visionary support of the project. “David provided resources, of course, but also wisdom and strategy to keep the project on time and on budget,” Hockfield says. “He took personal interest in the people and projects at what became the David H. Koch Institute for Integrative Cancer Research.” Koch’s embrace of an interdisciplinary center for fighting cancer advanced and enhanced MIT’s capabilities in this arena, notes Tyler Jacks, the David H. Koch Professor of Biology at MIT, and director of the David H. Koch Institute for Integrative Cancer Research. “As an MIT-trained engineer, David immediately saw the value in bringing together the great strengths in engineering on our campus with our cancer science efforts in order to solve the most challenging problems in cancer,” Jacks says. “As a cancer survivor, he has been deeply committed to supporting innovative approaches to improve outcomes for patients. David chose to invest in MIT because he believed that we were uniquely positioned to change the course of cancer, and his generosity has enabled us to do that.” Jacks added that MIT benefitted from Koch’s high level of interest in the the research projects he backed. “From the earliest days of planning the Koch Institute, David dug into the details,” Jacks says. “He was always inquisitive and really enjoyed asking probing questions, whether about the HVAC system in the building or the intricacies of nanotechnology-based cancer therapy. David was a huge supporter of what we do and rightly proud of what we have created in the Koch Institute. And we are extremely grateful for his support.” In addition to the named chair Jacks holds, Koch endowed other professorships that bear his name, held by MIT faculty in the fields of biology, biological engineering, chemical engineering, and materials science and engineering. David H. Koch was born in Wichita, Kansas, on May 3, 1940. He graduated from Deerfield Academy, a prep school in Massachusetts, and received his bachelor’s degree and master’s degree from MIT in chemical engineering, the Institute’s Course 10. He joined Koch Industries, the firm founded by his father, in 1970, and became president of a division of the company, Koch Engineering, in 1979. He served as executive vice president of Koch Industries until publicly announcing his retirement, due to his health, in June 2018. Koch was also a Life Member Emeritus of the MIT Corporation. He first became a Member of the Corporation in 1988, and was elected a Life Member in 1998. Beyond cancer research, Koch was also a significant supporter of MIT’s programs in chemical engineering. In the 1980s, Koch made a significant gift to sustain the School of Chemical Engineering Practice at MIT, whose roots go back to 1916. Now known as the David H. Koch School of Chemical Engineering Practice, this is a unique program for graduate students combining coursework with internships, to enhance both academic and professional development. “David Koch was a model philanthropist who funded initiatives across a swath of cultural, scientific, and medical institutions,” says Robert Millard, chair of the MIT Corporation. “His generosity has benefited humanity broadly — from the arts to cancer research to science. MIT is deeply thankful for his many contributions to our community.” In a different vein, Koch served as lead donor for the David H. Koch Childcare Center at MIT, which opened in 2013 and almost doubled the childcare capacity on campus. Situated on Vassar Street on the west side of the MIT campus, the center provides high-quality support for MIT faculty, postdocs, graduate students, and staff who are raising young families, often while pursuing intensive research careers. Koch decided to give $20 million for the facility after serving on the Biology Visiting Committee at MIT — one of many such groups that advise the Institute — and recognizing the need for more extensive childcare facilities in order to help attract and retain talented personnel on campus. Along with Koch, Charles W. Johnson ’55 and Jennifer C. Johnson also helped fund the facility. A less well-known but vital aspect of Koch’s relationship with MIT was his enduring support for the Institute’s basketball team. Koch was a standout basketball player as an undergraduate, and captained the MIT team during the 1961-62 season, his senior year; he played alongside his brother Bill on MIT’s varsity team. David Koch’s attachment to the program continued throughout his life. Indeed, Koch not only followed the team, and attended team banquets, but endowed the position of coach for the men’s basketball team, a role that has been filled since the 1995-96 season by Larry Anderson. During that time, MIT has had a superb run of success, which includes making the NCAA Division III Final Four in 2012. “My heart goes out the entire Koch family,"" says Anderson. “I know that David had lots of love and interests – we were lucky enough that MIT Basketball was one of them. He was proud to wear the MIT Cardinal red and silver gray as captain of the team. He was the record-holder for 47 years for the most points scored in a single game with 41, and his support meant so much to the MIT Basketball family.” “David’s generous philanthropy allowed us to do many impossible things at MIT, but I have valued equally his curiosity, interest, engagement, and enthusiasm,” Hockfield says. “Coming from an MIT family, David Koch was truly a son of MIT who made the Institute a better place, for its students and faculty, and for the lives they change through their work.”         ",Electronics and Technology,0.13087573072872916
19,MIT News,Overcoming obstacles with an electric hovercraft,Computer Science,2019-08-28,-,http://news.mit.edu/2019/overcoming-obstacles-hyperloop-competition-0828,"  Through dedication and a willingness to face challenges both expected and unforeseen, an MIT team recently brought the air-powered hovercraft from the world of Saturday-morning cartoons to reality, at the 2019 SpaceX Hyperloop Pod Competition. But that’s only part of the story. What’s past is prologue In a 2013 white paper, Elon Musk, technology entrepreneur, investor, and engineer, detailed a high-speed frictionless train — the Hyperloop. When drag and atmosphere were removed from a tunnel, he posited, trains could float within a vacuum tube at up to 700 miles per hour. Musk wasn’t the first to imagine an air-powered train. In the 1860s, Alfred Ely Beach, inventor, publisher, and patent lawyer, envisioned a subway under the streets of New York City. In 1870, his experiments in pneumatic power resulted in a demonstration run of the Beach Pneumatic Transit, a 10-passenger car propelled by a 100-horsepower fan, baffles, and blowers, through a tunnel beneath Broadway. His efforts were thwarted by Tammany Hall politics and the Panic of 1873. It took the MIT team, dubbed Hyperloop II, to once more embrace Beach’s concept. “We took Beach’s vision and accomplished a much more efficient pneumatic vehicle,” explains Vik Parthiban, team captain. Lofty goals Parthiban, a graduate researcher at the MIT Media Lab, was part of the 2017 SpaceX Hyperloop Pod Competition during his undergraduate years at the University of Texas. He came to MIT determined to further the technology and, in the fall of 2018, recruited nearly 30 undergraduate and graduate students to develop an autonomous electric hovercraft. “Imagine an air hockey puck,” explains Parthiban. “Instead of air coming out of a table, it comes out of pucks under the vehicle. A regulation system pumps air into these air castors, which then levitate the vehicle.” Four castors beneath the vehicle are operated by a pneumatic system controlled by a central computer. The propulsion system takes the 200-kilogram vehicle from zero to 200 miles per hour in 20 seconds with the push of a finger. High-speed passenger trains in China and Japan use magnetic levitation to create a gap between the train and the track to remove the drag, but Parthiban took a different approach. “Putting magnetic levitation in a hyperloop is expensive,” he says. “Our goal was to invent a new technology that would cost less and be more efficient than magnetic levitation, and to develop an electric hovercraft that would work even without a vacuum tunnel. The only thing needed is a flat surface.” The process With support from the Edgerton Center and industry sponsors including Arrow Electronics, Silicon Expert, and Texas Guadaloop, the group joined forces to contribute individual skills. “We worked together to figure out the best way to integrate the components. Every person brought their own knowledge,” says Nick Dowmon, software engineering lead and a System Design and Management (SDM) graduate student. “It was an awesome learning opportunity and a chance to collaborate and learn from each other.” Over the winter, the team met in the Edgerton Center’s build space to create a machine no one had ever built before. They brainstormed, designed, and redesigned. They machined parts, outsourcing the more complex components. They collaborated with the University of Texas on pneumatics and conducted analyses to determine the type of sensors needed to levitate and propel the pod at the required speed, adjusting here, fine-tuning there. They fashioned the 70-component wiring harness and constructed a test track in a 200-foot-long corridor beneath MIT’s Great Dome. On May 22, the completed pod was presented at the MIT Museum to an overflow crowd eager to view the world’s first electric hovercraft. A minor setback In early summer, the production schedule was on target. Team members were confident the pod would meet its delivery deadline and reach California by July 7. On June 18, Parthiban and two teammates bent over the pod in the build space, intent upon working out last-minute details. Then Parthiban saw flames. A tear in the battery insulation had caused a short, and he reached for a fire extinguisher. But the blaze quickly escalated, he recalls, and he reached for the fire alarm instead. “The battery insulation fire burned down most of the vehicle,” he says. “It was the saddest thing.” Parthiban called an emergency team meeting that evening, and within two hours, every team member had arrived — including those who’d left the project to focus on research and internships. Parthiban explained that rebuilding the pod in three weeks was virtually impossible. But in true MIT style, every team member came together in a resounding “Let’s do this!” “Everyone agreed we had to make it happen,” says Bowen Zeng, levitation lead and a graduate student in mechanical engineering. “There was no choice.” “We had to drop everything to rebuild the pod before we went to California,” says Dowmon. “Many times, I was still in the build space at midnight with someone that I didn’t normally work with toiling on a part of the pod, but we helped each other. We worked through it together.” Three days after the meeting, the pneumatic panel was rebuilt. In a week, the new chassis was finished. The electronic systems were recreated. Sponsors fast-tracked the delivery of replacement components. And a week before the shipping deadline, the pod was finished (again). “I don’t think I’ve ever worked with a team that was so dedicated, so able to keep on going after something so discouraging,” adds Jessica Harsono, braking team lead and graduate student in mechanical engineering. MIT’s entry was the only fully-functioning levitating pod in the competition at SpaceX headquarters in July. “Competition week was truly where our collaboration paid off,” says Parthiban. “With only a few people in California, we had to split the tasks and get parts and do the machining in a short amount of time, under deadline. But we made it.” The MIT team emerged as the No. 1 U.S. university at the annual competition and placed fifth worldwide. They also earned a SpaceX Innovation award. Only at MIT Beach laid the groundwork and Musk provided the opportunity, but in the end, it was the spirit of camaraderie and teamwork that made the MIT team’s hyperloop a reality. “My motivation wasn’t that I wanted to achieve this for myself,” says Harsono. “So many other people worked so hard, and I didn’t want to let them down. I was motivated out of respect for what they’d done and how much effort and care they put in.” “This only can happen at MIT,” Parthiban says. “We all have that same mindset, the same hard-work attitude.” ",Computer Science,0.13046000519464163
20,Science Daily,New Material State: Quantum Disordered Liquid-Like Magnetic Moments,Matter & Energy,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165240.htm,"   In the paper ""Field-tunable quantum disordered ground state in the triangular-lattice antiferromagnet NaYbO2,"" published in the journal Nature Physics, Wilson and colleagues Leon Balents, of the campus's Kavli Institute for Theoretical Physics, and Mark Sherwin, a professor in the Department of Physics, describe their discovery of a long-sought ""quantum spin liquid state"" in the material NaYbO2 (sodium ytterbium oxide). The study was led by materials student Mitchell Bordelon and also involved physics students Chunxiao Liu, Marzieh Kavand and Yuanqi Lyu, and undergraduate chemistry student Lorenzo Posthuma, as well as collaborators at Boston College and at the U.S. National Institute of Standards and Technology. At the atomic level, electrons in one material's lattice structure behave differently, both individually and collectively, from those in another material. Specifically, the ""spin,"" or the electron's intrinsic magnetic moment (akin to an innate bar magnet) and its tendency to communicate and coordinate with the magnetic moments of nearby electrons differs by material. Various types of spin systems and collective patterns of ordering of these moments are known to occur, and materials scientists are ever seeking new ones, including those that have been hypothesized but not yet shown to exist. ""There are certain, more classical moments that let you know to a very high degree of certainty that the spin is pointing in a particular direction,"" Wilson explained. ""In those, the quantum effects are small. But there are certain moments where the quantum effects are large, and you can't precisely orient the spin, so there is uncertainty, which we call 'quantum fluctuation.'"" Quantum magnetic states are those in which the magnetism of a material is primarily driven by such quantum fluctuations, generally derived from the uncertainty principle, intrinsic to magnetic moments. ""So, you envision a magnetic moment, but the uncertainty principle says that I can't perfectly orient that in any one direction,"" Wilson noted. Explaining the quantum spin liquid state, which was proposed long ago and is the subject of this paper, Wilson said, ""In conventional materials, the magnetic moments talk to one another and want to orient relative to one another to form some pattern of order."" In classical materials, this order is disrupted by thermal fluctuations, what Wilson describes as ""just heat from the environment."" ""If the material is warm enough, it is nonmagnetic, meaning the moments are all sort of jumbled relative to one another,"" he explained. ""Once the material is cooled, the moments start to communicate, such that their connection to one another outcompetes the thermal fluctuations and they form an ordered state. That's classical magnetism."" But things are different in the quantum world, and magnetic moments that fluctuate can actually be the inherent ""ground state"" of a material. ""So, you can ask if there is a magnetic state in which the moments are precluded from freezing or forming some pattern of long-range order relative to one another, not by thermal fluctuations, but instead, by quantum fluctuations,"" Wilson said. ""Quantum fluctuations become more relevant as a material cools, while thermal fluctuations increase as it heats up, so you want to find a magnet that doesn't order until you can get it cool enough such that the quantum fluctuations preclude it from ever ordering."" That quantum disorder is desirable because it is associated with entanglement, the quantum mechanical quality that makes it possible to encode quantum information. To determine whether NaYbO2 might exhibit that characteristic, the researchers had to determine the intrinsic, or ground state of the material's magnetic moments when all thermal fluctuations are removed. In this particular system, Wilson was able to determine experimentally that the magnetic moments are intrinsically in a fluctuating, disordered state, thus confirming that a quantum disordered state exists. To find the hypothesized state, said Wilson, ""First you have to put highly quantum magnetic moments into a material, but your material needs to be constructed such that the moments don't want to order. You do that by using the principle of 'magnetic frustration.'"" A simple way to think of that, according to Wilson, is to imagine a single triangle in the lattice structure of the material. ""Let's say I build my material so that the magnetic moments are all located on a triangular lattice,"" he said, ""and they all talk to one another in a way that has them wanting to orient antiferromagnetically, or antiparallel, to one another."" In that arrangement, any adjacent moment on the triangle wants to orient antiparallel to its neighbor. But because there are an odd number of points, you have one up at one point and one down (antiparallel to the first) at the second point, meaning that the third moment has a differently oriented moment on each side, so it doesn't know what to do. All of the moments are competing with one another. ""That's magnetic frustration, and, as it turns out, it reduces the temperature at which the moments are finally able to find some arrangement they all agree on,"" Wilson said. ""So, for instance, classically, nature decides that at some temperature the mismatched moments agree that they will all point to 120 degrees relative to each other. So they're not all 100 percent happy but it's some compromise that establishes an ordered state."" From there, he added, ""The idea is to take a frustrated lattice where you have already suppressed the ordered state, and add quantum fluctuations to it, which take over as you cool the material. Magnetic frustration lowers the ordering temperature enough so that quantum fluctuations eventually take over and the system can stabilize into a fundamentally disordered quantum spin state."" Wilson continued: ""That's the paradigm of what people are looking for; however, some materials may seem to display this state when actually, they don't. For instance, all real materials have disorder, such as chemical or structural disorder, and this can also prevent the magnetic moments from talking to each other effectively and becoming ordered. In such a case, Wilson says, ""They might form a disordered state, but it's more of a frozen, or static, disordered state than it is a dynamic quantum state. ""So, if I have a magnetic system that doesn't order at the lowest temperatures I can measure, it can be tricky trying to understand whether what I'm measuring is an intrinsic quantum spin liquid fluctuating type of state or a frozen, extrinsic, chemically driven disordered state. That is always debated."" Among the most interesting findings about this new material, Wilson said, is that even at the lowest measurable temperature -- .005 degree Centigrade above absolute zero -- it still doesn't order. ""However, in this material we can also apply a magnetic field, which breaks this competition engendered by magnetic frustration, and then we can drive it to order, inducing a special kind of antiferromagnetic state,"" he added. ""The reason that's important is because this special state is very delicate and a very good fingerprint for how much chemical disorder there is in the system and its influence on the magnetic ground state. The fact that we can drive this field-driven state tells us that the disordered state we see at low temperature with zero magnetic field is indeed an intrinsically quantum disordered state, consistent with being a quantum spin liquid state."" ",Electronics and Technology,0.12919851542997643
21,ACM,Entanglement Sent Over 50 km of Optical Fiber,ACM,2019-08-29,-,https://www.uibk.ac.at/newsroom/entanglement-sent-over-50-km-of-optical-fiber.html.en,"    Entanglement Sent Over 50 km of Optical Fiber     University of InnsbruckAugust 29, 2019   Researchers at Austria’s University at Innsbruck and the Austrian Academy of Sciences have transmitted a photon entangled with matter over 50 kilometers (about 31 miles) of fiber-optic cable, a step toward practical application of quantum networks and a quantum Internet. The team corralled a calcium atom in an ion trap, then uses lasers to write a quantum state onto the ion and concurrently induce emission of a photon to store quantum information. Because the emitted photon had an 854-nanometer wavelength that is rapidly absorbed by the optical fiber, the researchers initially transmitted the particle through a nonlinear crystal illuminated by a laser—converting the wavelength to the 1,550-nanometer value for long-distance travel. Measurements during transmission showed entanglement was maintained after the particle reached its destination. The next step in the team’s research will be to use their technique to support entanglement between ions separated by more than 100 kilometers.                 ",Electronics and Technology,0.12798080469541367
22,Science Daily,Artificial Intelligence Used to Recognize Primate Faces in the Wild,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165232.htm,"   'For species like chimpanzees, which have complex social lives and live for many years, getting snapshots of their behaviour from short-term field research can only tell us so much,' says Dan Schofield, researcher and DPhil student at Oxford University's Primate Models Lab, School of Anthropology. 'By harnessing the power of machine learning to unlock large video archives, it makes it feasible to measure behaviour over the long term, for example observing how the social interactions of a group change over several generations.' The computer model was trained using over 10 million images from Kyoto University's Primate Research Institute (PRI) video archive of wild chimpanzees in Guinea, West Africa. The new software is the first to continuously track and recognise individuals in a wide range of poses, performing with high accuracy in difficult conditions such as low lighting, poor image quality and motion blur. 'Access to this large video archive has allowed us to use cutting edge deep neural networks to train models at a scale that was previously not possible,' says Arsha Nagrani, co-author of the study and DPhil student at the Department of Engineering Science, University of Oxford. 'Additionally, our method differs from previous primate face recognition software in that it can be applied to raw video footage with limited manual intervention or pre-processing, saving hours of time and resources.' The technology has potential for many uses, such as monitoring species for conservation. Although the current application focused on chimpanzees, the software provided could be applied to other species, and help drive the adoption of artificial intelligence systems to solve a range of problems in the wildlife sciences. 'All our software is available open-source for the research community,' says Nagrani. 'We hope that this will help researchers across other parts of the world apply the same cutting-edge techniques to their unique animal data sets. As a computer vision researcher, it is extremely satisfying to see these methods applied to solve real, challenging biodiversity problems.' 'With an increasing biodiversity crisis and many of the world's ecosystems under threat, the ability to closely monitor different species and populations using automated systems will be crucial for conservation efforts, as well as animal behaviour research' adds Schofield. 'Interdisciplinary collaborations like this have huge potential to make an impact, by finding novel solutions for old problems, and asking biological questions which were previously not feasible on a large scale.'### ",Computer Science,0.12762370600536746
23,Science Daily,An Astonishing Parabola Trick: Unusual Magnetic Behavior,Computers & Math,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903113337.htm,"   We all know that our left hand is different from our right -- a left glove won't fit your right hand and vice versa. Scientists use the term ""chirality"" to describe objects that do not align with their mirror image. Chemists, in particular, are familiar with this property in molecules, as in left- and right-rotating lactic acid. Humans metabolize the right-rotating variant more easily than its ""mirror image."" Such chiral effects are known to occur in magnetic materials, where magnetic textures also have chiral properties: the arrangement of individual magnetic moments inside the material, or, figuratively speaking, the alignment of the many tiny ""compass needles"" that make up a magnet, could form right- and left-handed alignments. Under certain conditions, some textures behave like image and mirror image -- a left-handed texture cannot be made congruent with its right-handed version. The interesting aspect here is that ""the two textures can present different magnetic behaviors,"" as HZDR physicist Dr. Denys Makarov points out. ""To put it simply: a right-handed texture can be more energetically preferable than a left-handed texture. Since systems in nature tend to assume their lowest possible energetic state, the right-handed state is preferred."" Such chiral effects hold great technological promise. Among other things, they could be helpful in the future development of highly energy-efficient electronic components such as sensors, switches, and non-volatile storage devices. Magnetic curved architectures ""Helimagnets are materials with well-defined chiral magnetic properties, due to a lack of internal magnetic symmetry,"" explains the lead author of the paper, Dr. Oleksii Volkov from HZDR's Institute of Ion Beam Physics and Materials Research. ""Despite the fact that they have been known for a long time, these are rather exotic materials that are difficult to produce. Moreover, helimagnets usually exhibit their unique chiral properties at low temperatures."" That is why Makarov's team chose a different path. They used a common magnetic material, iron-nickel alloy (known as Permalloy), to build curved objects like parabola-shaped strips. Using lithography, they formed various parabolic strips of several micrometers from thin sheets of Permalloy. The physicists then exposed the samples to a magnetic field, thus orienting the magnetic moments in the parabola along this magnetic field. They then experimentally explored the magnetization reversal by using a highly sensitive analysis method at HZB's synchrotron. The team was able to show that the magnetic moments in the parabolic strip remained in their original direction until a reversed magnetic field of a certain critical value was applied. Surprisingly strong effect This delayed response is due to chiral effects caused by the curvature at the apex area of the parabola strips. ""Theorists have predicted this unusual behavior for some time, but it was actually considered more of a theoretical trick,"" explains Dr. Florian Kronast of Helmholtz-Zentrum Berlin. ""But now we have shown that this trick actually works in practice. We detected magnetic chiral response in a conventional soft ferromagnetic material, just through the geometric curvature of the strips we used."" In the process, the team were faced with two more surprises: On the one hand, the effect was remarkably strong, which means it could be used to influence the magneto-electric responses of materials. On the other hand, the effect was detected in a relatively large object: micrometer-sized parabolas that can be produced using conventional lithography. Previously, experts had assumed that these curvature-induced chiral effects could only be observed in magnetic objects with dimensions of about a dozen of nanometers. ""In terms of possible applications, we are looking forward to novel magnetic switches and data storage devices that utilize geometrically-induced chiral properties,"" Makarov emphasizes. There are concepts that envision future digital data storage in certain magnetic objects, so-called chiral domain walls or skyrmions. The recent discovery might help to produce such objects quite easily -- at room temperature, and using common materials. In addition, the newly discovered effect also paves the way for novel, highly sensitive magnetic field sensors. ",Electronics and Technology,0.12646570286732142
24,Science Daily,Secret Messages Hidden in Light-Sensitive Polymers,Matter & Energy,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904081316.htm,"   In a leap forward in this field, researchers at the Institut Charles Sadron (CNRS) and the Institut de Chimie Radicalaire (CNRS/Aix-Marseille Université) have developed light-sensitive polymers where light can change the information stored on the molecular scale. Three types of information change have been shown in this work: revealing, changing and erasing a message. These French scientists have shown that some polymers can act like invisible ink: when exposed to the appropriate wavelength, their monomers are transformed, and the sequence becomes legible. The message only appears if it is subjected to the right light source. This is the first example of a secret message stored on a molecule. This study also shows that monomers being changed by light can be used to erase or change the information contained in some polymers. Chemists have for example 'transformed copper into gold' by changing the chemical symbol for copper written on a polymer, Cu, into the chemical symbol for gold, Au. The polymers are 'read' using mass spectrometry, a technology used routinely in many analytical laboratories. The teams involved in this recent work now wish to continue it by exploring how to control the physical properties of the polymers using light, for applications other than information storage and decoding, such as design of new materials. - A polymer is composed of simple chemical units, monomers. A polymer can take the shape of a sequence of two different monomers that can be read as 0 or 1 in a message written in binary notation. ",Electronics and Technology,0.12595690383182784
25,MIT News,Students spearhead group to enhance the graduate experience,Computer Science,2019-09-04,-,http://news.mit.edu/2019/gradsage-leadership-engineering-0904,"  What do graduate students in engineering want? This was the question before a new advisory group launched by the MIT School of Engineering in late 2017 — the school’s first comprised entirely of graduate students. This fall the group is rolling out its inaugural initiatives: a graduate-level leadership minor or certificate and a set of recommendations intended to improve advisor-advisee relations. GradSAGE (short for Graduate Student Advisory Group for Engineering) was established by Anantha Chandrakasan just months after he became dean of the MIT School of Engineering. “I thought it would be great to get student engagement as we shaped new initiatives, and to learn their perspectives on important issues and challenges they face,” says Chandrakasan. “In a sense, we are listening to our customers.” The dean already counted department heads and other school stakeholders among his advisors. But Chandrakasan, the Vannevar Bush Professor of Electrical Engineering and Computer Science, felt he was missing the voice of students. “The beauty of this group is that the students came up with a list of topics and priorities for us to focus on,” Chandrakasan says. “This was an opportunity for them to tell me what was most important, and while I wasn’t surprised by their choices, I was surprised by how passionately they felt about these areas.” Soft skills matter The very first gathering of GradSAGE, on Dec. 5, 2017, was like “a brainstorming-schmooze session,” recalls Parker Vascik, a fifth-year graduate student in aeronautics and astronautics (AeroAstro). “But we quickly moved toward identifying specific topics where we felt we could make significant changes in the academic culture and environment.” One topic that immediately seized the interest of the group involved expanded opportunities to learn and practice leadership abilities. “Grad students come to MIT hoping to have an impact on the world, and they are probably in the top 1 percent in terms of technical skills,” says Lucio Milanese, a fourth-year graduate student in nuclear science and engineering. “But there are nontechnical skills, soft skills, that are essential to communicating ideas and managing people that are just as important in solving really important problems.” GradSAGE research suggested MIT engineering graduate students could benefit from more structured opportunities to learn and practice soft skills. “There is an ocean of knowledge to acquire around teamwork — giving and receiving feedback, conflict resolution, growth mindset, that the basic graduate school curriculum doesn’t address,” says Dhanushkodi Mariappan, a fourth-year graduate student in mechanical engineering. After working in industry and launching his own startup before grad school, Mariappan felt strongly about what was needed. “A formal leadership program could propel MIT graduate students in their careers, whether they are interested in taking on jobs in industry or in academia, where in some sense they will be running labs or research groups that are like little companies.” A readymade leadership curriculum Potential solutions to the leadership education challenge lay close at hand. Mariappan pointed the group to the Bernard M. Gordon-MIT Engineering Leadership Program (GEL), a center focused on helping undergraduates acquire leadership skills. Mariappan made particular note of a GEL course he had taken, 6.928 (Leading Creative Teams), taught by David Niño. “The class was eye-opening,” says Mariappan, “We were introduced to frameworks that can be applied to solve problems in an incredible range of real-world situations.” It was a course with a blueprint for the kind of curriculum GradSAGE hoped to advance, so Mariappan recruited Niño to the effort. “To achieve something great in engineering takes a team, but engineers often don’t know how to develop a vision, recruit a talented team, facilitate group decisions, negotiate, delegate, and lead everyone in the same direction,” says Niño, who now works closely with GradSAGE. “Our courses involve practice of these leadership skills, so students can continue to evolve after graduation, and apply these over a lifetime.” As a result of this collaboration, a new option for satisfying a doctoral minor requirement draws on GEL’s classes, including new ones offered this fall that can serve as cornerstones for the minor: 6.S978 (Negotiation and Influence Skills for Technical Leaders) and 6.S976 (Engineering Leadership in the Age of Artificial Intelligence). Students whose doctoral programs do not permit a minor can instead pursue the GEL Leadership Certificate, which will be launched in the spring of 2020. Leadership classes taken before then will be retroactively recognized and can count toward the certificate. “We envision hundreds of graduate students pursuing some sort of leadership development experience —not just in the school of engineering but in the other MIT schools,” says Milanese. “In 10 to 15 years, we want employers to recognize a unique brand of MIT leadership and value MIT graduate students as nearly universally possessing outstanding leadership skills.”  “A very special relationship” The second major thrust of GradSAGE focused on an aspect of graduate life universally acknowledged as critical. “Advisor-advisee relations arose in every single GradSAGE discussion as a root issue for nearly everything graduates experience, from mental health to taking on leadership opportunities,” says Vascik. “Graduate students have a very special relationship with one person who is boss, mentor, and a little bit of family, and this person guides your destiny while you’re here.” “Most problems between advisors and students boil down to two issues: poor advisor-advisee fit and poor communication,” according to Jessica Boles, who is starting her third year as a graduate student in electrical engineering and computer science (EECS). “Many students arrive at MIT thinking, ‘Here is a field I’d like to work in, here’s a prominent person in the field I’d like to work with,’” says Boles. “But there are lots of other things to consider: Who will directly mentor them, what’s the work environment like, what are the advisor’s expectations and policies?” From informal surveys, Boles and her GradSAGE colleagues knew that an unclear understanding of an advisor’s standards and styles could lead to friction, disappointment, stress, lab-switching, and sometimes even departure from MIT. Different professors have starkly different approaches to dealing with their graduate students, notes Vascik. “One might like to see students three times a week and micromanage research, while another wants to get together once per semester,” he says. “Factors such as these can dramatically shape a student’s experience in graduate school, and we believe these styles and expectations should be communicated to incoming and current students more effectively.” Transparency and communication Approaching the challenge like engineers, the GradSAGE students developed flow charts of specific advisor-advisee problems, interviewed faculty, reviewed literature, and derived a set of potential mitigations. They ran their proposals by the Office of Graduate Education, MIT Chancellor Cynthia Barnhart, MIT Vice Chancellor Ian Waitz, and then presented their recommendations to Chandrakasan. In a matter of months, the group had approval to pilot several initiatives. Among these efforts: requesting advisors to post online brief statements about their philosophies and policies related to research advising (an effort now being explored within the AeroAstro and EECS departments); and centralizing and publicizing resources for graduate students who encounter difficulties with their advisors. In addition, Boles produced a video that details the kinds of questions admitted students should consider during the graduate school selection process, which she unveiled online to admitted EECS students just prior to MIT's visit weekend last spring.  “It was well-received, especially among the populations of students we really hope to reach: international students, underrepresented minorities, and students without prior graduate school experience,” she says. “So many more students sought information on the roles advisors would play in their research and career, and on the work environments in potential research labs, including expectations around publications, work hours, and group interactions.” A new, enhanced video is in the works intended for all incoming engineering graduate students. “Our goal is to increase transparency of advising style so we can ensure better advisor-advisee fits from the beginning,” says Boles. Down the line, adds Vascik, this work could translate to reduced stress among graduate students, fewer students switching labs, and more cohesive and productive labs. “Prospective students stand to benefit the most, because with online information, and their ability to ask smart questions, they will have a good sense before they arrive of what awaits them here.” For both the advising and leadership GradSAGE ventures, this fall marks just the start of a longer process. Growing these programs will take both time and money, which Chandrakasan seems intent to provide. “What we have done so far is expose important issues, and now it’s a matter of actually converting them into actionable items, which we must do,” he says. ",Society,0.12507029869320666
26,Science Daily,"At the Edge of Chaos, Powerful New Electronics Could Be Created",Computers & Math,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903124002.htm,"   A team of physicists at the University of Groningen, led by Professor of Functional Nanomaterials Beatriz Noheda, made their observation in thin films of barium titanate (BaTiO3), a ferroelastic material. Ferroic materials are characterized by their ordered structure, in shape (ferroelastic), charge (ferroelectric) or magnetic moment (ferromagnetic), for example. 'These materials are always crystals in which the atoms are arranged with characteristic symmetries,' Noheda explains. Twins Electric or magnetic dipoles are aligned within domains in the crystals. 'However, the dipoles could be pointing up or down, as both states are equivalent.' As a result, crystals of these materials will have both types of domains. The same goes for ferroelastic materials, best known for their shape memory. In this case, however, the situation is a bit more complicated, Noheda explains: 'The unit cells in these crystals are elongated, which means that domains of the different unit cells do not easily match in shape. This creates an elastic strain that reduces the crystal stability.' The crystal can improve the stability naturally by forming twins of domains, which are slightly tilted in opposite directions to relieve the stress. The result is a material in which these twinned pairs form alternating domains, with a fixed periodicity. Heating causes a phase change in the material, in which both the direction and the periodicity of the domain walls is altered. 'The question was how this change takes place,' says Noheda. Domain walls Increasing the temperature increases the disorder (entropy) in the material. Thus, a tug-of-war starts between the intrinsic tendency for order and the increasing entropy. It is this process that was observed for the first time by the Groningen team, using atomic force microscopy. When heating samples from 25 °C to 70 °C, a phase change takes place, altering the position of domain walls. When the transition starts, domain walls of the new phase appear gradually and both phases exist together at intermediate temperatures (30 °C to 50 °C). 'This doesn't happen in a random way, but by repeated doubling,' says Noheda. Cooling the material reduces the periodicity of the domains by repeated halving. 'This doubling or halving is well known in non-linear dynamical systems, when they are close to the transition to chaotic behaviour,' explains Noheda, 'However, it had never been observed in spatial domains, but only in time periods.' The resemblance between the behaviour of the thin films and non-linear systems suggests that the material is itself at the edge of chaos during heating. 'This is an interesting observation, because it means that the response of the system is highly dependent on initial conditions. Thus, we could get very diverse responses following a small change in these conditions.' Neuromorphic computing The paper includes theoretical calculations from colleagues at Penn State University (US) and the University of Cambridge (UK), which show that the behaviour observed in the ferroelastic barium titanate is generic for ferroic materials. Thus, a ferroelectric material at the edge of chaos could give a highly diverse response over a small range of input voltages. 'That is exactly what you want, to create the type of adaptable response needed for neuromorphic computing, such as reservoir computing, which benefits from non-linear systems that can produce highly diverse input-output sets.' The paper in Physical Review Letters is a proof of principle, showing how a material can be designed to exist at the edge of chaos, where it is highly responsive. Noheda also points out how the doubling of domains creates a structure similar to the bifurcating dendrites connecting the pyramidal cells in the brain. These cells play an important role in cognitive abilities. Ultimately, ferroic materials on the edge of chaos may be used to create electronic brain-like systems for complex computing. ",Electronics and Technology,0.12443240280773053
27,Science Daily,Laser-Based Ultrasound Approach Provides New Direction for Nondestructive Testing,Matter & Energy,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904102631.htm,"   A team of researchers is using ultrasonic nondestructive testing (NDT) that involves amplifying the signal from a photoacoustic laser source using laser-absorbing patch made from an array of nanoparticles from candle soot and polydimethylsiloxane. They discuss their work in this week's Applied Physics Letters, from AIP Publishing. The approach marks one of the first NDT systems that combines elements of contact and noncontact ultrasound testing. The results of generating such ultrasonic waves with the photoacoustic patch demonstrate the promise of the broad range of noncontact applications for NDT. ""Laser-based NDT method has advantages of temperature-independent measurement and wide range of monitoring area by easily changing the position of devices,"" said Taeyang Kim, an author on the paper. ""This technique provides a very flexible and simple method for noncontact and remote generation of ultrasonic surface waves."" Ultrasound waves can be made when a high-powered laser strikes a surface. The heat produced by the pulses induces a pattern of expansion and compression on the illuminated area, yielding an ultrasonic signal. The waves produced, called Lamb waves, then travel through material as an elastic wave. The group used the candle soot nanoparticles paired with polydimethylsiloxane to absorb the laser. They turned to candle soot because it is readily available and efficient at absorbing lasers and can produce the elastic expansion needed to make the photoacoustic conversion that generates the Lamb wave. By placing the particle in the patch in a line array, they were able to narrow the bandwidth of the waves, filtering out unwanted wave signals and increasing analytical accuracy. The researchers opted for an aluminum sensing system for the receiving transducer. The patch increased the amplitude by more than twofold over conditions without the patch and confirmed it produced narrower bandwidth than other conditions. Kim said the question of how the approach's durability in an industrial setting remains, as well as how well the patches perform on curved and rough surfaces. ""New NDT systems will attract more attention to explore the optimal materials for the patch or various applications for NDT industries,"" he said. Next, the team looks to test the system in high-temperature nondestructive testing scenarios. ",Electronics and Technology,0.1243599901707605
28,Science Daily,Biophysics: Stretching Proteins With Magnetic Tweezers,Matter & Energy,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904113217.htm,"   The mechanical forces required to activate proteins like VWF are often so small that their magnitude could not be determined using existing methods. Now, a team of scientists led by LMU physicists Martin Benoit and Professor Jan Lipfert has developed a much more sensitive procedure. Their 'magnetic tweezers' can quantify forces that are 100 times smaller than the commonly used alternative method currently available. As Lipfert and colleagues report in the journal PNAS, they have employed the technique to observe the unfolding of the VWF protein under the influence of low mechanical forces. A powerful approach to study mechanoregulation is so-called protein force spectroscopy. This involves tugging on an individual protein molecule and observing how an applied force alters its three-dimensional structure. Up to now, the method of choice for pulling has been an atomic force microscope, which works best in the range of 100 piconewton (pN). ""However, many molecular processes are activated by forces that are much weaker than that,"" says Lipfert. ""So for measurements at the level of single molecules, we need more sensitive instrumentation -- there's little point in using a bathroom scale to weigh out the ingredients of a cake."" The researchers developed a method in which the proteins are attached at one end to a glass surface and carry a tag at the other end that binds to tiny magnetic beads and the assembly is then subjected to an externa magnetic field. Extension of the protein induced by the field results in the vertical displacement of each bead, which can be detected by microscopy. ""This sort of set-up is referred to as magnetic tweezers,"" Lipfert explains. ""It has the great advantage that it allows us to apply and resolve very weak forces -- significantly less than 1 piconewton -- to the protein of interest. In addition, magnetic tweezers enable very stable measurements over long periods of time -- up to one week!"" To test the new method, the LMU group used VWF as their target protein. In the bloodstream, VWF circulates as a multimer of dimers that are made of two identical subunits. Under normal conditions of blood flow, it has a relatively compact globular form. However, any increase in the shear forces in the bloodstream owing to injury of the vasculature causes vWF to unfold. This exposes binding sites for receptors on blood platelets. Binding of VWF to platelets in turn triggers a reaction cascade that leads to clotting, which seals the wound. ""The cascade is induced by the action on the molecule of mechanical forces acting that are much weaker than those that have been measured up to now,"" says Lipfert. Analysis of the unzipping of VWF dimers with magnetic tweezers showed that the so-called VWF stem opens up under an applied force of less than 1 pN, when the subunits of the dimer are pulled apart like the two halves of a zipper. ""We assume that this pattern of behavior, which we were able to observe for the first time, represents the first step in blood coagulation,"" says Lipfert. ""Our approach provides a detailed picture of the forces and the changes in extension involved in unfolding the protein. We are confident that future application of the method will contribute to a better understanding of the mode of action of VWF and of the role of clinically relevant mutations."" ",Electronics and Technology,0.12130865096000563
29,Science Daily,Automated Text Analysis: The Next Frontier of Marketing Innovation,Society,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904091123.htm,"   The study, forthcoming in the January issue of the Journal of Marketing, is titled ""Uniting the Tribes: Using Text for Marketing Insights"" and authored by Jonah Berger, Ashlee Humphreys, Wendy Moe, Oded Netzer, and David Schweidel. Online reviews, customer service calls, press releases, news articles, marketing communications, and other interactions create a wealth of textual data companies can analyze to optimize services and develop new products. By some estimates, 80-95% of all business data is unstructured, with most of that being text. This text has the potential to provide critical insights about its producers, including individuals' identities, their relationships, their goals, and how they display key attitudes and behaviors. This text can be aggregated to create insights about organizations and social institutions and how attitudes vary over cultural contexts, demographics, groups, and time. Berger explains that ""The digitization of information has made a wealth of textual data readily available. But by itself, all this data is just that. Data. For data to be useful, researchers have to be able to extract underlying insight -- to measure, track, understand, and interpret the causes and consequences of marketplace behavior."" But how can marketers do that? The research team explains how researchers and managers can use text to better understand the individuals and organizations who produce the text. The article also explores how the content of text affects various audiences. For example, how consumers may be influenced to change their behaviors or brands influenced to attend to issues raised by consumers depends in large part on the content of text. Moe adds that ""Automated text analysis opens the black-box of interactions, allowing researchers to directly access what is being said and how it is said in marketplace communication."" Given the volume of text data available, automated text analysis methods are critical, but need to be handled carefully. Researchers should avoid over-fitting and weigh the importance of features to glean and use the right predictors from text. Thus, this article also provides an overview of the methodologies and metrics used in text analysis, providing a set of guidelines and procedures for marketing researchers and marketing scholars. Understanding these methods help us understand how text is used and processed. For example, virtual assistants are currently under scrutiny for the fact that humans are listening to the audio recordings. However, this process is necessary to train the machines used for automated text analysis. The goal of this article is to further the collective understanding of text analysis and how it can be used for insights. Researchers and marketers can use this article to create frameworks, establish and communicate policies, and strengthen cross-functional collaboration with teams working on textual analytics projects. ",Computer Science,0.12103264161626454
30,MIT News,Robotic thread is designed to slip through the brain’s blood vessels,Research,2019-08-28,-,http://news.mit.edu/2019/robot-brain-blood-vessels-0828,"  MIT engineers have developed a magnetically steerable, thread-like robot that can actively glide through narrow, winding pathways, such as the labrynthine vasculature of the brain. In the future, this robotic thread may be paired with existing endovascular technologies, enabling doctors to remotely guide the robot through a patient’s brain vessels to quickly treat blockages and lesions, such as those that occur in aneurysms and stroke. “Stroke is the number five cause of death and a leading cause of disability in the United States. If acute stroke can be treated within the first 90 minutes or so, patients’ survival rates could increase significantly,” says Xuanhe Zhao, associate professor of mechanical engineering and of civil and environmental engineering at MIT. “If we could design a device to reverse blood vessel blockage within this ‘golden hour,’ we could potentially avoid permanent brain damage. That’s our hope.” Zhao and his team, including lead author Yoonho Kim, a graduate student in MIT’s Department of Mechanical Engineering, describe their soft robotic design today in the journal Science Robotics. The paper’s other co-authors are MIT graduate student German Alberto Parada and visiting student Shengduo Liu. In a tight spot To clear blood clots in the brain, doctors often perform an endovascular procedure, a minimally invasive surgery in which a surgeon inserts a thin wire through a patient’s main artery, usually in the leg or groin. Guided by a fluoroscope that simultaneously images the blood vessels using X-rays, the surgeon then manually rotates the wire up into the damaged brain vessel. A catheter can then be threaded up along the wire to deliver drugs or clot-retrieval devices to the affected region. Kim says the procedure can be physically taxing, requiring surgeons, who must be specifically trained in the task, to endure repeated radiation exposure from fluoroscopy. “It’s a demanding skill, and there are simply not enough surgeons for the patients, especially in suburban or rural areas,” Kim says. The medical guidewires used in such procedures are passive, meaning they must be manipulated manually, and are typically made from a core of metallic alloys, coated in polymer, a material that Kim says could potentially generate friction and damage vessel linings if the wire were to get temporarily stuck in a particularly tight space. The team realized that developments in their lab could help improve such endovascular procedures, both in the design of the guidewire and in reducing doctors’ exposure to any associated radiation.        Threading a needle Over the past few years, the team has built up expertise in both hydrogels — biocompatible materials made mostly of water — and 3-D-printed magnetically-actuated materials that can be designed to crawl, jump, and even catch a ball, simply by following the direction of a magnet. In this new paper, the researchers combined their work in hydrogels and in magnetic actuation, to produce a magnetically steerable, hydrogel-coated robotic thread, or guidewire, which they were able to make thin enough to magnetically guide through a life-size silicone replica of the brain’s blood vessels. The core of the robotic thread is made from nickel-titanium alloy, or “nitinol,” a material that is both bendy and springy. Unlike a clothes hanger, which would retain its shape when bent, a nitinol wire would return to its original shape, giving it more flexibility in winding through tight, tortuous vessels. The team coated the wire’s core in a rubbery paste, or ink, which they embedded throughout with magnetic particles. Finally, they used a chemical process they developed previously, to coat and bond the magnetic covering with hydrogel — a material that does not affect the responsiveness of the underlying magnetic particles and yet provides the wire with a smooth, friction-free, biocompatible surface. They demonstrated the robotic thread’s precision and activation by using a large magnet, much like the strings of a marionette, to steer the thread through an obstacle course of small rings, reminiscent of a thread working its way through the eye of a needle. The researchers also tested the thread in a life-size silicone replica of the brain’s major blood vessels, including clots and aneurysms, modeled after the CT scans of an actual patient’s brain. The team filled the silicone vessels with a liquid simulating the viscosity of blood, then manually manipulated a large magnet around the model to steer the robot through the vessels’ winding, narrow paths. Kim says the robotic thread can be functionalized, meaning that features can be added — for example, to deliver clot-reducing drugs or break up blockages with laser light. To demonstrate the latter, the team replaced the thread’s nitinol core with an optical fiber and found that they could magnetically steer the robot and activate the laser once the robot reached a target region. When the researchers ran comparisons between the robotic thread coated versus uncoated with hydrogel, they found that the hydrogel gave the thread a much-needed, slippery advantage, allowing it to glide through tighter spaces without getting stuck. In an endovascular surgery, this property would be key to preventing friction and injury to vessel linings as the thread works its way through. “One of the challenges in surgery has been to be able to navigate through complicated blood vessels in the brain, which has a very small diameter, where commercial catheters can’t reach,” says Kyujin Cho, professor of mechanical engineering at Seoul National University. “This research has shown potential to overcome this challenge and enable surgical procedures in the brain without open surgery.” And just how can this new robotic thread keep surgeons radiation-free? Kim says that a magnetically steerable guidewire does away with the necessity for surgeons to physically push a wire through a patient’s blood vessels. This means that doctors also wouldn’t have to be in close proximity to a patient, and more importantly, the radiation-generating fluoroscope. In the near future, he envisions endovascular surgeries that incorporate existing magnetic technologies, such as pairs of large magnets, the directions of which doctors can manipulate from just outside the operating room, away from the fluoroscope imaging the patient’s brain, or even in an entirely different location. “Existing platforms could apply magnetic field and do the fluoroscopy procedure at the same time to the patient, and the doctor could be in the other room, or even in a different city, controlling the magnetic field with a joystick,” Kim says. “Our hope is to leverage existing technologies to test our robotic thread in vivo in the next step.” This research was funded, in part, by the Office of Naval Research, the MIT Institute for Soldier Nanotechnologies, and the National Science Foundation (NSF). ",Electronics and Technology,0.12071629296375583
31,MIT News,Comparing primate vocalizations,Research,2019-09-03,-,http://news.mit.edu/2019/old-world-monkey-language-0903,"  The utterances of Old World monkeys, some of our primate cousins, may be more sophisticated than previously realized — but even so, they display constraints that reinforce the singularity of human language, according to a new study co-authored by an MIT linguist.  The study reinterprets evidence about primate language and concludes that Old World monkeys can combine two items in a language sequence. And yet, their ability to combine items together seems to stop at two. The monkeys are not able to recombine language items in the same open-ended manner as humans, whose languages generate an infinite variety of sequences. “We are saying the two systems are fundamentally different,” says Shigeru Miyagawa, an MIT linguist and co-author of a new paper detailing the study’s findings. That might seem apparent. But the study’s precise claim — that even if other primates can combine terms, they cannot do so in the way humans do — emphasizes the profound gulf in cognitive ability between humans and some of our closest relatives. “If what we’re saying in this paper is right, there’s a big break between two [items in a sentence], and [the potential for] infinity,” Miyagawa adds. “There is no three, there is no four, there is no five. Two and infinity. And that is the break between a nonhuman primate and human primates.” The paper, “Systems underlying human and Old World monkey communications: One, two, or infinite,” is published today in the journal Frontiers in Psychology. The authors are Miyagawa, who is a professor of linguistics at MIT; and Esther Clarke, an expert in primate vocalization who is a member of the Behavior, Ecology, and Evolution Research (BEER) Center at Durham University in the U.K. To conduct the study, Miyagawa and Clarke re-evaluated recordings of Old World monkeys, a family of primates with over 100 species, including baboons, macaques, and the probiscis monkey. The language of some of these species has been studied fairly extensively. Research starting in the 1960s, for example, established that vervet monkeys have specific calls when they see leopards, eagles, and snakes, all of which requires different kinds of evasive action. Similarly, tamarin monkeys have one alarm call to warn of aerial predators and one to warn of ground-based predators. In other cases, though, Old World monkeys seem capable of combining calls to create new messages. The putty-nosed monkey of West Africa, for example, has a general alarm call, which scientists call “pyow,” and a specific alarm call warning of eagles, which is “hack.” Sometimes these monkeys combine them in “pyow-hack” sequences of varying length, a third message that is used to spur group movement. However, even these latter “pyow-hack” sequences start with “pyow” and end with “hack”; the terms are never alternated. Although these sequences vary in length and consequently can sound a bit different from each other, Miyagawa and Clarke break with some other analysts and think there is no “combinatorial operation” at work with putty-nosed monkey language, unlike the process through which humans rearrange terms. It is only the length of the “pyow-hack” sequence that indicates how far the monkeys will relocate. “The putty-nose monkey’s expression is complex, but the important thing is the overall length, which predicts behavior and predicts how far they travel,” Miyagawa says. “They start with ‘pyow’ and end up with ‘hack.’ They never go back to ‘pyow.’ Never.” As a result, Miyagawa adds, “Yes, those calls are made up of two items. Looking at the data very carefully it is apparent. The other thing that is apparent is that they cannot combine more than two things. We decided there is a whole different system here,” compared to human language. Similarly, Campbell’s monkey, also of West Africa, deploys calls that might be interpreted as evidence of human-style combination of language items, but which Miyagawa and Clarke believe are actually a simpler system. The monkeys make sounds rendered as “hok,” for an eagle alarm, and “krak,” for a leopard alarm. To each, they add an “-oo” suffix to turn those utterances into generalized aerial alarms and land alarms. However, that does not mean the Campbell’s monkey has developed a suffix as a kind of linguistic building block that could be part of a more open-ended, larger system of speech, the researchers conclude. Instead, its use is restricted to a small set of fixed utterances, none of which have more than two basic items in them. “It’s not the human system,” Miyagawa says. In the paper, Miyagawa and Clarke contend that the monkeys’ ability to combine these terms means they are merely deploying a “dual-compartment frame” which lacks the capacity for greater complexity. Miyagawa also notes that when the Old World monkeys speak, they seem to use a part of the brain known as the frontal operculum. Human language is heavily associated with Broca’s area, a part of the brain that seems to support more complex operations. If the interpretation of Old World monkey language that Miyagawa and Clarke put forward here holds up, then humans’ ability to harness Broca’s area for language may specifically have enabled them to recombine language elements as other primates cannot — by enabling us to link more than two items together in speech.  “It seems like a huge leap,” Miyagawa says. “But it may have been a tiny [physiological] change that turned into this huge leap.” As Miyagawa acknowledges, the new findings are interpretative, and the evolutionary history of human language acquisition is necessarily uncertain in many regards. His own operating conception of how humans combine language elements follows strongly from Noam Chomsky’s idea that we use a system called “Merge,” which contains principles that not all linguists accept. Still, Miyagawa suggests, further analysis of the differences between human language and the language of other primates can help us better grasp how our unique language skills evolved, perhaps 100,000 years ago. “There’s been all this effort to teach monkeys human language that didn’t succeed,” Miyagawa notes. “But that doesn’t mean we can’t learn from them.” ",Computer Science,0.11847217370450308
32,IEEE,It Shouldn't Be This Hard to Responsibly Fly a Drone,Robotics,2019-09-04,-,https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone,"      A few weeks ago, I went home to Oregon to visit my family. On a whim, I brought along my Parrot Anafi drone—it’s small and lightweight and uses the same USB-C charger as my laptop, so it was easy to toss into my carry-on. Like you do in Oregon, our plans were to go river rafting, hiking, and kayaking, and I figured I’d pack the drone and try and find some interesting opportunities to fly it. I try hard to be a responsible drone owner. I don’t have my U.S. Federal Aviation Administration Part 107 certificate yet, but my drone is registered and labeled, and I’m an AMA member, which includes insurance for recreational drone pilots. I also do my best to make sure that the places I fly are places that I’m allowed to fly, and fortunately, the FAA has a handy app that is supposed to make that easy. And it would have been easy, if I didn’t bother to check whether what the app was telling me was accurate or not. But I did check, and as it turns out, the app is, in many situations, worse than useless. Finding somewhere to legally and safely fly a drone for recreational purposes can be tricky. Most people who buy a drone to fly recreationally don’t want to have to worry about airspace restrictions and all that stuff. The FAA knows this, and the agency has invested what I can only assume is a lot of resources into educating new drone owners about how to be responsible pilots. It’s easy enough to just buy a cheap drone and ignore the FAA completely and fly it wherever you want, but setting aside willful ignorance, the FAA seems to understand that most folks are willing to obey common sense rules in the name of safety as long as it’s easy to do so. The problem is that the rules that govern recreational drone operations often aren’t common sense and aren’t at all easy to access. To help with this, the FAA has developed an app called B4UFLY where you can tell it your location and it’ll let you know whether you’re “good to go” or whether there are flight restrictions in place. It’s a wonderful idea, and I imagine it would be very effective, if it didn’t have some serious issues. Don’t get me wrong—the app itself is relatively fast and easy to use and seems to function largely as intended. The issue is that it presents itself, explicitly, as a tool that can be used “so that recreational flyers know whether it is safe to fly their drone” featuring “a clear ‘status’ indicator that informs the operator whether it is safe to fly or not.” But buried in a PDF FAQ (now offline) about the app is this: “Additionally, there may be local laws or ordinances about flying unmanned aircraft affecting your intended flight that are not reflected in this app. It is the responsibility of the operator to know the rules and fly safely at all times.” And oh boy is that a huge responsibility that the app itself doesn’t even mention, and that enormous loophole means that the B4UFLY app’s “good to go” indicator is not just meaningless but in fact giving you the wrong idea entirely. To illustrate the problem, here are some specific examples of places I wanted to fly my drone around Oregon. In each case, the FAA’s B4YFLY app (powered by Kittyhawk) was telling me that I was “good to go” when I really wasn’t. I’ve also included whether other drone safety mapping services (DJI FlySafe and Airmap) thought that it was safe to fly there, along with what it took to figure out the actual rules: The issue here is that for all of these places, the FAA’s B4UFLY app shows you a friendly green “good to go” message when in reality, flying in any of these areas is either explicitly prohibited or would require explicit permission. You could argue that this is worse than no app at all, because the app is actively giving you bad information. You are not, in fact, good to go, and if you’re already going, you should stop immediately. We asked the FAA for comment on this, and the agency sent us the following statement: The B4UFLY app provides information regarding airspace access for recreational drone flyers. It provides land use information for take offs and landings for National Park Service lands, but it does not provide information regarding take offs and landings are allowed on other federal, state, and locally managed lands and parks. Operators are expected to make themselves aware of any additional land use or police restrictions that may exist in the area where they wish to fly.  What the FAA is saying, I think, is that the B4UFLY app provides information about airspace, which is all that the FAA is allowed to regulate. The FAA can’t regulate anything on the ground, which is where most other drone regulations come in—when a city or state says “you can’t fly drones here,” what they’re actually saying is, “you can’t take off or land a drone from here because we own this land.” However, I have two issues with this. The first issue is that the FAA threw National Parks Service lands into the B4UFLY app, which (as they say in their comment above) is explicitly about “land use” regulations, not something the FAA has jurisdiction over, as far as I know. In fact, the National Parks Service itself acknowledges that it’s not technically illegal to fly drones over national parks, as long as you launch, operate, and land from outside the park boundary. So, if the FAA is able to add national parks restrictions to the B4UFLY app, why not wilderness areas and other federal lands? The second issue here is a much more fundamental one—recreational drone pilots put their trust in the FAA to tell them where they can and cannot fly. When the FAA itself presents the B4UFLY app as a tool that can be used so that “recreational flyers know whether it is safe to fly their drone,” that’s exactly what it should do. Instead, the app provides only one very limited kind of information about recreational drone safety, without telling the user that it’s on them to somehow dig up all the rest of the information that may or may not affect their flight. And based on my experience in Oregon, it can be a frustrating process that’s frequently inconclusive. In general, there’s a difference between not being prohibited from flying somewhere, and having explicit permission to fly somewhere, and I feel like responsible drone owners should always try and obtain explicit permission to fly. By “explicit permission,” I mean something somewhere that says, “the folks in charge of this area say that it’s okay for you to fly here.” This may be inconvenient, but it’s the best way of making sure that what you’re doing is safe and respectful. Drones are so cheap and pervasive now that the only way to make sure that people fly them safely is to make the rules clear, concise, and easy to follow. The FAA seems to be trying to do this, but it’s simply not good enough. At the absolute minimum, the B4UFLY app should not tell users that they’re “good to go” unless they are flying from an area where drone use is explicitly permitted, like national forests. Anywhere else, users should be instructed to verify that their local laws allow drone use. Is that going to be a huge annoyance that drives users away from the app? Of course. But it’s the truth, and if the FAA doesn’t like that, they should work with local governments to put the necessary information into the app instead.  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11397708756491168
33,IEEE,New Double 3 Robot Makes Telepresence Easier than Ever,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever,"      Today, Double Robotics is announcing Double 3, the latest major upgrade to its line of consumer(ish) telepresence robots. We had a (mostly) fantastic time testing out Double 2 back in 2016. One of the things that we found out back then was that it takes a lot of practice to remotely drive the robot around. Double 3 solves this problem by leveraging the substantial advances in 3D sensing and computing that have taken place over the past few years, giving their new robot a level of intelligence that promises to make telepresence more accessible for everyone. Double 2’s iPad has been replaced by “a fully integrated solution”—which is a fancy way of saying a dedicated 9.7-inch touchscreen and a whole bunch of other stuff. That other stuff includes an NVIDIA Jetson TX2 AI computing module, a beamforming six-microphone array, an 8-watt speaker, a pair of 13-megapixel cameras (wide angle and zoom) on a tilting mount, five ultrasonic rangefinders, and most excitingly, a pair of Intel RealSense D430 depth sensors.  It’s those new depth sensors that really make Double 3 special. The D430 modules each uses a pair of stereo cameras with a pattern projector to generate 1280 x 720 depth data with a range of between 0.2 and 10 meters away. The Double 3 robot uses all of this high quality depth data to locate obstacles, but at this point, it still doesn’t drive completely autonomously. Instead, it presents the remote operator with a slick, augmented reality view of drivable areas in the form of a grid of dots. You just click where you want the robot to go, and it will skillfully take itself there while avoiding obstacles (including dynamic obstacles) and related mishaps along the way.  This effectively offloads the most stressful part of telepresence—not running into stuff—from the remote user to the robot itself, which is the way it should be. That makes it that much easier to encourage people to utilize telepresence for the first time. The way the system is implemented through augmented reality is particularly impressive, I think. It looks like it’s intuitive enough for an inexperienced user without being restrictive, and is a clever way of mitigating even significant amounts of lag.  Otherwise, Double 3’s mobility system is exactly the same as the one featured on Double 2. In fact, that you can stick a Double 3 head on a Double 2 body and it instantly becomes a Double 3. Double Robotics is thoughtfully offering this to current Double 2 owners as a significantly more affordable upgrade option than buying a whole new robot. For more details on all of Double 3  new features, we spoke with the co-founders of Double Robotics, Marc DeVidts and David Cann. IEEE Spectrum: Why use this augmented reality system instead of just letting the user click on a regular camera image? Why make things more visually complicated, especially for new users? Marc DeVidts and David Cann: One of the things that we realized about nine months ago when we got this whole thing working was that without the mixed reality for driving, it was really too magical of an experience for the customer. Even us—we had a hard time understanding whether the robot could really see obstacles and understand where the floor is and that kind of thing. So, we said “What would be the best way of communicating this information to the user?” And the right way to do it ended up drawing the graphics directly onto the scene. It’s really awesome—we have a full, real time 3D scene with the depth information drawn on top of it. We’re starting with some relatively simple graphics, and we’ll be adding more graphics in the future to help the user understand what the robot is seeing. How robust is the vision system when it comes to obstacle detection and avoidance? Does it work with featureless surfaces, IR absorbent surfaces, in low light, in direct sunlight, etc? We’ve looked at all of those cases, and one of the reasons that we’re going with the RealSense is the projector that helps us to see blank walls. We also found that having two sensors—one facing the floor and one facing forward—gives us a great coverage area. Having ultrasonic sensors in there as well helps us to detect anything that we can't see with the cameras. They're sort of a last safety measure, especially useful for detecting glass.  It seems like there’s a lot more that you could do with this sensing and mapping capability. What else are you working on? We're starting with this semi-autonomous driving variant, and we're doing a private beta of full mapping. So, we’re going to do full SLAM of your environment that will be mapped by multiple robots at the same time while you're driving, and then you'll be able to zoom out to a map and click anywhere and it will drive there. That  where we're going with it, but we want to take baby steps to get there. It  the obvious next step, I think, and there are a lot more possibilities there. Do you expect developers to be excited for this new mapping capability? We're using a very powerful computer in the robot, a NVIDIA Jetson TX2 running Ubuntu. There  room to grow. It’s actually really exciting to be able to see, in real time, the 3D pose of the robot along with all of the depth data that gets transformed in real time into one view that gives you a full map. Having all of that data and just putting those pieces together and getting everything to work has been a huge feat in of itself.  We have an extensive API for developers to do custom implementations, either for telepresence or other kinds of robotics research. Our system isn't running ROS, but we're going to be adding ROS adapters for all of our hardware components. Telepresence robots depend heavily on wireless connectivity, which is usually not something that telepresence robotics companies like Double have direct control over. Have you found that connectivity has been getting significantly better since you first introduced Double? When we started in 2013, we had a lot of customers that didn’t have WiFi in their hallways, just in the conference rooms. We very rarely hear about customers having WiFi connectivity issues these days. The bigger issue we see is when people are calling into the robot from home, where they don't have proper traffic management on their home network. The robot doesn't need a ton of bandwidth, but it does need consistent, low latency bandwidth. And so, if someone else in the house is watching Netflix or something like that, it’s going to saturate your connection. But for the most part, it’s gotten a lot better over the last few years, and it’s no longer a big problem for us. Do you think 5G will make a significant difference to telepresence robots? We’ll see. We like the low latency possibilities and the better bandwidth, but it  all going to be a matter of what kind of reception you get. LTE can be great, if you have good reception; it’s all about where the tower is. I’m pretty sure that WiFi is going to be the primary thing for at least the next few years. DeVidts also mentioned that an unfortunate side effect of the new depth sensors is that hanging a t-shirt on your Double to give it some personality will likely render it partially blind, so that  just something to keep in mind. To make up for this, you can switch around the colorful trim surrounding the screen, which is nowhere near as fun. When the Double 3 is ready for shipping in late September, US $2,000 will get you the new head with all the sensors and stuff, which seamlessly integrates with your Double 2 base. Buying Double 3 straight up (with the included charging dock) will run you $4,ooo. This is by no means an inexpensive robot, and my impression is that it’s not really designed for individual consumers. But for commercial, corporate, healthcare, or education applications, $4k for a robot as capable as the Double 3 is really quite a good deal—especially considering the kinds of use cases for which it’s ideal. [ Double Robotics ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.1138742701220733
34,ACM,Hurricane Forecasters Turn to New Tools to Predict When Storms Will Rapidly Intensify,ACM,2019-08-31,-,https://www.washingtonpost.com/weather/2019/08/31/hurricane-forecasters-turn-new-tools-predict-when-storms-will-rapidly-intensify/,"       Hurricane Forecasters Turn to New Tools to Predict When Storms Will Rapidly Intensify     The Washington PostTristram KortenAugust 31, 2019   Scientists with the U.S. National Oceanic and Atmospheric Administration (NOAA) are working to gain a better understanding of what happens within a hurricane during periods of rapid intensification. The latest experiments involve specialized drones that fly around a hurricane's eye, and plane-mounted radars for measuring wind motion. NOAA's Frank Marks and colleagues are always honing dynamical and statistical computer models for hurricane forecasts; the former analyzes current atmosphere conditions and calculates their behavior in the immediate future, and the latter analyzes past storm patterns to assess how a hurricane might behave. The addition of drones is expected to supplement data collected by dropsondes deployed by hurricane hunter aircraft, while Doppler radar and Doppler LiDAR, which can quantify wind motion and potentially record wind-speed data near the eye, could help predict intensification.               *May Require Paid Registration  ",Space & Time,0.111602868662392
35,Science Daily,Emotion-Reading Algorithms Cannot Predict Intentions Via Facial Expressions,Health,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165231.htm,"   Computers aren't very good at discerning misrepresentation, and that's a problem as the technologies are increasingly deployed in society to render decisions that shape public policy, business and people's lives. Turns out that algorithms fail basic tests as truth detectors, according to researchers who study theoretical factors of expression and the complexities of reading emotions at the USC Institute for Creative Technologies. The research team completed a pair of studies using science that undermines popular psychology and AI expression understanding techniques, both of which assume facial expressions reveal what people are thinking. ""Both people and so-called 'emotion reading' algorithms rely on a folk wisdom that our emotions are written on our face,"" said Jonathan Gratch, director for virtual human research at ICT and a professor of computer science at the USC Viterbi School of Engineering. ""This is far from the truth. People smile when they are angry or upset, they mask their true feelings, and many expressions have nothing to do with inner feelings, but reflect conversational or cultural conventions."" Gratch and colleagues presented the findings today at the 8th International Conference on Affective Computing and Intelligent Interaction in Cambridge, England. Of course, people know that people can lie with a straight face. Poker players bluff. Job applicants fake interviews. Unfaithful spouses cheat. And politicians can cheerfully utter false statements. Yet, algorithms aren't so good at catching duplicity, even as machines are increasingly deployed to read human emotions and inform life-changing decisions. For example, the Department of Homeland Security invests in such algorithms to predict potential threats. Some nations use mass surveillance to monitor communications data. Algorithms are used in focus groups, marketing campaigns, to screen loan applicants or hire people for jobs. ""We're trying to undermine the folk psychology view that people have that if we could recognize people's facial expressions, we could tell what they're thinking,"" said Gratch, who is also a professor of psychology. ""Think about how people used polygraphs back in the day to see if people were lying. There were misuses of the technology then, just like misuses of facial expression technology today. We're using naïve assumptions about these techniques because there's no association between expressions and what people are really feeling based on these tests."" To prove it, Gratch and fellow researchers Su Lei and Rens Hoegen at ICT, along with Brian Parkinson and Danielle Shore at the University of Oxford, examined spontaneous facial expressions in social situations. In one study, they developed a game that 700 people played for money and then captured how people's expressions impacted their decisions and how much they earned. Next, they allowed subjects to review their behavior and provide insights into how they were using expressions to gain advantage and if their expressions matched their feelings. Using several novel approaches, the team examined the relationships between spontaneous facial expressions and key events during the game. They adopted a technique from psychophysiology called ""event-related potentials"" to address the extreme variability in facial expressions and used computer vision techniques to analyze those expressions. To represent facial movements, they used a recently proposed method called facial factors, which captures many nuances of facial expressions without the difficulties modern analysis techniques provide. The scientists found that smiles were the only expressions consistently provoked, regardless of the reward or fairness of outcomes. Additionally, participants were fairly inaccurate in perceiving facial emotion and particularly poor at recognizing when expressions were regulated. The findings show people smile for lots of reasons, not just happiness, a context important in the evaluation of facial expressions. ""These discoveries emphasize the limits of technology use to predict feelings and intentions,"" Gratch said. ""When companies and governments claim these capabilities, the buyer should beware because often these techniques have simplistic assumptions built into them that have not been tested scientifically."" Prior research shows that people will make conclusions about other's intentions and likely actions simply based off of the other's expressions. While past studies exist using automatic expression analysis to make inferences, such as boredom, depression and rapport, less is known about the extent to which perceptions of expression are accurate. These recent findings highlight the importance of contextual information when reading other's emotions and support the view that facial expressions communicate more than we might believe. ",Society,0.11124961763019421
36,IEEE,ETH Zurich Demonstrates PuppetMaster Robot,Robotics,2019-08-30,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot,"      As far as I know, the universe does not have a desperate need for robot puppeteers, and considering the difficulty of making even a halfway decent robot puppeteer, you’d think that any sensible roboticist would keep well clear of the problem. But some folks over at ETH Zurich decided that they’d have a crack at it anyway, and they started by describing why they’d likely be better off if they hadn’t: Marionettes are underactuated, high-dimensional, highly non-linear coupled pendulum systems. They are driven by gravity, the tension forces generated by a small number of cables, and the internal forces arising from mechanical articulation constraints. As such, the map between the actions of a puppeteer and the motions performed by the marionette is notoriously unintuitive, and mastering this unique art form takes unfaltering dedication and a great deal of practice. Our goal is to enable autonomous robots to animate marionettes with a level of skill that approaches that of human puppeteers.   I’m not much of a puppeteer myself, but this looks not bad at all, considering that the ABB YuMi robot is missing quite a few degrees of freedom in its hands. For context, here’s someone who has mastered this unique artform through unfaltering dedication and a great deal of practice, master puppeteer Scott Land:   The ETH Zurich project can’t yet animate a complex marionette, but it’s a respectable showing with the dragon, I think. As input, all the robot needs to know is the design of the puppet at the target motion you want the puppet to make. While moving the puppet in real life, the robot is continuously simulating its motions over the next second while iteratively optimizing to try to get the puppet to move the way it’s supposed to. The usefulness of this research, thankfully, is not constrained to puppets: Our long term goal is to enable robots to manipulate various types of complex physical systems – clothing, soft parcels in warehouses or stores, flexible sheets and cables in hospitals or on construction sites, plush toys or bedding in our homes, etc – as skillfully as humans do. We believe the technical framework we have set up for robotic puppeteering will also prove useful in beginning to address this very important grand-challenge. [ Paper ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11005753237125886
37,IEEE,Video Friday: This Robotic Thread Could One Day Travel Inside Your Brain,Robotics,2019-08-31,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain,"      Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (send us your events!): Let us know if you have suggestions for next week, and enjoy today’s videos. Eight engineering students from ETH Zurich are working on a year-long focus project to develop a multimodal robot called Dipper, which can fly, swim, dive underwater, and manage that difficult air-water transition:    The robot uses one motor to selectively drive either a propeller or a marine screw depending on whether it’s in flight or not. We’re told that getting the robot to autonomously do the water to air transition is still a work in progress, but that within a few weeks things should be much smoother. [ Dipper ] Thanks Simon!   Giving a jellyfish a hug without stressing them out is exactly as hard as you think, but Harvard’s robot will make sure that all jellyfish get the emotional (and physical) support that they need.    The gripper’s six “fingers” are composed of thin, flat strips of silicone with a hollow channel inside bonded to a layer of flexible but stiffer polymer nanofibers. The fingers are attached to a rectangular, 3D-printed plastic “palm” and, when their channels are filled with water, curl in the direction of the nanofiber-coated side. Each finger exerts an extremely low amount of pressure — about 0.0455 kPA, or less than one-tenth of the pressure of a human’s eyelid on their eye. By contrast, current state-of-the-art soft marine grippers, which are used to capture delicate but more robust animals than jellyfish, exert about 1 kPA. The gripper was successfully able to trap each jellyfish against the palm of the device, and the jellyfish were unable to break free from the fingers’ grasp until the gripper was depressurized. The jellyfish showed no signs of stress or other adverse effects after being released, and the fingers were able to open and close roughly 100 times before showing signs of wear and tear. [ Harvard ]   MIT engineers have developed a magnetically steerable, thread-like robot that can actively glide through narrow, winding pathways, such as the labyrinthine vasculature of the brain. In the future, this robotic thread may be paired with existing endovascular technologies, enabling doctors to remotely guide the robot through a patient’s brain vessels to quickly treat blockages and lesions, such as those that occur in aneurysms and stroke.    [ MIT ]   See NASA’s next Mars rover quite literally coming together inside a clean room at the Jet Propulsion Laboratory. This behind-the-scenes look at what goes into building and preparing a rover for Mars, including extensive tests in simulated space environments, was captured from March to July 2019. The rover is expected to launch to the Red Planet in summer 2020 and touch down in February 2021.    The Mars 2020 rover doesn’t have a name yet, but you can give it one! As long as you’re not too old! Which you probably are!  [ Mars 2020 ]   I desperately wish that we could watch this next video at normal speed, not just slowed down, but it’s quite impressive anyway.    Here’s one more video from the Namiki Lab showing some high speed tracking with a pair of very enthusiastic robotic cameras:  [ Namiki Lab ]   Normally, tedious modeling of mechanics, electronics, and information science is required to understand how insects’ or robots’ moving parts coordinate smoothly to take them places. But in a new study, biomechanics researchers at the Georgia Institute of Technology boiled down the sprints of cockroaches to handy principles and equations they then used to make a test robot amble about better.    [ Georgia Tech ]   More magical obstacle-dodging footage from Skydio’s still secret new drone.    We’ve been hard at work extending the capabilities of our upcoming drone, giving you ways to get the control you want without the stress of crashing. The result is you can fly in ways, and get shots, that would simply be impossible any other way. How about flying through obstacles at full speed, backwards? [ Skydio ]   This is a cute demo with Misty:    [ Misty Robotics ]   We’ve seen pieces of hardware like this before, but always made out of hard materials—a soft version is certainly something new.    Utilizing vacuum power and soft material actuators, we have developed a soft reconfigurable surface (SRS) with multi-modal control and performance capabilities. The SRS is comprised of a square grid array of linear vacuum-powered soft pneumatic actuators (linear V-SPAs), built into plug-and-play modules which enable the arrangement, consolidation, and control of many DoF. [ RRL ]   The EksoVest is not really a robot, but it’ll make you a cyborg! With super strength!    ""This is NOT intended to give you super strength but instead give you super endurance and reduce fatigue so that you have more energy and less soreness at the end of your shift."" Drat! [ EksoVest ]   We have created a solution for parents, grandparents, and their children who are living separated. This is an amazing tool to stay connected from a distance through the intimacy that comes through interactive play with a child. For parents who travel for work, deployed military, and families spread across the country, the Cushybot One is much more than a toy; it is the opportunity for maintaining a deep connection with your young child from a distance.    Hmm. I think the concept here is great, but it’s going to be a serious challenge to successfully commercialize. [ Indiegogo ]   What happens when you equip RVR with a parachute and send it off a cliff? Watch this episode of RVR Launchpad to find out – then go Behind the Build to see how we (eventually) accomplished this high-flying feat.    [ Sphero ]   These omnidirectional crawler robots aren’t new, but that doesn’t keep them from being fun to watch.    [ NEDO ] via [ Impress ]     We’ll finish up the week with a couple of past ICRA and IROS keynote talks—one by Gill Pratt on The Reliability Challenges of Autonomous Driving, and the other from Peter Hart, on Making Shakey.     [ IEEE RAS ]    Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11001508254830898
38,MIT News,Soft robotics breakthrough manages immune response for implanted devices,Research,2019-09-04,-,http://news.mit.edu/2019/soft-robotics-breakthrough-manages-implanted-devices-immune-response-0904,"  Researchers from the Institute for Medical Engineering and Science (IMES) at MIT; the National University of Ireland Galway (NUI Galway); and AMBER, the SFI Research Centre for Advanced Materials and BioEngineering Research, recently announced a significant breakthrough in soft robotics that could help patients requiring in-situ (implanted) medical devices such as breast implants, pacemakers, neural probes, glucose biosensors, and drug and cell delivery devices. The implantable medical devices market is currently estimated at approximately $100 billion, with significant growth potential into the future as new technologies for drug delivery and health monitoring are developed. These devices are not without problems, caused in part by the body’s own protection responses. These complex and unpredictable foreign-body responses impair device function and drastically limit the long-term performance and therapeutic efficacy of these devices. One such foreign body response is fibrosis, a process whereby a dense fibrous capsule surrounds the implanted device, which can cause device failure or impede its function. Implantable medical devices have various failure rates that can be attributed to fibrosis, ranging from 30-50 percent for implantable pacemakers or 30 percent for mammoplasty prosthetics. In the case of biosensors or drug/cell delivery devices, the dense fibrous capsule which can build up around the implanted device can seriously impede its function, with consequences for the patient and costs to the health care system. A radical new vision for medical devices to address this problem was published in the internationally respected journal, Science Robotics. The study was led by researchers from NUI Galway, IMES, and the SFI research center AMBER, among others. The research describes the use of soft robotics to modify the body’s response to implanted devices. Soft robots are flexible devices that can be implanted into the body. The transatlantic partnership of scientists has created a tiny, mechanically actuated soft robotic device known as a dynamic soft reservoir (DSR) that has been shown to significantly reduce the build-up of the fibrous capsule by manipulating the environment at the interface between the device and the body. The device uses mechanical oscillation to modulate how cells respond around the implant. In a bio-inspired design, the DSR can change its shape at a microscope scale through an actuating membrane. IMES core faculty member, assistant professor at the Department of Mechanical Engineering, and W.M. Keck Career Development Professor in Biomedical Engineering Ellen Roche, the senior co-author of the study, is a former researcher at NUI Galway who won international acclaim in 2017 for her work in creating a soft robotic sleeve to help patients with heart failure. Of this research, Roche says “This study demonstrates how mechanical perturbations of an implant can modulate the host foreign body response. This has vast potential for a range of clinical applications and will hopefully lead to many future collaborative studies between our teams.” Garry Duffy, professor in anatomy at NUI Galway and AMBER principal investigator, and a senior co-author of the study, adds “We feel the ideas described in this paper could transform future medical devices and how they interact with the body. We are very excited to develop this technology further and to partner with people interested in the potential of soft robotics to better integrate devices for longer use and superior patient outcomes. It’s fantastic to build and continue the collaboration with the Dolan and Roche labs, and to develop a trans-Atlantic network of soft roboticists.” The first author of the study, Eimear Dolan, lecturer of biomedical engineering at NUI Galway and former researcher in the Roche and Duffy labs at MIT and NUI Galway, says “We are very excited to publish this study, as it describes an innovative approach to modulate the foreign-body response using soft robotics. I recently received a Science Foundation Ireland Royal Society University Research Fellowship to bring this technology forward with a focus on Type 1 diabetes. It is a privilege to work with such a talented multi-disciplinary team, and I look forward to continuing working together.” ",Robotics,0.10797810214126664
39,Science Daily,Earthquake Study Casts Doubt on Early Warnings but Hints at Improved Forecasting,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904130614.htm,"   Since the 1980s seismologists -- earthquake researchers -- have wondered how feasible it might be to predict how an earthquake will behave given some information about its initial conditions. In particular whether you can tell the eventual magnitude based on seismic measurements near the point of origin, or epicenter. Most researchers consider this idea too improbable given the randomness of earthquake behavior, but Ide thinks there's more to it than that. ""Taking inspiration from a study comparing different-sized earthquakes, I decided to analyze a seismic data set from a region known as the Tohoku-Hokkaido subduction zone in eastern Japan,"" said Ide. ""A systematic comparison of around 100,000 seismic events over 15 years leads me to believe earthquakes are not different in random ways but share many similarities."" To draw comparisons between different earthquakes, Ide first selected the larger examples from the data set with magnitudes greater than 4.5. He also selected smaller earthquakes in the same regions as these larger ones. Ide then ascertained mathematically how similar seismic signals were between pairs of large and small earthquakes. He used a statistical function for the comparison of signals called a cross-correlation on data from 10 seismic stations close to the pairs of earthquakes in each case. ""Some pairs of large and small earthquakes start with exactly the same shaking characteristics, so we cannot tell the magnitude of an earthquake from initial seismic observations,"" explained Ide. ""This is bad news for earthquake early warning. However, for future forecasting attempts, given this symmetry between earthquakes of different magnitudes, it is good to know they are not completely random."" ",Environment,0.10712001080996152
40,IEEE,Eben Upton on the Raspberry Pi’s Industrial Crossover and Why There Will Never Be a Pi 9,Electronics and Technology,2019-08-28,-,https://spectrum.ieee.org/semiconductors/processors/eben-upton-on-the-raspberry-pis-industrial-crossover-and-why-there-will-never-be-a-pi-9,     ,Computer Science,0.10500415742793791
41,IEEE,Parrot Adds Folding VR Goggles to Anafi Drone Kit,Robotics,2019-09-04,-,https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit,"      We've been fans of Parrot’s Anafi drone since its release just over a year ago. In a space dominated by DJI, the Anafi’s thoughtful and considerate design means that it’s the drone I usually prefer to fly rather than my Mavic Pro. The Anafi is small, quiet, compact, takes great pictures, and since it uses the same USB-C charger as my laptop and my phone, it’s easy to travel with. Over the last year, Parrot has been making consistent (if minor) upgrades to the Anafi’s software, as well as some subtle hardware upgrades that improve both ruggedness and efficiency. And this week, it’s announcing a new pair of foldable goggles that add a first-person-view flying experience that’s easy to take with you. If you haven’t tried flying a drone through a headset before, it’s a unique experience. Some might even say magical. I wouldn’t say that, because I’m a bitter and jaded tech reporter, but it’s seriously a lot of fun.  Parrot’s Cockpitglasses 3 (Parrot is still terrible at naming things) work a bit like Google’s Daydream headset, in that it uses your phone instead of a dedicated screen. The nice thing about relying on phones for this is when you get a better phone, you get a goggles upgrade as well, although the quality you get won’t be on par with (much more expensive) FPV systems with built-in screens. The headset has two buttons on it which aren't electronic, but instead are hinged to tap on different places on your phone  screen when you push them to send commands through to the app. When the goggles aren't in use, they fold down nearly flat, which is a neat trick for something that looks like it  probably somewhat comfortable to wear. The main FPV HUD shows contextual information, such as flight speed, direction, altitude, and drone location. But with the click of a button on top of the Cockpitglasses, you can easily swap to a minimal HUD interface, so you can fully immerse yourself in the FPV experience. You can even find your Anafi in mid-air without removing the Cockpit glasses: Switch to See-Through View, and you’ll see live video from your smartphone camera. An overlay shows the exact position of your drone -- and if your Anafi is off-screen, an icon points to its location. Parrot  Freeflight 6 software is also seeing some upgrades with the release of the FPV kit, on top of additions that have been made over the past year such as live histogram and zebra stripes for exposure adjustments, and automated little planet and tunnel shots. Brand new are two flight presets: cinematic, which locks the roll axis to record video like you're flying smoothly through the air; and racing, which does the same thing except in a more aggressive sport mode. While Parrot seems to be advertising the Anafi FPV as a new drone in places, it’s really the same Anafi that’s been available for the past year. There have been some incremental upgrades, though. The most noticeable is likely that the weight of the controller has been reduced by 30 percent, almost certainly by removing the interior balancing weight blocks, something that  trivial to do yourself. The drone has been lightened a bit (down to 310 grams), had its arms streamlined and reinforced, and configured to fold up into a slightly smaller package. It’s nothing crazy, but with these small optimizations, Parrot has managed to tease out an entire extra minute of flight time (now 26 minutes)—impressive for a drone this size. The Parrot FPV kit, including the custom backpack that doubles as a launch pad, will run you US $800 when it  available in “early September.” That’s $100 more than the current MSRP of the drone and controller by themselves. It  not a bad deal, especially if you want to try out FPV flight for the first time. Parrot says that for current Anafi owners, a software update will enable FPV mode, and we'd expect that the goggles will be available to add to your kit in the near future.  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.10313856517641697
42,Science Daily,Explosive Fireballs: Never-Before-Seen Insight,Matter & Energy,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903153819.htm,"   The ability to measure and monitor the dramatic changes during explosions could help scientists understand and even control them. Measurements using rugged temperature or pressure probes placed inside an exploding fireball can provide physical data but cannot measure chemical changes that may be generated during the explosion. Sampling the end products of a detonation is possible but provides information only once the explosion is over. In this work, molecules in the fireball are detected by monitoring the way they interact with light, especially in the infrared region. These measurements are fast and can be taken a safe distance away. Since fireballs are turbulent and full of strongly absorbing substances, lasers are needed. Using a new instrument built in their lab, the investigators measured explosive events at faster speeds, at higher resolutions and for longer time periods than previously possible using infrared laser light. ""The swept-ECQCL approach enables new measurements by combining the best features of high-resolution tunable laser spectroscopy with broadband methods such as FTIR,"" co-author Mark Phillips explained. The study looked at four types of high-energy explosives, all placed in a specially designed chamber to contain the fireball. A laser beam from the swept-ECQCL was directed through this chamber while rapidly varying the laser light's wavelength. The laser light transmitted through the fireball was recorded throughout each explosion to measure changes in the way infrared light was absorbed by molecules in the fireball. The explosion produces substances such as carbon dioxide, carbon monoxide, water vapor and nitrous oxide. These can all detected by the characteristic way each absorbs infrared light. Detailed analysis of the results provided the investigators with information about temperature and concentrations of these substances throughout the explosive event. They were also able to measure absorption and emission of infrared light from tiny solid particles (soot) created by the explosion. The swept-ECQCL measurements provide a new way to study explosive detonations that could have other uses. In future studies, the investigators hope to extend the measurements to more wavelengths, faster scan rates, and higher resolutions. ",Space & Time,0.10293197557034074
43,ACM,"Right-Wing WhatsApp Users in Brazil are Louder, More Active, More Effective",ACM,2019-08-28,-,https://news.northwestern.edu/stories/2019/08/right-wing-whatsapp-users-in-brazil-are-louder-more-active-more-effective/,"    Right-Wing WhatsApp Users in Brazil are Louder, More Active, More Effective     Northwestern University NewscenterAmanda MorrisAugust 28, 2019   Researchers at Northwestern University (NU) have confirmed that WhatsApp use played a key role in Brazil's 2018 presidential election. The researchers performed the first large-scale analysis of partisan WhatsApp groups in the context of Brazil's 2018 election, and found that right-wing users were more effective in using the platform to spread news, disinformation, and opinions. They also found there were more right-wing groups in Brazil than left-wing, and the right-wing groups shared significantly more multimedia content than did left-wing groups. Said NU's Larry Birnbaum, ""Our ultimate goal is to understand how information and misinformation spreads, so we can find technological interventions. We want to find ways to help people better evaluate the information they receive.”                 ",Computer Science,0.10291840012378634
44,IEEE,Algorithms Aid Search for Source of Spacetime Rumbles,Space & Time,2019-08-28,-,https://spectrum.ieee.org/tech-talk/aerospace/astrophysics/new-algorithms-lead-search-for-origins-of-spacetime-rumbles,"      One night last week around 7 p.m., Michael Lundquist was at his home in Tucson, Arizona when his cellphone rang. He knew from the ringtone that it was a robocall. So he took it immediately. He lives for moments like this. “A LIGO/VIRGO alert has been received,” the automated voice told him (generated by a Python script that he’d written). The LIGO gravitational wave detector in Louisiana and Washington state had just seconds earlier picked up a ripple in spacetime. Lundquist opened his email app to see the details of LIGO’s gravitational wave alert. The gravitational wavefront from a collision of two neutron stars some 115 million light years away had, LIGO reported, just passed through the earth. And it looked like this potential “kilonova” event could be visible in Tucson that night. (The alert arrived, as it happened, perfectly timed to kick off the evening’s telescope observations, under partly cloudy but clearing skies.) Lundquist is one of a handful of pioneers of what’s called “multi-messenger” astronomy, using in this case gravitational wave detections to trigger astronomical observations via an optical telescope. His group’s SAGUARO system (standing for Searches After Gravitational-waves Using ARizona Observatories) has automated access to a 1.5-meter survey telescope in the Catalina mountains outside Tucson. Within an hour of receiving notification of the gravitational wave candidate event “S190822c,” Lundquist and others in the SAGUARO loop began plotting their night’s observing run of the region of the sky where this gravitational wave source might be found. In this case, the LIGO data analysis programs had concluded from the shape of the gravitational wave front that two in-spiraling and ultimately colliding neutron stars had produced the spacetime blip. LIGO is also well tuned to detect neutron star-black hole collisions and—the predominant, vanilla flavor in LIGO-land—black hole-black hole collisions. The fact that LIGO is able to point in any direction to the potential source of the gravitational waves is remarkable in itself. A single gravitational wave interferometer provides precious little information about where the candidate wave originated. However, with two LIGO interferometers in different regions of the United States and one European gravitational wave interferometer (VIRGO) outside Pisa, Italy, careful timing of the wavefront’s arrival at multiple detectors allows astronomers to triangulate the direction of the signal. As it turns out, S190822c was ultimately retracted. Detecting changes in laser beam path length down to fractions of the width of a proton—which is how much spacetime wobbles and warps when a gravitational wave passes through—is often a game of false positives. S190822c was one such false positive. Yet, as Lundquist and co-authors describe in their recent paper on SAGUARO, published in Astrophysical Journal Letters, they have had three candidate gravitational wave events on which to test their increasingly automated system. The closest they’ve yet come to discovering the optical counterpart to a gravitational wave source came on 25 April of this year. In that case, the source was in a galaxy some 500 million light years distant. Lundquist says that, knowing the region of the sky where the gravitational wave had just been found, SAGUARO’s algorithms get telescope time as soon as possible to take pictures of between 12 and 24 tiled segments of the sky. The team already has on file previous images from that same telescope of 5,069 patches of sky, covering most of the sky that’s visible from Tucson, Arizona’s latitude. So the algorithm automatically aligns and then subtracts that archived image from the new image. Which creates a negative picture that only registers any changes in stars or galaxies (or asteroids or comets or planets) in the frame. Through this process, the 25 April event, Lundquist says, generated 2,711 candidate sources. Which would be impossible to sort out by hand in any reasonable timeframe, when the optical flare-up in the distant galaxy might only be visible for a day or two after the gravitational wave signal arrives. So Lundquist has also set up a machine learning pipeline that automatically considers each potential candidate source for more trivial explanations—a known variable star, for instance, or an over-saturated image that caused noisy pixel data. This way, the team can whittle the list of possible gravitational wave sources down to one or two dozen objects that could then be reasonably followed up on by astronomers. The thing they’re ultimately looking for looks like a supernova in a far away galaxy, only brighter. Called a kilonova, this recently discovered astronomical event (thanks in large part to the observations of another multi-messenger gravitational wave detection from 2017) is now believed to be the source of most of the universe’s heavy elements. That gold in your watch or ring or necklace is in fact very likely the byproduct of two colliding neutron stars somewhere nearby, sometime around or before 4.6 billion years ago, when our solar system formed. “We have it in a nice spot where we’re waiting for a really good event,” Lundquist says. “But in the meantime we’re just trying to improve the machine learning as much as possible—to make it just that much easier to find the kilonova.”  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Space & Time,0.1000177052104701
45,Science Daily,Astronomers Find a Golden Glow from a Distant Stellar Collision,Space & Time,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827123524.htm,"   The impact also created a kilonova -- a turbocharged explosion that instantly forged several hundred planets' worth of gold and platinum. The observations provided the first compelling evidence that kilonovae produce large quantities of heavy metals, a finding long predicted by theory. Astronomers suspect that all of the gold and platinum on Earth formed as a result of ancient kilonovae created during neutron star collisions. Based on data from the 2017 event, first spotted by the Laser Interferometer Gravitational-wave Observatory (LIGO), astronomers began to adjust their assumptions of how a kilonova should appear to Earth-bound observers. A team led by Eleonora Troja, an associate research scientist in the University of Maryland's Department of Astronomy, re-examined data from a gamma-ray burst spotted in August 2016 and found new evidence for a kilonova that went unnoticed during the initial observations. NASA's Neil Gehrels Swift Observatory began tracking the 2016 event, named GRB160821B, minutes after it was detected. The early catch enabled the research team to gather new insights that were missing from the kilonova observations of the LIGO event, which did not begin until nearly 12 hours after the initial collision. Troja and her colleagues reported these new findings in the journal Monthly Notices of the Royal Astronomical Society on August 27, 2019. ""The 2016 event was very exciting at first. It was nearby and visible with every major telescope, including NASA's Hubble Space Telescope. But it didn't match our predictions -- we expected to see the infrared emission become brighter and brighter over several weeks,"" said Troja, who also has an appointment at NASA's Goddard Space Flight Center. ""Ten days after the event, barely any signal remained. We were all so disappointed. Then, a year later, the LIGO event happened. We looked at our old data with new eyes and realized we had indeed caught a kilonova in 2016. It was a nearly perfect match. The infrared data for both events have similar luminosities and exactly the same time scale."" The similarities between the two events suggest that the 2016 kilonova also resulted from the merger of two neutron stars. Kilonovae may also result from the merger of a black hole and neutron star, but it is unknown whether such an event would yield a different signature in X-ray, infrared, radio and optical light observations. According to Troja, the information collected from the 2016 event does not contain as much detail as the observations of the LIGO event. But the coverage of those first few hours -- missing from the record of the LIGO event -- revealed important new insights into the early stages of a kilonova. For example, the team got their first look at the new object that remained after the collision, which was not visible in the LIGO event data. ""The remnant could be a highly magnetized, hypermassive neutron star known as a magnetar, which survived the collision and then collapsed into a black hole,"" said Geoffrey Ryan, a Joint Space-Science Institute (JSI) Prize Postdoctoral Fellow in the UMD Department of Astronomy and a co-author of the research paper. ""This is interesting, because theory suggests that a magnetar should slow or even stop the production of heavy metals, which is the ultimate source of a kilonova's infrared light signature. Our analysis suggests that heavy metals are somehow able to escape the quenching influence of the remnant object."" Troja and her colleagues plan to apply the lessons they learned to re-evaluate past events, while also improving their approach to future observations. A number of candidate events have been identified with optical light observations, but Troja is more interested in events with a strong infrared light signature -- the telltale indicator of heavy metal production. ""The very bright infrared signal from this event arguably makes it the clearest kilonova we have observed in the distant universe,"" Troja said. ""I'm very much interested in how kilonova properties change with different progenitors and final remnants. As we observe more of these events, we may learn that there are many different types of kilonovae all in the same family, as is the case with the many different types of supernovae. It's so exciting to be shaping our knowledge in real time."" ",Space & Time,0.09997415694667021
46,Science Daily,Busy Older Stars Outpace Stellar Youngsters,Space & Time,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828193809.htm,"   The findings provide fresh insights into the history of our Galaxy and increase our understanding of how stars form and evolve. Researchers calculate that the old stars are moving more quickly in and out of the disc -- the pancake-shaped mass at the heart of the Galaxy where most stars are located. A number of theories could explain this movement -- it all depends where the star is in the disc. Stars towards the outskirts could be knocked by gravitational interactions with smaller galaxies passing by. Towards the inner parts of the disc, the stars could be disturbed by massive gas clouds which move along with the stars inside the disc. They could also be thrown out of the disc by the movement of its spiral structure. Dr Ted Mackereth, a galactic archaeologist at the University of Birmingham, is lead author on the paper. He explains: ""The specific way that the stars move tells us which of these processes has been dominant in forming the disc we see today. We think older stars are move active because they have been around the longest, and because they were formed during a period when the Galaxy was a bit more violent, with lots of star formation happening and lots of disturbance from gasses and smaller satellite galaxies. There are lots of different processes at work, and untangling all these helps us to build up a picture of the history of our Galaxy."" The study uses data from the Gaia satellite, currently working to chart the movements of around 1 billion stars in the Milky Way. It also takes information from APOGEE, an experiment run by the Sloan Digital Sky Survey that uses spectroscopy to measure the distribution of elements in stars, as well as images from the recently-retired Kepler space telescope. Measurements provided by Kepler show how the brightness of stars varies over time, which gives insights into how they vibrate. In turn, that yields information about their interior structure, which enables scientists to calculate their age. The Birmingham team, working with colleagues at the University of Toronto and teams involved with the Sloan Digital Sky Survey, were able to take these different data strands and calculate the differences in velocity between different sets of stars grouped by age. They found that the older stars were moving in many different directions with some moving very quickly out from the galactic disk. Younger stars move closely together at much slower speeds out from the disc, although they are faster than the older stars as they rotate around the Galaxy within the disc. The eventual goal of the research is to link what is known about the Milky Way with information about how other galaxies in the universe formed, ultimately being able to place our Galaxy within the very earliest signatures of the universe. The research is published in the Monthly Notices of the Royal Astronomical Society and funded by the Science and Technology Facilities Council, the Royal Astronomical Society and the European Research Council. ",Space & Time,0.09996232076238758
47,Science Daily,Earth's Fingerprint Hints at Finding Habitable Planets Beyond the Solar System,Space & Time,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828140132.htm,"   McGill Physics student Evelyn Macdonald and her supervisor Prof. Nicolas Cowan used over a decade of observations of Earth's atmosphere taken by the SCISAT satellite to construct a transit spectrum of Earth, a sort of fingerprint for Earth's atmosphere in infrared light, which shows the presence of key molecules in the search for habitable worlds. This includes the simultaneous presence of ozone and methane, which scientists expect to see only when there is an organic source of these compounds on the planet. Such a detection is called a ""biosignature."" ""A handful of researchers have tried to simulate Earth's transit spectrum, but this is the first empirical infrared transit spectrum of Earth,"" says Prof. Cowan. ""This is what alien astronomers would see if they observed a transit of Earth."" The findings, published Aug. 28 in the journal Monthly Notices of the Royal Astronomical Society, could help scientists determine what kind of signal to look for in their quest to find Earth-like exoplanets (planets orbiting a star other than our Sun). Developed by the Canadian Space Agency, SCISAT was created to help scientists understand the depletion of Earth's ozone layer by studying particles in the atmosphere as sunlight passes through it. In general, astronomers can tell what molecules are found in a planet's atmosphere by looking at how starlight changes as it shines through the atmosphere. Instruments must wait for a planet to pass -- or transit -- over the star to make this observation. With sensitive enough telescopes, astronomers could potentially identify molecules such as carbon dioxide, oxygen or water vapour that might indicate if a planet is habitable or even inhabited. Cowan was explaining transit spectroscopy of exoplanets at a group lunch meeting at the McGill Space Institute (MSI) when Prof. Yi Huang, an atmospheric scientist and fellow member of the MSI, noted that the technique was similar to solar occultation studies of Earth's atmosphere, as done by SCISAT. Since the first discovery of an exoplanet in the 1990s, astronomers have confirmed the existence of 4,000 exoplanets. The holy grail in this relatively new field of astronomy is to find planets that could potentially host life -- an Earth 2.0. A very promising system that might hold such planets, called TRAPPIST-1, will be a target for the upcoming James Webb Space Telescope, set to launch in 2021. Macdonald and Cowan built a simulated signal of what an Earth-like planet's atmosphere would look like through the eyes of this future telescope which is a collaboration between NASA, the Canadian Space Agency and the European Space Agency. The TRAPPIST-1 system located 40 light years away contains seven planets, three or four of which are in the so-called ""habitable zone"" where liquid water could exist. The McGill astronomers say this system might be a promising place to search for a signal similar to their Earth fingerprint since the planets are orbiting an M-dwarf star, a type of star which is smaller and colder than our Sun. ""TRAPPIST-1 is a nearby red dwarf star, which makes its planets excellent targets for transit spectroscopy. This is because the star is much smaller than the Sun, so its planets are relatively easy to observe,"" explains Macdonald. ""Also, these planets orbit close to the star, so they transit every few days. Of course, even if one of the planets harbours life, we don't expect its atmosphere to be identical to Earth's since the star is so different from the Sun."" According to their analysis, Macdonald and Cowan affirm that the Webb Telescope will be sensitive enough to detect carbon dioxide and water vapour using its instruments. It may even be able to detect the biosignature of methane and ozone if enough time is spent observing the target planet. Prof. Cowan and his colleagues at the Montreal-based Institute for Research on Exoplanets are hoping to be some of the first to detect signs of life beyond our home planet. The fingerprint of Earth assembled by Macdonald for her senior undergraduate thesis could tell other astronomers what to look for in this search. She will be starting her Ph.D. in the field of exoplanets at the University of Toronto in the Fall. ",Space & Time,0.09995197402651484
48,Science Daily,The Dark Side of Extrasolar Planets Share Surprisingly Similar Temperatures,Space & Time,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827111114.htm,"   Using data from the Spitzer Space and the Hubble Space telescopes, the researchers from the McGill Space Institute found that the nightside temperature of 12 hot Jupiters they studied was about 800°C. Unlike our familiar planet Jupiter, so-called hot Jupiters circle very close to their host star -- so close that it typically takes fewer than three days to complete an orbit. As a result, hot Jupiters have daysides that permanently face their host stars and nightsides that always face the darkness of space, similarly to how the same side of the Moon always faces the Earth. The tight orbit also means these planets receive more light from their star, which is what makes them extremely hot on the dayside. But scientists had previously measured significant amounts of heat on the nightside of hot Jupiters, as well, suggesting some kind of energy transfer from one side to the other. ""Atmospheric circulation models predicted that the nightside temperatures should vary much more than they do,"" said Dylan Keating, a Physics PhD student under the supervision of McGill professor Nicolas Cowan. ""This is surprising because the planets we studied all receive different amounts of irradiation from their host stars and the dayside temperatures among them varies by almost 1700°C."" Keating, the first author of a new Nature Astronomy study describing the findings, said the nightside temperatures are probably the result of condensation of vaporized rock in these very hot atmospheres. ""The uniformity of the nightside temperatures suggests that clouds on this side of the planets are likely similar to one another in composition. Our analysis suggests that these clouds are likely made of minerals such as manganese sulfide or silicates: in other words, rocks,"" Keating explained. According to Cowan, because the basic physics of cloud formation are universal, the study of the nightside clouds on hot Jupiters could give insight into cloud formation elsewhere in the Universe, including on Earth. Keating said that future space telescope missions -- such as the James Webb Space Telescope and the European Space Agency's ARIEL mission -- could be used to further characterize the dominant cloud composition on hot Jupiter nightsides, as well as to improve models of atmospheric circulation and cloud formation of these planets. ""Observing hot Jupiters at both shorter and longer wavelengths will help us determine what types of clouds are on the nightsides of these planets,"" Keating explained. ",Space & Time,0.09992858363337102
49,Science Daily,Newly Discovered Giant Planet Slingshots Around Its Star,Space & Time,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828092502.htm,"   ""This planet is unlike the planets in our solar system, but more than that, it is unlike any other exoplanets we have discovered so far,"" says Sarah Blunt, a Caltech graduate student and first author on the new study publishing in The Astronomical Journal. ""Other planets detected far away from their stars tend to have very low eccentricities, meaning that their orbits are more circular. The fact that this planet has such a high eccentricity speaks to some difference in the way that it either formed or evolved relative to the other planets."" The planet was discovered using the radial velocity method, a workhorse of exoplanet discovery that detects new worlds by tracking how their parent stars ""wobble"" in response to gravitational tugs from those planets. However, analyses of these data usually require observations taken over a planet's entire orbital period. For planets orbiting far from their stars, this can be difficult: a full orbit can take tens or even hundreds of years. The California Planet Search, led by Caltech Professor of Astronomy Andrew W. Howard, is one of the few groups that watches stars over the decades-long timescales necessary to detect long-period exoplanets using radial velocity. The data needed to make the discovery of the new planet were first provided by W. M. Keck Observatory in Hawaii. In 1997, the team began using the High-Resolution Echelle Spectrometer (HIRES) on the Keck I telescope to take measurements of the planet's star, called HR 5183. ""The key was persistence,"" said Howard. ""Our team followed this star with Keck Observatory for more than two decades and only saw evidence for the planet in the past couple years! Without that long-term effort, we never would have found this planet."" In addition to Keck Observatory, the California Planet Search also used the Lick Observatory in Northern California and the McDonald Observatory in Texas. The astronomers have been watching HR 5183 since the 1990s, but do not have data corresponding to one full orbit of the planet, called HR 5183 b, because it circles its star roughly every 45 to 100 years. The team instead found the planet because of its strange orbit. ""This planet spends most of its time loitering in the outer part of its star's planetary system in this highly eccentric orbit, then it starts to accelerate in and does a slingshot around its star,"" explains Howard. ""We detected this slingshot motion. We saw the planet come in and now it's on its way out. That creates such a distinctive signature that we can be sure that this is a real planet, even though we haven't seen a complete orbit."" The new findings show that it is possible to use the radial velocity method to make detections of other far-flung planets without waiting decades. And, the researchers suggest, looking for more planets like this one could illuminate the role of giant planets in shaping their solar systems. Planets take shape out of disks of material left over after stars form. That means that planets should start off in flat, circular orbits. For the newly detected planet to be on such an eccentric orbit, it must have gotten a gravitational kick from some other object. The most plausible scenario, the researchers propose, is that the planet once had a neighbor of similar size. When the two planets got close enough to each other, one pushed the other out of the solar system, forcing HR 5183 b into a highly eccentric orbit. ""This newfound planet basically would have come in like a wrecking ball,"" says Howard, ""knocking anything in its way out of the system."" This discovery demonstrates that our understanding of planets beyond our solar system is still evolving. Researchers continue to find worlds that are unlike anything in our solar system or in solar systems we have already discovered. ""Copernicus taught us that Earth is not the center of the solar system, and as we expanded into discovering other solar systems of exoplanets, we expected them to be carbon copies of our own solar system,"" Howard explains, ""But it's just been one surprise after another in this field. This newfound planet is another example of a system that is not the image of our solar system but has remarkable features that make our universe incredibly rich in its diversity."" ",Space & Time,0.09989884961537852
50,MIT News,"For first time, astronomers catch asteroid in the act of changing color",Space & Time,2019-08-29,-,http://news.mit.edu/2019/asteroid-changing-color-first-0830,"  Last December, scientists discovered an “active” asteroid within the asteroid belt, sandwiched between the orbits of Mars and Jupiter. The space rock, designated by astronomers as 6478 Gault, appeared to be leaving two trails of dust in its wake — active behavior that is associated with comets but rarely seen in asteroids. While astronomers are still puzzling over the cause of Gault’s comet-like activity, an MIT-led team now reports that it has caught the asteroid in the act of changing color, in the near-infrared spectrum, from red to blue. It is the first time scientists have observed a color-shifting asteroid, in real-time. “That was a very big surprise,” says Michael Marsset, a postdoc in MIT’s Department of Earth, Atmospheric and Planetary Sciences (EAPS). “We think we have witnessed the asteroid losing its reddish dust to space, and we are seeing the asteroid’s underlying, fresh blue layers.” Marsset and his colleagues have also confirmed that the asteroid is rocky — proof that the asteroid’s tail, though seemingly comet-like, is caused by an entirely different mechanism, as comets are not rocky but more like loose snowballs of ice and dust. “It’s the first time to my knowledge that we see a rocky body emitting dust, a little bit like a comet,” Marsset says. “It means that probably some mechanism responsible for dust emission is different from comets, and different from most other active main-belt asteroids.” Marsset and his colleagues, including EAPS Research Scientist Francesca DeMeo and Professor Richard Binzel, have published their results today in the journal Astrophysical Journal Letters. A rock with tails Astronomers first discovered 6478 Gault in 1988 and named the asteroid after planetary geologist Donald Gault. Until recently, the space rock was seen as relatively average, measuring about 2.5 miles wide and orbiting along with millions of other bits of rock and dust within the inner region of the asteroid belt, 214 million miles from the sun. In January, images from various observatories, including NASA’s Hubble Space Telescope, captured two narrow, comet-like tails trailing the asteroid. Astronomers estimate that the longer tail stretches half a million miles out, while the shorter tail is about a quarter as long. The tails, they concluded, must consist of tens of millions of kilograms of dust, actively ejected by the asteroid, into space. But how? The question reignited interest in Gault, and studies since then have unearthed past instances of similar activity by the asteroid. “We know of about a million bodies between Mars and Jupiter, and maybe about 20 that are active in the asteroid belt,” Marsset says. “So this is very rare.” He and his colleagues joined the search for answers to Gault’s activity in March, when they secured observation time at NASA’s Infrared Telescope Facility (IRTF) on Mauna Kea, Hawaii. Over two nights, they observed the asteroid and used a high-precision spectrograph to divide the asteroid’s incoming light into various frequencies, or colors, the relative intensities of which can give scientists an idea of an object’s composition. From their analysis, the team determined that the asteroid’s surface is composed mainly of silicate, a dry, rocky material, similar to most other asteroids, and, more importantly, not at all like most comets. Comets typically come from the far colder edges of the solar system. When they approach the sun, any surface ice instantly sublimates, or vaporizes into gas, creating the comet’s characteristic tail. Since Marsset’s team has found 6478 Gault is a dry, rocky body, this means it likely is generating dust tails by some other active mechanism. A fresh change As the team observed the asteroid, they discovered, to their surprise, that the rock was changing color in the near-infrared, from red to blue. “We've never seen such a dramatic change like this over such a short period of time,” says co-author DeMeo. The scientists say they are likely seeing the asteroid’s surface dust, turned red over millions of years of exposure to the sun, being ejected into space, revealing a fresh, less irradiated surface beneath, that appears blue at near-infrared wavelengths. “Interestingly, you only need a very thin layer to be removed to see a change in the spectrum,” DeMeo says. “It could be as thin as a single layer of grains just microns deep.” So what could be causing the asteroid to turn color? The team and other groups studying 6478 Gault believe the reason for the color shift, and the asteroid’s comet-like activity, is likely due to the same mechanism: a fast spin. The asteroid may be spinning fast enough to whip off layers of dust from its surface, through sheer centrifugal force. The researchers estimate it would need to have about a two-hour rotation period, spinning around every couple of hours, versus Earth’s 24-hour period. “About 10 percent of asteroids spin very fast, meaning with a two- to three-hour rotation period, and it’s most likely due to the sun spinning them up,” says Marsset. This spinning phenomenon is known as the YORP effect (or, the Yarkovsky-O’Keefe-Radzievskii-Paddack effect, named after the scientists who discovered it), which refers to the effect of solar radiation, or photons, on small, nearby bodies such as asteroids. While asteroids reflect most of this radiation back into space, a fraction of these photons is absorbed, then reemitted as heat, and also momentum. This creates a small force that, over millions of years, can cause the asteroid to spin faster. Astronomers have observed the YORP effect on a handful of asteroids in the past. To confirm a similar effect is acting on 6478 Gault, researchers will have to detect its spin through light curves — measurements of the asteroid’s brightness over time. The challenge will be to see through the asteroid’s considerable dust tail, which can obscure key portions of the asteroid’s light. Marsset’s team, along with other groups, plan to study the asteroid for further clues to activity, when it next becomes visible in the sky. “I think [the group’s study] reinforces the fact that the asteroid belt is a really dynamic place,” DeMeo says. “While the asteroid fields you see in the movies, all crashing into each other, is an exaggeration, there is definitely a lot happening out there every moment.” This research was funded, in part, by the NASA Planetary Astronomy Program. ",Space & Time,0.09986691148111519
51,Science Daily,Hints of a Volcanically Active Exo-Moon,Space & Time,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829115425.htm,"   Sodium gas as circumstantial evidence Astronomers have not yet discovered a rocky moon beyond our solar system and it's on the basis of circumstantial evidence that the researchers in Bern conclude that the exo-Io exists: Sodium gas was detected at the WASP 49-b at an anomalously high-altitude. ""The neutral sodium gas is so far away from the planet that it is unlikely to be emitted solely by a planetary wind,"" says Oza. Observations of Jupiter and Io in our solar system, by the international team, along with mass loss calculations show that an exo-Io could be a very plausible source of sodium at WASP 49-b. ""The sodium is right where it should be"" says the astrophysicist. Tides keep the system stable Already in 2006, Bob Johnson of the University of Virginia and the late Patrick Huggins at New York University, USA had shown that large amounts of sodium at an exoplanet could point to a hidden moon or ring of material, and ten years ago, researchers at Virginia calculated that such a compact system of three bodies: star, close-in giant planet and moon, can be stable over billions of years. Apurva Oza was then a student at Virginia, and after his PhD on moons atmospheres in Paris, decided to pick up the theoretical calculations of these researchers. He now publishes the results of his work together with Johnson and colleagues in the Astrophysical Journal. ""The enormous tidal forces in such a system are the key to everything,"" explains the astrophysicist. The energy released by the tides to the planet and its moon keeps the moon's orbit stable, simultaneously heating it up and making it volcanically active. In their work, the researchers were able to show that a small rocky moon can eject more sodium and potassium into space through this extreme volcanism than a large gas planet, especially at high altitudes. ""Sodium and potassium lines are quantum treasures to us astronomers because they are extremely bright,"" says Oza, ""the vintage street lamps that light up our streets with yellow haze, is akin to the gas we are now detecting in the spectra of a dozen exoplanets."" ""We need to find more clues"" The researchers compared their calculations with these observations and found five candidate systems where a hidden exomoon can survive against destructive thermal evaporation. For WASP 49-b the observed data can be best explained by the existence of an exo-Io. However, there are other options. For example, the exoplanet could be surrounded by a ring of ionized gas, or non-thermal processes. ""We need to find more clues,"" Oza admits. The researchers are therefore relying on further observations with ground-based and space-based instruments. ""While the current wave of research is going towards habitability and biosignatures, our signature is a signature of destruction,"" says the astrophysicist. A few of these worlds could be destroyed in a few billion years due to the extreme mass loss. ""The exciting part is that we can monitor these destructive processes in real time, like fireworks,"" says Oza. ",Space & Time,0.09973059817288712
52,Science Daily,Ultra-Fast Bomb Detection Method Could Upgrade Airport Security,Science,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829101051.htm,"   In a comprehensive two-part paper published by the journal Propellants, Explosives, Pyrotechnics and Forensic Science International: Synergy, a team of researchers from Surrey detail how they have built on their previous ground-breaking work on super-fast fingerprint drug testing, to develop a technique that is able to detect key explosives in just 30 seconds. The new method, which uses swabbing material to collect samples of explosives, is able to detect substances such as nitrotoluenes, trinitrotriazine, hexamethylene triperoxide diamine and nitroglycerine. Detection of peroxide-based explosives is key as high-profile terrorist attacks such as the London bombings in 2007 used devices made from these materials. Surrey's swab spray technique is able to achieve higher sensitivity results than previously published works and was also tested on dirty surfaces such as new and used keyboards. Dr Melanie Bailey, co-author of the paper from the University of Surrey, said: ""It's the unfortunate reality that security, especially in our airports, has to stay several steps ahead of those that wish to cause harm and destruction. The current thermal based way of detecting explosive material is becoming outdated and has the propensity of producing false positives. What we demonstrate with our research is an extremely fast, accurate and sensitive detection system that is able to identify a wide range of explosive materials."" Dr Catia Costa, co-author of the paper from the University of Surrey, said: ""The need for fast screening methods with enhanced selectivity and sensitivity to explosives has reached a new boiling point with the recent terrorist activity. The use of paper spray for applications such as these may help reduce false-negative events whilst also allowing simultaneous detection of other substances such as drugs, as previously reported by our group."" Dr Patrick Sears, co-author of the paper from the University of Surrey, said: ""The critical advantage of this system is the ability to uniquely identify the explosive being detected, making it much less likely to create false alarms. The selectivity of this system means that it could also be used to identify a range of other threat materials whilst the sensitivity would allow the detection of invisible traces of explosives."" ",Space & Time,0.09932261899211782
53,IEEE,"SpaceX, OneWeb, or Kepler Communications: Who Really Launched the First Ku-band Satellite?",Space & Time,2019-08-30,-,https://spectrum.ieee.org/tech-talk/aerospace/satellites/spacex-oneweb-or-kepler-communications-who-really-launched-the-first-kuband-satellite,"      No one denies that the Soviet Union put the first man-made object into orbit nor, a few wackos aside, that American astronauts were first to reach the moon. But deciding which of three companies’ new broadband internet satellites was first to launch is proving somewhat more contentious. Elon Musk’s SpaceX, SoftBank’s OneWeb and Canadian start-up Kepler Communications are all claiming that they launched the first satellites capable of delivering high-speed internet using Ku-band (12-18 GHz) frequencies. At stake is far more than just bragging rights or national pride. According to US law, the first operator to launch gets first choice of back-up spectrum should there be any interference between the rival systems—a near certainty given the more than 10,000 satellites they intend to deploy. The Federal Communications Commission (FCC) now finds itself in the bizarre position of having to rule on something that might seem utterly obvious but that will affect the future of all three companies. It all seemed much simpler around the turn of the millennium, when two companies, Teledesic and Skybridge, announced plans for a few hundred low earth orbit (LEO) internet satellites. The FCC quickly settled on a plan for them to share the available spectrum. All the satellites could use the entire frequency range in the US, only switching to non-interfering frequencies during in-line events—when a ground station happened to line up with satellites from both systems. This would happen just six percent of the time, the FCC concluded. In those rare cases, the first operator to have launched would get to choose which half of the spectrum it would prefer to use. “But as there is no difference in quantity or kind between the halves of the spectrum,” the FCC wrote, “This first choice, a kind of coordination priority, has little significance.” In the end, it didn’t matter at all, as Teledesic and Skybridge both ran out of money before launching a single commercial satellite. But fast forward to 2019, and the significance of the FCC’s decision is looming much larger. “The rules were made for two systems and for just a few hundred satellites,” says Tim Farrar, president of satellite and telecommunications consulting firm TMF Associates. “If you’ve got thousands going around, you’ll have conjunctions happening almost all the time, and then you’re effectively splitting the band in two almost everywhere.” In a worst case scenario, operators would be operating with half the spectrum, and effectively half the capacity, their systems were designed for. Suddenly, the quality of an operator’s “home spectrum”—the dedicated frequencies it can retreat to—looks far more important. “Some portions of the Ku-band have more terrestrial incumbent users than other portions,” wrote OneWeb in a letter to the FCC. “Home spectrum matters... because it allows an operator to maximize network capacity and, in turn, service to customers by choosing the portion of the frequency band in which it prefers to operate.” Another issue is that only the first two systems can choose home spectrum bands anchored to either end of the available spectrum. “If there’s going to be more than two operators, you really, really want to be one of the first two,” says Farrar. “If you’re the third, you could be all hopping all over the place.” The home spectrum rules only come into play if the operators cannot agree beforehand how to coordinate with one another’s systems. But while everyone agrees that would be the best solution for all concerned, the challenges of such complex negotiations between multiple operators are considerable. “With operators all coming online at different times, we anticipate conversations will always be ongoing,” OneWeb told IEEE Spectrum. In order to hedge their bets, SpaceX, OneWeb and Kepler would all very much like to be first in line to choose their home spectrum. After launching its initial six satellites in February, OneWeb sent a letter to the FCC asserting victory. It wrote: “OneWeb hereby notifies the Commission that the first space station in the OneWeb System has met the requirement to be launched and capable of operating.... [Therefore] OneWeb hereby claims first priority in home spectrum selection in the Ku-band.” In May, Kepler put in its own claim, noting that its KIPP spacecraft reached orbit more than a year earlier, in January 2018, and had since been carrying out commercial operations. “As far as Kepler is aware, this launch represented the first deployment of a Ku-band satellite within the Processing Round, and as such it should have first priority in any selection of home spectrum within Ku-band,” the company wrote to the FCC. “Needless to say, it is disappointing to see OneWeb try and undercut Kepler’s position.” A few weeks after that, SpaceX made its own case. Despite launching its first experimental satellite in February 2018 (after Kepler), and its first 60 commercial satellites in May 2019 (after OneWeb), SpaceX believes that under FCC regulations, it still officially launched first. “The scope of this rule makes clear that to be considered ‘capable of operating,’ an operator must not only launch satellites but must also communicate with a U.S.-licensed earth station in the specific frequency band,” it wrote to the Commission. Although OneWeb applied for an FCC license for its earth stations before SpaceX, it has not yet been granted. SpaceX, however, applied for and was granted a special temporary license to communicate with its first batch of satellites shortly before their launch. OneWeb and Kepler dispute this interpretation, calling it “flawed” and “extraordinary,” with both sides insisting that their readings embody common sense. SpaceX notes that a foreign operator that launched first but had no intention of offering service in the US could hold domestic systems hostage. Kepler, on the other hand, points out that “to preserve fairness... home spectrum selection order cannot be based...on arbitrary barriers such as approval delays.” SpaceX has another Starlink mission scheduled for September, and OneWeb hopes to launch 30 satellites at a time on future rockets. With Tim Farrar estimating that the FCC could take 6 to 12 months to rule on home spectrum priority, the skies could be full of satellites by the time we find out, officially, who actually launched first.  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Space & Time,0.09795415462100274
54,Science Daily,'Radical' Wrinkle in Forming Complex Carbon Molecules in Space,Space & Time,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903134740.htm,"   The team's research has now identified several avenues by which ringed molecules known as polycyclic aromatic hydrocarbons, or PAHs, can form in space. The latest study is a part of an ongoing effort to retrace the chemical steps leading to the formation of complex carbon-containing molecules in deep space. PAHs -- which also occur on Earth in emissions and soot from the combustion of fossil fuels -- could provide clues to the formation of life's chemistry in space as precursors to interstellar nanoparticles. They are estimated to account for about 20 percent of all carbon in our galaxy, and they have the chemical building blocks needed to form 2D and 3D carbon structures. In the latest study, published in Nature Communications, researchers produced a chain of ringed, carbon-containing molecules by combining two highly reactive chemical species that are called free radicals because they contain unpaired electrons. The study ultimately showed how these chemical processes could lead to the development of carbon-containing graphene-type PAHs and 2D nanostructures. Graphene is a one-atom-thick layer of carbon atoms. Importantly, the study showed a way to connect a five-sided (pentagon-shaped) molecular ring with a six-sided (hexagonal) molecular ring and to also convert five-sided molecular rings to six-sided rings, which is a stepping stone to a broader range of large PAH molecules. ""This is something that people have tried to measure experimentally at high temperatures but have not done before,"" said Musahid Ahmed, a scientist in Berkeley Lab's Chemical Sciences Division. He led the chemical-mixing experiments at Berkeley Lab's Advanced Light Source (ALS) with Professor Ralf I. Kaiser at the University of Hawaii at Manoa. ""We believe this is yet another pathway that can give rise to PAHs."" Professor Alexander M. Mebel at Florida International University assisted in the computational work for the study. Previous studies by the same research team have also identified a couple of other pathways for PAHs to develop in space. The studies suggest there could be multiple chemical routes for life's chemistry to take shape in space. ""It could be all the above, so that it isn't just one,"" Ahmed said. ""I think that's what makes this interesting."" The experiments at Berkeley Lab's ALS -- which produces X-rays and other types of light supporting many different types of simultaneous experiments -- used a portable chemical reactor that combines chemicals and then jets them out to study what reactants formed in the heated reactor. Researchers used a light beam tuned to a wavelength known as ""vacuum ultraviolet"" or VUV produced by the ALS, coupled with a detector (called a reflectron time-of-flight mass spectrometer), to identify the chemical compounds jetting out of the reactor at supersonic speeds. The latest study combined the chemical radicals CH3 (aliphatic methyl radical) with C9H7 (aromatic 1-indenyl radical) at a temperature of about 2,105 Fahrenheit degrees to ultimately produce molecules of a PAH known as naphthalene (C10H8) that is composed of two joined benzene rings. The conditions required to produce naphthalene in space are present in the vicinity of carbon-rich stars, the study noted. The reactants produced from two radicals, the study notes, had been theorized but hadn't been demonstrated before in a high-temperature environment because of experimental challenges. ""The radicals are short-lived -- they react with themselves and react with anything else around them,"" Ahmed said. ""The challenge is, 'How do you generate two radicals at the same time and in the same place, in an extremely hot environment?' We heated them up in the reactor, they collided and formed the compounds, and then we expelled them out of the reactor."" Kaiser said, ""For several decades, radical-radical reactions have been speculated to form aromatic structures in combustion flames and in deep space, but there has not been much evidence to support this hypothesis."" He added, ""The present experiment clearly provides scientific evidence that reactions between radicals at elevated temperatures do form aromatic molecules such as naphthalene."" While the method used in this study sought to detail how specific types of chemical compounds form in space, the researchers noted that the methods used can also enlighten broader studies of chemical reactions involving radicals exposed to high temperatures, such as in the fields of materials chemistry and materials synthesis. ",Matter & Energy,0.09747424687145313
55,MIT News,How “information gerrymandering” influences voters,Research,2019-09-04,-,http://news.mit.edu/2019/information-gerrymandering-influences-voters-0904,"  Many voters today seem to live in partisan bubbles, where they receive only partial information about how others feel regarding political issues. Now, an experiment developed in part by MIT researchers sheds light on how this phenomenon influences people when they vote. The experiment, which placed participants in simulated elections, found not only that communication networks (such as social media) can distort voters’ perceptions of how others plan to vote, but also that this distortion can increase the chance of electoral deadlock or bias overall election outcomes in favor of one party.   “The structure of information networks can really fundamentally influence the outcomes of elections,” says David Rand, an associate professor at the MIT Sloan School of Management and a co-author of a new paper detailing the study. “It can make a big difference and is an issue people should be taking seriously.” More specifically, the study found that “information gerrymandering” can bias the outcome of a vote, such that one party wins up to 60 percent of the time in simulated elections of two-party situations where the opposing groups are equally popular. In a follow-up empirical study of the U.S. federal government and eight European legislative bodies, the researchers also identified actual information networks that show similar patterns, with structures that could skew over 10 percent of the vote in the study’s experiments. The paper, “Information gerrymandering and undemocratic decisions,” is being published today in Nature. The authors are Alexander J. Stewart of the University of Houston; Mohsen Mosleh, a research scientist at MIT Sloan; Marina Diakonova of the Environmental Change Institute at Oxford University; Antonio Arechar, an associate research scientist at MIT Sloan and a researcher at the Center for Research and Teaching in Economics (CIDE) in Aguascalientes, Mexico; Rand, who is also the principal investigator for MIT Sloan’s Human Cooperation Lab; and Joshua B. Plotkin of the University of Pennsylvania. Stewart is the lead author. Formal knowledge While there is a burgeoning academic literature on media preferences, political ideology, and voter choices, the current study is an effort to create general models of the fundamental influence that information networks can have. Through abstract mathematical models and experiments, the researchers can analyze how strongly networks can influence voter behavior, even when long-established layers of voter identity and ideology are removed from the political arena. “Part of the contribution here is to try to formalize how information about politics flows through social networks, and how that can influence voters’ decisions,” says Stewart. The study used experiments involving 2,520 particpants, who played a “voter game” in one of a variety of conditions. (The participants were recruited via Amazon’s Mechanical Turk platform and took part in the simulated elections via Breadboard, a platform generating multiplayer network interactions.) The players were divided into two teams, a “yellow” team and a “purple” team, usually with 24 people on each side, and were allowed to change their voting intentions in response to continuously updated polling data. The participants also had incentives to try to produce certain vote outcomes reflective of what the authors call a “compromise worldview.” For instance, players would receive a (modest) payoff if their team received a super-majority vote share; a smaller payoff if the other team earned a super-majority; and zero payoff if neither team reached that threshold. The election games usually lasted four minutes, during which time each voter had to decide how to vote. In general, voters almost always voted for their own party when the polling data showed it had a chance of reaching a super-majority share. They also voted for their own side when the polling data showed a deadlock was likely. But when the opposing party was likely to achieve a super-majority, half the players would vote for it, and half would continue to vote for their own side. During a baseline series of election games where all the players had unbiased, random polling information, each side won roughly a quarter of the time, and a deadlock without a super-majority resulted about half the time. But the researchers also varied the game in multiple ways. In one iteration of the game, they added information gerrymandering to the polls, such that some members of one team were placed inside the other team’s echo chamber. In another iteration, the research team deployed online bots, comprising about 20 percent of voters, to behave like “zealots,” as the scholars called them; the bots would strongly support one side only. After months of iterations of the game, the researchers concluded that election outcomes could be heavily biased by the ways in which the polling information was distributed over the networks, and by the actions of the zealot bots. When members of one party were led to believe that most others were voting for the other party, they often switched their votes to avoid deadlock. “The network experiments are important, because they allow us to test the predictions of the mathematical models,” says Mosleh, who led the experimental portion of the research “When we added echo chambers, we saw that deadlock happened much more often — and, more importantly, we saw that information gerrymandering biased the election results in favor of one party over the other.” The empirical case As part of the larger project, the team also sought out some empirical information about similar scenarios among elected governments. There are many instances where elected officials might either support their first-choice legislation, settle for a cross-partisan compromise, or remain in deadlock. In those cases, having unbiased information about the voting intentions of other legislators would seem to be very important. Looking at the co-sponsorship of bills in the U.S. Congress from 1973 to 2007, the researchers found that the Democratic Party had greater “influence assortment” — more exposure to the voting intentions of people in their own party — than the Republican Party of the same time. However, after Republicans gained control of Congress in 1994, their own influence assortment became equivalent to that of the Democrats, as part of a highly polarized pair of legislative influence networks. The researchers found similar levels of polarization in the influence networks of six out of the eight European parliaments they evaluated, generally during the last decade. Rand says he hopes the current study will help generate additional research by other scholars who want to keep exploring these dynamics empirically. “Our hope is that laying out this information gerrymandering theory, and introducing this voter game, we will spur new research around these topics to understand how these effects play out in real-world networks,” Rand says. Support for the research was provided by the U.S. Defense Advanced Research Projects Agency, the Ethics and Governance of Artificial Intelligence Initiative of the Miami Foundation, the Templeton World Charity Foundation and the John Templeton Foundation, the Army Research Office, and the David and Lucile Packard Foundation. ",Society,0.09335446265480059
56,MIT News,"New science blooms after star researchers die, study finds",Research,2019-08-29,-,http://news.mit.edu/2019/life-science-funding-researchers-die-0829,"  The famed quantum physicist Max Planck had an idiosyncratic view about what spurred scientific progress: death. That is, Planck thought, new concepts generally take hold after older scientists with entrenched ideas vanish from the discipline. “A great scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it,” Planck once wrote. Now a new study co-authored by MIT economist Pierre Azoulay, an expert on the dynamics of scientific research, concludes that Planck was right. In many areas of the life sciences, at least, the deaths of prominent researchers are often followed by a surge in highly cited research by newcomers to those fields. Indeed, when star scientists die, their subfields see a subsequent 8.6 percent increase, on average, of articles by researchers who have not previously collaborated with those star scientists. Moreover, those papers published by the newcomers to these fields are much more likely to be influential and highly cited than other pieces of research. “The conclusion of this paper is not that stars are bad,” says Azoulay, who has co-authored a new paper detailing the study’s findings. “It’s just that, once safely ensconsed at the top of their fields, maybe they tend to overstay their welcome.” The paper, “Does Science Advance one Funeral at a Time?” is co-authored by Azoulay, the International Programs Professor of Management at the MIT Sloan School of Management; Christian Fons-Rosen, an assistant professor of economics at the University of California at Merced; and Joshua Graff Zivin, a professor of economics at the University of California at San Diego and faculty member in the university’s School of Global Policy and Strategy. It is forthcoming in the American Economic Review. To conduct the study, the researchers used a database of life scientists that Azoulay and Graff Zivin have been building for well over a decade. In it, the researchers chart the careers of life scientists, looking at accomplishments that include funding awards, published papers and the citations of those papers, and patent statistics. In this case, Azoulay, Graff Zivin, and Fons-Rosen studied what occurred after the unexpected deaths of 452 life scientists, who were still active in their disciplines. In addition to the 8.6 percent increase in papers by new entrants to those subfields, there was a 20.7 percent decrease in papers by the rather smaller number of scientists who had previously co-authored papers with the star scientists. Overall, Azoulay notes, the study provides a window into the power structures of scientific disciplines. Even if well-established scientists are not intentionally blocking the work of researchers with alternate ideas, a group of tightly connected colleagues may wield considerable influence over journals and grant awards. In those cases, “it’s going to be harder for those outsiders to make a mark on the domain,” Azoulay notes. “The fact that if you’re successful, you get to set the intellectual agenda of your field, that is part of the incentive system of science, and people do extraordinary positive things in the hope of getting to that position,” Azoulay notes. “It’s just that, once they get there, over time, maybe they tend to discount ‘foreign’ ideas too quickly and for too long.” Thus what the researchers call “Planck’s Principle” serves as an unexpected — and tragic — mechanism for diversifying bioscience research. The researchers note that in referencing Planck, they are extending his ideas to a slightly different setting than the one he himself was describing. In his writing, Planck was discussing the birth of quantum physics — the kind of epochal, paradigm-setting shift that rarely occurs in science. The current study, Azoulay notes, examines what happens in everyday “normal science,” in the phrase of philosopher Thomas Kuhn. The process of bringing new ideas into science, and then hanging on to them, is only to be expected in many areas of research, according to Azoulay. Today’s seemingly stodgy research veterans were once themselves innovators facing an old guard. “They had to hoist themselves atop the field in the first place, when presumably they were [fighting] the same thing,” Azoulay says. “It’s the circle of life.” Or, in this case, the circle of life science. The research received support from the National Science Foundation, the Spanish Ministry of Economy and Competitiveness, and the Severo Ochoa Programme for Centres of Excellence in R&D. ",Society,0.09054789349791724
57,Science Daily,Realistic Robots Get Under Galápagos Lizards' Skin,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904213718.htm,"   To avoid injury from male-to-male contests, some animal species display behaviours such as color changes or sequences of movements that showcase body size and fighting ability. In lizards, one of the most recognised behaviours is the bobbing or pushup display. Dr David Clark at Alma College, US and colleagues investigated whether lizards would react more quickly and strongly to their opponent's bobbing display, if that display occurred immediately or with a delay following an initial challenge. The authors used remote-controlled realistic lizard robots made from hand carved wood, high resolution photos and latex limbs to simulate an opponent's reaction to a wild lizards' display. The authors positioned lizard robots approximately 1-3m from 20 wild Galápagos Lava Lizards (Microlophus bivitattus) found on the island of San Cristóbal. After provoking an initial response by the native lizard, the researchers remotely activated the lizard robot to respond with a pre-set counter movement either immediately, or after a 30-second delay. Dr David Clark, the corresponding author of the study said: ""We had hypothesized that our Lava Lizard subjects would respond differently if the robot responded immediately to their bobbing display than if the response from the robot was delayed. The results suggest that our hypothesis was correct. We found that an immediate response by the robot stimulated the wild lizard to respond more quickly and significantly more often than when the robot's response was delayed by 30 seconds."" The authors suggest that the live lizards may have perceived a rapid response from their robotic contestant as more aggressive than a delayed response. This ability to assess their contestant's level of aggression may help the lizard size up their competitor and may influence their decision to retreat or instigate a contest, helping them avoid disadvantageous injury. Dr Clark said: ""Ours is the first study to use a lizard robot that interacts with wild subjects in real-time. Previous research in this area has used either pre-recorded video playback or robots with movements set on a ""loop."" The findings confirm that realistic robotic stimuli can be used to interact with animals, to communicate with them and even manipulate their behaviour. Our results further our understanding of how lava lizards communicate with each other in their natural habitat."" The authors say that bobbing display communication in lizards could now be explored further by altering display speeds, bobbing height and the distance between the robot and subject. ",Society,0.08787100898287643
58,Science Daily,Nuclear Winter Would Threaten Nearly Everyone on Earth,Science,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828080543.htm,"   Indeed, death by famine would threaten nearly all of the Earth's 7.7 billion people, said co-author Alan Robock, a Distinguished Professor in the Department of Environmental Sciences at Rutgers University-New Brunswick. The study in the Journal of Geophysical Research-Atmospheres provides more evidence to support The Treaty on the Prohibition of Nuclear Weapons passed by the United Nations two years ago, Robock said. Twenty-five nations have ratified the treaty so far, not including the United States, and it would take effect when the number hits 50. Lead author Joshua Coupe, a Rutgers doctoral student, and other scientists used a modern climate model to simulate the climatic effects of an all-out nuclear war between the United States and Russia. Such a war could send 150 million tons of black smoke from fires in cities and industrial areas into the lower and upper atmosphere, where it could linger for months to years and block sunlight. The scientists used a new climate model from the National Center for Atmospheric Research with higher resolution and improved simulations compared with a NASA model used by a Robock-led team 12 years ago. The new model represents the Earth at many more locations and includes simulations of the growth of the smoke particles and ozone destruction from the heating of the atmosphere. Still, the climate response to a nuclear war from the new model was nearly identical to that from the NASA model. ""This means that we have much more confidence in the climate response to a large-scale nuclear war,"" Coupe said. ""There really would be a nuclear winter with catastrophic consequences."" In both the new and old models, a nuclear winter occurs as soot (black carbon) in the upper atmosphere blocks sunlight and causes global average surface temperatures to plummet by more than 15 degrees Fahrenheit. Because a major nuclear war could erupt by accident or as a result of hacking, computer failure or an unstable world leader, the only safe action that the world can take is to eliminate nuclear weapons, said Robock, who works in the School of Environmental and Biological Sciences. ",Matter & Energy,0.08744453133968584
59,ACM,MRI Computing Technique Can Spot Scarred Heart Muscles Without Damaging Kidneys,ACM,2019-08-28,-,https://warwick.ac.uk/newsandevents/pressreleases/new_mri_computing/,"    MRI Computing Technique Can Spot Scarred Heart Muscles Without Damaging Kidneys     University of WarwickAlice ScottAugust 28, 2019   Researchers at the University of Warwick in the U.K. have developed a new three-dimensional (3D) magnetic resonance imaging (MRI) computing technique to calculate strain in heart muscles and reveal muscular dysfunction without damaging the patient's kidneys. The Hierarchical Template Matching (HTM) methodology consists of a numerically stable technique for left ventricular myocardial tracking, a 3D extension of local weighted mean function to convert MRI pixels, and a 3D extension of the HTM model for myocardial tracking problems. This makes use of the contrast agent gadolinium unnecessary, lowering the risk of kidney damage. Warwick's Mark Williams said the technique allows doctors to ""see in more depth what is happening to the heart, more precisely to each heart muscle, and diagnose any issues such as remodeling of heart that causes heart failure.""                 ",Health,0.08454646232280529
60,Science Daily,Natural Ways of Cooling Cities,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165242.htm,"   Urban heat islands are a phenomenon where the temperature in a city is noticeably higher than in the surrounding rural area. When combined with the sort of heatwave that hit many parts of Europe at the beginning of July, urban heat can pose a real threat to the elderly, sick or other vulnerable people. Scientists at ETH Zurich have researched urban heat islands across the globe and have found that the effectiveness of heat-reduction strategies in cities varies depending on the regional climate. ""We already know that plants create a more pleasant environment in a city, but we wanted to quantify how many green spaces are actually needed to produce a significant cooling effect,"" says Gabriele Manoli, former postdoc with the Chair of Hydrology and Water Resources Management at ETH Zurich and lead author of the recently published article in the journal Nature. More green spaces: not always the most efficient solution Manoli and his colleagues from ETH Zurich, Princeton University and Duke University studied data from some 30,000 cities worldwide and their surrounding environment, taking into consideration the average summer temperature, the population size and the annual rainfall. The urban heat island phenomenon is more pronounced the bigger the city and the more rainfall in that region. As a general rule, more rain encourages plant growth in the surrounding area, making this cooler than the city. This effect is the strongest when annual rainfall averages around 1500 millimetres as in Tokyo, but does not increase further with more rain. Two climate extremes illustrate well the role of vegetation on the urban heat island phenomenon: very dry regions on the one hand, and tropical areas on the other. Through carefully targeted planting, a city like Phoenix in the USA could achieve cooler temperatures than the surrounding countryside, where conditions are almost desert-like. By comparison, a city surrounded by tropical forests, such as Singapore, would need far more green spaces to reduce temperatures, but this would also create more humidity. In cities located in tropical zones, other cooling methods are therefore expected to be more effective, such as increased wind circulation, more use of shade and new heat-dispersing materials. ""There is no single solution,"" Manoli says. ""It all depends on the surrounding environment and regional climate characteristics."" Useful information for city planners Manoli explains that the main benefit of the study is a preliminary classification of cities, in the form of a clear visualisation guiding planners on possible approaches to mitigate the urban heat island effect. ""Even so, searching for solutions to reduce temperatures in specific cities will require additional analysis and in-depth understanding of the microclimate,"" he stresses. ""Such information, however, is based on data and models available to city planners and decision-makers only in a handful of cities, such as Zurich, Singapore or London."" Manoli is currently analysing data from other periods of the year and is studying which types of plant are most suitable for reducing temperatures. The support provided by the Branco Weiss Fellowship allowed the environmental engineer to work with scientists from the areas of physics, urban studies and social sciences with a specific focus on interdisciplinary research topics. ",Environment,0.08452676065115006
61,Stanford,Vintage film reveals Antarctic glacier melting,Science,2019-09-05,-,https://news.stanford.edu/2019/09/02/vintage-film-reveals-antarctic-glacier-melting/,"   Newly digitized vintage film has doubled how far back scientists can peer into the history of underground ice in Antarctica, and revealed that an ice shelf on Thwaites Glacier in West Antarctica is being thawed by a warming ocean more quickly than previously thought. This finding contributes to predictions for sea-level rise that would impact coastal communities around the world.  New research shows the Thwaites Glacier in Antarctica, pictured here, may be at risk of melting further inland than previously thought. (Image credit: NASA)  The researchers made their findings by comparing ice-penetrating radar records of Thwaites Glacier with modern data. The research appeared in Proceedings of the National Academy of Sciences Sept. 2. “By having this record, we can now see these areas where the ice shelf is getting thinnest and could break through,” said lead author Dustin Schroeder, an assistant professor of geophysics at Stanford University’s School of Earth, Energy & Environmental Sciences (Stanford Earth) who led efforts to digitize the historical data from airborne surveys conducted in the 1970s. “This is a pretty hard-to-get-to area and we’re really lucky that they happened to fly across this ice shelf.” Researchers digitized about 250,000 flight miles of Antarctic radar data originally captured on 35mm optical film between 1971 and 1979 as part of a collaboration between Stanford and the Scott Polar Research Institute (SPRI) at Cambridge University in the U.K. The data has been released to an online public archive through Stanford Libraries, enabling other scientists to compare it with modern radar data in order to understand long-term changes in ice thickness, features within glaciers and baseline conditions over 40 years. Sea-level predictions The information provided by historic records will help efforts like the Intergovernmental Panel on Climate Change (IPCC) in its goal of projecting climate and sea-level rise for the next 100 years. By being able to look back 40 to 50 years at subsurface conditions rather than just the 10 to 20 years provided by modern data, scientists can better understand what has happened in the past and make more accurate projections about the future, Schroeder said. “You can really see the geometry over this long period of time, how these ocean currents have melted the ice shelf – not just in general, but exactly where and how,” said Schroeder, who is also a faculty affiliate at the Stanford Woods Institute for the Environment. “When we model ice sheet behavior and sea-level projections into the future, we need to understand the processes at the base of the ice sheet that made the changes we’re seeing.” The film was originally recorded in an exploratory survey using ice-penetrating radar, a technique still used today to capture information from the surface through the bottom of the ice sheet. The radar shows mountains, volcanoes and lakes beneath the surface of Antarctica, as well as layers inside the ice sheet that reveal the history of climate and flow. ",Environment,0.0844737929041725
62,Science Daily,"Remora-Inspired Suction Disk Mimics Fish's Adhesion Ability, Offers Evolutionary Insight",Matter & Energy,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903162537.htm,"   Key to the remora's adhesion are the disk's well-known capabilities for generating suction, as well as friction created by spiky bones within the disk called lamellae to maintain hold on its host. However, the factors driving the evolution of remora's unique disc morphology have long eluded researchers seeking to understand, and even engineer new devices and adhesives that mimic, the fish's uncanny ability to lock on to various surface types without harming their host or expending much energy, often for hours at a time under extreme oceanic forces. In a study led at New Jersey Institute of Technology (NJIT), researchers have showcased a new biologically inspired remora disc capable of replicating the passive forces of suction and friction that power the fish's ability, demonstrating up to 60% greater hold than has been measured for live remoras attached to shark skin. Using the disc model to explore evolutionary drivers of the remora's disc, researchers say the study's findings provide evidence that today's living species of remora have evolved a greater number of lamellae over time to enhance their holding power and ability to attach to a broader range of hosts with smoother surfaces, thereby increasing their chance for survival. The study, featured in Bioinspiration and Biomimetics, indicates the disc model may be used to inform the design of more effective, lower-cost adhesive technologies in the future. ""The beauty behind the remora's adhesive mechanism is that biological tissues inherently do most of the work,"" said Brooke Flammang, professor of biological sciences at NJIT who led the study. ""The most significant aspect of this research is that our robotic disc relies completely on the fundamental physics driving the adhesive mechanism in remoras, allowing us to determine biologically relevant performance and gain insight into the evolution of the remora's disc. This was previously not possible with past designs that required a human operator to control the system."" Diverging from many of their closest scavenger-like ancestors, such as cobia (Rachycentron canadum), the remora fish (of the family Echeneidae) is believed to have first begun attaching to hosts with rough surfaces, akin to sharks, after having evolved its suction disc from dorsal fin spines nearly 32 million years ago. The disc of living remoras today now features a fleshy-soft outer lip for suction while the disc's interior houses many more linear rows of tissue (lamellae) with tooth-like tissue projections (spinules), which the fish raises to generate friction against various host bodies to prevent slipping during hitchhiking. According to Flammang, while scientists have shed some light on the origins of the remora's modified fin structure, fundamental aspects of the disk's evolution have largely remained unclear. ""The evolution of the remora's disc is largely unknown,"" said Flammang. ""There is one fossil remora, Opisthomyzon, in the fossil record that has a disc with fewer lamellae [than today's remoras] without spinules towards the back of the head."" Flammang says this raises two questions: ""how"" and ""why."" ""The 'how' is from the dorsal fin, although the intermediate evolutionary stages aren't known,"" explained Flammang. ""If you look at a phylogeny of remoras it shows that those species that are thought to be more derived have more lamellae ... the 'why' has been assumed to be for adhesive performance, but that was never tested before this paper."" To learn more, Kaelyn Gamel, the study's first author and former graduate researcher in the Flammang lab, designed a remora-inspired disc from commercially available 3-D printed materials that could autonomously maintain attachment to various surfaces and be modified by adding and removing lamellae, enabling the team to investigate the performance of increased lamellar number on shear adhesion. ""Our disc's capability to add and remove lamellae while acting as a passive system allowed us to change the amount of friction along with the ambient pressure within the disc,"" said Gamel, now a Ph.D. researcher at the University of Akron. ""We were able to compare the difference between no friction, some friction and a lot of friction based on the variation in lamellae number."" In collaboration with Austin Garner, a researcher at the University of Akron, the team conducted pull-off tests with their model disc underwater, experimenting with the model's lamellar number (up to 12 lamellae) to measure the shear force and time it took to pull the disc from silicon molds with surfaces ranging from completely smooth to those exceeding shark skin roughness (350-grit, 180-grit and 100-grit). Overall, the team found that their disc's adhesive performance was strongly correlated with an increase in the disc's lamellae, observing a ""sweet spot"" in suction power between nine and 12 lamellae. When modified to 12 lamellae and 294 spinules, the team's disc weighed just 45 grams and withstood 27 N (newton) forces for 50 seconds -- almost three times the force that would typically pull a remora from a shark. The tests also revealed a minimum of six lamellae -- the number coincidentally found on the 32-million-year-old fossil Opisthomyzon -- were needed to maintain adhesion. ""What is most striking about these results is that for a given disc shape, there is an optimal range in which the friction and suction phenomena are balanced, and [as their disc size has gotten longer] remoras have evolved to maintain this sweet spot of high-performance adhesion,"" explained Flammang. The team now says their remora disk model will be used for future evolutionary studies to learn whether suction or friction predominated attachment in earliest remora ancestors and how evolution of disc shape affects adhesion. The disc may also have engineering applications in everything from medical biosensors and drug delivery devices to geo-sensing tags for ecological studies and tracking marine life. ""One of the greatest advantages to our design is that it operates autonomously because it relies only on the physics of the system for operation,"" said Flammang. ""This makes it easily scalable for a multitude of new technologies, both for medical and scientific purposes."" ",Matter & Energy,0.08410923950756816
63,Science Daily,Deep Transformations Needed to Achieve Sustainable Development Goals,Society,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112705.htm,"   The UN Sustainable Development Goals (SDGs) focus on time-bound targets for prosperity, people, planet, peace, and partnership -- collectively known as the five Ps. By adopting the 2030 Agenda with its 17 SDGs and the Paris Climate Agreement, UN member states effectively created a framework for national action and global cooperation on sustainable development, while the Paris Agreement committed signatory countries to achieving net-zero greenhouse gas emissions by the middle of the century. SDG 13 on climate change specifically links to the Paris Agreement noting that the UN Framework Convention on Climate Change ""is the primary international, intergovernmental forum for negotiating the global response to climate change."" Despite the interconnectivity and clear aims of these global goals, stakeholders seem to lack a shared understanding of how the 17 SDGs can be operationalized. Building on previous work by The World in 2050 -- a global research initiative established by IIASA -- the authors of the study published in the journal Nature Sustainability propose six transformations to organize SDG interventions through a semi-modular action agenda that can be designed by discrete, yet interacting, parts of government. According to the paper, the proposed framework may be operationalized within the structures of governments while still respecting the strong interdependencies across the 17 SDGs. The authors also outline an action agenda for science to provide the knowledge required for designing, implementing, and monitoring the SDG Transformations. ""The 2030 Agenda and the Paris Agreement have given the world an aspirational narrative and an actionable agenda to achieve a just, safe, and sustainable future for all within planetary boundaries. The six transformations provide an integrated and holistic framework for action that reduces the complexity, yet encompasses the 17 SDGs, their 169 targets, and the Paris Agreement. They provide a new approach to shift from incremental to transformational change; to identify synergies using sustainable development pathways; formulate actionable roadmaps; and a focus on inter-relationships to uncover multiple benefits and synergies,"" explains study co-author Nebojsa Nakicenovic, executive director of The World in 2050 (TWI2050) research initiative at IIASA. In their paper the researchers considered which key interventions would be necessary to achieve the SDG outcomes and how their implementation might be organized into a limited set of six transformations namely education, gender, and inequality; health, wellbeing, and demography; energy decarbonization and sustainable industry; sustainable food, land, water, and oceans; sustainable cities and communities; and digital revolution for sustainable development. To simplify the discussion of interlinkages between interventions and SDGs, the authors further identified intermediate outputs generated by combinations of interventions, which in turn contribute to the achievement of each SDG. Each SDG transformation describes a major change in societal structure (economic, political, technological, and social) to achieve long-term sustainable development, while also each contributing to multiple SDGs. Excluding any of them would make it virtually impossible to achieve the SDGs. Pursuing the six transformations will require deep, deliberate, long-term structural changes in resource use, infrastructure, institutions, technologies, and social relations, which have to happen in a relatively short time window. Previous societal transformations, like industrialization in 19th century Europe, were initiated by technological changes like the steam engine and were largely undirected, while 20th century technologies like semiconductors, the Internet and Global Positioning Systems, were promoted through directed innovation to meet military aims. The authors emphasize that it is crucial that SDG transformations are formally directed in order to meet time-bound, quantitative targets, such as net-zero carbon emissions by mid-century. ""By achieving change in these six key areas, we can save both people and planet. To deliver on both ambitious climate targets and meet all the Sustainable Development Goals, we identify very concrete levers that governments can pull. For instance, investing in agriculture with known technologies and management practices can enable both food security, human health, and climate mitigation. Investing in young children's education is another example. It improves human wellbeing, increases economic development, and stabilizes population growth,"" says study co-author Johan Rockström from the Potsdam Institute for Climate Impact Research in Germany. ""The six transformations in this paper have the ultimate goal of enhancing human prosperity and reducing inequalities. This is of course not easy. In fact, it is the largest human endeavor of all time. Science is here to provide governments with a fact-based framework. If political leadership fails to act, however, we would face unprecedented risks for the stability of societies, and for our Earth system."" ",Environment,0.0828111001950103
64,Stanford,Ancient die-off greater than dinosaur extinction,Science,2019-09-05,-,https://news.stanford.edu/2019/08/28/ancient-die-off-greater-dinosaur-extinction/,"   Clues from Canadian rocks formed billions of year ago reveal a previously unknown loss of life even greater than that of the mass extinction of the dinosaurs 65 million years ago, when Earth lost nearly three-quarters of its plant and animal species.  This photograph shows rocks from the Belcher Islands in Hudson Bay, Canada, from which doctoral candidate Malcolm Hodgskiss collected barite samples dating 2.02 to 1.87 billion years old. (Image credit: Malcolm Hodgskiss)  Rather than prowling animals, this die-off involved miniscule microorganisms that shaped the Earth’s atmosphere and ultimately paved the way for those larger animals to thrive. “This shows that even when biology on Earth is comprised entirely of microbes, you can still have what could be considered an enormous die-off event that otherwise is not recorded in the fossil record,” said Malcolm Hodgskiss, co-lead author of a new study published in Proceedings of the National Academy of Sciences. Invisible clues Because this time period preceded complex life, researchers cannot simply dig up fossils to learn what was living 2 billion years ago. Even clues left behind in mud and rocks can be difficult to uncover and analyze. Instead, the group turned to barite, a mineral collected from the Belcher Islands in Hudson Bay, Canada, that encapsulates a record of oxygen in the atmosphere. Those samples revealed that Earth experienced huge changes to its biosphere – the part of the planet occupied by living organisms – ending with an enormous drop in life approximately 2.05 billion years ago that may also be linked to declining oxygen levels. “The fact that this geochemical signature was preserved was very surprising,” Hodgskiss said. “What was especially unusual about these barites is that they clearly had a complex history.” Looking at the Earth’s productivity through ancient history provides a glimpse into how life is likely to behave over its entire existence – in addition to informing observations of atmospheres on planets outside our solar system. “The size of the biosphere through geologic time has always been one of our biggest questions in studying the history of the Earth,” said Erik Sperling, an assistant professor of geological sciences at Stanford who was not involved with the study. “This new proxy demonstrates how interlinked the biosphere and levels of oxygen and carbon dioxide in the atmosphere are.” Biological angle This relationship between the proliferation of life and atmospheric oxygen has given researchers new evidence of the hypothesized “oxygen overshoot.” According to this theory, photosynthesis from ancient microorganisms and the weathering of rocks created a huge amount of oxygen in the atmosphere that later waned as oxygen-emitting organisms exhausted their nutrient supply in the ocean and became less abundant. This situation is in contrast to the stable atmosphere we know on Earth today, where the oxygen created and consumed balances out. The researchers’ measurements of oxygen, sulfur and barium isotopes in barite support this oxygen overshoot hypothesis. The research helps scientists hone their estimates of the size of the oxygen overshoot by revealing the significant biological consequences of oxygen levels above or below the capacity of the planet. “Some of these oxygen estimates likely require too many microorganisms living in the ocean in Earth’s past,” said co-lead author Peter Crockford, a postdoctoral researcher at the Weizmann Institute of Science and Princeton University. “So we can now start to narrow in on what the composition of the atmosphere could have been through this biological angle.” Co-authors include researchers from Nanjing University, the University of Colorado Boulder and Woods Hole Oceanographic Institution The research was supported by Stanford University McGee and Compton Grants, the Northern Scientific Training Program, NSERC, National Geographic, the American Philosophical Society, Geological Society of America and the Agouron Institute. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Matter & Energy,0.08264827320998318
65,Science Daily,Methane-Producing Microorganism Makes a Meal of Iron,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904175713.htm,"   ""The microorganism Methanosarcina acetivorans is a methanogen that plays an important part in the carbon cycle, by which dead plant material is recycled back into carbon dioxide that then generates new plant material by photosynthesis,"" said James Ferry, Stanley Person Professor of Biochemistry and Molecular Biology at Penn State, who led the research team. ""Methanogens produce about 1 billion metric tons of methane annually, which plays a critical role in climate change. Understanding the process by which this microorganism produces methane is important for predicting future climate change and for potentially manipulating how much of this greenhouse gas the organism releases."" Methanosarcina acetivorans, which is found in environments like the ocean floor and rice paddies where it helps to decompose dead plant material, converts acetic acid into methane and carbon dioxide. Prior to this study, however, researchers were not certain how the microorganism had enough energy to survive in the oxygen-free -- anaerobic -- environments where it lives. The researchers determined that an oxidized form of iron called ""iron three,"" essentially rust, allows the microorganism to work more efficiently, using more acetic acid, creating more methane, and creating more ATP -- a chemical that provides energy for biological reactions essential for growth. ""Most organisms like humans use a process called respiration to create ATP, but this requires oxygen,"" said Ferry. ""When no oxygen is present, many organisms instead use a less efficient process called fermentation to create ATP, like the processes used by yeast in the production of wine and beer. But the presence of iron allows M. acetivorans to use respiration even in the absence of oxygen."" The findings allowed the researchers to update the biological pathway by which M. acetivorans converts acetic acid to methane, which now includes respiration. Pathways like this one involve many intermediate steps, during which energy is often lost in the form of heat. The researchers also determined that in the presence of iron, energy loss in this microorganism is reduced due to a recently discovered process called electron bifurcation. ""Electron bifurcation takes one of those steps that has the potential for tremendous heat loss and harvests that energy in the form of ATP rather than heat,"" said Ferry. ""This makes the process more efficient."" This updated pathway could allow researchers to predict the amount of methane that the microorganism will release into the atmosphere. ""Rice paddies -- a major source of the methane in the atmosphere -- contain decaying rice plants submerged in water that are ultimately processed by M. acetivorans. If we measure the amount of iron three present in the paddies, we can predict how much methane will be released by the microorganisms, which can improve our climate change models."" In the absence of iron, the microorganism produces roughly equal amounts of methane and carbon dioxide from acetic acid. But with increasing amounts of iron, it produces more carbon dioxide relative to methane, so providing the organism with additional iron could alter the relative amounts of these greenhouse gasses that are produced. ""Methane is 30 times more potent as a greenhouse gas than carbon dioxide, which makes it more problematic in terms of our warming planet,"" said Ferry. ""Now that we better understand this biochemical pathway, we see that we can use iron to alter the ratios of the gasses being produced. In the future, we might even be able to go further and inhibit the production of methane by this microorganism. ""In addition to the practical applications, this is a major addition to understanding the biology of the largely unseen but hugely important anaerobic world."" In addition to Ferry, the research team includes Divya Prakash and Shikha Chauhan at Penn State. The research was supported by US Department of Energy and the Penn State Eberly College of Science. ",Environment,0.08178046580939621
66,ACM,3.8-Million-Year-Old Fossil Cranium Unveils More About Human Ancestry,ACM,2019-08-28,-,https://news.psu.edu/story/585192/2019/08/28/research/38-million-year-old-fossil-cranium-unveils-more-about-human,"       3.8-Million-Year-Old Fossil Cranium Unveils More About Human Ancestry     Penn State NewsA'ndrea Elyse MesserAugust 28, 2019   Researchers at Pennsylvania State University (Penn State), Case Western Reserve University, and Italy's University of Bologna have gained new insights into human evolution via digital reconstruction of 3.8-million-year-old fossilized cranium fragments of a hominid from Ethiopia. Penn State's Timothy M. Ryan said the team scanned the fossils with micro-computed tomography (CT), then processed the data to build three-dimensional models. The Bologna scientists computer-modeled the fossil based on CT scan data to predict the hominid's likely appearance. By identifying and dating the fossil, the team was able to compare its facial features with those of a partial cranium discovered decades earlier, and determined the latter belonged to a close relative. Case Western's Yohannes Haile-Selassie said, ""This is a game changer in our understanding of human evolution during the Pliocene [5.3 million to 2.6 million years ago].""                 ",Matter & Energy,0.08143499957151552
67,Science Daily,Ancient Civilizations Were Already Messing Up the Planet,Science,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150702.htm,"   ""Through this crowdsourced data, we can see that there was global environmental impact by land use at least 3,000 years ago,"" says Gary Feinman, MacArthur Curator of Anthropology at the Field Museum and one of the study's 250 authors. ""And that means that the idea of seeing human impact on the environment as a newer phenomenon is too focused on the recent past."" Feinman says that to understand our current climate crisis, we need to understand the history of humans altering their environments. The study, led by Lucas Stephens of the University of Pennsylvania, is a part of a larger project called ArchaeoGLOBE, where online surveys are used to gather information from regional experts on how land use has changed over time in 146 different areas around the world. Land use can be anything from hunting and gathering to farming to grazing animals. And as it turns out, many of the ways ancient people used the land weren't as ""leave-no-trace"" as many have imagined. ""About 12,000 years ago, humans were mainly foraging, meaning they didn't interact with their environments as intensively as farmers generally do,"" says Feinman. ""And now we see that 3,000 years ago, we have people doing really invasive farming in many parts of the globe."" Humans in these time periods began clearing out forests to plant food and domesticating plants and animals to make them dependent on human interaction. Early herders also changed their surroundings through land clearance and selective breeding. While these changes were at varying paces, the examples are now known to be widespread and can provide insight on how we came to degrade our relationship with the Earth and its natural resources. ""We saw an accelerated trajectory of environmental impact,"" says Ryan Williams, associate curator and head of anthropology at the Field Museum and co-author of the study. ""While the rate at which the environment is currently changing is much more drastic, we see the effects that human impacts had on the Earth thousands of years ago."" The results, however, are more optimistic than they seem. Now that researchers know the beginnings of environmental impact, they can use this data to study what solutions ancient civilizations used to mitigate the negative effects of deforestation, water scarcity, and more. In addition to pointing out the history behind what most assume is a recent phenomenon, the study is one of the first of its kind to operate on such a large scale. Use of online resources and professional connections helped the project span across the world. The emphasis now, however, is on the parts we often miss. ""We need to invest in these regions that haven't been as intensively studied,"" says Williams. ""If we incentivize and create opportunities for researchers there then you can just imagine what the results of the next study like this could be."" For a long time, war, environment, transportation and colonization prevented researchers from being able to work together and share their findings about certain parts of the world. As a result, today's archaeologists are still adding to and growing the network of expertise in these regions. ""What really got me here was not so much the results, although I think that the results provide a foundation to support what many archeologists suspected,"" says Feinman. ""But I think the most innovative aspect of this was the whole research design. To gather information from 250 scholars and to make sure that the whole world was covered, that's really something."" While today's climate change and environmental destruction are happening more quickly and on a far larger scale than the world has ever seen, Feinman notes that this study helps provide a historical context to today's problems. ""There's such a focus on how the present is different from the past in contemporary science. I think this study provides a check, a counter-weight to that, by showing that yes, there have been more accelerated changes in land use recently, but humans have been doing this for a long time. And the patterns start 3,000 years ago,"" says Feinman. ""It shows that the problems we face today are very deep-rooted, and they are going to take more than simple solutions to solve. They cannot be ignored."" ",Matter & Energy,0.08071228515456703
68,Science Daily,What If We Paid Countries to Protect Biodiversity?,Science,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830112815.htm,"   After long negotiations, the international community has agreed to safeguard the global ecosystems and improve on the status of biodiversity. The global conservation goals for 2020, called the Aichi targets, are an ambitious hallmark. Yet, effective implementation is largely lacking. Biodiversity is still dwindling at rates only comparable to the last planetary mass extinction. Additional effort is required to reach the Aichi targets and even more so to halt biodiversity loss. ""Human well-being depends on ecological life support. Yet, we are constantly losing biodiversity and therefore the resilience of ecosystems. At the international level, there are political goals, but the implementation of conservation policies is a national task. There is no global financial mechanism that can help nations to reach their biodiversity targets,"" says lead author Nils Droste from Lund University, Sweden. Brazil has successfully implemented Ecological Fiscal Transfer systems that compensate municipalities for hosting protected areas at a local level since the early 1990's. According to previous findings, such mechanisms help to create additional protected areas. The international research team has therefore set out to scale this idea up to the global level where not municipalities but nations are in charge of designating protected areas. They developed and compared three different design options: An ecocentric model: where only protected area extent per country counts -- the bigger the protected area, the better; A socio-ecological model: where protected areas and Human Development Index count, adding development justice to the previous model; An anthropocentric model: where population density is also considered, as people benefit locally from protected areas. The socio-ecological design was the one that proved to be the most efficient. The model provided the highest marginal incentives -- that is, the most additional money for protecting an additional percent of a country's area -- for countries that are the farthest from reaching the global conservation goals. The result surprised the researchers. ""While we developed the socio-ecological design with a fairness element in mind, believing that developing countries might be more easily convinced by a design that benefits them, we were surprised how well this particular design aligns with the global policy goals,"" says Nils Droste. ""It would most strongly incentivize additional conservation action where the global community is lacking it the most,"" he adds. As the study was aimed at providing options, not prescriptions for policy makers, the study did not detail who should be paying or how large the fund should exactly be. Rather, it provides a yet unexplored option to develop a financial mechanism for biodiversity conservation akin to what the Green Climate Fund is for climate change. ""We know that we need to change land use in order to preserve biodiversity. Protecting land from degradation and providing healthy ecosystems, clean air or clean rivers is a function of the state. Giving a financial reward to governments for such public ecosystem services will ease the provision of corresponding conservation efforts and will help to put this on the agenda,"" concludes Nils Droste. ",Environment,0.08022707945914313
69,Science Daily,Putting a Price on Carbon Pollution Alone Unlikely to Help Reach Climate Goals,Science,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904113219.htm,"   The Paris Agreement, signed in 2015, requires nations to collectively limit global warming to 2°C by 2100, and to pursue efforts to limit the temperature increase even further to 1.5°C. This goal requires human-caused carbon dioxide (CO2) emissions to reach zero by 2070 and become negative afterwards, using strategies that remove CO2 from the air, such as carbon capture technologies or planting trees. However, a new study by Imperial College London researchers shows that carbon taxes, which are the currently favoured system for reaching this target, will not be enough to avoid catastrophic climate change. They instead suggest that alongside carbon taxes, which put a price on emissions, there also need to be incentives for strategies that remove CO2 from the atmosphere. They say this will encourage these strategies to be implemented at a commercial scale in order to reach the Paris Agreement goals. The study is published in Joule. Study lead author Habiba Daggash, from the Centre for Environmental Policy at Imperial, said: ""The current system of penalising greenhouse gas emissions through carbon taxes is not sufficient to avoid catastrophic climate change, even if very high taxes are enforced. Therefore, using this strategy alone, the Paris Agreement that most countries have committed to could not be delivered. ""The system needs to be adapted to recognise that not only do emissions need to be penalised, but actions that result in permanent removal of greenhouse gases from the atmosphere must also be credited."" Placing a price on carbon, usually in the form of taxes on emissions, has been touted as a way of allowing market forces to produce a low-carbon economy, in which using low-carbon forms of energy is seen as an advantage. Using the UK as an example, Habiba and Dr Niall Mac Dowell, also from the Centre for Environmental Policy, modelled the future UK energy system based on several scenarios concerning levels of carbon taxation and incentives for carbon removal. Their analysis shows that much higher carbon taxes than current levels are enough to create a push for low-carbon technologies that satisfy emissions goals in the short term. However, higher carbon taxes are not enough to incentivise the development and deployment of carbon removal strategies, which are necessary to reach long-term goals. If, instead, governments incentivised carbon removal strategies much earlier, then carbon taxes could remain lower while still encouraging removal strategies to be developed and deployed on a large scale. Habiba said: ""Early incentives could both reduce the cost of delivering the Paris Agreement and satisfy our long-term need for negative emissions."" The team say that the UK case study could apply to other regions, and are now investigating the situation in developing economies, using Nigeria as a case study. ",Environment,0.08005524909596184
70,Science Daily,Birds in Serious Decline at Lake Constance,Environment,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903105223.htm,"   This seeming contradiction is due to the fact that the most common species are disappearing particularly rapidly. Six of the ten most common bird species around Lake Constance have declined dramatically in number, while two have remained the same and only two have increased. The population of house sparrows, for example, has declined by 50 percent since 1980, at which time it was still the most common species. ""These are really shocking figures -- particularly when you consider that the bird population started declining decades before the first count in 1980,"" explains Hans-Günther Bauer from the Max Planck Institute of Animal Behavior. Viewed over a lengthier period, the fall in numbers may therefore be even greater. Agricultural landscape hostile to birds It is particularly noticeable how differently the various habitats have been affected. The study indicates that bird populations around Lake Constance are dwindling particularly rapidly in countryside, which is intensively used by humans. This applies above all to modern farmland: 71 percent of the species that inhabit fields and meadows have declined in numbers, in some cases drastically. The partridge, for example, which was once a common inhabitant of the region's farmland, has completely died out around Lake Constance. The great grey shrike, the meadow pipit and the little owl have also disappeared from the area. One of the main reasons for this decline is the scarcity of food. According to the ornithologists, 75 percent of the bird species that eat flying insects and 57 percent of those that eat terrestrial invertebrates have decreased in number around Lake Constance. ""This confirms what we have long suspected: the human extermination of insects is having a massive impact on our birds,"" says Bauer. In addition, today's efficient harvesting methods leave hardly any seeds behind for granivorous species. Moreover, the early, frequent mowing of large areas of grassland, the agricultural practice of monoculture, the early ripening of winter grains, the implementation of drainage measures and the shortage of fallow land are destroying the habitats of many species that live in the open countryside. However, the birds are disappearing not only from the fields and meadows but also from the towns and villages around Lake Constance. ""The increasing need for order and decreasing tolerance of dirt and noise are making life more and more difficult for local birds. It appears that successful breeding is becoming increasingly rare since the birds are being forced to nest amid tower blocks, ornamental trees and immaculate kitchen gardens,"" says Bauer. Even species that can survive virtually anywhere, such as blackbirds (down 28 percent), chaffinches and robins (each down 24 percent) are suffering greatly due to the deteriorating conditions in settled areas. Winners and losers in the woods and on the water In contrast, the woodland birds around Lake Constance appear to be doing comparatively well. 48 percent of the forest-dwelling species are increasing in number, while only 35 percent are dwindling. One example is the spotted woodpecker, whose numbers have grown by 84 percent. Like other woodpeckers, it seems to have benefited from the larger quantities of timber in the forest. Furthermore, more of the species that inhabit the wetlands around Lake Constance have increased than decreased. The winners here include the mute swan. Nevertheless, the numbers of many forest-dwelling species are also declining. The wood warbler population, for example, has fallen by 98 percent, firecrest numbers by 61 percent. This is how the intensive use of timber around Lake Constance and the shorter felling intervals are making themselves felt. Trees containing nests are being felled even in protected areas, and breeding seasons are largely being ignored. Older trees are often felled for traffic safety reasons; new paths are laid in the forests and wet areas are drained. All in all, the last population count in 2010-2012 documents the same developments and causes as those that preceded it. However, the situation has clearly worsened in some cases. There is hardly any indication that things have changed for the better since then. ""The living conditions for birds around Lake Constance have in fact deteriorated further over the last seven years. This means that their numbers have presumably fallen still further in this time,"" says Bauer. More food and living space for birds With its diverse structure and location in the foothills of the Alps, the Lake Constance region actually provides excellent living conditions for birds. However, the changes it has undergone over the last few decades are typical of densely populated regions with intensive farming and forestry. ""This means that the rapid decline in the populations of many species that we have observed around Lake Constance is sure to be happening in other regions as well,"" says Bauer. The study is one of only a few long-term investigations of breeding bird populations ever conducted in Germany. In order to collect the most recent data, which dates from between 2010 and 2012, 90 volunteers joined the scientists and counted all the birds in an area of approximately 1,100 square kilometres surrounding Lake Constance. The ornithologists first recorded the bird population between 1980 and 1981 and have repeated the count every ten years ever since. The next count will take place between 2020 and 2022.- Measures that would benefit the bird populations include:    - The scientists are calling for agricultural and forestry policy to be reconsidered in order to counteract the rapid loss of biodiversity. - Drastically restricting the use of insecticides and herbicides in forestry and agriculture, in public spaces and in private gardens - Significantly reducing the use of fertilisers - Converting at least ten percent of agricultural land to ecological conservation areas - Leaving some areas of arable land and grassland uncultivated in winter and during the breeding season - Late mowing outside the grassland birds' breeding season, maintenance of flower strips and fallow areas for seed production - At least five percent of woodland should be left completely unused - Creating natural gardens using indigenous plants ",Environment,0.07994437620319231
71,Science Daily,Most-Comprehensive Analysis of Fentanyl Crisis Urges Innovative Action,Science,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829081407.htm,"   ""This crisis is different because the spread of synthetic opioids is largely driven by suppliers' decisions, not by user demand,"" said Bryce Pardo, lead author of the study and an associate policy researcher at RAND, a nonprofit research organization. ""Most people who use opioids are not asking for fentanyl and would prefer to avoid exposure."" While fentanyl had appeared in U.S. illicit drug markets before, production was limited to one or a few capable chemists, and bottlenecks in production and distribution slowed the drug's diffusion. Law enforcement was able to detect and shut down illicit manufacture to contain these outbreaks. RAND researchers found that today's synthetic opioid surge is fueled by multiple sources. Mexican drug trafficking organizations smuggle fentanyl into the U.S., and China's pharmaceutical and chemical industries are inadequately regulated, allowing producers to advertise and ship synthetic opioids to buyers anywhere in the world. While traditional criminal organizations play a role in the spread of fentanyl, the internet also has made it easier to traffic these drugs and to share information about their synthesis. Overdose deaths involving fentanyl and other synthetic opioids have increased from about 3,000 in 2013 to more than 30,000 in 2018. These deaths have remained concentrated in Appalachia, the mid-Atlantic and New England. ""While synthetic opioids have not yet become entrenched in illicit drug markets west of the Mississippi River, authorities must remain vigilant,"" said Jirka Taylor, study co-author and senior policy analyst at RAND. ""Even delaying the onset in these markets by a few years could save thousands of lives."" For U.S. policymakers, nontraditional strategies may be required to address this new challenge. The researchers avoid making specific policy recommendations, but advocate consideration of a broad array of innovative approaches such as supervised consumption sites, creative supply disruption, drug content testing, and increasing access to novel treatments that are available in other countries, such as heroin-assisted treatment. ""Indeed, it might be that the synthetic opioid problem will eventually be resolved with approaches or technologies that do not currently exist or have yet to be tested,"" said Beau Kilmer, study co-author and director of the RAND Drug Policy Research Center. ""Limiting policy responses to existing approaches will likely be insufficient and may condemn many people to early deaths."" RAND researchers say that since the diffusion of fentanyl is driven by suppliers' decisions, it makes sense to consider supply disruption as one piece of a comprehensive response, particularly where that supply is not yet firmly entrenched. But the researchers note there is little reason to believe that tougher sentences, including drug-induced homicide laws for low-level retailers and couriers, will make a difference. Instead, they call for an exploration of innovative disruption efforts that confuse or dissuade online sourcing. The study is the most comprehensive document to be published on the past, present and future of illicit synthetic opioids. RAND researchers analyzed mortality and drug seizure data, reviewed existing literature, and conducted expert interviews and international case studies. RAND researchers examined synthetic opioid markets across the U.S. and in other parts of the world, such as Estonia (where fentanyl first appeared 20 years ago). Canada's experience with synthetic opioids is most similar to that in the United States in terms of its timing, sudden increase in drug-related harms, and regional concentration. ""Problems in parts of Canada are as severe as in the Eastern United States despite substantial differences in drug policy, and the delivery of public health and social services,"" said Jonathan Caulkins, study co-author and Stever University Professor at Carnegie Mellon University. A handful of other countries in Europe also have seen synthetic opioids increasingly displace heroin. Their experience is varied and shows a range of directions some future markets in the United States may take. For instance, Sweden developed an online market with fentanyl analogs sold primarily as nasal sprays. Evidence from abroad suggests synthetic opioids may be here to stay: the study found no instance where fentanyl lost ground to another opioid after attaining a dominant position in drug markets. Funding for the study was provided by RAND Ventures, which is supported by gifts from RAND supporters and income from operations. The study: The Future of Fentanyl and Other Synthetic Opioids. ",Society,0.07988811873352637
72,Science Daily,New Model Predicts Painted Lady Butterfly Migrations Based on Breeding Sites Data,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904100743.htm,"   The new approach could be used to study potential effects of climate change in the behaviour of migratory insects Researchers from the Institute of Evolutionary Biology (IBE), a joint research institute of the Spanish National Research Council (CSIC) and Pompeu Fabra University (UPF), in Barcelona, Spain, and from the University of Grenoble-Alpes in France, have developed a method for predicting where the populations of the migratory Painted Lady butterfly (Vanessa cardui) distribute along the year and across their Europe-Africa migratory range. Their findings are published today in the journal Proceedings of the Royal Society B. In a previously published study, the researchers demonstrated that Painted Lady butterflies migrate from Europe to tropical Africa by the end of summer, crossing the Mediterranean Sea and Sahara Desert. In a follow-up study, the researchers showed that the offspring of these migrants reverse their migration towards Europe in spring. Thus, the Painted Lady butterfly travels 15,000 km between Africa and Europe through multiple generations to seasonally exploit resources and favourable climates in both continents. ""The challenge now is to understand how migratory species are able to optimize time and space as to properly find the environmental requirements that each generation need for their survival"" states Gerard Talavera, the leading author, researcher at IBE and a National Geographic Explorer. - The key is to find the caterpillars - Migratory insects are in a continuous move, and it is difficult to track from where to where they migrate. One of the main reasons for species to migrate is to find the optimal environmental conditions to raise a new generation. The immatures (eggs, caterpillars and cocoons) are key stages in the butterfly life cycle, which, unlike the adults, cannot escape from adverse situations. Thus, their breeding habitat is a very good indicator of the specific requirements that the species need to survive. The present study has gathered information of up to 646 breeding occurrences of Painted Lady butterflies in 30 countries. By using time-series of 35 years of monthly climatic data, the researchers have built a model that defines the breeding requirements of the species and produced a map of the most probable areas for the species to breed every month. ""We thought that we could learn about the movements of the adults by looking at where the caterpillars grow at different times of the year"" says Mattia Menchetti, member of the research team. ""If we can map in space and time the sites where they breed along the year, then we can understand from where to where the adults can migrate."" The species rely on their reproductive success in both continents: Africa and Europe The model shows that the species is forced to move across its overall range, since suitable breeding habitat is rarely permanent all the year. ""Because the species breeds continuously for the entire year, its reproductive success relies on both continents. The results show the relevance of the sub-Saharan winter population stock in sustaining the migrations of the species into Europe,"" says Talavera. However, the situation could eventually revert if the overall permanent suitable extent grows substantially in the future, as a consequence of global warming. ""We cannot discard that the impact of rapid climate change may affect the butterfly migratory phenomena in unpredictable ways, as has already been shown to happen in migratory birds,"" adds Talavera. The overwintering missing generations might be near the equator  Even if it has been proved that most populations of the Painted Lady butterfly spend the winters in the sub-Sahara, many of the precise localities are still unknown. Thanks to this new modelling approach, the researchers have identified the potential niche requirements of the species during the winter in Africa, and thus the sites where these could aggregate to breed. According to the results of the study, the butterflies could locate near the equatorial latitudes between December and February. This scenario confirms that the overall migratory circuit undertaken by the annual successive generations might encompass up to 15,000 km, from the equator (e.g. Kenyan and Cameroonian highlands) to northern Scandinavia. A global project  The findings published in Proceedings of the Royal Society B are part of a wider project aimed at studying the Painted Lady's migratory behaviour and routes worldwide. With that goal in mind, the team lead a long-term global citizen science project called The Worldwide Painted Lady Migration, which invites citizens from all over the word to communicate observations of the Painted Lady butterfly. ",Environment,0.07981056740223721
73,Science Daily,"Similarities in Human, Chimpanzee, and Bonobo Eye Color Patterns Revealed",Health,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904100801.htm,"   In contrast, as the sclerae of apes' eyes is often darker than human eyes, researchers have long argued that their gaze is 'cryptic', or hidden. This means that nonhuman apes would not be able to see where other members of their species are looking. Now, researchers from the National University of Singapore (NUS), together with collaborators from the University of St Andrews and Leiden University, have discovered that ape eyes possess the same pattern of colour differences as human beings. Doctoral student Mr Juan O. Perea-García and Associate Professor Antónia Monteiro from the Department of Biological Sciences at the NUS Faculty of Science suggest that this discovery may mean apes also follow each other's gaze. Their findings were published in Proceedings of the National Academy of Sciences (PNAS) on 3 September 2019. Eye-opening results The research team compared the darkness of the sclerae contrasted with irises of over 150 humans, bonobos and chimpanzees. The researchers found that bonobos, like humans, have paler sclerae and darker irises. Chimpanzees were found to have a different pattern -- with very dark sclerae, and paler irises. Both of these colour patterns show the same type of contrast seen in human eyes, and could help other apes find out where they are looking. ""Humans are unique in many ways, as no other animal can communicate with similar intricate language or build tools of such complexity. Gaze following is an important component of many behaviours that are thought to be characteristically human, so our findings suggest that apes might also engage in these behaviours,"" said Mr Perea-García. Furthering our ancestral understanding Before humans had language, our ancestors might have used the gaze of those around them to help communicate dangers or other useful information. They might not have been able to say, ""Look over there!."" However, a look in the direction of the predator might be sufficient, as long as it was possible to follow the direction of their gaze. Apart from helping us understand how our ancestors communicated, this study suggests some interesting new research directions. These include questions pertaining to why human beings and bonobos evolve in a similar way, despite bonobos being more closely related to chimpanzees. ""We know that some gorillas and orangutans have eye colouration like our own, and some members of these species have eye colouration similar to the chimpanzees, but why is there this variation within a species? We are working with several zoos to find out more,"" shared Mr Perea-García. ",Society,0.07730562260259286
74,Stanford,Traditional fire management could help revitalize American Indian cultures,Science,2019-09-05,-,https://news.stanford.edu/2019/08/27/traditional-fire-management-help-revitalize-american-indian-cultures/,"   It costs more than a new iPhone XS, and it’s made out of hazelnut shrub stems. Traditional baby baskets of Northern California’s Yurok and Karuk tribes come at a premium not only because they are handcrafted by skilled weavers, but because the stems required to make them are found only in forest understory areas experiencing a type of controlled burn once practiced by the tribes but suppressed for more than a century.  Traditional tribal fire treatments can increase production of high-quality raw materials for baskets while reducing the danger of uncontrolled wildfires. (Image credit: Tony Marks-Block)  A new Stanford-led study with the U.S. Forest Service in collaboration with the Yurok and Karuk tribes found that incorporating traditional techniques into current fire suppression practices could help revitalize American Indian cultures, economies and livelihoods, while continuing to reduce wildfire risks. The findings could inform plans to incorporate the cultural burning practices into forest management across an area one and a half times the size of Rhode Island. “Burning connects many tribal members to an ancestral practice that they know has immense ecological and social benefit especially in the aftermath of industrial timber activity and ongoing economic austerity,” said study lead author Tony Marks-Block, a doctoral candidate in anthropology who worked with Lisa Curran, the Roger and Cynthia Lang Professor in Environmental Anthrolopogy. “We must have fire in order to continue the traditions of our people,” said Margo Robbins, a Yurok basket weaver and director of the Yurok Cultural Fire Management Council who advised the researchers. “There is such a thing as good fire.” The study, published in Forest Ecology and Management, replicates Yurok and Karuk fire treatments that involve cutting and burning hazelnut shrub stems. The approach increased the production of high-quality stems (straight, unbranched and free of insect marks or bark blemishes) needed to make culturally significant items such as baby baskets and fish traps up to 10-fold compared with untreated shrubs. Reducing fuel load  Sisters Lillian Rentz and Janet Morehead of the Karuk Tribe examine recently harvested California hazel stems from a prescribed burn area. (Image credit: Frank K. Lake / U.S. Forest Service)  Previous studies have shown that repeated prescribed burning reduces fuel for wildfires, thus reducing their intensity and size in seasonally dry forests such as the one the researchers studied in the Klamath Basin area near the border with Oregon. This study was part of a larger exploration of prescribed burns being carried out by Stanford and U.S. Forest Service researchers who collaborated with the Yurok and Karuk tribes to evaluate traditional fire management treatments. Together, they worked with a consortium of federal and state agencies and nongovernmental organizations across 5,570 acres in the Klamath Basin. The consortium has proposed expanding these “cultural burns” – which have been greatly constrained throughout the tribes’ ancestral lands – across more than 1 million acres of federal and tribal lands that are currently managed with techniques including less targeted controlled burns or brush removal. Tribes traditionally burned specific plants or landscapes as a way of generating materials or spurring food production, as opposed to modern prescribed burns that are less likely to take these considerations into account. The authors argue that increasing the number of cultural burns could ease food insecurity among American Indian communities in the region. Traditional food sources have declined precipitously due in part to the suppression of prescribed burns that kill acorn-eating pests and promote deer populations by creating beneficial habitat and increasing plants’ nutritional content.  Nicholas Nix sleeps in a traditional baby basket woven out of hazelnut stems by his grandmother Margo Robbins of the Yurok Tribe. (Image credit: Margo Robbins)  “This study was founded upon tribal knowledge and cultural practices,” said co-author Frank Lake, a research ecologist with the U.S. Forest Service and a Karuk descendant with Yurok family. “Because of that, it can help us in formulating the best available science to guide fuels and fire management that demonstrate the benefit to tribal communities and society for reducing the risk of wildfires.” The researchers write that it would be easy and efficient to include traditional American Indian prescribed burning practices in existing forest management strategies. For example, federal fire managers could incorporate hazelnut shrub propane torching and pile burning into their fuel reduction plans to meet cultural needs. Managers would need to consult and collaborate with local tribes to plan these activities so that the basketry stems could be gathered post-treatment. Larger-scale pile burning treatments typically occur over a few days and require routine monitoring by forestry technicians to ensure they do not escape or harm nearby trees. As these burn, it would be easy for a technician to simultaneously use a propane torch to top-kill nearby hazelnut shrubs. This would not require a significant increase in personnel hours. Fires with a purpose “These are fires with a purpose, said Curran, who is also a senior fellow at the Stanford Woods Institute for the Environment. “Now that science has quantified and documented the effectiveness of these practices, fire managers and scientists have the information they need to collaborate with tribes to implement them on a large scale.” Marks-Block will teach a course at Stanford this fall on the socio-ecology of fire. It will include field trips to a prescribed fire site in the Santa Cruz mountains and tribal prescribed fire training exchanges in the Klamath Basin area of Northern California. The research was funded by the National Science Foundation, the U.S. Joint Fire Science Program, Stanford’s Department of Anthropology, the Stanford Office of the Vice Provost for Graduate Education’s Diversity Dissertation Research Opportunity and the Stanford School of Humanities and Sciences Community Engagement grant. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Environment,0.07711251147973708
75,Science Daily,New Whale Species Discovered Along the Coast of Hokkaido,Environment,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903113335.htm,"   In a collaboration between the National Museum of Nature and Science, Hokkaido University, Iwate University, and the United States National Museum of Natural History, a beaked whale species which has long been called Kurotsuchikujira (black Baird's beaked whale) by local Hokkaido whalers has been confirmed as the new cetacean species Berardius minimus (B. minimus). Beaked whales prefer deep ocean waters and have a long diving capacity, making them hard to see and inadequately understood. The Stranding Network Hokkaido, a research group founded and managed by Professor Takashi F. Matsuishi of Hokkaido University, collected six stranded un-identified beaked whales along the coasts of the Okhotsk Sea. The whales shared characteristics of B. bairdii (Baird's beaked whale) and were classified as belonging to the same genus Berardius. However, a number of distinguishable external characteristics, such as body proportions and color, led the researchers to investigate whether these beaked whales belong to a currently unclassified species. ""Just by looking at them, we could tell that they have a remarkably smaller body size, more spindle-shaped body, a shorter beak, and darker color compared to known Berardius species,"" explained Curator Emeritus Tadasu K. Yamada of the National Museum of Nature and Science from the research team. In the current study, the specimens of this unknown species were studied in terms of their morphology, osteology, and molecular phylogeny. The results, published in the journal Scientific Reports, showed that the body length of physically mature individuals is distinctively smaller than B. bairdii (6.2-6.9m versus 10.0m). Detailed cranial measurements and DNA analyses further emphasized the significant difference from the other two known species in the genus Berardius. Due to it having the smallest body size in the genus, the researchers named the new species B. minimus. ""There are still many things we don't know about B. minimus,"" said Takashi F. Matsuishi. ""We still don't know what adult females look like, and there are still many questions related to species distribution, for example. We hope to continue expanding what we know about B. minimus."" Local Hokkaido whalers also refer to some whales in the region as Karasu (crow). It is still unclear whether B. minimus (or Kurotsuchikujira) and Karasu are the same species or not, and the research team speculate that it is possible Karasu could be yet another different species. This study was conducted in collaboration with multiple institutions. Dr. Shino Kitamura and Dr. Shuichi Abe of Iwate University carried out the DNA analyses while Dr. Tadasu K. Yamada and Dr. Yuko Tajima of the National Museum of Nature and Science made osteological specimens, morphological observations and detailed measurements to depict systematic uniqueness. Dr. Takashi F. Matsuishi and Dr. Ayaka Matsuda of Hokkaido University made the multivariate analyses. Dr. James G. Mead of Smithsonian Institution contributed to discussions related to systematic comparison. ",Environment,0.07687435522885254
76,Science Daily,Clues to Early Social Structures May Be Found in Ancient Extraordinary Graves,Science,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828143053.htm,"   As early farming communities gave rise to larger, more complex sedentary societies, new social hierarchies arose, presenting opportunities for individual people to achieve positions of importance. The authors cite two archetypal ""pathways to power"" such individuals might follow: one self-aggrandizing and often autocratic, and the other more group-oriented and egalitarian. But how these ""pathways"" were expressed in early cultures remains unclear. This study focused on a single burial in the Ba'ja settlement of southern Jordan, dating between 7,500-6,900 BC, during the Late Pre-Pottery B Period. The elaborate construction of this grave and sophistication of associated symbolic objects suggest the deceased was a person of importance in the ancient society. The authors suggest that the presence of exotic items in the grave indicate a person who achieved individual prestige by access to trade networks, while the proximity of the grave to other less elaborate graves indicates that they were nonetheless considered close in status to the broader community, not neatly fitting either archetype of a powerful individual. The authors propose that this sort of data can provide insights into cultural views toward leadership and social hierarchy in early cultures. They also suggest that further investigations of this body and others in Ba'ja, including ancient DNA analysis to illuminate familial relationships, may combine with grave information to create a more refined picture of early community social structures. The authors add: ""We suggest that leadership can be understood only by studying the social contexts and the pathways to power (not only the burials of extraordinary individuals). In fact, studying rich tombs to interpret social structures has been done before, but our new approach emphasizes the social environments of leadership. The key study of the elaborate burial of the late PPNB site of Ba'ja lets us surmise that access to leadership was possible through corporate leadership-type of primus inter pares than by autocratic coercive power."" ",Society,0.07578316244423125
77,MIT News,MIT report examines how to make technology work for society,Computer Science,2019-09-04,-,http://news.mit.edu/2019/work-future-report-technology-jobs-society-0904,"  Automation is not likely to eliminate millions of jobs any time soon — but the U.S. still needs vastly improved policies if Americans are to build better careers and share prosperity as technological changes occur, according to a new MIT report about the workplace. The report, which represents the initial findings of MIT’s Task Force on the Work of the Future, punctures some conventional wisdom and builds a nuanced picture of the evolution of technology and jobs, the subject of much fraught public discussion. The likelihood of robots, automation, and artificial intelligence (AI) wiping out huge sectors of the workforce in the near future is exaggerated, the task force concludes — but there is reason for concern about the impact of new technology on the labor market. In recent decades, technology has contributed to the polarization of employment, disproportionately helping high-skilled professionals while reducing opportunities for many other workers, and new technologies could exacerbate this trend. Moreover, the report emphasizes, at a time of historic income inequality, a critical challenge is not necessarily a lack of jobs, but the low quality of many jobs and the resulting lack of viable careers for many people, particularly workers without college degrees. With this in mind, the work of the future can be shaped beneficially by new policies, renewed support for labor, and reformed institutions, not just new technologies. Broadly, the task force concludes, capitalism in the U.S. must address the interests of workers as well as shareholders. “At MIT, we are inspired by the idea that technology can be a force for good. But if as a nation we want to make sure that today’s new technologies evolve in ways that help build a healthier, more equitable society, we need to move quickly to develop and implement strong, enlightened policy responses,” says MIT President L. Rafael Reif, who called for the creation of the Task Force on the Work of the Future in 2017. “Fortunately, the harsh societal consequences that concern us all are not inevitable,” Reif adds. “Technologies embody the values of those who make them, and the policies we build around them can profoundly shape their impact. Whether the outcome is inclusive or exclusive, fair or laissez-faire, is therefore up to all of us. I am deeply grateful to the task force members for their latest findings and their ongoing efforts to pave an upward path.” “There is a lot of alarmist rhetoric about how the robots are coming,” adds Elisabeth Beck Reynolds, executive director of the task force, as well as executive director of the MIT Industrial Performance Center. “MIT’s job is to cut through some of this hype and bring some perspective to this discussion.” Reynolds also calls the task force’s interest in new policy directions “classically American in its willingness to consider innovation and experimentation.” Anxiety and inequality The core of the task force consists of a group of MIT scholars. Its research has drawn upon new data, expert knowledge of many technology sectors, and a close analysis of both technology-centered firms and economic data spanning the postwar era. The report addresses several workplace complexities. Unemployment in the U.S. is low, yet workers have considerable anxiety, from multiple sources. One is technology: A 2018 survey by the Pew Research Center found that 65 to 90 percent of respondents in industrialized countries think computers and robots will take over many jobs done by humans, while less than a third think better-paying jobs will result from these technologies. Another concern for workers is income stagnation: Adjusted for inflation, 92 percent of Americans born in 1940 earned more money than their parents, but only about half of people born in 1980 can say that. “The persistent growth in the quantity of jobs has not been matched by an equivalent growth in job quality,” the task force report states. Applications of technology have fed inequality in recent decades. High-tech innovations have displaced “middle-skilled” workers who perform routine tasks, from office assistants to assembly-line workers, but these innovations have complemented the activities of many white-collar workers in medicine, science and engineering, finance, and other fields. Technology has also not displaced lower-skilled service workers, leading to a polarized workforce. Higher-skill and lower-skill jobs have grown, middle-skill jobs have shrunk, and increased earnings have been concentrated among white-collar workers. “Technological advances did deliver productivity growth over the last four decades,” the report states. “But productivity growth did not translate into shared prosperity.” Indeed, says David Autor, who is the Ford Professor of Economics at MIT, associate head of MIT’s Department of Economics, and a co-chair of the task force, “We think people are pessimistic because they’re on to something. Although there’s no shortage of jobs, the gains have been so unequally distributed that most people have not benefited much. If the next four decades of automation are going to look like the last four decades, people have reason to worry.” Productive innovations versus “so-so technology” A big question, then, is what the next decades of automation have in store. As the report explains, some technological innovations are broadly productive, while others are merely “so-so technologies” — a term coined by economists Daron Acemoglu of MIT and Pascual Restrepo of Boston University to describe technologies that replace workers without markedly improving services or increasing productivity. For instance, electricity and light bulbs were broadly productive, allowing the expansion of other types of work. But automated technology allowing for self-check-out at pharmacies or supermarkets merely replaces workers without notably increasing efficiency for the customer or productivity. “That’s a strong labor-displacing technology, but it has very modest productivity value,” Autor says of these automated systems. “That’s a ‘so-so technology.’ The digital era has had fabulous technologies for skill complementarity [for white-collar workers], but so-so technologies for everybody else. Not all innovations that raise productivity displace workers, and not all innovations that displace workers do much for productivity.” Several forces have contributed to this skew, according to the report. “Computers and the internet enabled a digitalization of work that made highly educated workers more productive and made less-educated workers easier to replace with machinery,” the authors write. Given the mixed record of the last four decades, does the advent of robotics and AI herald a brighter future, or a darker one? The task force suggests the answer depends on how humans shape that future. New and emerging technologies will raise aggregate economic output and boost wealth, and offer people the potential for higher living standards, better working conditions, greater economic security, and improved health and longevity. But whether society realizes this potential, the report notes, depends critically on the institutions that transform aggregate wealth into greater shared prosperity instead of rising inequality. One thing the task force does not foresee is a future where human expertise, judgment, and creativity are less essential than they are today.   “Recent history shows that key advances in workplace robotics — those that radically increase productivity — depend on breakthroughs in work design that often take years or even decades to achieve,” the report states. As robots gain flexibility and situational adaptability, they will certainly take over a larger set of tasks in warehouses, hospitals, and retail stores — such as lifting, stocking, transporting, cleaning, as well as awkward physical tasks that require picking, harvesting, stooping, or crouching. The task force members believe such advances in robotics will displace relatively low-paid human tasks and boost the productivity of workers, whose attention will be freed to focus on higher-value-added work. The pace at which these tasks are delegated to machines will be hastened by slowing growth, tight labor markets, and the rapid aging of workforces in most industrialized countries, including the U.S. And while machine learning — image classification, real-time analytics, data forecasting, and more — has improved, it may just alter jobs, not eliminate them: Radiologists do much more than interpret X-rays, for instance. The task force also observes that developers of autonomous vehicles, another hot media topic, have been “ratcheting back” their timelines and ambitions over the last year. “The recent reset of expectations on driverless cars is a leading indicator for other types of AI-enabled systems as well,” says David A. Mindell, co-chair of the task force, professor of aeronautics and astronautics, and the Dibner Professor of the History of Engineering and Manufacturing at MIT. “These technologies hold great promise, but it takes time to understand the optimal combination of people and machines. And the timing of adoption is crucial for understanding the impact on workers.” Policy proposals for the future Still, if the worst-case scenario of a “job apocalypse” is unlikely, the continued deployment of so-so technologies could make the future of work worse for many people. If people are worried that technologies could limit opportunity, social mobility, and shared prosperity, the report states, “Economic history confirms that this sentiment is neither ill-informed nor misguided. There is ample reason for concern about whether technological advances will improve or erode employment and earnings prospects for the bulk of the workforce.” At the same time, the task force report finds reason for “tempered optimism,” asserting that better policies can significantly improve tomorrow’s work. “Technology is a human product,” Mindell says. “We shape technological change through our choices of investments, incentives, cultural values, and political objectives.” To this end, the task force focuses on a few key policy areas. One is renewed investment in postsecondary workforce education outside of the four-year college system — and not just in the STEM skills (science, technology, engineering, math) but reading, writing, and the “social skills” of teamwork and judgment. Community colleges are the biggest training providers in the country, with 12 million for-credit and non-credit students, and are a natural location for bolstering workforce education. A wide range of new models for gaining educational credentials is also emerging, the task force notes. The report also emphasizes the value of multiple types of on-the-job training programs for workers. However, the report cautions, investments in education may be necessary but not sufficient for workers: “Hoping that ‘if we skill them, jobs will come,’ is an inadequate foundation for constructing a more productive and economically secure labor market.” More broadly, therefore, the report argues that the interests of capital and labor need to be rebalanced. The U.S., it notes, “is unique among market economies in venerating pure shareholder capitalism,” even though workers and communities are business stakeholders too. “Within this paradigm [of pure shareholder capitalism], the personal, social, and public costs of layoffs and plant closings should not play a critical role in firm decision-making,” the report states. The task force recommends greater recognition of workers as stakeholders in corporate decision making. Redressing the decades-long erosion of worker bargaining power will require new institutions that bend the arc of innovation toward making workers more productive rather than less necessary. The report holds that the adversarial system of collective bargaining, enshrined in U.S. labor law adopted during the Great Depression, is overdue for reform. The U.S. tax code can be altered to help workers as well. Right now, it favors investments in capital rather than labor — for instance, capital depreciation can be written off, and R&D investment receives a tax credit, whereas investments in workers produce no such equivalent benefits. The task force recommends new tax policy that would also incentivize investments in human capital, through training programs, for instance. Additionally, the task force recommends restoring support for R&D to past levels and rebuilding U.S. leadership in the development of new AI-related technologies, “not merely to win but to lead innovation in directions that will benefit the nation: complementing workers, boosting productivity, and strengthening the economic foundation for shared prosperity.” Ultimately the task force’s goal is to encourage investment in technologies that improve productivity, and to ensure that workers share in the prosperity that could result. “There’s no question technological progress that raises productivity creates opportunity,” Autor says. “It expands the set of possibilities that you can realize. But it doesn’t guarantee that you will make good choices.” Reynolds adds: “The question for firms going forward is: How are they going to improve their productivity in ways that can lead to greater quality and efficiency, and aren’t just about cutting costs and bringing in marginally better technology?” Further research and analyses In addition to Reynolds, Autor, and Mindell, the central group within MIT’s Task Force on the Work of the Future consists of 18 MIT professors representing all five Institute schools. Additionally, the project has a 22-person advisory board drawn from the ranks of industry leaders, former government officials, and academia; a 14-person research board of scholars; and eight graduate students. The task force also counsulted with business executives, labor leaders, and community college leaders, among others. The task force follows other influential MIT projects such as the Commission on Industrial Productivity, an intensive multiyear study of U.S. industry in the 1980s. That effort resulted in the widely read book, “Made in America,” as well as the creation of MIT’s Industrial Performance Center. The current task force taps into MIT’s depth of knowledge across a full range of technologies, as well as its strengths in the social sciences. “MIT is engaged in developing frontier technology,” Reynolds says. “Not necessarily what will be introduced tomorrow, but five, 10, or 25 years from now. We do see what’s on the horizon, and our researchers want to bring realism and context to the public discourse.” The current report is an interim finding from the task force; the group plans to conduct additional research over the next year, and then will issue a final version of the report. “What we’re trying to do with this work,” Reynolds concludes, “is to provide a holistic perspective, which is not just about the labor market and not just about technology, but brings it all together, for a more rational and productive discussion in the public sphere.” ",Society,0.07085576692952296
78,MIT News,"A summer at the MSRP-Bio reveals connections between proteins, people, and passions",Research,2019-09-03,-,http://news.mit.edu/2019/summer-mit-reveals-connections-between-proteins-people-passions-0903,"  Meucci Ilunga seems to know something about everything. He’s a videographer who’s branching out into podcasting. He’s researched cancer therapies and volunteered in a hospital. He grew up on a Navajo reservation, and he’s a year away from completing a biochemistry degree at the University of Arizona. “I’m excited about life in general,” he says. At the moment, though, he’s especially excited about a cellular conundrum that he investigated during the 10-week internship in the MIT Department of Biology that he completed as part of the MIT Summer Research Program in Biology (MSRP-Bio). “Your cells are really, really complicated,” he says. “They’re packed with lots of different kinds of proteins. Yet when you look at how proteins interact, they’re specific.” How do proteins find the appropriate binding partners amongst all the noise? Ilunga and his MSRP-Bio supervisor, biology and biological engineering Professor Amy Keating, think that short sequences of amino acids — the units that comprise proteins — can mediate binding interactions more intricate than researchers had previously appreciated. Just as proteins home in on their binding partners, Ilunga has always been drawn to science. As a kid, he told everyone he wanted to be an astrophysicist. “I had no idea what that meant,” he says, “but I loved the idea of exploring the unknown and being able to generate knowledge.” Ilunga grew up on the Navajo reservation in Kinlichee, Arizona, however, and he didn’t have the same opportunities to engage in science as kids in urban centers. “Only about 60 percent of people on the reservation have running water and electricity,” he says, “so most people are pressed with more urgent matters than following their curiosities.” Ilunga notes the myriad of difficulties his reservation faces, from prevalent diabetes to corrupt politicians and poor school systems, but says that the hardest part about being Navajo is feeling like his people’s problems are invisible to those outside the tribe. “A lot of us feel very forgotten about,” he says. Ilunga quickly exhausted the opportunities that his high school in Fort Defiance, Arizona, had to offer, leading him to graduate early and leave for the University of Arizona at age 16. But he was determined to remember his roots. Balancing his love of science with his connection to the reservation — and finding a career that will let him return — has proven challenging. “You can become an engineer, but there are no engineering jobs on the reservation. You can become a computer scientist, but there are no computer science jobs,” he says. So he decided to pursue biochemistry, as it would lay the foundation for medical school, and the reservation is always in need of doctors. At his university, Ilunga started shadowing physicians and volunteering in a hospital. His path to medical school seemed clear. There was only one problem: He found medicine unfulfilling. “There’s so much more I could be doing. So I started looking at what else I could do to get back home,” he says. This desire for balance is what made Ilunga choose to join the MSRP-Bio program, for which he received sponsorship from the Gould Fund. Ilunga met the MSRP-Bio coordinator, Mandana Sassanfar, at a conference for minority students, and she told him that MSRP-Bio promotes a balance between lab work and life. “What sold me on this program is that it understands that I’m more than just a scientist,” he says. Over the summer, Ilunga has spoken with many MIT professors about the diverse professional paths scientists can take, and these conversations have inspired him to consider a career in policy. “I could be someone who goes to Congress to fight — not only for Native American affairs, but also for scientific affairs,” he says. Ilunga plans to pursue a PhD in life sciences in preparation for this career, possibly studying protein interactions like the ones he’s been working on all summer. He finds research most interesting when it has a clear clinical application, and understanding protein interactions lets researchers design drugs that disrupt them. The protein interactions that Ilunga researched are mediated by sequences called short linear motifs, or SLiMs, which consist of contiguous stretches of only three to 10 amino acids — a small subset of the hundreds of amino acids that make up the typical protein. While larger domains are able to form tighter and more sustained interactions, SLiMs mediate weaker, transient interactions. SLiMs make up in speed what they lack in strength. Allowing proteins to quickly bind and release each other is beneficial for some biological processes, and SLiMs can also evolve rapidly and let organisms adapt to change quickly. Researchers think this is why SLiMs have persisted in many different organisms over the course of evolution, despite being relatively unintuitive tools for forming protein complexes. The Keating lab noticed that sometimes proteins that contain SLiMs recognize their binding partners with a specificity that’s unexpected, given that so many proteins contain these short sequences. Ilunga spent his summer looking into how small domains and short sequences can play a large role in protein pairing. His weeks began with culturing large quantities of bacteria that were used to produce SLiM-containing peptides; then he isolated these peptides and used a technique called biolayer interferometry to determine how tweaking their amino acid sequences affected how strongly they bound their target protein. When he altered the amino acid sequence directly adjacent to the SLiMs, Ilunga found that the strength of their binding interactions could vary quite wildly. The Keating lab doesn’t understand how this occurs, and Ilunga’s findings pave the way for testing different biochemical mechanisms to explain this phenomenon. When he wasn’t isolating proteins or chatting with the MIT faculty, Ilunga got to know the MIT community. “At a lot of top schools there’s a sense of prestige that fills the air, but it wasn’t like that at MIT. Everyone here is so humble,” he says. He especially enjoyed getting to know his fellow MSRP-Bio students. Whether they were going on a boat cruise along the Charles River or helping each other troubleshoot lab work, he says it was an amazing group of people to spend the summer with. As he heads back to the University of Arizona, Ilunga is taking many technical skills back with him, as well as a new outlook on life. He has always been hopeful that life will get easier for Navajos and other minorities. Now he’s confident that the medical and technological advances that institutions like MIT are creating can improve living conditions for people like his family back on the reservation. “I used to think my optimism was blind,” he says. “Now I think my optimism is informed.” ",Health,0.06732961463981946
79,Stanford,Poverty as a disease trap,Science,2019-09-05,-,https://news.stanford.edu/2019/09/03/poverty-disease-trap/,"   No drug can cure a paradox. That basic truth is at the heart of a new Stanford-led study highlighting how poverty traps make it impossible to eradicate a potentially deadly disease with current approaches.  A woman bathes in the Senegal River, while her children play nearby. Common livelihoods, daily chores, hygiene practices and children’s play depend on the waterway, where residents are repeatedly exposed to reinfection by the parasitic worms that cause schistosomiasis. (Image credit: Andrea Lund)  The study, published in the American Journal of Tropical Medicine and Hygiene, looks at why years of mass drug administration in Senegal have failed to dramatically alter infection rates of schistosomiasis, a parasitic disease that lurks in waterborne snails and affects more than 200 million people worldwide. It finds that neither drugs nor people’s relatively sophisticated understanding of disease risks can overcome the inevitable exposure caused by imperatives of subsistence living. The researchers call for greater focus on the role of socio-economic and environmental systems, and engaging communities in the design of disease control programs. “The field of tropical medicine has focused primarily on mass drug administration programs,” said lead author Andrea Lund, a PhD student in the Emmett Interdisciplinary Program in Environment and Resources within Stanford’s School of Earth, Energy & Environmental Sciences. “These have worked in many places, but there are persistent hot spots where you need to come at the problem from social and environmental angles too.” Although charity evaluation services consistently rank mass drug administration programs among the most effective developing world public health interventions, the efforts often fail to eradicate disease in the long run. That’s because they don’t address the root causes that lead to reinfection time after time, according to Lund. Obstacles to a cure Schistosomiasis is a disease caused by a parasitic worm and transmitted to humans by freshwater snails that serve as the parasite’s intermediate host. The disease is widespread across tropical latitudes, with the vast majority of cases in sub-Saharan Africa. The snails release infective larvae into freshwater, where they burrow into people’s skin. Symptoms range from abdominal pain and diarrhea to infertility, permanent organ damage and bladder cancer. Chronic schistosomiasis can affect cognitive development and labor productivity, according to some studies. Nearly 40 years after being introduced, praziquantel – a drug used to clear schistosome parasites from people – has yet to make a dent in the global burden of the disease. That’s because treated people often re-enter contaminated water, repeatedly exposing themselves to reinfection. Lund is part of a team that has been trying to understand the obstacles to a cure and ways around them. Led by Stanford disease ecologist Susanne Sokolow and biologist Giulio De Leo (both co-authors on the study), the group has shown that ecological tactics aimed at controlling schistosomiasis are the most effective way to reduce the disease’s prevalence. The team received early funding from the Stanford Woods Institute for the Environment for a project to reintroduce native snail-eating prawns to local water sources, and has since established the Program for Disease Ecology, Health and Environment at Stanford with a grant from the Stanford Institute for Innovation in Developing Economies. The program, supported by Woods and the Stanford Center for Innovation in Global Health, focuses on finding sustainable ecological solutions to a range of diseases. Sophisticated understanding For the study, Lund and her colleagues surveyed residents of villages along the Senegal River, a region with persistently high rates of schistosomiasis despite yearly school-based mass administration of praziquantel since 1999. People explained how life in their rural, resource-poor area is inextricably intertwined with the river. Common livelihoods, such as agriculture and fishing, depend on contact with the waterway. So do chores, such as washing clothes, and hygiene practices, such as bathing and children’s play. A 53-year-old man from one riverside village who spoke with one of the researchers summed up the catch-22: “That water, we cannot touch it. We cannot abandon it. If we abandon it, we will all become unemployed.”   “Investment in environmental solutions … may be the only way to reduce the risk of schistosomiasis in settings where the disease burden remains high even in the presence of treatment programs.” —Andrea Lund PhD Student, Emmett Interdisciplinary Program in Environment and Resources   “There is a feeling of inevitability around schistosomiasis infection, given the constraints of poverty,” said Sokolow, a senior research scientist at Woods. “That jibes with the experience of the many years of efforts to distribute pills and carry out educational campaigns in the region without a huge drop in schisto transmission or infection. It’s the quintessential wicked problem.” Residents expressed a relatively sophisticated knowledge about the environmental nature of schistosomiasis, including the fact that infection risks increase at midday – an observation borne out by the tendency of snails to release free-swimming parasite larvae into the water at the same time of day. With this knowledge, some residents had developed personal strategies or village-wide policies – enforceable by fines – to minimize exposure by avoiding the river at certain times. Good leadership and community engagement were among the strongest indicators of success in overcoming these obstacles. This capacity to organize suggests that communities could take the lead in implementing environmental and social interventions – ranging from prawn re-introduction to the construction and maintenance of water and sanitation facilities or behavior change programs. This would ensure interventions are locally acceptable and can be sustained over time. This type of engagement with communities could reduce the amount of parasite transmission in the environment and improve outcomes of mass drug administration in areas where they have had limited success, according to the researchers. “Ultimately, I see these findings making a case for further investment in environmental solutions – such as prawn re-introduction,” Lund said. “This may be the only way to reduce the risk of schistosomiasis in settings where the disease burden remains high even in the presence of treatment programs.” Sokolow is also a senior fellow at the Stanford Center for Innovation in Global Health. De Leo is a professor of biology in Stanford’s School of Humanities and Sciences and a senior fellow at the Stanford Woods Institute for the Environment and the Stanford Center for Innovation in Global Health. Co-authors of the study include Omar Sow, a former research assistant in computer science; Sofia Ali, a former undergraduate; Sylvia Bereknyei Merrell, a research scholar; Janine Bruce, a senior research scholar; and researchers at the Centre de Recherche Biomédicale Espoir Pour La Santé in Senegal, the Station d’Innovation Aquacole in Senegal and the University of California, Santa Barbara. The research was funded by the National Science Foundation; Stanford’s School of Earth, Energy & Environmental Sciences; Stanford’s Emmett Interdisciplinary Program in Environment and Resources (E-IPER); the Stanford Interdisciplinary Graduate Fellowship; the National Institutes of Health; the Bill and Melinda Gates Foundation; and the Freeman Spogli Institute for International Studies. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Environment,0.06645260618300755
80,MIT News,A comprehensive catalogue of human digestive tract bacteria,Research,2019-09-02,-,http://news.mit.edu/2019/catalogue-human-digestive-gut-bacteria-0902,"  The human digestive tract is home to thousands of different strains of bacteria. Many of these are beneficial, while others contribute to health problems such as inflammatory bowel disease. Researchers from MIT and the Broad Institute have now isolated and preserved samples of nearly 8,000 of these strains, while also clarifying their genetic and metabolic context. This data set (BIO-ML), which is available to other researchers who want to use it, should help to shed light on the dynamics of microbial populations in the human gut and may help scientists develop new treatments for a variety of diseases, says Eric Alm, director of MIT’s Center for Microbiome Informatics and Therapeutics and a professor of biological engineering and of civil and environmental engineering at MIT. “There’s a lot of excitement in the microbiome field because there are associations between these bacteria and health and disease. But we’re lacking in being able to understand why that is, what’s the mechanism, and what are the functions of those bacteria that are causing them to associate with disease,” says Alm, who is the senior author of the study. The researchers collected stool samples from about 90 people, for up to two years, allowing them to gain insight into how microbial populations change over time within individuals. This study focused on people living in the Boston area, but the research team is now gathering a larger diversity of samples from around the globe, in hopes of preserving microbial strains not found in people living in industrialized societies. “More than ever before, modern techniques allow us to isolate previously uncultured human gut bacteria. Exploring this genetic and functional diversity is fascinating — everywhere we look, we discover new things. I’m convinced that enriching biobanks with a large diversity of strains from individuals living diverse lifestyles is essential for future advancements in human microbiome research,” says Mathilde Poyet, a senior postdoc at MIT and one of the lead authors of the study. MIT research associate Mathieu Groussin and former postdoc Sean Gibbons are also lead authors of the study, which appears in the Sept. 2 issue of Nature Medicine. Ramnik Xavier, a professor of medicine at Harvard Medical School and member of the Broad Institute, is a senior author of the study along with Alm. Microbiome dynamics Humans have trillions of bacterial cells in their digestive tracts, and while scientists believe that these populations change and evolve over time, there has been little opportunity to observe this. Through the OpenBiome organization, which collects stool samples for research and therapeutic purposes, Alm and his colleagues at MIT and the Broad Institute had access to fecal samples from about 90 people. For most of their analysis, the researchers focused on microbes found in about a dozen individuals who had provided samples over an extended period, up to two years. “That was a unique opportunity, and we thought that would be a great set of individuals to really try to dig down and characterize the microbial populations more thoroughly,” Alm says. “To date there hadn’t been a ton of longitudinal studies, and we wanted to make that a key focus of our study, so we could understand what the variation is day-to-day.” The researchers were able to isolate a total of 7,758 strains from the six major phyla of bacteria that dominate the human GI tract. For 3,632 of these strains, the researchers sequenced their full genomes, and they also sequenced partial genomes of the remaining strains. Analyzing how microbial populations changed over time within single hosts allowed the researchers to discover some novel interactions between strains. In one case, the researchers found three related strains of Bacteroides vulgatus coexisting within a host, all of which appeared to have diverged from one ancestor strain within the host. In another case, one strain of Turicibacter sanguinis completely replaced a related strain of the same species nearly overnight. “This is the first time we’re getting a glimpse of these really different dynamics,” Alm says. Population variation The researchers also measured the quantities of many metabolites found in the stool samples. This analysis revealed that variations in amino acid levels were closely linked with changes in microbial populations over time within a single person. However, differences between the composition of microbial populations in different people were more closely associated with varying levels of bile acids, which help with digestion. The researchers don’t know exactly what produces these differences in amino acid and bile acid levels, but say they could be influenced by diet — a connection that they hope to investigate in future studies. They have also made all of their data available online and are offering samples of the strains of bacteria they isolated, allowing other scientists to study the functions of these strains and their potential roles in human health. “Comprehensive and high-resolution collections of bacterial isolates open the possibility to mechanistically investigate how our lifestyle shapes our gut microbiome, metabolism, and inflammation. We aim to provide such a resource to the research community worldwide, including to lower-income research institutions,” Groussin says. The researchers have also begun a larger-scale project to collect microbiome samples from a greater diversity of populations around the world. They are especially focusing on underrepresented populations who live in nonindustrialized societies, as their diet and microbiomes are expected to be very different from those of people living in industrialized societies. “It may be that as populations that have been living traditional lifestyles start to switch to a more industrialized lifestyle, they may lose a lot of that biodiversity. So one of the main things we want to do is conserve it, and then later we can go back and characterize it as well,” Alm says. The research was funded by a Broad Next 10 grant from the Broad Institute. ",Health,0.059056611143695426
81,Science Daily,"Millennials, Think You're Digitally Better Than Us? Yes, According to Science",Society,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828092457.htm,"   Legend has it that millennials, specifically the ""Net Generation,"" use many technologies simultaneously, masterfully switching from one to the next. They claim that it's easy and that they can do it much better than older generations. Research, so far, hasn't proven this claim and the consequences of these incessant interruptions on attention and performance. Florida Atlantic University researchers in the Charles E. Schmidt College of Science are one of the first to examine this phenomenon in college-age students. The study provides some of the first results on whether or not ""Net Genners,"" who have grown up with widespread access to technology, are developing greater digital literacy than generations before them, and if this has enriched them with an ability to switch their attention more efficiently. For the study, researchers simulated a typical working environment, complete with IT interruptions, to allow them to track the effects on participants' inhibitory processes. One hundred and seventy-seven mostly college-age participants were divided into three groups: those who received IT interruptions; those who did not, and a control group. Researchers compared the three groups' accuracy and response time on completing tasks, gauging their level of anxiety. Results, published in the journal Applied Neuropsychology: Adult, indicate that there is no need to ""pardon these interruptions,"" at least for this younger generation. Findings show that switching between technologies did not deplete or diminish performance in the group that had the IT interruptions compared to the control group or the group that did not receive IT interruptions. Unexpectedly, however, researchers discovered diminished performance in the participants from the group that did not receive any IT interruptions. All three groups reported low levels of anxiety during the study. Seventy-five percent of two of the groups reported their anxiety as ""not at all"" or ""a little bit,"" and the researchers did not find any significant differences between groups. ""We were really surprised to find impaired performance in the group that did not receive any information technology interruptions. It appears that the Net Generation thrives on switching their attention and they can do it more efficiently because information technology is woven throughout their daily lives,"" said Mónica Rosselli, Ph.D., senior author, professor and assistant chair of psychology in FAU's Charles E. Schmidt College of Science, and a member of the FAU Brain Institute (I-BRAIN), one of the University's four research pillars. ""Because younger generations are so accustomed to using instant messaging, pop-ups like the ones we used for our study, may blend into the background and may not appear surprising or unplanned, and therefore may not produce anxiety."" Prior research in the general population has found that it takes about 25 minutes to return to an original task following an IT interruption and 41 percent of these interruptions result in discontinuing the interrupted task altogether. Emails alone cause about 96 interruptions in an eight-hour day with an added one-and-a-half hours of recovery time per day. Results of the new FAU study sheds light on younger generations who have commonly used instant messaging as a major communication tool and this communication preference may reveal a perception gap between generations. ""How we adapt to technology and leverage it to our advantage by deciding what information we attend to at any given moment has substantial implications on our ability to remain valuable and productive in our respective work and education domains,"" said Deven M. Christopher, co-author and a graduate psychology student at FAU. ""Results from our study may provide a basis for further research, especially because younger generations are developing in a more connected world than preceding generations."" ",Society,0.05595715801691694
82,Science Daily,Some Vaccine Doubters May Be Swayed by Proximity to Disease Outbreak,Science,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828143108.htm,"   In both the US and globally, there is growing vaccine hesitancy, which can manifest itself in increased non-medical exemption rates, decreased vaccination rates and increased outbreaks of vaccine-preventable diseases. The formation of attitudes about vaccination is complex and linked to many factors including media and peer group influence, distrust of science, information access, and socio-economic barriers. In the new study, researchers surveyed 1,006 online respondents across the United States about their political beliefs, vaccination attitudes and demographics. The survey was carried out in January 2017, following local outbreaks of measles in 2016. The respondent pool was generated by a market research firm to be a nationally representative sample of the U.S. voting age population and the final sample matched known population in terms of gender, age, income race and Census region. The researchers found that an individual's proximity to a measles outbreak independent had no independent effect on measles vaccination attitudes (p = 0.43). However, they found that trust in government medical experts is strongly and positively related to vaccination attitudes (p=0.01). Moreover, the study uncovered an interactive relationship between the two variables. People who are skeptical of the CDC and similar institutions and live farther away from a disease outbreak harbor less favorable vaccination views than those who are skeptical but live in close proximity to an outbreak. People who have high levels of trust are not affected by disease proximity. The research therefore suggests that, unlike people who trust government experts, people who are skeptical of the CDC and similar institutions may consider whether or not a given disease occurs nearby when making decisions about vaccination. Justwan adds: ""In this paper, we explore whether people's vaccination attitudes with regards to measles are shaped by how far away they live from a recent outbreak. We find that this is the case -- but only for individuals who also distrust government medical experts. Put differently: citizens who are skeptical of the CDC and similar institutions base their vaccination decision-making to some degree on whether or not a given disease occurs in close vicinity to their community."" ",Society,0.05429499749049888
83,Science Daily,Benefits of Cognitive Behavioral Therapy for IBS Continue 2 Years After Treatment,Health,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904102629.htm,"   Previous research (the ACTIB trial) led by Professor Hazel Everitt at the University of Southampton in collaboration with researchers at King's College London, showed that that Cognitive Behavioural Therapy (CBT) tailored specifically for IBS and delivered over the telephone or through an interactive website is more effective in relieving the symptoms of IBS than current standard care one year after treatment. This 24 month follow up research published in Lancet Gastroenterology and Hepatology this week has shown that benefits continue two years after treatment despite patients having no further therapy after the initial CBT course. These results are important as previously there was uncertainty whether the initial benefits could be sustained in the long term. Currently there is limited availability of CBT for IBS in a resource constrained NHS but this research indicates that easily accessible treatment could be provided to a large number of patients and provide them with effective, long-term relief. Professor Everitt added: ""the fact that both telephone and web based CBT sessions were shown to be effective treatments is a really important and exciting discovery. Patients are able to undertake these treatments at a time convenient to them, without having to travel to clinics and we now know that the benefits can last long term.'' ",Health,0.05411120294967076
84,Science Daily,New Way to Reduce Food Waste,Environment,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903153825.htm,"   That, in turn, raises the cost and environmental impact of feeding the world's population. Researchers are suggesting a potential solution -- they found that 'humanizing' produce can change consumer attitudes toward fresh fruits and vegetables that are showing signs of age. The work, published in the Journal of the Association for Consumer Research, found that depicting imperfect-looking but still nutritious produce with human characteristics enhanced the food's appeal. ""We suggest that when old produce is humanized, it is evaluated more favorably, since it leads consumers to evaluate the old produce with a more compassionate lens,"" the researchers wrote. Vanessa Patrick, Bauer Professor of Marketing at the University of Houston and a coauthor on the paper, said the researchers examined how attitudes toward human aging -- ""old is gold,"" vs. ""young is good"" -- translated to attitudes toward so-called ""mature"" produce. The project involved anthropomorphizing bananas, cucumbers and zucchini, or depicting the produce in ways that suggest human-like traits. Bananas, for example, were depicted sunbathing while reclining in a chaise. Cucumber slices were used to create a picture of a human face. ""With fresh produce, aging promotes visible changes, much as it does in humans,"" Patrick said. ""That can create a connection with human qualities of aging when the food is anthropomorphized."" In the study, participants were shown depictions of both fresh and slightly-past-its-prime produce in both anthropomorphized and unadorned states. Those who saw the anthropomorphized aging produce rated it as more desirable than participants who saw the same produce without the anthropomorphic effects. Anthropomorphism didn't affect perceptions of fresh produce. The researchers said the results suggest grocery store managers and other marketers should consider using similar strategies to promote produce that has begun to show signs of aging but remains nutritious and tasty. ""Making food that would otherwise go to waste more appealing to consumers may allow store managers to avoid having to reduce the price for that older produce, which would improve the bottom line,"" Patrick said. ",Health,0.052851293031421887
85,Science Daily,Family-School Engagement Has Specific Perks for Young Students,Society,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827123536.htm,"   After surveying more than 3,170 students and 200 teachers, researchers at the University of Missouri found that families are less engaged with their child's schooling in middle school than they are when their child is in elementary school. However, the researchers also found a silver lining: Both elementary school children and middle school children are less likely to have concentration problems and behavioral issues at the end of a school year if their parents made a greater effort to be engaged with their schooling earlier in the year. ""In addition to being less likely to have emotional or behavioral issues in class, we also found that students with engaged parents ended the year with better social skills and were able to focus on tasks easier,"" said Tyler Smith, a senior research associate in the College of Education. ""This means that when parents are more involved at school, the benefits to their child grow over time."" The researchers said that family-school engagement often drops from elementary to middle school for several reasons, including a change in student-teacher ratio and a desire to respect their child's growing sense of independence. ""Keeping in contact with multiple teachers can be more challenging for parents with children in middle school, but our study shows evidence that parents and teachers should continue to make an effort to connect,"" said Keith Herman, a professor in the College of Education and co-author on the study. ""There are many options for parents to become more involved at both levels without feeling intrusive."" Herman suggests that parents can explore getting involved with their child's schooling in a variety of ways. Options outside of the home include attending school functions, volunteering at events and joining parent groups. However, parents and family members can also take a more active role by helping with homework and keeping in touch with the child's teacher(s). Smith adds that teachers can also do their part in encouraging families to get more involved by providing opportunities for parents to connect with them. ""Teachers have a lot on their hands, obviously, but even small efforts to help build better family-teacher relationships can have big payoffs for everyone involved,"" Smith said. ""Teachers might consider inviting parents to special events or giving students assignments that involve their parents so that the students can help begin to build that relationship naturally."" ",Society,0.052688160064931655
86,MIT News,A tech intervention to tame tuberculosis,Research,2019-09-04,-,http://news.mit.edu/2019/tech-phone-tuberculosis-kenya-0904,"  For tuberculosis patients, complying with a full course of treatment can be daunting and difficult. But a new experiment conducted by MIT researchers in Kenya, in collaboration with the digital health company Keheala, shows that a digital program used on mobile phones helps patients successfully finish their treatments. The program created interactive communication between patients and providers — rather than, say, one-way reminders about medication — and also used behavioral-science insights to help motivate patients to continue their recovery regimens. After the experimental intervention, only 4 percent of tuberculosis patients had unsuccessful treatment outcomes. For comparison, 13 percent of patients in a control group, who did not use the platform, didn’t finish their treatment. “Patients who we supported with our mobile platform were two-thirds less likely to fail to complete treatment,” says Erez Yoeli, a research scientist at the MIT Sloan School of Management and co-author of a newly published paper outlining the experiment’s results.  The paper, “Mobile Self-verification and Support for Successful Tuberculosis Treatment,” appears today in the New England Journal of Medicine. The co-authors are Yoeli; David Rand, an associate professor in the MIT Sloan School of Management; Jon Rathauser, CEO of Keheala, a digital health care firm based in Tel Aviv; Syon P. Bhanot, an assistant professor of economics at Swarthmore College; Maureen K. Kimenye and Eunice Mailu of the Kenya Ministry of Health; Enos Masini of the World Health Organization; and Philip Owiti of the International Union Against Tuberculosis and Lung Disease.  Tuberculosis treatments often take six months, and a substantial number of patients break off treatment when they are feeling better but have not fully recovered. If individuals stop taking medicine and relapse, it can also have harmful effects for larger communities as well, since tuberculosis is contagious.  So why do people break off their treatments? “Stigma, access to care challenges, burdensome treatment protocols, and a lack of information, motivation, and support make it difficult for patients to do the right thing and take their medication,” says Rathauser, who founded Keheala in 2014 to try to create tools to try to overcome logistical hurdles to health care delivery in the developing world. To conduct the study, the researchers teamed up with 17 health care clinics in Nairobi, the capital of Kenya, to create a randomized trial. There were 569 patients who participated in the intervention, and 535 patients in the control group who did not use the mobile digital program. The study was approved by the institutional review boards of Kenyatta National Hospital and the University of Nairobi. The researchers, working with Keheala, developed a health platform for the tuberculosis patients that works on “feature phones,” which are generally limited to talk and texting functionality, and are relatively common in Kenya in areas more prone to contagious disease outbreaks. Among other things, the program sent daily messages to patients asking them to verify that they were sticking to their medical routines. If patients did not respond to the daily messages, they would get follow-up messages and then ultimately phone calls from members of the research team who themselves had experience with TB treatment. The clinic treating the patient would be notified as well. In this way, Yoeli explains, the program used two key behavioral principles to improve patient actions: both “increased observability” of treatment adherence and “eliminating plausible deniability,” that is, reducing their ability to make excuses for not taking their medication. The program also provided information about tuberculosis, motivational messages, an “adherence contest,” and emphasized the community benefits of continuing treatment. “Throughout, we tried to give the individual as much credit as possible for their good deed toward the community,” adds Yoeli. The success of the experiment, Rand says, reinforces how crucial the behavior and psychology of patients can be in these situations.  “Nonadherence with treatment regimes is a major problem in medicine that leads to serious negative health outcomes,” Rand says. “But critically, the challenge is not medical — it’s behavioral.” As a result, Rand adds, “this is a space where behavioral science can play a major role in improving health outcomes. To me, what is so exciting about this paper is that we show how an intervention which is technologically quite simple has a really large positive impact because it is designed in a psychologically sophisticated way.”  Researchers with global expertise in the field say the findings are valuable. Jessica Haberer, an associate professor of medicine at Harvard Medical School and the director of global health research at Massachusetts General Hospital, who has read the paper, observes that “the study was well done,” adding, “The primary outcome shows great promise for the intervention.” As she notes, tuberculosis causes more deaths worldwide than any other infectious disease — an estimated 1.7 million in 2017 — and methods like the one in this experiment could reduce the incomplete treatment courses that are one source of the problem. Haberer also notes that although the long-term tracking of tuberculosis patients is costly and difficult, in the future, the “impact of the intervention could be better assessed through long-term follow-up,” in order to find out how many patients have reached, for instance, 18 months of disease-free survival. Such longitudinal data, she notes, has been historically “underutilized” in the area of tuberculosis research. Indeed, as the researchers acknowledge, any one study can have limitations. In this case, they would also like to see how the method fares in rural settings, which may present even greater health care access challenges. “One key thing the next study needs to show is that this approach works not just in the city of Nairobi, but for a more diverse population, including rural patients,” Yoeli states. In fact, the researchers are now in the midst of a three-year randomized controlled trial which expands the geographic scope of the experiment, and also evaluates its cost-effectiveness more thoroughly. The team says it would also like to apply the concept to HIV treatment programs in the future as well. The researchers, and Keheala, received support from Development Innovation Ventures (DIV), a fund of the United States Agency for International Development (USAID) that tests solutions to global development challenges through a year-round grant competition. ",Health,0.04954320885265137
87,Science Daily,Victorian Child Hearing-Loss Databank to Go Global,Society,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830092107.htm,"   The Victorian Childhood Hearing Impairment Longitudinal Databank, which has collected information for eight years, is featured in the latest International Journal of Epidemiology. Its data shows that language development and speech in hearing-impaired children lags behind other children, despite advancements in earlier detection and intervention in the past decade. The paper's* lead author, Murdoch Children's Research Institute's (MCRI) Dr Valerie Sung, says researchers world-wide can use the databank to answer questions around childhood hearing loss. ""This register can help us understand why some children with a hearing loss do so well, while others experience greater difficulties,"" she says. ""Universal newborn hearing screening is detecting hearing loss earlier than ever before, usually within a few weeks of birth. ""Children with hearing loss have very early access to hearing aids, early intervention services and for some, cochlear implantation. It was expected that hearing-impaired children would quickly come to enjoy the same language and educational outcomes as their hearing peers. ""However, early clinical diagnosis and intervention does not guarantee equality in health outcomes, with language and related outcomes of children with hearing loss remaining on average well below population means and the children's true cognitive potential. ""Demonstrating the reasons for this inequality has been hampered until now by the lack of population based prospective research."" The Victorian Childhood Hearing Impairment Longitudinal Databank (VicCHILD) is a population-based longitudinal databank open to every child with permanent hearing loss in Victoria. VicCHILD started in 2012 and stems from 25 years of work by The Royal Children's Hospital and MCRI. At the end 2018, 807 children were enrolled and provided baseline data. By 2020 more than 1000 children will be taking part, making it the largest hearing databank in the world. VicCHILD collects data at enrolment, two years of age, school entry and late primary /early high school. It involves parent questionnaires, child assessments and taking saliva samples. Dr Sung, who is also a honorary fellow at the University of Melbourne, says about 600 Australian infants each year are diagnosed with congenital hearing loss within weeks of birth. ""As these children grow, they can face challenges in things that come naturally to others like language and learning. This can impact their quality of life,"" she says. ""Hearing loss incurs significant burden and medical costs and impacts adversely on educational attainment and employment opportunities. ""This important bank of information could improve interventions and ultimately the lives of children with hearing loss and their families. It will also act as a platform for research trials to understand the effectiveness of different interventions."" ",Health,0.048292447119185844
88,Science Daily,Surgical Masks as Good as Respirators for Flu and Respiratory Virus Protection,Matter & Energy,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903134732.htm,"   A study published today in JAMA compared the ubiquitous surgical (or medical) mask, which costs about a dime, to a less commonly used respirator called an N95, which costs around $1. The study reported ""no significant difference in the effectiveness"" of medical masks vs. N95 respirators for prevention of influenza or other viral respiratory illness. ""This study showed there is no difference in incidence of viral respiratory transmission among health care workers wearing the two types of protection,"" said Dr. Trish Perl, Chief of UT Southwestern's Division of Infectious Diseases and Geographic Medicine and the report's senior author. ""This finding is important from a public policy standpoint because it informs about what should be recommended and what kind of protective apparel should be kept available for outbreaks."" Medical personnel -- in particular nurses, doctors, and others with direct patient contact -- are at risk when treating patients with contagious diseases such as influenza (flu). A large study conducted in a New York hospital system after the 2009 outbreak of H1N1, or swine flu, found almost 30 percent of health care workers in emergency departments contracted the disease themselves, Dr. Perl said. During that pandemic, the U.S. Centers for Disease Control and Prevention (CDC) recommended using the tighter-fitting N95 respirators, designed to fit closely over the nose and mouth and filter at least 95 percent of airborne particles, rather than the looser-fitting surgical masks routinely worn by health care workers, Dr. Perl said. But some facilities had trouble replenishing N95s as supplies were used. In addition, there are concerns health care workers might be less vigilant about wearing the N95 respirators since many perceive them to be less comfortable than medical masks, such as making it harder to breathe and being warmer on the wearer's face. Earlier clinical studies comparing the masks and respirators yielded mixed results, said Dr. Perl, also a Professor of Internal Medicine who holds the Jay P. Sanford Professorship in Infectious Diseases. The new study was performed at multiple medical settings in seven cities around the country, including Houston, Denver, Washington, and New York, by researchers at the University of Texas, the CDC, Johns Hopkins University, the University of Colorado, Children's Hospital Colorado, the University of Massachusetts, the University of Florida, and several Department of Veterans Affairs hospitals. Researchers collected data during four flu seasons between 2011 and 2015, examining the incidence of flu and acute respiratory illnesses in the almost 2,400 health care workers who completed the study. The project was funded by the CDC, the Veterans Health Administration, and the Biomedical Advanced Research and Development Authority (BARDA), which is part of the U.S. Health and Human Services Department and was founded in the years after Sept. 11, 2001, to help secure the nation against biological and other threats. ""It was a huge and important study -- the largest ever done on this issue in North America,"" Dr. Perl said. In the end, 207 laboratory-confirmed influenza infections occurred in the N95 groups versus 193 among medical mask wearers, according to the report. In addition, there were 2,734 cases of influenza-like symptoms, laboratory-confirmed respiratory illnesses, and acute or laboratory-detected respiratory infections (where the worker may not have felt ill) in the N95 groups, compared with 3,039 such events among medical mask wearers. ""The takeaway is that this study shows one type of protective equipment is not superior to the other,"" she said. ""Facilities have several options to provide protection to their staff -- which include surgical masks -- and can feel that staff are protected from seasonal influenza. Our study supports that in the outpatient setting there was no difference between the tested protections."" Dr. Perl said she expects more studies to arise from the data collected in this report; she now plans to investigate the dynamics of virus transmission to better understand how respiratory viruses are spread. ",Health,0.04468861384413049
89,Science Daily,New Insight Into Motor Neuron Death Mechanisms Could Be a Step Toward ALS Treatment,Health,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904125327.htm,"   The study into the role a protein known as heat shock protein 90 plays in intracellular signaling is a key step on the way to figuring out the reason some motor neurons in the spinal cord die and some do not. Findings, which could eventually lead to therapies to counter motor neuron death, were published in Experimental Biology and Medicine. Neurons are cells in the nervous system that carry information to muscles, glands and other nerves. Motor neurons are large neurons in the spine and brain stem, with long axons extending outside the nervous system to contact muscles and control their movements via contraction. Researchers led by Alvaro Estevez and Maria Clara Franco of the OSU College of Science have shown that a ubiquitous ""protein chaperone,"" heat shock protein 90, is particularly sensitive to inhibition in motor neurons that depend for survival on ""trophic factors"" -- small proteins that serve as helper molecules. Trophic factors attach to docking sites on the surface of nerve cells, setting in motion processes that help keep a cell alive. Research in animal models has shown trophic factors may have the ability to salvage dying neurons. ""It is well known that there are some motor neuron subpopulations resistant to degeneration in ALS, and other subpopulations that are highly susceptible to degeneration,"" said Estevez, associate professor of biochemistry and biophysics and the corresponding author on this research. ""Understanding the mechanisms involved in these different predispositions could provide new insight into how ALS progresses and open new alternatives for the development of novel treatments for the disease."" In this study, a motor-neuron-specific pool of heat shock protein 90, also known as Hsp90, repressed activation of a key cellular receptor and thus was shown to be critical to neuron survival; when Hsp90 was inhibited, motor neuron death was triggered. The Hsp90 inhibitor used in this research was geldanamycin, an antitumor antibiotic used in chemotherapy. Findings suggest the drug may have the unintended consequence of decreasing motor neurons' trophic pathways and thus putting those nerve cells at risk. ""The inhibition of Hsp90 as a therapeutic approach may require the development of inhibitors that are more selective so the cancer cells are targeted and healthy motor neurons are not,"" said Franco, assistant professor of biochemistry and biophysics. ALS, short for amyotrophic lateral sclerosis and also known as Lou Gehrig's disease, is caused by the deterioration and death of motor neurons in the spinal cord. It is progressive, debilitating and fatal. ALS was first identified in the late 1800s and gained international recognition in 1939 when it was diagnosed in a mysteriously declining Gehrig, ending the Hall of Fame baseball career of the New York Yankees first baseman. Known as the Iron Horse for his durability -- he hadn't missed a game in 15 seasons -- Gehrig died two years later at age 37. ",Health,0.04444224203651083
90,Science Daily,Possible Genetic Link Between Children's Language and Mental Health,Society,2019-08-20,-,https://www.sciencedaily.com/releases/2019/08/190820081851.htm,"   The University of York-led study examined genetic variants in six genes that are thought to contribute to language development in children. The researchers used Polygenic scoring, a statistical technique which adds up the effect of different genetic variants, to determine whether variants that are associated with children's language are also associated with poor mental health. They found that nearly half of the genetic variants which contribute to children's language were also associated with poor mental health. As part of the study, the team analysed genetic data from over 5,000 children, as well as parental responses to questionnaires and clinical assessments on children's language ability. If future research confirms these findings, it may have important implications for timing of mental health provision for children with language disorders, the researchers say. Lead author of the study, Dr Umar Toseeb, from the Department of Education at the University of York, said: ""This study provides very preliminary evidence that children with language disorders, such as developmental language disorder (DLD), may experience poor mental health due to shared biological mechanisms. ""This means that children with DLD may have poor mental health because the genes that are responsible for building neural systems responsible for language might also be responsible for mental health. ""If our findings are confirmed in future work, it could mean that, rather than wait for children with developmental language disorder to show symptoms of poor mental health before intervening, mental health support is put in place as soon as language difficulties become apparent, as a preventative measure."" The mental health difficulties often experienced my children with a DLD have commonly been thought to be caused by their struggles with language, but this study is the first to suggest that there may also be genetic factors which put children with DLD at risk of poor mental health. First author of the study, Dr Dianne Newbury, from the Department of Biological and Medical Sciences at Oxford Brookes University, said: ""This is the first study to demonstrate these genetic effects but they need to be replicated in larger independent datasets to confirm the findings. ""We looked at genetic variation across six genes, but there are many thousands more in the human genome that we did not investigate, so these results only represent a subset of the relevant networks. ""The study illustrates the complexity of language related genetic networks and shows that this is an area that should be investigated further."" ",Health,0.043544571931740515
91,Science Daily,Diet's Effect on Gut Bacteria Could Play Role in Reducing Alzheimer's Risk,Health,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903120514.htm,"   According to researchers at Wake Forest School of Medicine, that is a fair possibility. In a small pilot study, the researchers identified several distinct gut microbiome signatures -- the chemicals produced by bacteria -- in study participants with mild cognitive impairment (MCI) but not in their counterparts with normal cognition, and found that these bacterial signatures correlated with higher levels of markers of Alzheimer's disease in the cerebrospinal fluid of the participants with MCI. Through cross-group dietary intervention, the study also showed that a modified Mediterranean-ketogenic diet produced changes in the gut microbiome and its metabolites that correlated with reduced levels of Alzheimer's markers in the members of both study groups. The study appears in the current issue of EBioMedicine, a journal published by The Lancet. ""The relationship of the gut microbiome and diet to neurodegenerative diseases has recently received considerable attention, and this study suggests that Alzheimer's disease is associated with specific changes in gut bacteria and that a type of ketogenic Mediterranean diet can affect the microbiome in ways that could impact the development od dementia,"" said Hariom Yadav, Ph.D., assistant professor of molecular medicine at Wake Forest School of Medicine, who co-authored the study with Suzanne Craft, Ph.D., professor gerontology and geriatric medicine at the medical school and director of Wake Forest Baptist Health's Alzheimer's Disease Research Center. The randomized, double-blind, single-site study involved 17 older adults, 11 with diagnosed MCI and six with normal cognition. These participants were randomly assigned to follow either the low-carbohydrate modified Mediterranean-ketogenic diet or a low-fat, higher carbohydrate diet for six weeks then, after a six-week ""washout"" period, to switch to the other diet. Gut microbiome, fecal short-chain fatty acids and markers of Alzheimer's, including amyloid and tau proteins, in cerebrospinal fluid were measured before and after each dieting period. The study's limitations include the subject group's size, which also accounts for the lack of diversity in terms of gender, ethnicity and age. ""Our findings provide important information that future interventional and clinical studies can be based on,"" Yadav said. ""Determining the specific role these gut microbiome signatures have in the progression of Alzheimer's disease could lead to novel nutritional and therapeutic approaches that would be effective against the disease."" The research was supported by grant P30 AG049638 and award R01 AG055122 from the National Institute on Aging, Department of Defense grant W81XWH-18-1-0118, National Center for Advancing Translational Sciences grant UL1 TR001420 and a grant from the Hartman Family Foundation. ",Health,0.04212632875007382
92,Science Daily,Review: Biofeedback Could Help Treat a Number of Conditions,Health,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827101624.htm,"   Dr. Karli Kondo of the VA Evidence Synthesis Program and OHSU, first author on the study, explained how the research could advance the use of biofeedback: ""We are encouraged by the positive findings and the additional findings of potential benefits for a wide range of conditions. Biofeedback is a low-risk, cost-effective intervention. We hope that this report will help to make biofeedback more widely available to veterans across the U.S., and that it will serve as a roadmap for future research in the field."" The results appeared online Aug. 14, 2019, in the Journal of General Internal Medicine. Biofeedback refers to using instruments to measure and provide real-time feedback on patients' physiological responses. It can help patients learn to control and change those responses. Since biofeedback does not involve medication and is relatively noninvasive compared to other treatments, it could benefit patients with a low risk of any adverse effects, say the researchers. Biofeedback measures include muscle activity, heart rate, blood pressure, and brainwaves. It is often paired with treatments to change behavior, thoughts, or emotions. For example, using electromyography (EMG) to measure how muscles tighten in response to a medical condition may help patients consciously control those muscles. Biofeedback increasingly is being used as a complementary or alternative treatment for a wide range of conditions. In 2017, about 70 VA facilities reported offering some form of biofeedback. However, the evidence on how biofeedback is used and its effectiveness is scattered across studies on the individual conditions. Because of this, the practice is not well-integrated with usual care. Kondo and colleagues created an ""evidence map"" to get a high-level overview of the research available on biofeedback. They searched for previously conducted systematic reviews and studies on biofeedback to summarize what has been found so far. In total, the researchers used 16 systematic reviews on the topic. The review showed clear evidence that biofeedback is effective at reducing headache pain. A variety of biofeedback measures have been used for headache, including EMG, skin temperature, and blood pressure monitoring. These techniques appear to help decrease the frequency, duration, and intensity of both migraine and tension headaches. The largest improvements were in decreasing frequency of headaches. Moderate-confidence evidence shows that biofeedback can also improve secondary outcomes of headache, such as medication use, muscle tension, anxiety, and depression. Strong evidence also exists showing that biofeedback can help with urinary incontinence for men who have had their prostate removed. In this case, EMG is used to assist with pelvic floor muscle training. Adding biofeedback provides both immediate and long-term improvements beyond those seen with muscle training alone. The evidence map shows consistent evidence that biofeedback helps with several other conditions, although with fewer trials than were found for headaches or incontinence. EMG biofeedback can help with fecal incontinence in both older people of both sexes and in young women who recently gave birth. Adding biofeedback to therapy for lower-limb activity after stroke also appears to help patients. Stroke therapy can include several different types of biofeedback, such as platforms that measure weight distribution to help with balance, sensors to measure the angle of the joints during walking, and EMG to record muscle activity. The researchers found studies on biofeedback use for several other conditions, but no compelling evidence that it was effective in those cases. Reviews showed no benefits from biofeedback for urinary incontinence in women or for high blood pressure management. However, the studies covering these conditions were limited. Likewise, the review turned up insufficient evidence for the use of biofeedback for other conditions, such as bruxism (grinding or clenching the teeth, often while sleeping), labor pain, and Reynaud's disease (a condition involving reduced blood flow to the extremities). The researchers point out that their evidence map, in addition to showing several conditions for which biofeedback has been proven useful, also shows areas of uncertainty where more research is needed. They identified several targets for further research: balance and gait training, fibromyalgia, and intradialytic hypotension (a decrease in blood pressure that can cause a number of symptoms, including nausea, dizziness, and anxiety). Overall, the evidence map gives a ""lay of the land"" that shows what evidence exists for using biofeedback to treat medical conditions or symptoms and what research is still needed. The study was funded by the VA Office of Research and Development Quality Enhancement Research Initiative. ",Health,0.041663391602750156
93,Science Daily,Receptor Protein in Brain Promotes Resilience to Stress,Health,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903160619.htm,"   ""We have found that a specific cell receptor promotes resilience to the adverse effects of stress in animals,"" said study leader Seema Bhatnagar, PhD, a neuroscientist in the Department of Anesthesiology and Critical Care at Children's Hospital of Philadelphia (CHOP). ""Because we found links to the same receptor in patients with PTSD, we may have insights into developing more effective treatments for human psychiatric disorders."" The research appeared online July 17, 2019 in Nature Communications. Bhatnagar leads CHOP's Stress Neurobiology Program, which includes first author Brian F. Corbett, PhD, of CHOP, who performed much of the laboratory analysis. Other key collaborators were psychiatrists Philip Gerhman, MD, and Richard Ross, MD, of the Perelman School of Medicine at the University of Pennsylvania, who are attending physicians at the Corporal Michael J. Crescenz Veterans Affairs Medical Center in Philadelphia. The researchers focused on the sphingosine-1-phosphate receptor 3 (S1PR3), a lipid molecule found on cell membranes that is active in many cellular processes, including inflammation, cell migration and proliferation. It is one of a broader set of molecules called sphingolipid receptors. Scientists previously knew little about S1PR3's function in the brain. Bhatnagar said the current study points to this receptor as important in neural signaling, and added, ""We found that manipulating SIRPR3 levels affected how well animals cope with stress."" Because current psychiatric treatments succeed in only a subset of patients with stress-related psychiatric disorders, neurobiologists often model stress in laboratory animals, such as rats, to understand what makes some animals vulnerable to stress and others more resilient. Social hierarchies and territoriality are sources of stress in rats. Bhatnagar's team used validated behavioral tools, such as a forced swim test or a social defeat test, to investigate how rats use coping strategies to deal with stress. Rats that cope more passively, showing anxiety- and depressive-type behaviors, are classified as vulnerable; those that cope more actively are classified as resilient. In the current study, the researchers detected higher levels of the S1PR3 protein in resilient rats and lower levels in the vulnerable group. The study team then adjusted the expression of the S1PR3 gene to raise or reduce the gene's product -- the S1PR3 protein. Their results confirmed that increasing the protein levels increased stress-resilient behaviors, while ""knocking down"" or reducing protein levels raised vulnerable behaviors. The scientists also measured S1PR3 levels in the blood of patients at the Veterans Affairs hospital, all of whom had experienced combat. The veterans with PTSD had lower levels of S1PR3 than those without PTSD. Furthermore, those with more severe PTSD symptoms had lower levels of S1PR3. ""Our findings in both laboratory models and patients suggest that this protein is a potential blood-based biomarker for PTSD,"" said Bhatnagar. She added that follow-up studies in larger patient samples will be necessary to validate these initial findings. ""If we can establish that SIPR3 or related sphingolipid receptors are valid biomarkers for PTSD and other stress-related disorders, we may have a new tool to predict a person's risk for PTSD, or to predict the severity of a patient's symptoms. It may help us to better evaluate potential treatments, and perhaps to design better treatments,"" she added. ",Health,0.04109991815052728
94,Science Daily,Promising New Target to Combat Alzheimer's Disease,Health,2019-09-03,-,https://www.sciencedaily.com/releases/2019/09/190903153827.htm,"   The new research, published online in the journal Nature Communications, is the first to link maladaptive changes in calcium transport by mitochondria -- the energy-generating powerhouses of cells -- to the progression of Alzheimer's disease. ""Amyloid-beta deposition and tau pathology are considered the major contributors to Alzheimer's disease and, as a result, they have been the main focus of therapeutic development,"" explained John W. Elrod, PhD, Associate Professor in the Center for Translational Medicine at LKSOM and senior investigator on the new study. ""Large clinical trials targeting these pathways have universally failed, however."" Altered calcium regulation and metabolic dysfunction have been suspected of contributing to neuronal dysfunction and Alzheimer's development. ""But up to now, no one has investigated the impact of altered calcium transport into and out of the mitochondria on the progression of Alzheimer's disease,"" Dr. Elrod noted. ""Our current study provides a missing link between these two hypotheses of Alzheimer's pathogenesis."" Calcium transport into mitochondria plays an important part in many cellular functions and requires the involvement of multiple proteins to be carried out effectively. Among the key regulators of this process is a protein known as NCLX, which previously was discovered by Dr. Elrod's laboratory to mediate calcium efflux from heart cells. NCLX expression is also important in mitochondrial calcium efflux in neurons. In their new study, Dr. Elrod and colleagues examined the role of mitochondrial calcium uptake by neurons in Alzheimer's disease. To do so, the team used a mouse model of familial Alzheimer's disease in which animals harbored three gene mutations that give rise to age-progressive pathology comparable to Alzheimer's progression in human patients. As mice carrying the three mutations aged, the researchers observed a steady reduction in NCLX expression. This reduction was accompanied by decreases in the expression of proteins that limit mitochondrial calcium uptake, resulting in damaging calcium overload. NCLX loss was further linked to increases in the production of cell-damaging oxidants. To better understand the physiological relevance of NCLX loss, Dr. Elrod's team next completely eliminated NCLX expression in the forebrain of Alzheimer's disease mice. In tests for memory and cognitive function, the animals exhibited significant impairments. Analyses of brain tissue from these mice showed that NCLX reduction and the consequent loss of calcium efflux from mitochondria accelerated the development of amyloid beta and tau pathology. When NCLX expression was restored, levels of harmful protein aggregates declined, neuronal mitochondrial calcium homeostasis was reestablished, and mice were rescued from cognitive decline. ""Our findings indicate that maladaptive remodeling of pathways to compensate for abnormalities in calcium regulation, which perhaps are meant to maintain energy production in cells, lead to neuronal dysfunction and Alzheimer's pathology,"" Dr. Elrod said. ""Moreover, our data suggest that amyloid beta and tau pathology actually lie downstream of mitochondrial dysfunction in the progression of Alzheimer's disease, which opens up a new therapeutic angle."" Dr. Elrod and colleagues plan next to carry out a more detailed investigation of metabolic dysfunction that arises before Alzheimer's disease pathology emerges. ",Health,0.0410144824397189
95,Science Daily,Concussions Linked to Erectile Dysfunction in Former Pro Football Players,Science,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826121940.htm,"   The research -- based on a survey of more than 3,400 former NFL players representing the largest study cohort of former professional football players to date -- was conducted by investigators at the Harvard T.H. Chan School of Public Health and Harvard Medical School as part of the ongoing Football Players Health Study at Harvard University, a research program that encompasses a constellation of studies designed to evaluate various aspects of players' health across their lifespans. The researchers caution that their findings are observational -- based on self-reported concussion symptoms and indirect measures of ED and low testosterone. The results do not prove a cause-effect link between concussion and ED, nor do they explain exactly how head trauma might precipitate the onset of ED, the investigators noted. However, the findings do reveal an intriguing and powerful link between history of concussions and hormonal and sexual dysfunction, regardless of player age. Notably, the ED risk persisted even when researchers accounted for other possible causes such as diabetes, heart disease or sleep apnea, for example. Taken together, these findings warrant further study to tease out the precise mechanism behind it. One possible explanation, the research team said, could be injury to the brain's pituitary gland that sparks a cascade of hormonal changes culminating in diminished testosterone and ED. This biological mechanism has emerged as a plausible explanation in earlier studies that echo the current findings, such as reports of higher ED prevalence and neurohormonal dysfunction among people with head trauma and traumatic brain injury, including military veterans and civilians with head injuries. The new findings also suggest that sleep apnea and use of prescription pain medication contribute to low testosterone and ED. It remains unclear whether they do so independently, as consequences of head injury or both, the researchers said. Sexual function is not only a critical marker of overall health but also central to overall well-being, the researchers note. Understanding the mechanisms behind the possible downstream effects of head injury, they said, can inform treatments and preventive strategies. ""Former players with ED may be relieved to know that concussions sustained during their NFL careers may be contributing to a condition that is both common and treatable,"" said study lead author Rachel Grashow, a researcher at the Harvard T.H. Chan School of Public Health. The results are based on a survey of 3,409 former NFL players, average age 52 years (age range 24 to 89), conducted between 2015 and 2017. Participants were asked to report how often blows to the head or neck caused them to feel dizzy, nauseated or disoriented, or to experience headaches, loss of consciousness or vision disturbances -- all markers of concussion. Responders were grouped in four categories by number of concussive symptoms. Next, the former players were asked whether a clinician had recommended medication for either low testosterone or ED, and whether they were currently taking such medications. Men who reported the highest number of concussion symptoms were two and a half times more likely to report receiving either a recommendation for medication or to be currently taking medication for low testosterone, compared to men who reported the fewest concussion symptoms. Men with the most concussion symptoms were nearly two times more likely to report receiving a recommendation to take ED medication or to be currently taking ED medication than those reporting the fewest symptoms. Players who reported losing consciousness following head injury had an elevated risk for ED even in the absence of other concussion-related symptoms. Notably, even former players with relatively few concussion symptoms had an elevated risk for low testosterone, a finding that suggests there may be no safe threshold for head trauma, the team said. Of all participants, 18 percent reported low testosterone and nearly 23 percent reported ED. Slightly less than 10 percent of participants reported both. As expected, individuals with cardiovascular disease, diabetes, sleep apnea and depression, as well as those taking prescription pain medication -- all of which are known to affect sexual health -- were more likely to report low testosterone levels and ED. Yet, the link between concussion history and low testosterone levels and ED persisted even after researchers accounted for these other conditions. The link between history of concussion and ED was present among both the older and the younger players -- those under age 50 in this case -- the analysis showed, and it persisted over time. ""We found the same association of concussions with ED among both younger and older men in the study, and we found the same risk of ED among men who had last played twenty years ago,"" said study senior author Andrea Roberts, a researcher at the Harvard T.H. Chan School of Public Health. ""These findings suggest that increased risk of ED following head injury may occur at relatively young ages and may linger for decades thereafter."" Given that ED is both fairly common and easily treatable, those who experience symptoms are encouraged to report them to their physicians, the researchers said. Importantly, prompt evaluation of ED is critical because it can signal the presence of other conditions, including heart disease and diabetes. The findings also suggest that it may be important for clinicians to assess all patients with concussion history for the presence of neurohormonal changes. ""ED is a fact of life for many men,"" said Herman Taylor, director of player engagement and education and director of the Cardiovascular Research Institute at Morehouse School of Medicine. ""Anyone with symptoms should seek clinical attention and thorough evaluation, particularly since ED can be fueled by cardiovascular and metabolic disorders. The good news is that this is a treatable condition."" Co-investigators included Marc Weisskopf, Karen Miller, David M. Nathan, Ross Zafonte, Frank Speizer, Theodore Courtney, Aaron Baggish, Alvaro Pascual-Leone and Lee Nadler. The research was supported by the National Football League Players Association (NFLPA). ",Health,0.04074711045244099
