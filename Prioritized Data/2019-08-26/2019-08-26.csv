,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,Science Daily,Helping NASA Spacecraft Travel Faster and Farther With Math,Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822165020.htm,"   Randy Paffenroth, associate professor of mathematical sciences, computer science, and data science, has a multi-part mission in this research project. Using machine learning, neural networks, and an old mathematical equation, he has developed an algorithm that will significantly enhance the resolution of density scanning systems that are used to detect flaws in carbon nanotube materials. Higher resolution scans provide more accurate images (nine times ""super resolution"") of the material's uniformity, detecting imperfections in Miralon® materials -- a strong, lightweight, flexible nanomaterial produced by Nanocomp Technologies, Inc. Miralon® yarns, which can be as thin as a human hair, can be wrapped around structures like rocket fuel tanks, giving them the strength to withstand high pressures. Imperfections and variations in thickness can cause weak spots in the yarn and the resulting composite. Paffenroth, with a team of graduate students, is analyzing data from the manufacturing process to help ensure a more consistent end product. Nanocomp uses a modified commercial ""basis weight"" scanning system that scans the nanomaterial for mass uniformity and imperfections, creating a visual image of density; Paffenroth and his team are using machine learning to train algorithms to increase the resolution of the images, allowing the machine to detect more minute variations in the material. They have developed a unique mathematical ""compressed sensing / super resolution"" algorithm that has increased the resolution by nine times. Built with the Python programming language and based on an artificial neural network, the algorithm was ""trained"" on thousands of sets of nanomaterial images in which Paffenroth had already identified and located flaws. He essentially gave the algorithm a series of practice tests where he already knew the answers (known as ""ground truth""). Then, he gave it other tests without the answers. ""I give it a sheet of material. I know the imperfections going in but the algorithm doesn't. If it finds those imperfections, I can trust its accuracy,"" said Paffenroth. To make the machine learning algorithm more effective at making a high-resolution image out of a low-resolution image, he combined it with the Fourier Transform, a mathematical tool devised in the early 1800s that can be used to break down an image into its individual components. ""We take this fancy, cutting-edge neural network and add in 250-year-old mathematics and that helps the neural network work better,"" said Paffenroth. ""The Fourier Transform makes creating a high-resolution image a much easier problem by breaking down the data that makes up the image. Think of the Fourier Transform as a set of eyeglasses for the neural network. It makes blurry things clear to the algorithm. We're taking computer vision and virtually putting glasses on it. ""It's exciting to use this combination of modern machine learning and classic math for this kind of work,"" he added. Paffenroth's work is funded by an $87,353 grant WPI received from Nanocomp Technologies, a New Hampshire-based subsidiary of Huntsman Corporation that makes advanced carbon-nanotube materials for aerospace, defense, and the automotive industry. WPI is a sub-contractor to Nanocomp, which received an $8.1 million contract from NASA to advance its carbon nanotube sheets and yarns. Miralon® has already been proven in space. For instance, it was wrapped around structural supports in NASA's Juno probe orbiting the planet Jupiter to help a challenging problem with vibration damping and static discharge. NASA has also used Miralon® nanomaterials to make and test prototypes of new carbon composite pressure vessels, the precursors to next generation rocket fuel tanks. NASA spacecraft will need that added strength and durability as they travel farther from home and deeper into space. As part of its current NASA contract, Nanocomp is trying to make Miralon® yarns that are three times stronger, and the work by Paffenroth's team is a big part of making that happen. ""Randy is helping us achieve this goal of tripling our strength by improving the tools in our toolbox so that we can make stronger, better, next-generation materials to be used in space applications,"" said Bob Casoni, Quality Manager at Nanocomp. ""If NASA needs to build a new rocket system strong enough to get to Mars and back, it has a big set of challenges to face. Better materials are needed to allow NASA to design rockets that can go farther, faster and survive longer."" Casoni noted that with the higher resolution from WPI's algorithm, Nanocomp can see patterns and variations in its materials that they couldn't see before. ""We can not only pick up features, but we also have a better idea of the magnitude of those features,"" he said. ""Before, it was like seeing a blurry satellite image. You might think you're seeing the rolling hills of Pennsylvania, but with better resolution you see it's really Mount Washington or the Colorado Rockies. It's pretty amazing stuff."" And with better measurement tools, Nanocomp also will be able to improve its manufacturing process by testing whether changes in factors like temperature, tension control, pressure, and flow rates create better materials. ""We can use better measurements to optimize our ultimate product performance,"" said Casoni. ""Randy is helping us understand our manufacturing process better. He's doing his ""magic math"" to help us better understand variations in our product. The uniformity of that material plays a big part in its ultimate strength."" Paffenroth and his team will also develop algorithms to be used in active feedback control systems to predict how good a particular piece of material will be as it's first being made, helping to ensure a more consistent end product. The algorithm analyzes the properties measured at the beginning of the manufacturing run to effectively predict the properties at the end of the run, including mechanical properties and length of run. ""We can use machine learning to predict that Nanocomp won't get a useful length of material out of a particular production run,"" said Paffenroth. ""It helps them with waste. If they can tell in the first few meters of the run that there will be a problem, they can stop and start over. The Holy Grail of process engineering is that the more you understand about your process, the better your process is."" WPI will present its findings on Aug. 25 at the 2019 International Conference on Image, Video Processing and Artificial Intelligence in Shanghai, China. ",Computer Science,0.18018093950581984
1,IEEE,3 Easy Ways to Evaluate AI Claims,Robotics,2019-08-23,-,https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims,"      When every other tech startup claims to use artificial intelligence, it can be tough to figure out if an AI service or product works as advertised. In the midst of the AI “gold rush,” how can you separate the nuggets from the fool’s gold?  There’s no shortage of cautionary tales involving overhyped AI claims. And applying AI technologies to health care, education, and law enforcement mean that getting it wrong can have real consequences for society—not just for investors who bet on the wrong unicorn.  So IEEE Spectrum asked experts to share their tips for how to identify AI hype in press releases, news articles, research papers, and IPO filings. “It can be tricky, because I think the people who are out there selling the AI hype—selling this AI snake oil—are getting more sophisticated over time,” says Tim Hwang, director of the Harvard-MIT Ethics and Governance of AI Initiative. The term “AI” is perhaps most frequently used to describe machine learning algorithms (and deep learning algorithms, which require even less human guidance) that analyze huge amounts of data and make predictions based on patterns that humans might miss. These popular forms of AI are mostly suited to specialized tasks, such as automatically recognizing certain objects within photos. For that reason, they are sometimes described as “weak” or “narrow” AI. Some researchers and thought leaders like to talk about the idea of “artificial general intelligence” or “strong AI” that has human-level capacity and flexibility to handle many diverse intellectual tasks. But for now, this type of AI remains firmly in the realm of science fiction and is far from being realized in the real world.  “AI has no well-defined meaning and many so-called AI companies are simply trying to take advantage of the buzz around that term,” says Arvind Narayanan, a computer scientist at Princeton University. “Companies have even been caught claiming to use AI when, in fact, the task is done by human workers.” Here are three ways to recognize AI hype.  One red flag is what Hwang calls the “hype salad.” This means stringing together the term “AI” with many other tech buzzwords such as “blockchain” or “Internet of Things.” That doesn’t automatically disqualify the technology, but spotting a high volume of buzzwords in a post, pitch, or presentation should raise questions about what exactly the company or individual has developed. Other experts agree that strings of buzzwords can be a red flag. That’s especially true if the buzzwords are never really explained in technical detail, and are simply tossed around as vague, poorly-defined terms, says Marzyeh Ghassemi, a computer scientist and biomedical engineer at the University of Toronto in Canada. “I think that if it looks like a Google search—picture ‘interpretable blockchain AI deep learning medicine’—it  probably not high-quality work,” Ghassemi says. Hwang also suggests mentally replacing all mentions of “AI” in an article with the term “magical fairy dust.” It’s a way of seeing whether an individual or organization is treating the technology like magic. If so—that’s another good reason to ask more questions about what exactly the AI technology involves. And even the visual imagery used to illustrate AI claims can indicate that an individual or organization is overselling the technology. “I think that a lot of the people who work on machine learning on a day-to-day basis are pretty humble about the technology, because they’re largely confronted with how frequently it just breaks and doesn't work,” Hwang says. “And so I think that if you see a company or someone representing AI as a Terminator head, or a big glowing HAL eye or something like that, I think it’s also worth asking some questions.”              It can be hard to evaluate AI claims without any relevant expertise, says Ghassemi at the University of Toronto. Even experts need to know the technical details of the AI algorithm in question and have some access to the training data that shaped the AI model’s predictions. Still, savvy readers with some basic knowledge of applied statistics can search for red flags. To start, readers can look for possible bias in training data based on small sample sizes or a skewed population that fails to reflect the broader population, Ghassemi says. After all, an AI model trained only on health data from white men would not necessarily achieve similar results for other populations of patients.  How machine learning and deep learning models perform also depends on how well humans labeled the sample datasets use to train these programs. This task can be straightforward when labeling photos of cats versus dogs, but gets more complicated when assigning disease diagnoses to certain patient cases.  Medical experts frequently disagree with each other on diagnoses—which is why many patients seek a second opinion. Not surprisingly, this ambiguity can also affect the diagnostic labels that experts assign in training datasets. “For me, a red flag is not demonstrating deep knowledge of how your labels are defined,” Ghassemi says. Such training data can also reflect the cultural stereotypes and biases of the humans who labeled the data, says Narayanan at Princeton University. Like Ghassemi, he recommends taking a hard look at exactly what the AI has learned: “A good way to start critically evaluating AI claims is by asking questions about the training data.” Another red flag is presenting an AI system’s performance through a single accuracy figure without much explanation, Narayanan says. Claiming that an AI model achieves “99 percent” accuracy doesn’t mean much without knowing the baseline for comparison—such as whether other systems have already achieved 99 percent accuracy—or how well that accuracy holds up in situations beyond the training dataset. Narayanan also emphasized the need to ask questions about an AI model’s false positive rate—the rate of making wrong predictions about the presence of a given condition. Even if the false positive rate of a hypothetical AI service is just one percent, that could have major consequences if that service ends up screening millions of people for cancer. Readers can also consider whether using AI in a given situation offers any meaningful improvement compared to traditional statistical methods, says Clayton Aldern, a data scientist and journalist who serves as managing director for Caldern LLC. He gave the hypothetical example of a “super-duper-fancy deep learning model” that achieves a prediction accuracy of 89 percent, compared to a “little polynomial regression model” that achieves 86 percent on the same dataset. “We're talking about a three-percentage-point increase on something that you learned about in Algebra 1,” Aldern says. “So is it worth the hype?”  The hype surrounding AI isn’t just about the technical merits of services and products driven by machine learning. Overblown claims about the beneficial  impacts of AI technology—or vague promises to address ethical issues related to deploying it—should also raise red flags. “If a company promises to use its tech ethically, it is important to question if its business model aligns with that promise,” Narayanan says. “Even if employees have noble intentions, it is unrealistic to expect the company as a whole to resist financial imperatives.” One example might be a company with a business model that depends on leveraging customers’ personal data. Such companies “tend to make empty promises when it comes to privacy,” Narayanan says. And, if companies hire workers to produce training data, it’s also worth asking whether the companies treat those workers ethically. The transparency—or lack thereof—about any AI claim can also be telling. A company or research group can minimize concerns by publishing technical claims in peer-reviewed journals or allowing credible third parties to evaluate their AI without giving away big intellectual property secrets, Narayanan says. Excessive secrecy is a big red flag. With these strategies, you don’t need to be a computer engineer or data scientist to start thinking critically about AI claims. And, Narayanan says, the world needs many people from different backgrounds for societies to fully consider the real-world implications of AI.  Editor’s Note: The original version of this story misspelled Clayton Aldern’s last name as Alderton.   Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Computer Science,0.17882897196706002
2,Science Daily,The Technology Behind Bitcoin May Improve the Medications of the Future,Computers & Math,2019-08-23,-,https://www.sciencedaily.com/releases/2019/08/190823091740.htm,"   Big data. Machine Learning. Internet of Things. Blockchain. Futuristic concepts from the world of technology will likely soon find their way into your medicine cabinet -- and onto your mobile phone. Using a prototype app for smartphones, researchers from the University of Copenhagen have taken the next step in the dosing, production and distribution of the pharmaceutical products of the future. And the time for innovation is more than ripe, says Professor Jukka Rantanen of the Department of Pharmacy: '200 years ago, the first patent on making tablets was filed and the products have not changed much since. We are still having the same tablets. What we are doing now is suggesting a totally new type of product', he says. 'By rethinking the product design principles, related manufacturing solutions and distribution models for the pharmaceutical products, it is possible to dramatically reduce the overall price of medicine while also improving the safety and efficacy of the medication'. App-othecary The core of Jukka Rantanen and his research group's wager for a future solution for pharmaceutical products is the new concept of cryptopharmaceuticals, embodying the mentioned prototype of an app for smartphones. The app is called 'MedBlockChain' and has been developed by the group's former MSc-student Lasse Nørfeldt. It is, among other things, based on the research group's earlier work on digitalisation of pharmaceutical products, for example in the form of printing medications as edible QR codes. With the app, patients will be able to scan a medication and receive confirmation that it is a genuine product and not a fake item. A problem that, according to Jukka Rantanen, is particularly serious in countries with less structured medicines regulatory agencies. At the same time, patients can choose to provide access to a range of personal data -- everything from heart rate monitor watches, pedometers and internet-connected bath scales to genetic profiles, screen time and social media usage -- all contributing with knowledge that can enable computer systems based on artificial intelligence to gradually pin down the optimal dose for each patient. 'This type of data already exist in our information-rich society. It would be logical to employ this big data for something useful. Not just for sharing on Facebook, your exercise app or something like that, but also for defining your optimal dose of given medicine', says Jukka Rantanen. Builds on Blockchain With the growing mass of personal data, data security is also gaining importance, Jukka Rantanen points out. To guarantee data security, the app uses the so-called blockchain technology, which is probably best known in connection with the cryptocurrency Bitcoin. With blockchain, information -- or data blocks -- are linked in a chain that cannot be changed without simultaneously altering all other links of information in the chain. Thus, all changes will be detected and may be traced. If something looks suspicious, the system can also generate an alarm. As an example, a patient who scans a QR code on his medication may be alerted by an alarm if the code does not match the one that the pharmaceutical company has entered into the system, or if the medication does not match with the prescription. Conversely, the pharmaceutical company may be alerted if an otherwise unique medication code is registered more than once. Likewise, an absence of registrations may form the basis for alarms as it may reveal that the patient is not taking his or her medication as planned. This information may for example be shared with the patient's doctor or relatives. Cryptopharmaceuticals The blockchain concept may still seem distant to most people, but in fact, the technology is already being used in similar ways for everything from insurance and finance to shipping and food, explains Jukka Rantanen. As an example, Chinese consumers have already become accustomed to scanning items in the supermarket to confirm that the product they are buying is, for example, indeed bacon produced in Denmark, and not a counterfeit product. 'All of this is technologically possible. Now, the big question is how we should handle all of this data and who should get access to it. That is the discussion we hope to start with this new concept of cryptopharmaceuticals', says Jukka Rantanen. He emphasises Denmark as an obvious candidate as a pioneer country for the technology. Among other things based on the country's existing tradition of storing citizens' health data and prominent pharmaceutical industry. 'I think it has huge potential for Denmark to be among the first movers on this type of product. It is not limited to only one clinical condition. There could be a completely new type of product family coming out of this', says Jukka Rantanen. For the research team at the University of Copenhagen, the next step is to test the app on a test group of patients. This could for example be diabetes, where patients are most often accustomed to taking medication and measuring their personal blood sugar on a regular basis. The 'MedBlockChain' app may be downloaded from the App Store and Google PLAY. Note that this product is not final, but an illustrative prototype. * What is Blockchain? * A blockchain is a growing chain of data where each link -- or block -- is connected by means of a special, encrypted code. Each block has its own timestamp and contains information on previous blocks. Therefore, you can always go back and trace what has happened along the way. A network of computers, often in a huge number, shares the chain of data blocks. When new blocks are added, it will be confirmed by all the computers in the network, using a consensus mechanism. The design makes blockchains almost impossible to manipulate. It is thus a very secure way to process and store data. ",Computer Science,0.16567662375523795
3,ACM,"Artificial Muscles Bloom, Dance, Wave",ACM,2019-08-15,-,https://in.bgu.ac.il/en/pages/news/guest_networks.aspx,"    Router Guest Networks Lack Adequate Security, According to BGU Researchers     Ben-Gurion University of the Negev (Israel)August 15, 2019   A study by researchers at Israel’s Ben-Gurion University of the Negev (BGU) found routers made by leading manufacturers are vulnerable to cross-router data leakage via an attack on one of the separate host and guest networks. The researchers noted the presence of different levels of cross-router covert channels, which can be integrated and taken advantage of to either direct a malicious implant or to exfiltrate data. These flaws in certain cases can be patched as a software bug, but more pervasive and concealed cross-channel communication is impervious to prevention unless data streams are partitioned on different hardware. Said BGU's Adar Ovadya, ""A hardware-based solution seems to be the safest approach to guaranteeing isolation between secure and non-secure network devices.""                 ",Computer Science,0.14735049028996142
4,Science Daily,Materials Scientists Build a Synthetic System With Compartments Like Real Cells,Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822113405.htm,"   Now a research team led by Thomas Russell at the University of Massachusetts Amherst and the Lawrence Berkeley National Laboratory, with postdoctoral researcher Ganhua Xie and others, describe in a new paper how they take advantage of differences in electrical charge to create an ""all aqueous,"" water-in-water construct that achieves compartmentalization in a synthetic system. ""Our results point to new opportunities for manipulating and improving continuous separation and compartmentalized reactions. I feel we have developed a strategy to mimic the behavior of living cells,"" Russell notes. ""People have tried before to build synthetic systems that mimic nature and haven't done it, but we have. I think this is the first time this has been demonstrated."" Details appear in the current issue of Chem. Evan Runnerstrom, program manager in materials design at the Army Research Office, which supported this work with the U.S. Department of Energy, says, ""This ability to program stable structure and chemical functionality in all-aqueous systems that are environmentally friendly and bio-compatible will potentially provide unprecedented future capabilities for the Army. The knowledge generated by this project could be applicable to future technologies for all-liquid batteries, water purification or wound treatment and drug delivery in the field."" Russell and colleagues have been interested in liquid interfaces for several years and earlier conducted many oil-and-water experiments to observe results under various conditions. ""This led us to start looking at water-in-water liquid interfaces,"" he notes. For this work, Xie used two polymer aqueous solutions, one of polyethylene glycol (PEG) and water, the other dextran and water, with different electrical charges; they can be combined but do not mix. It's a ""classic example"" of coacervation, they suggest -- the solution undergoes liquid-liquid phase separation and forms two separate domains, like the non-mixing wax-and-water in a lava lamp. Next, Xie used a needle to send a high velocity jet of the dextran-plus-water solution into the PEG-plus-water solution, something Russell calls ""3D printing water-in-water."" This operation creates a coacervate-membrane-stabilized aqueous or water-filled tubule where the path-length of the tube can be kilometers long, he says. This 3D water-on-water printing forms a membranous layer of a coacervate that separates the two solutions. Another feature of the water tube formed this way is that electrical charge regulates whether and in which direction a material can pass through the coacervate membrane, the authors explain. A negatively charged dye or other molecule can only pass through a negatively charged wall of the asymmetrical membrane, and likewise for positively charged materials. Xie says, ""It effectively forms a diode, a one-sided gate. We can do a reaction inside this tube or sac that will generate a positively charged molecule that can only diffuse into the positive phase through the coacervate."" He adds, ""If we design the system right, we can separate things out easily by charge, so it can be used for separations media in all-aqueous compartmentalized reaction systems. We can also trigger one reaction that will allow a coordinated reaction cascade, just as it happens in our bodies."" Xie explains that the 3D water-on-water printing allows them to direct where they put these domains. ""We can build multi-layered structures with positive/negative/positive layers. We can use the sac-shaped ones as reaction chambers,"" he says. Advantages of separating functions and materials in cells by compartmentalization include allowing many processes to occur at once, many different chemical environments to coexist and otherwise incompatible components to work side by side. Among other tests and experiments, the researchers report on how they designed an all-aqueous tubular system and attached needles and syringe pumps at each end to allow water to pump through the entire structure without leakage, creating a flow-through coordinated reaction system. ""Once we'd done it, we looked at the biological mimicry,"" Russell says. ""There have been lots of efforts to mimic biological systems, and a biologist might object and say this is too simple. But I do think that even though it involves simple materials, it works. It's treading very close to vasculature, and it mimics any place where chemicals flow through a membrane. Is it in the body? No, but it does mimic a real metabolic process, a compartmental reaction."" The Army Research Office is an element of the U.S. Army Combat Capabilities Development Command's Army Research Laboratory. ",Electronics and Technology,0.1411475569477966
5,Science Daily,Study Models New Method to Accelerate Nanoparticles,Space & Time,2019-08-23,-,https://www.sciencedaily.com/releases/2019/08/190823094829.htm,"   The team simulated a system that uses light to generate an electromagnetic field. Neutral nanoparticles made from glass or some other material that insulates rather than conducts electric charges are used. The nanoparticles become polarized. All of the positive charges are displaced in the direction of the field and negative charges shift in the opposite direction. It creates an internal electric field that produces a force to move the particles from a reservoir, funneled through an injector, then shot out of an accelerator to produce thrust. The study, that has been about eight years in the making, analytically showed that the technique can work, and suggested parameters for success. ""The challenge is in selecting the right permittivity of the medium, the right amount of charge, in which all of this happens,"" said Joshua Rovey, associate professor in the Department of Aerospace Engineering in The Grainger College of Engineering at the U of I. ""You have to choose the right materials for the nanoparticles themselves as well as the material surrounding the nanoparticles as they move through the structure."" The technique is based on a field of physics called plasmonics that studies how optical light or optical electromagnetic waves, interact with nanoscale structures, such as a bar or prism. Rovey explained when the light hits the nanoscale structure, a resonant interaction occurs. It creates strong electromagnetic fields right next to that structure. And those electromagnetic fields can manipulate particles by applying forces to nanoscale particles that are near those structures. The study focused on how to feed the nanoparticles into the accelerator structure, or injector and how the angles of the plates in the injector affect the forces on these nanoparticles. ""One of the main motivating factors for the concept was the absence of or lack of a power supply in space,"" Rovey said. ""If we can just harness the sun directly, have the sun shine directly on the nanostructures themselves, there's no need for an electrical power supply or solar panel to provide power."" Rovey said this study was a numerical simulation. The next step will be to create nanoscale structures in a lab, load then into the system, apply a light source, and observe how the nanoparticles move. The study, ""Nanoparticle injector for photonic manipulators using dielectrophoresis,"" was written by Jaykob Maser and Joshua L. Rovey. It appears in AIP Advances. This project was supported by the Air Force Office of Scientific Research, a grant from the NASA Innovative Advanced Concepts program, and Missouri University of Science and Technology through the Chancellor's Fellowship. ",Electronics and Technology,0.13920635041819227
6,Science Daily,Insights Into High Quality Fabrication of Nanocomposites,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104827.htm,"   Selective laser melting (SLM), also known as laser powder bed fusion (L-PBF), is an additive manufacturing (AM) technology applied to metals and ceramics, and has shown promising potential for fabrication of unique structures and properties such as MMNCs. Using high power laser, SLM allows for quick production of three-dimensional (3D) parts with complicated shapes directly from powder materials without the time-consuming mold design process. This reduces production cost and lead time while delivering customized MMNCs parts for automotive, aerospace, electronics and biomedical industries. However, due to the lack of comprehensive understanding of the defects unique to SLM as well as the fabrication and performance of nanocomposites with SLM, researchers from Singapore University of Technology and Design (SUTD) and their research collaborators set out to gain a thorough understanding of the scientific and technological knowledge. They reviewed state of the art research from the perspective of materials and SLM processing parameters. Their paper was published in Progress in Materials Science, a journal that publishes authoritative reviews of recent advances in the science of materials. An in-depth review of the fabrication considerations related to nanocomposites was also conducted including the materials and SLM processing parameters, emphasizing on physical properties and preparation of powders (refer to image). Thereafter, mechanical properties of MMNCs and the corresponding enhancing mechanisms were addressed to provide a deeper understanding of MMNCs. ""MMNCs have always been a huge interest for material scientists. With the advancement in advanced manufacturing, particularly additive manufacturing, there is now greater potential in achieving high quality MMNCs. In our review, laser powder bed fusion is chosen as the process in focus as it has proven its capabilities in fabricating functional parts from metals and ceramics,"" explained principal investigator and co-author Professor Chua Chee Kai from SUTD. The review paper also addressed the defects unique to SLM technology associated with nanoparticles. The applications of MMNCs especially those fabricated with SLM processing were also listed and compared. ""One of the key challenges in AM is the lack of 'printable' materials. We believe this comprehensive review provides a timely overview and understanding of SLM for MMNCs by focusing on the merits while not ignoring the limitations. This hopefully will encourage more researchers to explore this highly interesting area,"" said co-author Dr Sing Swee Leong from Nanyang Technological University. ",Electronics and Technology,0.13654677995342362
7,Science Daily,Quantum Criticality Could Be a Boon for Qubit Designers,Computers & Math,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112651.htm,"   In a study in the Proceedings of the National Academy of Sciences, researchers from Rice University and the Vienna University of Technology (TU Wien) in Austria examined the behavior of an intermetallic crystal of cerium, palladium and silicon as it was subjected to extreme cold and a strong magnetic field. To their surprise, they found they could transform the quantum behavior of the material in two unique ways, one in which electrons compete to occupy orbitals and another where they compete to occupy spin states. ""The effect is so pronounced with one degree of freedom that it ends up liberating the other one,"" said Rice's Qimiao Si, co-corresponding author of the study and the director of the Rice Center for Quantum Materials (RCQM). ""You can essentially tune the system to maximize damage to one of these, leaving the other well-defined."" Si said the result could be important for companies like Google, IBM, Intel and others who are competing to develop quantum computers. Unlike today's digital computers, which use electricity or light to encode bits of information, quantum computers use the quantum states of subatomic particles like electrons to store information in qubits. A practical quantum computer could outperform its digital counterpart in many ways, but the technology is still in its infancy, and one of the chief obstacles is the fragility of the quantum states inside the qubits. ""You need a well-defined quantum state if you wish to be assured that the information that is stored in a qubit will not change due to background interference,"" Si said. Every electron acts like a spinning magnet, and its spin is described in one of two values, up or down. In many qubit designs, information is encoded in these spins, but these states can be so fragile that even tiny amounts of light, heat, vibration or sound can cause them to flip from one state to another. Minimizing the information that's lost to such ""decoherence"" is a major concern in qubit design, Si said. In the new study, Si worked with longtime collaborator Silke Paschen of TU Wien to study a material where the quantum states of electrons were scrambled not just in terms of their spins but also in terms of their orbitals. ""We designed a system, realized in some theoretical models and concurrently realized in a material, where spins and orbitals are almost on an equal footing and are strongly coupled together,"" he said. From previous research in 2012, Si, Paschen and colleagues knew that electrons in the compound could be made to interact so strongly that the material would undergo a dramatic change at a critically cold temperature. On either side of this ""quantum critical point,"" electrons in key orbitals would arrange themselves in a completely different way, with the shift occurring solely due to the quantum interactions between them. The earlier study invoked a well-known theory Si and collaborators developed in 2001 that prescribes how the spins of these localized electrons, which are part of atoms inside the alloy, strongly couple with free-flowing conduction electrons at the quantum critical point. According to this ""local quantum critical"" theory, as the material is cooled and approaches the critical point, the spins of localized electrons and conduction electrons begin to compete to occupy particular spin states. The quantum critical point is the tipping point where this competition destroys the ordered arrangement of the localized electrons and they instead become completely entangled with the conduction electrons. Even though Si has studied quantum criticality for almost 20 years, he was surprised by the results of Paschen's latest experiments. ""The new data was completely baffling to all of us,"" he said. ""That is, until we realized that the system contained not only spins but also orbitals as active degrees of freedom."" With that realization, Si's team, including Rice graduate student Ang Cai, built a theoretical model that contains both the spins and orbitals. Their detailed analysis of the model revealed a surprising form of quantum criticality that provided a clear understanding of the experiments. ""It was a shock to me, both from the theoretical model perspective and the experiments,"" he said. ""Even though this is a soup of things -- spins, orbitals that are all strongly coupled to each other and to background conduction electrons -- we could resolve two quantum critical points in this one system under the tuning of one parameter, which is the magnetic field. And at each one of the quantum critical points, only the spin or the orbital is driving the quantum criticality. The other one is more or less a bystander."" Si is the Harry C. and Olga K. Wiess Professor in Rice's Department of Physics and Astronomy. The study's co-lead authors are Cai and Valentina Martelli, formerly of TU Wien and now with the University of São Paulo in Brazil. Additional co-authors include Chia-Chuan Liu and Hsin-Hua Lai, both of Rice; Emilian Nica, formerly of Rice and currently at the University of British Columbia; Rong Yu, formerly of Rice and currently at Renmin University of China; Mathieu Taupin, Andrey Prokofiev, Diana Geiger, Jonathan Haenel and Julio Larrea, all of TU Wien; Kevin Ingersent of the University of Florida; Robert Küchler of the Max Planck Institute for Chemical Physics of Solids in Dresden, Germany; and Andre Strydom of the University of Johannesburg in South Africa. The research was supported by the National Science Foundation (DMR-1920740, CNS-1338099, PHY-1607611, DMR-1508122), the Robert A. Welch Foundation (C-1411), the Army Research Office (ARO-W911NF-14-1-0525, ARO-W911NF-14-1-0496), the Austrian Science Fund (P29296-N27, DK W1243), the European Research Council (Advanced Grant 227378), the Carlos Chagas Filho Foundation for Research Support of the State of Rio de Janeiro (201.755/2015), the National Natural Science Foundation of China (11674392), the Ministry of Science and Technology of China (2016YFA0300504), the South African National Research Foundation (93549), the University of Johannesburg and RCQM. RCQM leverages global partnerships and the strengths of more than 20 Rice research groups to address questions related to quantum materials. RCQM is supported by Rice's offices of the provost and the vice provost for research, the Wiess School of Natural Sciences, the Brown School of Engineering, the Smalley-Curl Institute and the departments of Physics and Astronomy, Electrical and Computer Engineering, and Materials Science and NanoEngineering. ",Electronics and Technology,0.13293821350496327
8,Science Daily,"Lasers Enable Engineers to Weld Ceramics, No Furnace Required",Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822141946.htm,"   The process, published in the Aug. 23 issue of Science, uses an ultrafast pulsed laser to melt ceramic materials along the interface and fuse them together. It works in ambient conditions and uses less than 50 watts of laser power, making it more practical than current ceramic welding methods that require heating the parts in a furnace. Ceramics have been fundamentally challenging to weld together because they need extremely high temperatures to melt, exposing them to extreme temperature gradients that cause cracking, explained senior author Javier E. Garay, a professor of mechanical engineering and materials science and engineering at UC San Diego, who led the work in collaboration with UC Riverside professor and chair of mechanical engineering Guillermo Aguilar. Ceramic materials are of great interest because they are biocompatible, extremely hard and shatter resistant, making them ideal for biomedical implants and protective casings for electronics. However, current ceramic welding procedures are not conducive to making such devices. ""Right now there is no way to encase or seal electronic components inside ceramics because you would have to put the entire assembly in a furnace, which would end up burning the electronics,"" Garay said. Garay, Aguilar and colleagues' solution was to aim a series of short laser pulses along the interface between two ceramic parts so that heat builds up only at the interface and causes localized melting. They call their method ultrafast pulsed laser welding. To make it work, the researchers had to optimize two aspects: the laser parameters (exposure time, number of laser pulses, and duration of pulses) and the transparency of the ceramic material. With the right combination, the laser energy couples strongly to the ceramic, allowing welds to be made using low laser power (less than 50 watts) at room temperature. ""The sweet spot of ultrafast pulses was two picoseconds at the high repetition rate of one megahertz, along with a moderate total number of pulses. This maximized the melt diameter, minimized material ablation, and timed cooling just right for the best weld possible,"" Aguilar said. ""By focusing the energy right where we want it, we avoid setting up temperature gradients throughout the ceramic, so we can encase temperature-sensitive materials without damaging them,"" Garay said. As a proof of concept, the researchers welded a transparent cylindrical cap to the inside of a ceramic tube. Tests showed that the welds are strong enough to hold vacuum. ""The vacuum tests we used on our welds are the same tests that are used in industry to validate seals on electronic and optoelectronic devices,"" said first author Elias Penilla, who worked on the project as a postdoctoral researcher in Garay's research group at UC San Diego. The process has so far only been used to weld small ceramic parts that are less than two centimeters in size. Future plans will involve optimizing the method for larger scales, as well as for different types of materials and geometries. ",Electronics and Technology,0.12911352915004914
9,Science Daily,Augmented Reality Glasses May Help People With Low Vision Better Navigate Their Environment,Computers & Math,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092252.htm,"   In a new study of patients with retinitis pigmentosa, an inherited degenerative eye disease that results in poor vision, Keck School of Medicine of USC researchers found that adapted augmented reality (AR) glasses can improve patients' mobility by 50% and grasp performance by 70%. ""Current wearable low vision technologies using virtual reality are limited and can be difficult to use or require patients to undergo extensive training,"" said Mark Humayun, MD, PhD, director of the USC Dr. Allen and Charlotte Ginsburg Institute for Biomedical Therapeutics, codirector of the USC Roski Eye Institute and University Professor of Ophthalmology at the Keck School. ""Using a different approach -- employing assistive technology to enhance, not replace, natural senses -- our team adapted AR glasses that project bright colors onto patients' retinas, corresponding to nearby obstacles,"" Humayun said. Patients with retinitis pigmentosa wore adapted AR glasses as they navigated through an obstacle course based on a U.S. Food and Drug Administration-validated functional test. Using video of each test, researchers recorded the number of times patients collided with obstacles, as well as the time taken to complete the course. Patients averaged 50% fewer collisions with the adapted AR glasses. Patients also were asked to grasp a wooden peg against a black background -- located behind four other wooden pegs -- without touching the front items. Patients demonstrated a 70% increase in grasp performance with the AR glasses. ""Patients with retinitis pigmentosa have decreased peripheral vision and trouble seeing in low light, which makes it difficult to identify obstacles and grasp objects. They often require mobility aids to navigate, especially in dark environments,"" said Anastasios N. Angelopoulos, study project lead in Humayun's research laboratory at the Keck School. ""Through the use of AR, we aim to improve the quality of life for low vision patients by increasing their confidence in performing basic tasks, ultimately allowing them to live more independent lives,"" Angelopoulos says. How the AR system works The AR system overlays objects within a 6-foot wireframe with four bright, distinct colors. In doing so, the glasses provide visual color cues that help people with constricted peripheral vision interpret complex environments, such as avoiding obstacles in dimly lit environments. To accomplish this, researchers used a process called simultaneous location and mapping, allowing the AR glasses to fully render the 3D structure of a room in real time. The glasses then translated this information into a semitransparent colored visual overlay, which highlighted potential obstacles with bright colors to help patients with spatial understanding and depth perception. This technology can work on commercially available devices. According to Humayun, while major cost and technical issues remain, this type of assistive technology could eventually become more practical for everyday use in the near future. ",Computer Science,0.1289096177031757
10,Science Daily,Disappearing Act: Device Vanishes on Command After Military Missions,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092302.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""This is not the kind of thing that slowly degrades over a year, like the biodegradable plastics that consumers might be familiar with,"" says Paul Kohl, Ph.D., whose team developed the material. ""This polymer disappears in an instant when you push a button to trigger an internal mechanism or the sun hits it."" The disappearing polymers were developed for the Department of Defense, which is interested in deploying electronic sensors and delivery vehicles that leave no trace of their existence after use, thus avoiding discovery and alleviating the need for device recovery. The key to making a polymer disappear, or break apart, is ""ceiling temperature."" Below the ceiling temperature, a polymer configuration is favored, but above that temperature, the polymer will break apart into its component monomers. Common polymers, like polystyrene, have a ceiling temperature above ambient temperature and are very stable. And even when they are warmed above their ceiling temperature, some of these materials can take a long time to decompose. For example, thousands of chemical bonds link all of the monomers together in polystyrene, and all of these bonds must be broken for the materials to decompose. But with low ceiling-temperature polymers, such as the cyclic ones Kohl is using, only one bond needs to break, and then all of the other bonds come apart, so the depolymerization happens quickly. The process can be initiated by a temperature spike from an outside or embedded source, or by a light-sensitive catalyst. For many years, researchers have attempted to make these polymers, but were unsuccessful because of the materials' instability at room temperature. Kohl's research group at the Georgia Institute of Technology discovered that they could overcome this issue if they were careful to remove all impurities formed during the synthesis. In addition, they found a number of aldehydes, including phthalaldehyde, that readily form cyclic polymers. Once they had optimized this polymer's synthesis, they focused on ways to make it disappear. To do this, the researchers incorporated into the polymer a photosensitive additive, which absorbs light and catalyzes depolymerization. ""Initially, we made it photosensitive to just ultraviolet light so we could make the parts in a well-lit room with fluorescent lighting, and it was just fine; it was stable,"" Kohl says. But when the polymer was placed outside, exposure to sunlight vaporized it (or reverted it back to a liquid, in some cases). A vehicle deployed at night would, therefore, disappear with the sunrise. Kohl's group has since discovered new additives that can trigger depolymerization at different wavelengths of visible light, so the polymer can decompose indoors. ""We have polymers designed for applications in which you come in the room, you turn the light on, and the thing disappears,"" Kohl says. The group has also determined how to stall depolymerization. ""We have a way to delay the depolymerization for a specific amount of time -- one hour, two hours, three hours,"" he says. ""You would keep it in the dark until you were going to use it, but then you would deploy it during the day, and you would have three hours before it decomposes."" The team has considered chemical methods to start the decomposition process, as well. In addition, they are testing various copolymers that can be added to phthalaldehyde to change the material's properties without altering its ability to vanish. Kohl says that this ""James Bond""-like material is already being incorporated in military devices by other researchers. But he also sees the potential of the materials for non-military applications. For example, the researchers have made a disappearing epoxy for a temporary adhesive that could be used in building materials. They also imagine the material could be used as sensors for environmental monitoring. Once the sensors are finished collecting data, there is no risk of littering the environment since they can be triggered to vaporize. The material can also be used for delivery vehicles in remote areas where recovery is difficult. ",Electronics and Technology,0.12813919774550006
11,Science Daily,Physicists Create World's Smallest Engine,Computers & Math,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821125519.htm,"   Work performed by Professor John Goold's QuSys group in Trinity's School of Physics describes the science behind this tiny motor. The research, published today in international journal Physical Review Letters, explains how random fluctuations affect the operation of microscopic machines. In the future, such devices could be incorporated into other technologies in order to recycle waste heat and thus improve energy efficiency. The engine itself -- a single calcium ion -- is electrically charged, which makes it easy to trap using electric fields. The working substance of the engine is the ion's ""intrinsic spin"" (its angular momentum). This spin is used to convert heat absorbed from laser beams into oscillations, or vibrations, of the trapped ion. These vibrations act like a ""flywheel,"" which captures the useful energy generated by the engine. This energy is stored in discrete units called ""quanta,"" as predicted by quantum mechanics. ""The flywheel allows us to actually measure the power output of an atomic-scale motor, resolving single quanta of energy, for the first time,"" said Dr Mark Mitchison of the QuSys group at Trinity, and one of the article's co-authors. Starting the flywheel from rest -- or, more precisely, from its ""ground state"" (the lowest energy in quantum physics) -- the team observed the little engine forcing the flywheel to run faster and faster. Crucially, the state of the ion was accessible in the experiment, allowing the physicists to precisely assess the energy deposition process. Assistant Professor in Physics at Trinity, John Goold said: ""This experiment and theory ushers in a new era for the investigation of the energetics of technologies based on quantum theory, which is a topic at the core of our group's research. Heat management at the nanoscale is one of the fundamental bottlenecks for faster and more efficient computing. Understanding how thermodynamics can be applied in such microscopic settings is of paramount importance for future technologies."" ",Electronics and Technology,0.12463219795151971
12,Science Daily,"In a Quantum Future, Which Starship Destroys the Other?",Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822165018.htm,"   ""The sequence of events can become quantum mechanical,"" said co-author Igor Pikovski, a physicist at the Center for Quantum Science and Engineering at Stevens Institute of Technology. "" We looked at quantum temporal order where there is no distinction between one event causing the other or vice versa."" The work, reported in the August 22 issue of Nature Communications, is among the first to reveal the quantum properties of time, whereby the flow of time doesn't observe a straight arrow forward, but one where cause and effect can co-exist both in the forward and backward direction. In the upcoming era of quantum computers, the work holds particular promise: quantum computers that exploit the quantum order of performing operations might beat devices that operate using only fixed sequences. To show this scenario, Pikovski and colleagues merged two seemingly conflicting theories -- quantum mechanics and general relativity -- to conduct a Gedanken experiment, a way of using the imagination to investigate the nature of things. The team, consisting of Pikovski, Magdalena Zych, Fabio Costa and Caslav Brukner, started by asking the question, ""what would a clock measure if it was influenced by a massive object in a quantum superposition state, i.e. both near and far at the same time?"" According to general relativity, the presence of a massive object slows down the flow of time, such that a clock placed close to a massive object will run slower compared to an identical one that is farther away. To illustrate what happens, imagine a pair of starships training for a mission. They are asked to fire at each other at a specified time and dodge the fire at another time, whereby each ship knows the exact time when to fire and when to dodge. If either ship fires too early, it will destroy the other, and this establishes an unmistakable time order between the firing events. However, if a powerful agent could place a sufficiently massive object, say a planet, closer to one ship it would slow down its flow of time. As a result, the ship would dodge the fire too late and would be destroyed. Quantum mechanics complicates the matter. When placing the planet in a state of superposition near one ship or the other, both can be destroyed or survive at the same time. The sequence of events exists in a state of superposition, such that each starship simultaneously destroys the other. The authors illustrate for the first time how this quantum scenario can occur and how it can be verified. ""Moving planets around is hard,"" said Pikovski. ""But imagining it helped us examine a quantum aspect of time that was previously unknown."" ",Space & Time,0.1240996096529284
13,Science Daily,A 2 Nm Sized Nanomachine Able to Spin and Transfer Its Rotational Energy,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104854.htm,"   Nature has proven exceptional at designing similar machines by using molecules that can convert optical, chemical or electrical energy into interactions with the surface to generate motion. ""For many nanomachines, we look at nature as our model. There are many examples of propellers with which organisms move in dynamic environments. Surprisingly, these natural nanomachines take the shape of large-scale propellers,"" says NAIST Professor Gwénaël Rapenne, who contributed to the new study. Consistently, the propeller Rapenne and his colleagues designed consists of three components: three blades each composed of an indazole, a stator consisting of five phenyl groups, and a ruthenium atom that binds to the two and allow the rotation like a ball bearing. One of the major differences is the conditions in which artificial nanopropellers work. Where natural nanomachines tend to work in environments comfortable for life, artificial nanomachines can work in much harsher conditions. Rapenne demonstrates this point by attaching his machine to a gold surface and observing that some begin swirling at extremely cold temperatures (near -200 oC). At the same time no propellers move at -275 oC, verifying their ability to convert thermal energy into movement. The propellers also showed the capacity to rotate in different directions in a controlled manner, but never to switch directions. This was the result of how the propeller was attached to the gold surface, which caused a slight tilt in the stator. The direction of the tilt determined the direction of the spin. This feature is reminiscent to macroscopic propellers we see in the real world. ""The stator acts as a ratchet-shaped gear that imposes a unidirectional rotation,"" notes Rapenne. This is not the first time Rapenne has used gold to prove the capabilities of his nanomachines. Two years ago, he and colleagues organized the world's first nanocar competition using gold tracks. While he does not expect to follow that effort with the first nano single propeller competition, he does believe the new machines will serve an important purpose in the nanoworld. ""Our propellers can displace nearby molecules, showing that they can be used to move molecular loads for faster transfer of energy or information,"" he says. ",Electronics and Technology,0.12369422620427585
14,Science Daily,Making Polyurethane Degradable Gives Its Components a Second Life,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092324.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""Millions of tons of polyurethanes are produced every day, and they're widely used in foams, plastics, sneakers, insulation and other products,"" says Ephraim Morado, a doctoral student who is presenting the work at the meeting. ""But when people finish using them, these materials are usually discarded."" Waste polyurethane either ends up in landfills, or it's incinerated, which requires a lot of energy and generates toxic byproducts, he notes. ""As an alternative, we want to develop the next generation of polyurethane that can degrade easily and be reprocessed into a new material that can then be commercialized, such as adhesives or paint,"" he says. Of course, Morado isn't alone in seeking ways to reuse polymers. ""A lot of people interested in recycling are trying to make polymers that will break down into their original starting materials and then remake the same polymer,"" says Steven Zimmerman, Ph.D., the project's principal investigator. ""We're taking a very different, intermediate approach, which industry might be more interested in pursuing in the short term because it would be easier and cheaper,"" adds Zimmerman, whose lab is based at the University of Illinois at Urbana-Champaign. ""We're trying to break our polymers down into some other starting materials that are familiar to industry."" The key difference between standard polyurethane and Morado's version is the incorporation of a hydroxy acetal as one of the monomers, alongside the traditional monomers. Zimmerman's team had first used a special iodine-containing acetal to make degradable polymers and polyacrylamide gels. In that earlier work, the polymer could be dissolved in slightly acidic water. Morado invented a new type of acetal to incorporate in his unconventional polyurethane so he could dissolve the polymer in the absence of water. After months of investigation, he discovered that a solution of trichloroacetic acid in dichloromethane, an organic solvent, could dissolve the polyurethane at room temperature in just three hours. That's in contrast to the harsher conditions of the typical incineration method, which requires more than 1,400 F to avoid toxic gas formation. Unlike water, dichloromethane causes the material to swell. That expansion enables the acid to reach the backbone of polyurethane's molecular chains, which it can break at positions where the acetal groups are located. Degradation releases alcohol monomers that can then be used to make new products such as adhesives whose performance rivals superglue. Morado created other acetal-containing polyurethanes that can be triggered to degrade when exposed to light. He used these materials to make microcapsules that could contain herbicides or even biocides for killing barnacles and other creatures that stick to ship hulls. He and Zimmerman are also developing adhesives that dissolve when treated with a few drops of acid in dichloromethane solvent. One potential application is on circuit boards, where a chip that had been securely glued to the board could be swapped out for a replacement if the original chip had failed. In addition, the team is working on polyurethanes that can degrade under even milder conditions, such as exposure to vinegar. That would be particularly useful for, say, degradable sutures or household applications such as removable picture hangers. ",Electronics and Technology,0.1209713136468774
15,Science Daily,"Physicists Mash Quantum and Gravity and Find Time, but Not as We Know It",Space & Time,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826122010.htm,"   UQ physicist Dr Magdalena Zych said the discovery arose from an experiment the team designed to bring together elements of the two big -- but contradictory -- physics theories developed in the past century. ""Our proposal sought to discover: what happens when an object massive enough to influence the flow of time is placed in a quantum state?"" Dr Zych said. She said Einstein's theory described how the presence of a massive object slowed time. ""Imagine two space ships, asked to fire at each other at a specified time while dodging the other's attack,"" she said. ""If either fires too early, it will destroy the other."" ""In Einstein's theory, a powerful enemy could use the principles of general relativity by placing a massive object -- like a planet -- closer to one ship to slow the passing of time."" ""Because of the time lag, the ship furthest away from the massive object will fire earlier, destroying the other."" Dr Zych said the second theory, of quantum mechanics, says any object can be in a state of ""superposition"" ""This means it can be found in different states -- think Schrodinger's cat,"" she said. Dr Zych said using the theory of quantum mechanics, if the enemy put the planet into a state of ""quantum superposition,"" then time also should be disrupted. ""There would be a new way for the order of events to unfold, with neither of the events being first or second -- but in a genuine quantum state of being both first and second,"" she said. UQ researcher Dr Fabio Costa said although ""a superposition of planets"" as described in the paper -- may never be possible, technology allowed a simulation of how time works in the quantum world -- without using gravity. ""Even if the experiment can never be done, the study is relevant for future technologies,"" Dr Costa said. ""We are currently working towards quantum computers that -- very simply speaking -- could effectively jump through time to perform their operations much more efficiently than devices operating in fixed sequence in time, as we know it in our 'normal' world."" ",Space & Time,0.11680598303909823
16,IEEE,ANYbotics Introduces Sleek New ANYmal C Quadruped,Robotics,2019-08-23,-,https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped,"      Quadrupedal robots are making significant advances lately, and just in the past few months we’ve seen Boston Dynamics’ Spot hauling a truck, IIT’s HyQReal pulling a plane, MIT’s MiniCheetah doing backflips, Unitree Robotics’ Laikago towing a van, and Ghost Robotics’ Vision 60 exploring a mine. Robot makers are betting that their four-legged machines will prove useful in a variety of applications in construction, security, delivery, and even at home. ANYbotics has been working on such applications for years, testing out their ANYmal robot in places where humans typically don’t want to go (like offshore platforms) as well as places where humans really don’t want to go (like sewers), and they have a better idea than most companies what can make quadruped robots successful. This week, ANYbotics is announcing a completely new quadruped platform, ANYmal C, a major upgrade from the really quite research-y ANYmal B. The new quadruped has been optimized for ruggedness and reliability in industrial environments, with a streamlined body painted a color that lets you know it means business.   ANYmal C’s physical specs are pretty impressive for a production quadruped. It can move at 1 meter per second, manage 20-degree slopes and 45-degree stairs, cross 25-centimeter gaps, and squeeze through passages just 60 centimeters wide. It’s packed with cameras and 3D sensors, including a lidar for 3D mapping and simultaneous localization and mapping (SLAM). All these sensors (along with the vast volume of gait research that’s been done with ANYmal) make this one of the most reliably autonomous quadrupeds out there, with real-time motion planning and obstacle avoidance. ANYmal C is also one of the ruggedest legged robots in existence. The 50-kilogram robot is IP67 rated, meaning that it’s completely impervious to dust and can withstand being submerged in a meter of water for an hour. If it’s submerged for longer than that, you’re absolutely doing something wrong. The robot will run for over 2 hours on battery power, and if that’s not enough endurance, don’t worry, because ANYmal can autonomously impale itself on a weird cone-shaped docking station to recharge.  As far as what ANYmal C is designed to actually do, it’s mostly remote inspection tasks where you need to move around through a relatively complex environment, but where for whatever reason you’d be better off not sending a human. ANYmal C has a sensor payload that gives it lots of visual options, like thermal imaging, and with the ability to handle a 10-kilogram payload, the robot can be adapted to many different environments. Over the next few months, we’re hoping to see more examples of ANYmal C being deployed to do useful stuff in real-world environments, but for now, we do have a bit more detail from ANYbotics CTO Christian Gehring. IEEE Spectrum: Can you tell us about the development process for ANYmal C? Christian Gehring: We tested the previous generation of ANYmal (B) in a broad range of environments over the last few years and gained a lot of insights. Based on our learnings, it became clear that we would have to re-design the robot to meet the requirements of industrial customers in terms of safety, quality, reliability, and lifetime. There were different prototype stages both for the new drives and for single robot assemblies. Apart from electrical tests, we thoroughly tested the thermal control and ingress protection of various subsystems like the depth cameras and actuators.  What can ANYmal C do that the previous version of ANYmal can’t? ANYmal C was redesigned with a focus on performance increase regarding actuation (new drives), computational power (new hexacore Intel i7 PCs), locomotion and navigation skills, and autonomy (new depth cameras). The new robot additionally features a docking system for autonomous recharging and an inspection payload as an option. The design of ANYmal C is far more integrated than its predecessor, which increases both performance and reliability. How much of ANYmal C’s development and design was driven by your experience with commercial or industry customers? Tests (such as the offshore installation with TenneT) and discussions with industry customers were important to get the necessary design input in terms of performance, safety, quality, reliability, and lifetime. Most customers ask for very similar inspection tasks that can be performed with our standard inspection payload and the required software packages. Some are looking for a robot that can also solve some simple manipulation tasks like pushing a button. Overall, most use cases customers have in mind are realistic and achievable, but some are really tough for the robot, like climbing 50° stairs in hot environments of 50°C. Can you describe how much autonomy you expect ANYmal C to have in industrial or commercial operations? ANYmal C is primarily developed to perform autonomous routine inspections in industrial environments. This autonomy especially adds value for operations that are difficult to access, as human operation is extremely costly. The robot can naturally also be operated via a remote control and we are working on long-distance remote operation as well.  Do you expect that researchers will be interested in ANYmal C? What research applications could it be useful for? ANYmal C has been designed to also address the needs of the research community. The robot comes with two powerful hexacore Intel i7 computers and can additionally be equipped with an NVIDIA Jetson Xavier graphics card for learning-based applications. Payload interfaces enable users to easily install and test new sensors. By joining our established ANYmal Research community, researchers get access to simulation tools and software APIs, which boosts their research in various areas like control, machine learning, and navigation. [ ANYmal C ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11049027267176584
17,Science Daily,New Technique Could Streamline Design of Intricate Fusion Device,Computers & Math,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821173718.htm,"   ""Our main result is that we came up with a new method of identifying the irregular magnetic fields produced by stellarator coils,"" said physicist Caoxiang Zhu, lead author of a paper reporting the results in Nuclear Fusion. ""This technique can let you know in advance which coil shapes and placements could harm the plasma's magnetic confinement, promising a shorter construction time and reduced costs."" Fusion, the power that drives the sun and stars, is the fusing of light elements in the form of plasma -- the hot, charged state of matter composed of free electrons and atomic nuclei -- that generates massive amounts of energy. Twisty, cruller-shaped stellarators are an alternative to doughnut-shaped tokamaks that are more commonly used by scientists seeking to replicate fusion on Earth for a virtually inexhaustible supply of power to generate electricity. A key benefit of stellarators is their production of highly stable plasmas that are less liable to the damaging disruptions that tokamaks can incur. But the complexity of stellarator coils has been a factor holding back development of such facilities. The coils of a stellarator must be constructed and arranged around the vacuum chamber very precisely, since deviations from the best coil arrangement create bumps and wiggles in the magnetic field that degrade the magnetic confinement and allow the plasma to escape. These problematic magnetic fields can easily be caused by misplacement of the magnetic coils, so engineers stipulate strict tolerances for these components. ""The big challenge of building stellarators is figuring out how to make them simply and economically,"" said PPPL Chief Scientist Michael Zarnstorff. ""Zhu's research is important because he is trying to look more carefully and quantitatively at some of the drivers of the cost. His results suggest that we can simplify the construction of stellarators and thereby make them easier and less expensive to build, by not insisting on tight tolerances for things that don't matter."" In the past, scientists have used computer simulations to determine which coil placements would be best, checking the plasma's reactions to all possible magnetic configurations before the stellarator was built. But because there are many ways for the coils to vary, ""this approach requires massive computation resources and man-hours,"" said Zhu. ""In this paper, we propose a new mathematical method to rapidly identify dangerous coil deviations that could appear during fabrication and assembly."" The method relies on a Hessian matrix, a mathematical tool that allows researchers to determine which variations of the magnetic coils can make the plasma change its properties. ""The idea is to figure out which perturbations you really have to control or avoid, and which you can ignore,"" Zhu said. The team recently confirmed the accuracy of the new method by using it to analyze coil placements for a configuration similar to the Columbia Non-Neutral Torus, a small fusion facility operated by Columbia University. They compared the results to those produced by past studies relying on conventional methods and found that they agreed. The team is now collaborating with researchers in China to use the method to optimize coil placement on the Chinese First Quasi-axisymmetric Stellarator (CFQS), currently under construction. The new technique could help scientists design better stellarators, Zhu said. It could make possible ways to identify an optimal coil arrangement that no one had considered before. Included on the research team were scientists from China's Southwest Jiaotong University and Japan's National Institute for Fusion Science. The research was supported by the DOE's Office of Science and the Max Planck Princeton Center for Plasma Physics. ",Matter & Energy,0.11015906029593432
18,IEEE,Video Friday: AlienGo Quadruped Robot Can Now Do Backflips,Robotics,2019-08-23,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot,"      Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (send us your events!): Let us know if you have suggestions for next week, and enjoy today’s videos. I know you’ve all been closely following our DARPA Subterranean Challenge coverage here and on Twitter, but here are short recap videos of each day just in case you missed something.        [ DARPA SubT ]   After Laikago, Unitree Robotics is now introducing AlienGo, which is looking mighty spry:    We’ve seen MIT’s Mini Cheetah doing backflips earlier this year, but apparently AlienGo is now the largest and heaviest quadruped to perform the maneuver. [ Unitree ]   The majority of soft robots today rely on external power and control, keeping them tethered to off-board systems or rigged with hard components. Now, researchers from the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS) and Caltech have developed soft robotic systems, inspired by origami, that can move and change shape in response to external stimuli, paving the way for fully untethered soft robots.    The Rollbot begins as a flat sheet, about 8 centimeters long and 4 centimeters wide. When placed on a hot surface, about 200°C, one set of hinges folds and the robot curls into a pentagonal wheel.  Another set of hinges is embedded on each of the five sides of the wheel. A hinge folds when in contact with the hot surface, propelling the wheel to turn to the next side, where the next hinge folds. As they roll off the hot surface, the hinges unfold and are ready for the next cycle. [ Harvard SEAS ]   A new research effort at Caltech aims to help people walk again by combining exoskeletons with spinal stimulation. This initiative, dubbed RoAM (Robotic Assisted Mobility), combines the research of two Caltech roboticists: Aaron Ames, who creates the algorithms that enable walking by bipedal robots and translates these to govern the motion of exoskeletons and prostheses; and Joel Burdick, whose transcutaneous spinal implants have already helped paraplegics in clinical trials to recover some leg function and, crucially, torso control.    [ Caltech ]   Once ExoMars lands, it’s going to have to get itself off of the descent stage and onto the surface, which could be tricky. But practice makes perfect, or as near as you can get on Earth.    That wheel walking technique is pretty cool, and it looks like ExoMars will be able to handle terrain that would scare NASA’s Mars rovers away. [ ExoMars ]   I am honestly not sure whether this would make the game of golf more or less fun to watch:    [ Nissan ]   Finally, a really exciting use case for Misty!    It can pick up those balls too, right? [ Misty ]   You know you’re an actual robot if this video doesn’t make you crave Peeps.    [ Soft Robotics ]   COMANOID investigates the deployment of robotic solutions in well-identified Airbus airliner assembly operations that are tedious for human workers and for which access is impossible for wheeled or rail-ported robotic platforms. This video presents a demonstration of autonomous placement of a part inside the aircraft fuselage. The task is performed by TORO, the torque-controlled humanoid robot developed at DLR.    [ COMANOID ]   It’s a little hard to see in this video, but this is a cable-suspended robot arm that has little tiny robot arms that it waves around to help damp down vibrations.    [ CoGiRo  ]     This week in Robots in Depth, Per speaks with author Cristina Andersson.    In 2013 she organized events in Finland during European robotics week and found that many people was very interested but that there was also a big lack of knowledge. She also talks about introducing robotics in society in a way that makes it easy for everyone to understand the benefits as this will make the process much easier. When people see the clear benefits in one field or situation they will be much more interested in bringing robotics in to their private or professional lives. [ Robots in Depth ]    Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11002563422942237
19,Science Daily,"Producing Protein Batteries for Safer, Environmentally Friendly Power Storage",Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092322.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition.  ""The trend in the battery field right now is to look at how the electrons are transported within a polymer network,"" says Tan Nguyen, a Ph.D. student who helped develop the project. ""The beauty of polypeptides is that we can control the chemistry on their side chains in 3D without changing the geometry of the backbone, or the main part of the structure. Then we can systematically examine the effect of changing different aspects of the side chains."" Current lithium-ion batteries can harm the environment, and because the cost of recycling them is higher than manufacturing them from scratch, they often accumulate in landfills. At the moment, there is no safe way of disposing of them. Developing a protein-based, or organic, battery would change this situation. ""The amide bonds along the peptide backbone are pretty stable -- so the durability is there, and we can then trigger when they break down for recycling,"" says Karen Wooley, Ph.D., who leads the team at Texas A&M University. She envisions that polypeptides could eventually be used in applications such as flow batteries for storing electrical energy. ""The other advantage is that by using this protein-like architecture, we're building in the kinds of conformations that are found in proteins in nature that already transport electrons efficiently,"" Wooley says. ""We can also optimize this to control battery performance."" The researchers built the system using electrodes made of composites of carbon black, constructing polypeptides that contain either viologen or 2,2,6,6-tetramethylpiperidine 1-oxyl (TEMPO). They attached viologens to the matrix used for the anode, which is the negative electrode, and used a TEMPO-containing polypeptide for the cathode, which is the positive electrode. The viologens and TEMPO are redox-active molecules. ""What we've measured so far for the range, the potential window between the two materials, is about 1.5 volts, suitable for low-energy requirement applications, such as biosensors,"" Nguyen says. For potential use in an organic battery, Nguyen has synthesized several polymers that adopt different conformations, such as a random coil, an alpha helix and a beta sheet, to investigate their electrochemical characteristics. With these peptides in hand, Nguyen is now collaborating with Alexandra Danielle Easley, a Ph.D. student in the laboratory of Jodie Lutkenhaus, Ph.D., also at Texas A&M University, to build the battery prototypes. Part of that work will include testing to better understand how the polymers function when they're organized on a substrate. While this early stage research has far to go before organic-based batteries are commercially available, the flexibility and variety of structures that proteins can provide promise wide potential for sustainable energy storage that is safer for the environment. ",Matter & Energy,0.10642451222910884
20,Science Daily,Temperatures of 800 Billion Degrees in the Cosmic Kitchen,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822101419.htm,"   Simulation of electromagnetic radiation Collisions between stars cannot be directly observed -- not least of all because of their extreme rarity. According to estimates, none has ever happened in our galaxy, the Milky Way. The densities and temperatures in merging processes of neutron stars are similar to those occurring in heavy ion collisions, however. This enabled the HADES team to simulate the conditions in merging stars at the microscopic level in the heavy ion accelerator at the Helmholtzzentrum für Schwerionenforschung (GSI) in Darmstadt. As in a neutron star collision, when two heavy ions are slammed together at close to the speed of light, electromagnetic radiation is produced. It takes the form of virtual photons that turn back into real particles after a very short time. However, the virtual photons occur very rarely in experiments using heavy ions. ""We had to record and analyze about 3 billion collisions to finally reconstruct 20,000 measurable virtual photons,"" says Dr. Jürgen Friese, the former spokesman of the HADES collaboration and researcher at Laura Fabbietti's Professorship on Dense and Strange Hadronic Matter at TUM. Photon camera shows collision zone To detect the rare and transient virtual photons, researchers at TUM developed a special 1.5 square meter digital camera. This instrument records the Cherenkov effect: the name given to certain light patterns generated by decay products of the virtual photons. ""Unfortunately the light emitted by the virtual photons is extremely weak. So the trick in our experiment was to find the light patterns,"" says Friese. ""They could never be seen with the naked eye. We therefore developed a pattern recognition technique in which a 30,000 pixel photo is rastered in a few microseconds using electronic masks. That method is complemented with neural networks and artificial intelligence."" Observing the material properties in the laboratory The reconstruction of thermal radiation from compressed matter is a milestone in the understanding of cosmic forms of matter. It enabled the scientists to place the temperature of the new system resulting from the merger of stars at 800 billion degrees celsius. As a result, the HADES team was able to show that the merging processes under consideration are in fact the cosmic kitchens for the fusion of heavy nucleii. ",Space & Time,0.10329650754937872
21,Science Daily,Mission to Jupiter's Icy Moon Confirmed,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822143218.htm,"   ""We are all excited about the decision that moves the Europa Clipper mission one key step closer to unlocking the mysteries of this ocean world,"" said Thomas Zurbuchen, associate administrator for the Science Mission Directorate at NASA Headquarters in Washington. ""We are building upon the scientific insights received from the flagship Galileo and Cassini spacecraft and working to advance our understanding of our cosmic origin, and even life elsewhere."" The mission will conduct an in-depth exploration of Jupiter's moon Europa and investigate whether the icy moon could harbor conditions suitable for life, honing our insights into astrobiology. To develop this mission in the most cost-effective fashion, NASA is targeting to have the Europa Clipper spacecraft complete and ready for launch as early as 2023. The agency baseline commitment, however, supports a launch readiness date by 2025. NASA's Jet Propulsion Laboratory in Pasadena, California, leads the development of the Europa Clipper mission in partnership with the Johns Hopkins University Applied Physics Laboratory for the Science Mission Directorate. Europa Clipper is managed by the Planetary Missions Program Office at NASA's Marshall Space Flight Center in Huntsville, Alabama. ",Space & Time,0.10024465895566959
22,Science Daily,Maximum Mass of Lightest Neutrino Revealed Using Astronomical Big Data,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822113407.htm,"   It's important to better understand neutrinos and the processes through which they obtain their mass as they could reveal secrets about astrophysics, including how the universe is held together, why it is expanding and what dark matter is made of. First author, Dr Arthur Loureiro (UCL Physics & Astronomy), said: ""A hundred billion neutrinos fly through your thumb from the Sun every second, even at night. These are very weakly interactive ghosts that we know little about. What we do know is that as they move, they can change between their three flavours, and this can only happen if at least two of their masses are non-zero."" ""The three flavours can be compared to ice cream where you have one scoop containing strawberry, chocolate and vanilla. Three flavours are always present but in different ratios, and the changing ratio-and the weird behaviour of the particle-can only be explained by neutrinos having a mass."" The concept that neutrinos have mass is a relatively new one with the discovery in 1998 earning Professor Takaaki Kajita and Professor Arthur B. McDonald the 2015 Nobel Prize in Physics. Even so, the Standard Model used by modern physics has yet to be updated to assign neutrinos a mass. The study, published today in Physical Review Letters by researchers from UCL, Universidade Federal do Rio de Janeiro, Institut d'Astrophysique de Paris and Universidade de Sao Paulo, sets an upper limit for the mass of the lightest neutrino for the first time. The particle could technically have no mass as a lower limit is yet to be determined. The team used an innovative approach to calculate the mass of neutrinos by using data collected by both cosmologists and particle physicists. This included using data from 1.1 million galaxies from the Baryon Oscillation Spectroscopic Survey (BOSS) to measure the rate of expansion of the universe, and constraints from particle accelerator experiments. ""We used information from a variety of sources including space- and ground-based telescopes observing the first light of the Universe (the cosmic microwave background radiation), exploding stars, the largest 3D map of galaxies in the Universe, particle accelerators, nuclear reactors, and more,"" said Dr Loureiro. ""As neutrinos are abundant but tiny and elusive, we needed every piece of knowledge available to calculate their mass and our method could be applied to other big questions puzzling cosmologists and particle physicists alike."" The researchers used the information to prepare a framework in which to mathematically model the mass of neutrinos and used UCL's supercomputer, Grace, to calculate the maximum possible mass of the lightest neutrino to be 0.086 eV (95% CI), which is equivalent to 1.5 x 10-37 Kg. They calculated that three neutrino flavours together have an upper bound of 0.26 eV (95% CI). Second author, PhD student Andrei Cuceu (UCL Physics & Astronomy), said: ""We used more than half a million computing hours to process the data; this is equivalent to almost 60 years on a single processor. This project pushed the limits for big data analysis in cosmology."" The team say that understanding how neutrino mass can be estimated is important for future cosmological studies such as DESI and Euclid, which both involve teams from across UCL. The Dark Energy Spectroscopic Instrument (DESI) will study the large scale structure of the universe and its dark energy and dark matter contents to a high precision. Euclid is a new space telescope being developed with the European Space Agency to map the geometry of the dark Universe and evolution of cosmic structures. Professor Ofer Lahav (UCL Physics & Astronomy), co-author of the study and chair of the UK Consortiums of the Dark Energy Survey and DESI said: ""It is impressive that the clustering of galaxies on huge scales can tell us about the mass of the lightest neutrino, a result of fundamental importance to physics. This new study demonstrates that we are on the path to actually measuring the neutrino masses with the next generation of large spectroscopic galaxy surveys, such as DESI, Euclid and others."" The research was funded by National Council for Scientific and Technological Development (CNPq) Science without Borders (Brazil), the Royal Astronomical Society, the UK Science and Technology Facilities Council (STFC), the Royal Society and the European Research Council. ",Space & Time,0.10000835247774563
23,Science Daily,Storms on Jupiter Are Disturbing the Planet's Colorful Belts,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822130438.htm,"   Thanks to coordinated observations of the planet in January 2017 by six ground-based optical and radio telescopes and NASA's Hubble Space Telescope, a University of California, Berkeley, astronomer and her colleagues have been able to track the effects of these storms -- visible as bright plumes above the planet's ammonia ice clouds -- on the belts in which they appear. The observations will ultimately help planetary scientists understand the complex atmospheric dynamics on Jupiter, which, with its Great Red Spot and colorful, layer cake-like bands, make it one of the most beautiful and changeable of the giant gas planets in the solar system. One such plume was noticed by amateur astronomer Phil Miles in Australia a few days before the first observations by the Atacama Large Millimeter/Submillimeter Array (ALMA) in Chile, and photos captured a week later by Hubble showed that the plume had spawned a second plume and left a downstream disturbance in the band of clouds, the South Equatorial Belt. The rising plumes then interacted with Jupiter's powerful winds, which stretched the clouds east and west from their point of origin. Three months earlier, four bright spots were seen slightly north of the North Equatorial Belt. Though those plumes had disappeared by 2017, the belt had since widened northward, and its northern edge had changed color from white to orangish brown. ""If these plumes are vigorous and continue to have convective events, they may disturb one of these entire bands over time, though it may take a few months,"" said study leader Imke de Pater, a UC Berkeley professor emerita of astronomy. ""With these observations, we see one plume in progress and the aftereffects of the others."" The analysis of the plumes supports the theory that they originate about 80 kilometers below the cloud tops at a place dominated by clouds of liquid water. A paper describing the results has been accepted for publication in the Astronomical Journal and is now online. Into the stratosphere Jupiter's atmosphere is mostly hydrogen and helium, with trace amounts of methane, ammonia, hydrogen sulfide and water. The top-most cloud layer is made up of ammonia ice and comprises the brown belts and white zones we see with the naked eye. Below this outer cloud layer sits a layer of solid ammonium hydrosulfide particles. Deeper still, at around 80 kilometers below the upper cloud deck, is a layer of liquid water droplets. The storm clouds de Pater and her team studied appear in the belts and zones as bright plumes and behave much like the cumulonimbus clouds that precede thunderstorms on Earth. Jupiter's storm clouds, like those on Earth, are often accompanied by lightning. Optical observations cannot see below the ammonia clouds, however, so de Pater and her team have been probing deeper with radio telescopes, including ALMA and also the Very Large Array (VLA) in New Mexico, which is operated by the National Science Foundation-funded National Radio Astronomy Observatory. ALMA array's first observations of Jupiter were between Jan. 3 and 5 of 2017, a few days after one of these bright plumes was seen by amateur astronomers in the planet's South Equatorial Belt. A week later, Hubble, the VLA, the Gemini, Keck and Subaru observatories in Hawaii and the Very Large Telescope (VLT) in Chile captured images in the visible, radio and mid-infrared ranges. De Pater combined the ALMA radio observations with the other data, focused specifically on the newly brewed storm as it punched through the upper deck clouds of ammonia ice. The data showed that these storm clouds reached as high as the tropopause -- the coldest part of the atmosphere -- where they spread out much like the anvil-shaped cumulonimbus clouds that generate lightning and thunder on Earth. ""Our ALMA observations are the first to show that high concentrations of ammonia gas are brought up during an energetic eruption,"" de Pater said. The observations are consistent with one theory, called moist convection, about how these plumes form. According to this theory, convection brings a mix of ammonia and water vapor high enough -- about 80 kilometers below the cloud tops -- for the water to condense into liquid droplets. The condensing water releases heat that expands the cloud and buoys it quickly upward through other cloud layers, ultimately breaking through the ammonia ice clouds at the top of the atmosphere. The plume's momentum carries the supercooled ammonia cloud above the existing ammonia-ice clouds until the ammonia freezes, creating a bright, white plume that stands out against the colorful bands encircling Jupiter. ""We were really lucky with these data, because they were taken just a few days after amateur astronomers found a bright plume in the South Equatorial Belt,"" said de Pater. ""With ALMA, we observed the whole planet and saw that plume, and since ALMA probes below the cloud layers, we could actually see what was going on below the ammonia clouds."" Hubble took images a week after ALMA and captured two separate bright spots, which suggests that the plumes originate from the same source and are carried eastward by the high altitude jet stream, leading to the large disturbances seen in the belt. Coincidentally, three months before, bright plumes had been observed north of the Northern Equatorial Belt. The January 2017 observations showed that that belt had expanded in width, and the band where the plumes had first been seen turned from white to orange. De Pater suspects that the northward expansion of the North Equatorial Belt is a result of gas from the ammonia-depleted plumes falling back into the deeper atmosphere. De Pater's colleague and co-author Robert Sault of the University of Melbourne in Australia used special computer software to analyze the ALMA data to obtain radio maps of the surface that are comparable to visible-light photos taken by Hubble. ""Jupiter's rotation once every 10 hours usually blurs radio maps, because these maps take many hours to observe,"" Sault said. ""In addition, because of Jupiter's large size, we had to 'scan' the planet, so we could make a large mosaic in the end. We developed a technique to construct a full map of the planet."" The VLT data were contributed by Leigh Fletcher and Padraig Donnelly of the University of Leicester in the United Kingdom, while Glenn Orton and James Sinclair of the Jet Propulsion Laboratory in California and Yasuma Kasaba of Tokyo University in Japan supplied the SUBARU data. Gordon Bjoraker of the NASA Goddard Space Flight Center in Maryland and Máté Ádámkovics of Clemson University in South Carolina analyzed the Keck data. The work was supported by a NASA Planetary Astronomy award (NNX14AJ43G) and a Solar System Observations award (80NSSC18K1001). ",Space & Time,0.09990687362422271
24,Stanford,Scientist models exoplanet’s atmosphere,Science,2019-08-26,-,https://news.stanford.edu/2019/08/22/scientist-models-exoplanets-atmosphere/,"   In the search for life beyond our galaxy, many scientists have their eyes turned toward orbs like Earth: rocky planets. So after the Transiting Exoplanet Survey Satellite (TESS) detected a rocky planet slightly larger than Earth last fall, a team of researchers launched a campaign to take additional images with the Spitzer Space Telescope, the only telescope currently in space that can directly detect a planet’s infrared light. The telescope produced pictures smaller than 1 pixel – 1/94 of an inch – like a speck of dust with which to make predictions about the planet’s habitability.  Go to the web site to view the video.  Video by NASA/JPL-Caltech/R. Hurt (IPAC)    This artist’s animation depicts the exoplanet LHS 3844b, which is 1.3 times the mass of Earth and orbits an M dwarf star.  Looking at several orbits of the planet allowed scientists to map the temperature of its surface and create models of its atmosphere – capabilities that scientists are only just starting to develop for rocky planets. Much of what researchers learn about exoplanets is based on what they know about the stars they circle. “People say we only know a planet as well as we know the star, because we’re basically inferring things based on what we’re measuring about the star,” said Laura Schaefer, an assistant professor of geological sciences at Stanford’s School of Earth Energy & Environmental Sciences (Stanford Earth) and co-author on a study characterizing a planet that was published in Nature Aug. 19. The team’s analyses show that this planet, LHS 3844b, located 48.6 light-years away, is much hotter than Earth and may be covered in dark volcanic rock. It orbits a star smaller than the sun in just 11 hours. The star is an M dwarf – the most common and long-lived type of star that could therefore potentially host a high percentage of the galaxy’s planets – and the rocky planet’s atmosphere is the first orbiting an M dwarf to be characterized. Researchers found the planet has little to no atmosphere, and thus could not support life – an important finding for understanding atmospheres of similar rocky planets around M dwarfs. Stanford News Service spoke with Schaefer to understand more about the findings and what they mean.   Why do scientists want to explore exoplanets? Very broadly, it’s to try to understand planet formation better. We understand in pretty good detail the planets in our own solar system, but that only gives us one snapshot of how planet formation works. By going out and finding planets around other stars, we have discovered many crazy new things that we didn’t realize happened when planets formed. For example, we found one class of planets that nobody expected to exist, called hot Jupiters. These are actually the first kind of exoplanets that were discovered. The other major goal with looking at exoplanets is to find another planet like Earth that might have life on it. I focus on the smaller rocky planets, not the big gas giants. The goal is eventually to find a planet in what we call the “habitable zone,” which is a region of orbital space where liquid water might be stable on the surface of a planet like Earth. In order to determine if a planet has life, we need to be able to measure its atmosphere and see if life has influenced it, as we know it has here on Earth, where our atmosphere of oxygen is produced by life. Before life was widespread on Earth, its atmosphere was very different. So we think if we can look at the atmospheres of planets in the habitable zone and determine what they’re made of, then maybe we could say if those planets have life. This is a first baby step on the way to doing that.   How did the team map the temperature of a planet that’s so far away? By observing the planet at different points along its orbit, we see different fractions of the day side of the planet. If we’re looking at the light from the star, we see a big dip when the planet is passing in front of the star, which we call the transit. As it goes behind the star, we see a smaller dip that we call the secondary eclipse. The amount of this dip gives us a constraint on the surface temperature of the planet. We can look also for variations in the stellar light that give us a temperature map with the day side and night side. We can constrain the orbit pretty well; we know how close it is to its star and we know the star’s brightness, so we know essentially how much light the planet is receiving from the star. We use models of the star’s evolution to try to understand how much light that planet has received over its entire lifetime.   What did the data tell you about its atmosphere? An atmosphere can take the heat from the star and move it around. If the planet doesn’t have an atmosphere, then you would expect a big contrast between the day side and the night side. Two signatures of the atmosphere are a shift in the highest temperature point and a lower amplitude of this signature, which indicates that heat is being moved around. With this particular planet – one of the first rocky planets from which this kind of measurement has been able to be done – we found a large temperature contrast between the day and night side and no offset of that temperature point. That indicated that the atmosphere had to be really thin. My contribution was to then determine if the atmosphere was stable by running models to look at how much atmosphere the planet could possibly lose for a range of parameters over the planet’s lifetime. If the planet started off with about the same amount of gases, such as water and carbon dioxide, as Earth or even more than that, then it would have lost all of them over the course of its lifetime due to the star heating up the atmosphere and causing it to escape – that is one mechanism for atmosphere escape. We looked at another model that constrained the lower end of atmosphere the planet could have and determined these thin atmospheres are not stable on this planet.   Why do you focus your research on atmospheric escape models? I started working on understanding early planetary atmospheres a number of years ago, before I even started grad school. To me, it’s one of the most interesting problems because it’s the early state of the planet that seems to really set how it evolves over its lifetime. That’s really important for Earth, because we don’t know a lot about its early history in the first half-billion years – but that’s the time period when life started. So my perspective is you have to start at the beginning. And that actually means starting before the planet forms and trying to understand all the processes that go into making the planet and what sets the initial conditions that it eventually evolves from. By looking at these hot, rocky exoplanets, we get to test our understanding of these processes. Additional co-authors of the study are affiliated with the Massachusetts Institute of Technology, the University of Maryland, the California Institute of Technology, the University of Texas at Austin, Vanderbilt University, the Space Telescope Science Institute and the Center for Astrophysics | Harvard & Smithsonian. The work was supported by NASA and the John Templeton Foundation. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Space & Time,0.09984722038745365
25,Stanford,"The power of language: How words shape people, culture",Others,2019-08-26,-,https://news.stanford.edu/2019/08/22/the-power-of-language-how-words-shape-people-culture/,"   Speaking, writing and reading is integral to everyday life, where language is the primary tool for expression and communication. Studying how people use language – what words and phrases they unconsciously choose and combine – can help us better understand ourselves and why we behave the way we do. Linguistics scholars seek to determine what is unique and universal about the language we use, how it is acquired and the ways it changes over time. They consider language as a cultural, social and psychological phenomenon. “Understanding why and how languages differ tells about the range of what is human,” said Dan Jurafsky, the Jackson Eli Reynolds Professor in Humanities and chair of the Department of Linguistics in the School of Humanities and Sciences at Stanford. “Discovering what’s universal about languages can help us understand the core of our humanity.” The stories below represent some of the ways linguists have investigated many aspects of language, including its semantics and syntax, phonetics and phonology, and its social, psychological and computational aspects. ",Society,0.09306568304481531
26,Science Daily,Early Disease Detection: Individual Exosomes Identified,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092254.htm,"   Exosomes are released from all cells in the body. They convey protein and nucleic acid cargos between the cells as a form of intercellular communication, and they represent potential circulating biomarkers for tumor progression and metastasis, as well as for early detection of neurodegenerative disease. In order to use exosomes as biomarkers of diseases in different tissues it is vital to distinguish them according to their surface protein complements. Researchers at Uppsala University and Vesicode AB, along with collaborators, have developed a method that can map surface protein complements on large numbers of individual exosomes. The novel proximity-dependent barcoding assay (PBA) reveals the surface protein composition of individual exosomes using antibody-DNA conjugates and next-generation sequencing. The method identifies proteins on individual exosomes using micrometer-sized, uniquely tagged single-stranded DNA clusters generated by rolling circle amplification. ""This technology will not only benefit researchers studying exosomes, but also enable high-throughput biomarker discovery. We will further develop and validate the PBA technology and provide service to researchers starting later this year. We believe single exosome analysis will allow this exciting class of biomarkers to reach its full potential,"" says Di Wu, researcher and inventor of the PBA technology and founder of Vesicode AB, commercializing the technique. ""This new technology will allow large-scale screens for biomarkers in disease, complementing a panel of methods for sensitive and specific detection of exosomes that we have previously established,"" says Masood Kamali-Moghaddam one of the group leaders at the Molecular Tools unit at Uppsala University. ",Health,0.09137219591520072
27,Science Daily,A Lack of Background Knowledge Can Hinder Reading Comprehension,Society,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092303.htm,"   ""Background knowledge plays a key role in students' reading comprehension -- our findings show that if students don't have sufficient related knowledge, they'll probably have difficulties understanding text,"" says lead researcher Tenaha O'Reilly of Educational Testing Service (ETS)'s Center for Research on Human Capital in Education. ""We also found that it's possible to measure students' knowledge quickly by using natural language processing techniques. If a student scores below the knowledge threshold, they'll probably have trouble comprehending the text."" Previous research has shown that students who lack sufficient reading skills, including decoding and vocabulary, fare poorly relative to their peers. But the research of O'Reilly and ETS colleagues Zuowei Wang and John Sabatini suggests that a knowledge threshold may also be an essential component of reading comprehension. The researchers examined data from 3,534 high-school students at 37 schools in the United States. The students completed a test that measured their background knowledge on ecosystems. For the topical vocabulary section of the test, the students saw a list of 44 words and had to decide which were related to the topic of ecosystems. They also completed a multiple-choice section that was designed to measure their factual knowledge. Then, after reading a series of texts on the topic of ecosystems, the students completed 34 items designed to measure how well they understood the texts. These comprehension items tapped into their ability to summarize what they had read, recognize opinions and incorrect information, and apply what they had read to reason more broadly about the content. The researchers used a statistical technique called broken-line regression -- often used to identify an inflection point in a data set -- to analyze the students' performance. The results revealed that a background-knowledge score of about 33.5, or about 59% correct, functioned as a performance threshold. Below this score, background knowledge and comprehension were not noticeably correlated; above the threshold score, students' comprehension appeared to increase as their background knowledge increased. Additional results indicated that the pattern could not be fully explained by the level of students' knowledge on a different topic -- what mattered was their background knowledge of ecosystems. The researchers found that students' ability to identify specific keywords was a fairly strong predictor whether they would perform above or below the threshold. That is, correctly identifying ecosystems, habitat, and species as topically relevant was more strongly linked with students' comprehension than was identifying bioremediation, densities, and fauna. The findings underscore the importance of having reached a basic knowledge level to be able to read and comprehend texts across different subjects: ""Reading isn't just relevant to English Language Arts classes but also to reading in the content areas,"" says O'Reilly. ""The Common Core State Standards highlight the increasing role of content area and disciplinary reading. We believe that the role of background knowledge in students' comprehension and learning might be more pronounced when reading texts in the subject areas."" The researchers plan to explore whether a similar kind of knowledge threshold emerges in other topic areas and domains; they note that it will be important to extend the research by focusing on diverse measures and populations. If the pattern holds, the findings could have important applications for classroom teaching, given the availability of knowledge assessments that can be administered without taking valuable time away from instruction. ""If we can identify whether a given student does not have sufficient knowledge to comprehend a text, then teachers can provide background material -- for example, knowledge maps -- so that students have a context for the texts they are about to read,"" O'Reilly concludes.. ",Society,0.08746981862028289
28,Science Daily,Cleaning Pollutants from Water With Pollen and Spores -- Without the 'Achoo!',Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092326.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""Even very low levels of certain compounds, such as hormones, pharmaceuticals or those in household and personal care products, can cause toxic effects. However, they often can escape normal cleanup processes at wastewater treatment plants,"" says Andrew Boa, Ph.D., whose lab is working on the pollen project. ""We're trying to find alternative ways to remove these chemicals from water so we can reduce the amount going into the environment."" The project is part of the larger ""Sullied Sediments"" program in which Boa and many other scientists are assessing pollutant levels in sediments from European waterways, with a view to assessing dredged sediment, managing sediment reuse and reducing future contaminations. These contaminants include pharmaceuticals such as the pain reliever diclofenac and household chemicals such as triclosan, an antimicrobial compound used in toothpaste, and other personal care products. Some of these chemicals, including triclosan, are either banned or their use heavily restricted. The European Union will begin officially monitoring levels of all of these ""Watch List"" chemicals from 2020 onwards. The spore grains used in the study are extracted from Lycopodium clavatum -- the common club moss. In their natural state, each of these microscopic grains carries genetic material inside a hard shell that's coated with an outer layer of wax and proteins, explains Aimilia Meichanetzoglou, a doctoral student in Boa's lab at the University of Hull. Boa first became interested in pollen thanks to his work with Grahame Mackenzie, Ph.D., a Hull professor (now emeritus) who developed the original method to form non-allergenic, hollowed-out pollen and spore shells. Mackenzie's company, Sporomex, uses the inert shells to encapsulate active ingredients for controlled release in pharmaceutical, food, cosmetic and medical applications. Boa has taken the concept in an entirely different direction. When he and Meichanetzoglou were studying the empty shells' interactions with a variety of chemicals, they noticed that some of the compounds became adsorbed, or stuck to, the surface of the shells. Boa realized this stickiness could potentially be used to grab low levels of pollutants, and so he pursued this type of application. Meichanetzoglou uses hydrolysis to rid the pollen of its genetic cargo and waxy coat, which makes the grains hypoallergenic. To target particular pollutants, she can vary the hydrolysis conditions and make modifications to the surface of the grains. For example, to remove phosphate, which is used in many fertilizers, Meichanetzoglou deposits iron oxide on the surface of the shells. Iron oxide reacts with the phosphate to form insoluble iron phosphate, which precipitates out of the water and gets adsorbed onto the grains. The researchers found that the grains could remove almost all of the phosphate from water samples and nearly 80% of several other pollutants. Treating wastewater will require consideration of various factors, such as scale and the degree of contamination. For example, homes that use a septic tank; particular buildings with a high level of pharmaceuticals in their waste water, such as hospitals or care homes for the elderly; or municipal waste water treatment plants that serve a whole city will all have different requirements. Boa is exploring options with local water authorities for implementation of this technology. Boa has purposely tried to keep process costs low to make the method commercially feasible. The moss is already harvested for other applications, Boa notes, and it can grow on poor-quality soil, so it won't compete with food crops for arable land. His collaborators have also begun testing the bioavailability of pollutants captured by the grains. ",Matter & Energy,0.08722152766811797
29,MIT News,What’s the best way to cut vehicle greenhouse-gas emissions?,Research,2019-08-26,-,http://news.mit.edu/2019/lightweight-vehicle-electric-emissions-0826,"  Policies to encourage reductions in greenhouse gas emissions tend to stress the need to switch as many vehicles as possible to electric power. But a new study by MIT and the Ford Motor Company finds that depending on the location, in some cases an equivalent or even bigger reduction in emissions could be achieved by switching to lightweight conventional (gas-powered) vehicles instead — at least in the near term. The study looked at a variety of factors that can affect the relative performance of these vehicles, including the role of low temperatures in reducing battery performance, regional differences in average number of miles driven annually, and the different mix of generating sources in different parts of the U.S. The results are being published today in the journal Environmental Science & Technology, in a paper by MIT Principal Research Scientist Randolph Kirchain, recent graduate Di Wu PhD ’18, graduate student Fengdi Guo, and three researchers from Ford. The study combined a variety of datasets to examine the relative impact of different vehicle choices down to a county-by-county level across the nation. It showed that while electric vehicles provide the greatest impact in reducing greenhouse gas emissions for most of the country, especially on both coasts and in the south, significant parts of the Midwest had the opposite result, with lightweight gasoline-powered vehicles achieving a greater reduction. The biggest factor leading to that conclusion was the mix of generating sources going into the grid in different regions, Kirchain says. That mix is “cleaner” on both the East and West coasts, with higher usage of renewable energy sources and relatively low-emissions natural gas, while in the upper Midwest there is still a much higher proportion of coal-burning power plants. That means that even though electric vehicles produce no greenhouse emissions while they are being driven, the process of recharging the car’s batteries results in significant emissions. In those locations, buying a lightweight car, defined as one whose structure is built largely from aluminum or specialized lightweight steel, would actually result in fewer emissions than buying a comparable electric car, the study found. The research was made possible by Ford’s collection of vehicle-performance data from about 30,000 cars, over a total of about 300 million miles of driving. They come from conventional midsize conventional gasoline cars, and the researchers used standard modeling techniques to calculate the performance of equivalent vehicles that were either hybrid-electric, battery-electric, or lightweight versions of conventional cars.  “We tried to add as much spatial resolution as possible, compared to other studies in the literature, to try to get a sense of the combined effects” of the various factors of temperature, the grid, and driving conditions, Kirchain explains. That combination of data showed, among other things, that “some of the areas with more carbon-heavy grids also happen to be colder, and somewhat more rural,” he says. “All three of those things can tilt emissions in a negative way for electric vehicles” in terms of their impact on reducing emissions. The combined effects are strongest in parts of Wisconsin and Michigan, where lightweight cars would have a significant advantage over EVs in reducing emissions, the study showed. The impact of cold weather on battery performance, he says, “is something that is discussed in the EV literature, but not as much in the popular discussions of the topic.” Conversely, gasoline-powered vehicles suffer an efficiency penalty in urban driving, but they have lower emissions in regions that are more rural and spread out. The data on car performance the team had to work with thanks to their collaboration with Ford researchers “was unique,” Kirchain says. “In the past, a ‘large’ study of this type would be a few dozen vehicles,” and those would mainly come from people who volunteered to share their data and therefore were more likely to be concerned about environmental impact. The extensive Ford data, by contrast, provide “a broader cross-section of drivers and driving conditions.” Kirchain stresses that the intent of this study is not in any way to minimize the importance of switching over ground transportation to electric power in order to curb greenhouse emissions. “We’re not trying to undermine the fact that electrification is the long-term solution — and the short-term solution for most of the country,” he says. But over the next few decades, which is considered a critical period in determining the planet’s climate outcomes, it’s important to know what measures will actually be most effective in reducing carbon emissions in order to set policies and incentives that will produce the best outcomes, he says. The relative advantage of lightweight vehicles compared to electric ones, according to their modeling, “goes down over time, as the grid improves,” he says. “But it doesn’t go away completely until you get to close to 2050 or so.” Lightweight aluminum is now used in the Ford F-150 pickup truck, and in the all-electric Tesla sedans. Currently, there are no high-volume lightweight gasoline-powered midsize cars on the market in the U.S., but they could be built if incentives similar to those used to encourage the production of electric cars were in place, Kirchain suggests. Right now, he says, the U.S. has “a patchwork of regulations and incentives that are providing extra incentives for electrification.” But there are certain parts of the country, he says, where it would make more sense to provide incentives “for any option that provides sufficient fuel savings, not just for electrification,” he says. “At least for the north central part of the country, policymakers should consider a more nuanced approach,” he adds. “This is a significant advance,” says Heather MacLean, professor of civil and mineral engineering at the University of Toronto, who was not associated with this work. This study, she says, “illustrates the importance of the regional disaggregation in the analysis, and that if it were absent results would be incorrect. This is an unequivocal call for regional policies that use the latest research to build rational agendas, rather than prescribing overarching global solutions.” The research team included Robert De Kleine, Hyung Chul Kim, and Timothy Wallington of the Research and Innovation Center of Ford Motor Company, in Dearborn, Michigan. ",Matter & Energy,0.08522207557431176
30,Science Daily,The Case for Retreat in the Battle Against Climate Change,Science,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822141856.htm,"   That's the case for carefully planned ""managed retreat"" made by three environmental researchers in an article published Aug. 22 in the Policy Forum section of the journal Science. The article was written by lead author A.R. Siders of the University of Delaware, with co-authors Miyuki Hino and Katharine J. Mach of Stanford University and the University of Miami. ""We need to stop picturing our relationship with nature as a war,"" said Siders, who is a core faculty member of UD's Disaster Research Center and an assistant professor of public policy and administration and of geography. ""We're not winning or losing; we're adjusting to changes in nature. Sea levels rise, storms surge into flood plains, so we need to move back."" Moving away from coastal and other endangered areas usually occurs after disaster strikes, she said, with emergency evacuations and their aftermath often handled inefficiently and haphazardly. Instead, the researchers argue that retreating from those areas should be done thoughtfully, with planning that is strategic as well as managed. ""Retreat is a tool that can help achieve societal goals like community revitalization, equity and sustainability if it is used purposefully,"" Siders said. ""People sometimes see retreat as defeatist, but I see it as picking your battles."" In the Science paper, the researchers point out that retreat is a difficult and complex issue for many reasons, including the short-term economic gains of coastal development, subsidized insurance rates and disaster recovery costs, and people's attachment to the place where they live and to the status quo. Also, when disaster strikes, the more affluent residents are more able to relocate, often leaving behind those who don't have the financial resources to move. ""No matter the circumstances, moving is hard,"" Hino said. ""People have chosen where to live for a reason, and it is often difficult to find a place to move to that meets all their social, cultural and financial requirements. ""One major challenge with retreat is that we're so focused on getting people out of harm's way, we miss the chance to help them move to opportunity."" The researchers take the long view, noting that retreat may be the answer to climate change in some areas, but it may not be a step that's necessary this year or even this decade. ""The challenge is to prepare for long-term retreat by limiting development in at-risk areas,"" they write, and making plans for further action based on responding to specific triggers and constantly monitoring and evaluating conditions. ""The story of retreat as a climate response is just beginning,"" Mach said. ""Retreat is compelling because it brings together so many aspects of how societies work, what individuals are trying to achieve and what it takes to ensure preparedness and resilience in a changing climate."" The paper makes note of a variety of areas where additional work is needed, including coordination of various levels of government and support for relocation assistance programs. First, Siders said, communities must identify which areas they most want to protect and how to encourage and assist relocation. ""Managed retreat needs to be embedded in larger conversations and social programs,"" she said. ""Retreat can't be just about avoiding risk. It needs to be about moving toward something better."" ",Environment,0.08474990733346884
31,MIT News,Using CRISPR to program gels with new functions,Research,2019-08-22,-,http://news.mit.edu/2019/crispr-edit-materials-gels-0822,"  The CRISPR genome-editing system is best-known for its potential to correct disease-causing mutations and add new genes into living cells. Now, a team from MIT and Harvard University has deployed CRISPR for a completely different purpose: creating novel materials, such as gels, that can change their properties when they encounter specific DNA sequences. The researchers showed they could use CRISPR to control electronic circuits and microfluidic devices, and to release drugs, proteins, or living cells from gels. Such materials could be used to create diagnostic devices for diseases such as Ebola, or to deliver treatments for diseases such as irritable bowel disease. “This study serves as a nice starting point for showing how CRISPR can be utilized in materials science for a really wide range of applications,” says James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering, and the senior author of the study. The lead authors of the study, which appears in the Aug. 22 online edition of Science, are MIT graduate students Max Atti English, Luis Soenksen, and Raphael Gayet, and postdoc Helena de Puig. DNA interactions CRISPR is based on DNA-cutting proteins called Cas enzymes, which bind to short RNA guides that direct them to specific areas of the genome. Cas cuts DNA in those locations, deleting a gene or allowing new genetic sequences to be introduced. Over the past several years, much research has been devoted to developing CRISPR as a gene-editing tool for treating disease by cutting out or repairing faulty genes. The MIT and Harvard team set out to adapt it for creating materials that could respond to external cues such as the presence of a certain sequence of DNA. For this work, they used an enzyme known as Cas12a, which can be programmed to bind to specific sequences of double-stranded DNA by simply changing the guide RNA sequence that is given along with the enzyme. Once Cas12a encounters a target DNA sequence, also called a trigger, it cleaves the double-stranded DNA and transforms into an enzyme that can slice any single-stranded DNA it encounters. “By incorporating DNA into materials, you can use this enzyme to control the properties of the materials in response to a specific biological cue in the environment,” English says. The researchers took advantage of this to design gels that incorporate single-stranded DNA in key functional or structural roles. In one example, they created a gel made of polyethylene glycol (PEG) and used DNA to anchor enzymes or other large biomolecules to the gel. When activated by a trigger sequence, Cas12a cuts the DNA anchors, releasing the payload. That type of gel could be useful for releasing therapeutic compounds such as drugs or growth factors, the researchers say. In another example, they created an acrylamide gel in which single-stranded DNA forms an integral part of the gel structure. In that case, when Cas12a is activated by the trigger, the entire gel breaks down, enabling the release of larger cargoes such as cells or nanoparticles. “In that context, we consider the single-stranded DNA as a structural scaffold,” Gayet says. “The enzyme is able to catalyze the cleavage of the single-stranded DNA, which acts as a structural linker, and release all of those molecules.” The researchers are now exploring the possibility of using this approach to deliver engineered bacterial cells to help treat gastrointestinal diseases. Inexpensive diagnostics The researchers also created two CRISPR-controlled diagnostic devices, one based on an electronic circuit and the other on a microfluidic chip. To create the electronic circuit, the researchers designed a gel that includes single-stranded DNA and a material called carbon black, which conducts electricity. When attached to the surface of an electrode, this conductive gel allows electrical current to flow. However, if Cas12a is activated by a trigger sequence, such as a strand of viral DNA from a blood sample, the gel becomes detached from the electrode and current stops flowing. For their microfluidic sensor, the researchers created a DNA-containing gel that acts as a valve that controls the flow of a solution through the microfluidic channel. If the solution contains a blood sample with a target DNA sequence, the gel breaks down, turning off the valve, and the solution stops flowing. This microfluidic sensor can be connected to an RFID chip, allowing it to wirelessly transmit the results of the test. “A health care worker can be monitoring dozens of patients, and the presence or absence of the Ebola trigger will automatically relay a binary signal,” Soenksen says. While the researchers used fluid samples containing Ebola virus RNA to test this approach, it could also be adapted to detect other infectious diseases, as well as cancer cells circulating in a patient’s bloodstream. Philip LeDuc, a professor of mechanical engineering at Carnegie Mellon University, describes the work as “tremendously creative.” “This is a very non-obvious intersection of two different fields, and the influence of this work will be far-reaching,” says LeDuc, who was not involved in the study. “This transdisciplinary work may enable an entire new generation of approaches for applications from building artificial organs to improving the environment.” The research was funded by the Defense Threat Reduction Agency, the Paul G. Allen Frontiers Group, and the Wyss Institute for Biologically Inspired Engineering at Harvard University. ",Health,0.08400103187437727
32,Science Daily,Northern White Rhino Eggs Successfully Fertilized,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104835.htm,"   ""We were surprised by the high rate of maturation achieved as we do not get such high rate (comparable to what we get with horse oocytes) with southern white rhino females in European zoos. The semen of Saut was very difficult to work with and to find three live sperms needed for the eggs of Najin we had to thaw two batches of semen. Now the injected oocytes are incubated and we need to wait to see if any viable embryo develop to the stage where it can be cryopreserved for later transfer,"" said Cesare Galli of Avantea in Cremona (Italy) who led the fertilization procedure. The international research consortium to save the northern white rhino from extinction is led by Thomas Hildebrandt from the Leibniz Institute for Zoo and Wildlife Research (IZW). Avantea is responsible for maturing the egg cells and creating viable embryos, further key project partners are Dvur Kralove Zoo, Ol Pejeta Conservancy and the Kenya Wildlife Service. The results of possible embryo development are to be announced around September 10th. The research program (BioRescue) is funded by the German Federal Ministry of Education and Research (BMBF). ",Environment,0.0838191169841635
33,IEEE,"To Stop Transformers From Exploding, Make Their Tanks Flexible",Matter & Energy,2019-08-23,-,https://spectrum.ieee.org/energywise/energy/the-smarter-grid/to-stop-transformers-from-exploding-make-their-tanks-flexible,"      When an eerie blue glow lit up the sky above New York City last December, some were disappointed to learn that aliens weren’t involved. The cause was, in fact, terrestrial: a transformer had exploded at a local power plant.  For the most part, transformers—which help power companies transmit electricity efficiently by altering voltages—are relatively safe. Fewer than one percent explode—but those explosions can be deadly, and result in flying projectiles, toxic fires, or oil spills. Transformers rupture due to a buildup of excess pressure in the tank in which they are encased, which is usually filled with mineral oil that acts as a coolant. Contaminants within the oil, the degradation of transformer parts, and electrical storms can all cause a fault, called an internal arc, that results in a rapid release of energy.  “When you have an arc inside the transformer, it heats up the oil and the oil burns to create a gas, which causes high pressure,” explains Samuel Brodeur, a senior mechanical engineer at the power and technology firm ABB, who is based in Varennes, Canada. Conventional tank designs are limited in their capacity to withstand such fault energies—which in severe cases, can reach as high as 150 megajoules (MJ), the equivalent of 150 sticks of dynamite—and so Brodeur and his colleagues have spent the past seven years working to devise a stronger, more resilient transformer tank. Their solution, described in a paper published 12 June in IEEE Transactions on Power Delivery, is called TXpand. The idea is startlingly simple: design a tank that’s flexible enough to deform to absorb all that extra pressure without rupturing. “It’s a bit like blowing [up] a balloon,” says Jean-Bernard Dastous, a research scientist from Canadian power supplier Hydro-Québec, which collaborated with ABB on the project. “If it’s very rigid, it will be difficult to expand the balloon. But if it’s made of a very flexible material, it’s easier for you to inflate it.” ABB, which currently supplies roughly 70 percent of Hydro-Québec’s transformers, alters tank flexibility by using different types of steel, varying the thickness of the wall and cover, and reinforcing weak points such as the corners, among other things. To design what ABB calls an “arc resistant” tank, the team first had to create a mechanical model that could predict the pressure at which a given tank would deform and subsequently rupture, based on its size and material properties. Hydro-Québec, in particular, wanted ABB to build a tank that could withstand 20 MJ of energy without rupturing—a level that would cause a “catastrophic failure” in most transformers and one that Dastous says “would cover 95 percent of faults occurring on the network.” And so ABB plugged equations into its numerical model and spent months building a full-size tank (roughly 5 meters long, 2.5 meters wide, and 4 meters high). For safety reasons, the tank was filled with water instead of oil, and contained a replica of the active part of a transformer. The first test, carried out on a frigid winter’s day in November 2017 in an open field at Hydro-Québec’s research facilities close to Montreal, was meant to demonstrate the tank could withstand the specified 20 MJ. Until then, the highest energy levels tested were just over half that value. The team injected pressurized air measuring 200 atmospheres, which is equivalent to the pressure experienced two kilometers below sea level. The tank bulged at its sides, but did not explode. The second test, they hoped, would demonstrate that at a given pressure, the tank would rupture at a chosen point. “We wanted to make sure that failure happens at the top of the transformer because when it happens there, less oil will spill into the environment,” explains Dastous.  Following an injection of 30 MJ of energy, the test tank did exactly this, proving that “our calculations and numerical test methodologies worked,” he says. The results have enabled Hydro-Québec to come up with “new improved arc-resistant specifications” for its suppliers to follow. The specifications, to be implemented in the coming months, will hopefully lead to fewer transformers exploding. ABB, on the other hand, has since applied its TXpand solution to more than 50 transformer designs. Brodeur says: “Because we are able to prevent most of the tank rupture cases, it’s safer for the people who work around the transformer and it’s also very good for the environment because we can prevent major oil spills and toxic fires.”  Monthly newsletter on latest news & opinion for the power & energy industry, green technology, and conservation.  IEEE Spectrum’s energy, power, and green tech blog, featuring news and analysis about the future of energy, climate, and the smart grid. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Matter & Energy,0.08329207914639569
34,Science Daily,Flame Retardants -- From Plants,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092330.htm,"   The researchers will present their results at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""The best flame-retardant chemicals have been organohalogen compounds, particularly brominated aromatics,"" says Bob Howell, Ph.D., the project's principal investigator. ""The problem is, when you throw items away, and they go into a landfill, these substances can leach into the environment."" Most organohalogen flame retardants are very stable. Microorganisms in the soil or water can't degrade them, so they persist for many years in the environment, working their way up the food chain. In addition, some of the compounds can migrate out of items to which they are added, such as electronics, and enter household dust. Although the health effects of ingesting or breathing organohalogen flame retardants are largely unknown, some studies suggest they could be harmful, prompting California to ban the substances in children's products, mattresses and upholstered furniture in 2018. ""A number of flame retardants are no longer available because of toxicity concerns, so there is a real need to find new materials that, one, are nontoxic and don't persist, and two, don't rely upon petroleum,"" Howell says. His solution was to identify compounds from plants that could easily be converted into flame retardants by adding phosphorus atoms, which are known to quench flames. ""We're making compounds that are based on renewable biosources,"" he says. ""Very often they are nontoxic; some are even food ingredients. And they're biodegradable -- organisms are accustomed to digesting them."" To make their plant-derived compounds, Howell and colleagues at the Center for Applications in Polymer Science at Central Michigan University began with two substances: gallic acid, commonly found in fruits, nuts and leaves; and 3,5-dihydroxybenzoic acid from buckwheat. Using a fairly simple chemical reaction, the researchers converted hydroxyl groups on these compounds to flame-retardant phosphorus esters. Then, the team added the various phosphorus esters individually to samples of an epoxy resin, a polymer often used in electronics, automobiles and aircraft, and examined the different esters' properties with several tests. In one of these tests, the researchers showed that the new flame retardants could strongly reduce the peak heat release rate of the epoxy resin, which reflects the intensity of the flame and how quickly it is going to spread. The plant-derived substances performed as well as many organohalogen flame retardants on the market. ""As a matter of fact, they may be better,"" Howell says. ""Because gallic acid has three hydroxyl groups within the same molecule that can be converted to phosphorus esters, you don't have to use as much of the additive, which reduces cost."" The researchers also studied how the new compounds quench flames, finding that the level of oxygenation at the phosphorus atom determined the mode of action. Compounds with a high level of oxygenation (phosphates) decomposed to a substance that promoted char formation on the polymer surface, starving the flame of fuel. In contrast, compounds with a low level of oxygenation (phosphonates) decomposed to species that scavenged combustion-promoting radicals. Howell's team hasn't yet performed toxicity tests, but he says that other groups have done such studies on similar compounds. ""In general, phosphorus compounds are much less harmful than the corresponding organohalogens,"" he notes. In addition, the plant-derived substances are not as volatile and are less likely to migrate from items into household dust. Howell hopes that the new flame retardants will attract the attention of a company that could help bring them to market, he says. ",Matter & Energy,0.08314768657619304
35,Science Daily,First Direct Evidence for Mantle Plume Origin of Jurassic Flood Basalts in Southern Africa,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104821.htm,"   The great Jurassic lava flows that flooded across southern Africa and parts of East Antarctica prior to the splitting of the Pangea supercontinent make up one of the largest volcanic systems on Earth. The magma eruptions caused global environmental turmoil and the extinctions of species. The rapid origin of this Karoo flood basalt province in southern Africa has been frequently associated with the melting of a large plume that ascended from the deep mantle around 180 million years ago. However, the plume model has lacked confirmation from lava compositions that preserve a geochemical 'plume signature'. ""To our knowledge, the Luenha picrites are the first lava samples that could originate from the plume source that has been previously inferred from various geological and geophysical data on the Karoo province. Therefore they allow compositional analysis of this source,"" says Sanni Turunen, the leading author and a doctoral student at the Finnish Museum of Natural History, which is part of the University of Helsinki. In the case of the Luenha picrites, named after the research area near the Luenha River, the geochemical compositions indicate a hot magma source that is in many respects different from previously reported magma sources in the Karoo province. They show compositional similarities to magmas formed in other deep mantle plume-related volcanic provinces worldwide. ""It is very important to realise that in huge and complex volcanic systems, such as the Karoo province, large amounts of magmas may be produced from several magma sources,"" explains Daúd Jamal, professor at the Eduardo Mondlane University, in Mozambique. ""Previous studies of Karoo picrites in Africa and Antarctica by us and by other groups have suggested the generation of magmas in the upper mantle, but our new results indicate plume sources were also involved,"" adds Jussi Heinonen, an Academy of Finland fellow at the Department of Geosciences and Geography at the University of Helsinki. Importantly, the Luenha picrites appear to represent the main source of the voluminous flood basalts of southern Africa. ""We were fascinated to realise that the Luenha picrites revealed a type of magma source that was recently predicted using lava compositions, but which had not been confirmed by observational evidence,"" as characterised by Arto Luttinen, senior curator at the Finnish Museum of Natural History. According to the study, the presently available data are compatible with a plume source that has retained the composition of Earth's primitive mantle remarkably well. This is quite unusual because of the 4.5 billion year evolution of the convecting mantle. Confirmation of the age and evolution of the primitive mantle-like source of the Luenha picrites requires further constraints from future isotopic studies. ""Whatever the exact nature of the Luenha source turns out to be, we feel confident that we have uncovered rocks that help to address the complex origin of large eruptions in new detail,"" Turunen concludes. ",Matter & Energy,0.08223022253198946
36,Science Daily,Rising Summer Heat Could Soon Endanger Travelers on Annual Muslim Pilgrimage,Science,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822103834.htm,"   Hajj, or Muslim Pilgrimage, is one of the five pillars of the Muslim faith. It is an annual pilgrimage to Mecca, Saudi Arabia, that involves living in the hot weather conditions of Saudi Arabia. Muslims are expected to make the pilgrimage at least once in their lifetimes. Islam follows a lunar calendar, so the dates for Hajj change every year. But for five to seven years at a time, the trip falls over summer. A new study projecting future summer temperatures in the region around Mecca finds that as soon as 2020, summer days in Saudi Arabia could surpass the United States National Weather Service's extreme danger heat-stress threshold, at a wet-bulb temperature of 29.1 degrees C (84.3 degrees Fahrenheit). Wet-bulb temperature is a measurement combining temperature with the amount of moisture in the air. At the extreme danger threshold defined by the National Weather Service, sweat no longer evaporates efficiently, so the human body cannot cool itself and overheats. Exposure to these conditions for long periods of time, such as during Hajj, could cause heat stroke and possibly death. ""When the Hajj happens in summer, you can imagine with climate change and increasing heat-stress levels conditions could be unfavorable for outdoor activity,"" said Elfatih Eltahir, a civil and environmental engineer at Massachusetts Institute of Technology and co-author of the new study in the AGU journal Geophysical Research Letters. ""Hajj may be the largest religious tourism event,"" Eltahir said. ""We are trying to bring in the perspective of what climate change could do to such large-scale outdoor activity."" Adapting to rising temperatures Middle Eastern temperatures are rising because of climate change and scientists project them to keep rising in the coming decades. In the new study, Eltahir and his colleagues wanted to know how soon and how frequently temperatures during summer Hajj would pass the extreme danger threshold. The researchers examined historical climate models and used past data to create a projection for the future. In the past 30 years, they found that wet-bulb temperature surpassed the danger threshold 58 percent of the time, but never the extreme danger threshold. At the danger threshold, heat exhaustion is likely and heat stroke is a potential threat from extended exposure. Passing the extreme danger threshold for extended periods of time means heat stroke is highly likely. The researchers then calculated how climate change is likely to impact wet-bulb temperature in Saudi Arabia in the future. They found that in the coming decades, pilgrims will have to endure extremely dangerous heat and humidity levels in years when Hajj falls over summer. Their projections estimate heat and humidity levels during Hajj will exceed the extreme danger threshold six percent of the time by 2020, 20 percent of the time from 2045 and 2053, and 42 percent of the time between 2079 and 2086. Climate change mitigation initiatives make passing the threshold during these years less frequent, projecting one percent by 2020, 15 percent of the time between 2045 and 2053, and 19 percent of the time between 2079 and 2086, according to the study. The study authors stress that their projections are meant not to cause anxiety among pilgrims but instead to help them adapt, and to help authorities plan for safe Hajj. ""These results are not meant to spread any fears, but they are meant to inform policies about climate change, in relation to both mitigation and adaptation"" Eltahir said. ""There are ways people could adapt, including structural changes by providing larger facilities to help people perform Hajj as well as nonstructural changes by controlling the number of people who go."" ""They've provided a very compelling example of an iconic way that 2 to 3 million people per year that can be really vulnerable to what to me is the biggest underrated climate hazard -- this combination of high temp and high humidity,"" said Radley Horton, a climate scientist at Columbia University Lamont Doherty Earth Observatory who was not involved with the study. ""I believe as the century progresses if we don't reduce our greenhouse gases [this] could become every much as an existential threat as sea level rising and coastal flooding."" ",Environment,0.08158983956224024
37,Science Daily,Salt Marshes' Capacity to Sink Carbon May Be Threatened by Nitrogen Pollution,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092340.htm,"   However, a new study indicates that a common pollutant of coastal waters, nitrate, stimulates the decomposition of organic matter in salt marsh sediments that normally would have remained stable over long periods of time. This increase in decomposition, which releases CO2, could alter the capacity of salt marshes to sequester carbon over the long term. The study, led by scientists at the Marine Biological Laboratory (MBL), Woods Hole, and Northeastern University, is published in Global Change Biology. ""Traditionally, we have viewed salt marshes as resilient to nitrogen pollution, because the microbes there remove much of the nitrogen as gas through a process called denitrification,"" writes first author Ashley Bulseco, a postdoctoral scientist at the MBL. ""But this research suggests that when nitrate is abundant, a change occurs in the microbial community in salt marsh sediments that increases the microbes' capacity to degrade organic matter. This potentially reduces the ability of the marsh to store carbon,"" Bulseco writes. As global temperatures continue to rise, a number of carbon capture strategies have been proposed, including sequestering CO2 in ""blue carbon"" habitats such as salt marshes, mangroves and seagrass meadows. However, coastal nitrogen pollution is also still rising in many areas due to agricultural and urban runoff, and sewage. ""Given the extent of nitrogen loading along our coastlines, it is imperative that we better understand the resilience of salt marsh systems to nitrate, especially if we hope to rely on salt marshes and other blue carbon systems for long-term carbon storage,"" the authors write. The next phase of this research, already in progress, is to analyze the microbial community responsible for degrading carbon in a salt marsh ecosystems, especially when exposed to high concentrations of nitrate. ",Environment,0.08144079351368172
38,MIT News,Study: Climate change could pose danger for Muslim pilgrimage,Research,2019-08-22,-,http://news.mit.edu/2019/climate-muslim-hajj-0822,"  For the world’s estimated 1.8 billion Muslims — roughly one-quarter of the world population — making a pilgrimage to Mecca is considered a religious duty that must be performed at least once in a lifetime, if health and finances permit. The ritual, known as the Hajj, includes about five days of activities, of which 20 to 30 hours involve being outside in the open air. According to a new study by researchers at MIT and in California, because of climate change there is an increasing risk that in coming years, conditions of heat and humidity in the areas of Saudi Arabia where the Hajj takes place could worsen, to the point that people face “extreme danger” from harmful health effects. In a paper in the journal Geophysical Review Letters, MIT professor of civil and environmental engineering Elfatih Eltahir and two others report the new findings, which show risks to Hajj participants could already be serious this year and next year, as well as when the Hajj, whose timing varies, again takes place in the hottest summer months, which will be from 2047 to 2052 and from 2079 to 2086. This will happen even if substantial measures are taken to limit the impact of climate change, the study finds, and without those measures, the dangers would be even greater. Planning for countermeasures or restrictions on participation in the pilgrimage may thus be needed. The timing of the Hajj varies from one year to the next, Eltahir explains, because it is based on the lunar calendar rather than the solar calendar. Each year the Hajj occurs about 11 days earlier, so there are only certain spans of years when it takes place during the hottest summer months. Those are the times that could become dangerous for participants, says Eltahir, who is the Breene M. Kerr Professor at MIT. “When it comes in the summer in Saudi Arabia, conditions become harsh, and a significant fraction of these activities are outdoors,” he says. There have already been signs of this risk becoming real. Although the details of the events are scant, there have been deadly stampedes during the Hajj in recent decades: one in 1990 that killed 1,462 people, and one in 2015 that left 769 dead and 934 injured. Eltahir says that both of these years coincided with peaks in the combined temperature and humidity in the region, as measured by the “wet bulb temperature,” and the stress of elevated temperatures may have contributed to the deadly events. “If you have crowding in a location,” Eltahir says, “the harsher the weather conditions are, the more likely it is that crowding would lead to incidents” such as those. Wet bulb temperature (abbreviated as TW), which is measured by attaching a wet cloth to the bulb of a thermometer, is a direct indicator of how effectively perspiration can cool off the body. The higher the humidity, the lower the absolute temperature that can trigger health problems. At anything above a wet bulb temperature of about 77 degrees Fahrenheit, the body can no longer cool itself efficiently, and such temperatures are classified as a “danger” by the U.S. National Weather Service. A TW above about 85 F is classified as “extreme danger,” at which heat stroke, which can damage the brain, heart, kidneys, and muscles and can even lead to death, is “highly likely” after prolonged exposure. Climate simulations carried out by Eltahir and his co-investigators, using both “business as usual” scenarios and scenarios that include significant countermeasures against climate change, show that the likelihood of exceeding these thresholds for extended periods will increase steadily over the course of this century with the countermeasures, and very severely so without them. Because evaporation is so crucial to maintaining a safe body temperature, the level of humidity in the air is key. Even an actual temperature of just 90 F, if the humidity rises to 95 percent, is enough to reach the deadly 85 degree TW threshold for “extreme danger.” At a lower humidity of 45 percent, the 85 TW threshold would not be reached until the actual temperature climbed to 104 F or more. (At very high humidity, the wet bulb temperature equals the actual temperature). Climate change will significantly increase the number of days each summer where wet bulb temperatures in the region will exceed the “extreme danger” limit. Even with mitigation measures in place, Eltahir says, “it will still be severe. There will still be problems, but not as bad” as would occur without those measures. The Hajj is “a very strong part of the culture” in Muslim communities, Eltahir says, so preparing for these potentially unsafe conditions will be important for officials in Saudi Arabia. A variety of protective measures have been in place in recent years, including nozzles that provide a mist of water in some of the outdoor locations to provide some cooling for participants, and widening some of the locations to reduce overcrowding. In the most potentially risky years ahead, Eltahir says, it may become necessary to severely limit the number of participants allowed to take part in the ritual. This new research “should help in informing policy choices, including climate change mitigation policies as well as adaptation plans,” he says. The research team included Suchul Kang, an MIT postdoc, and Jeremy Pal, a professor of civil engineering and environmental science at Loyola Marymount University in Los Angeles. The work was supported by a seed grant from the MIT Environmental Solutions Initiative. ",Environment,0.08055897854432092
39,Science Daily,Big Increase in Ocean Carbon Dioxide Absorption Along West Antarctic Peninsula,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112656.htm,"   The study, led by scientists at Rutgers University-New Brunswick, is published in the journal Nature Climate Change. The West Antarctic Peninsula is experiencing some of the most rapid climate change on Earth, featuring dramatic increases in temperatures, retreats in glaciers and declines in sea ice. The Southern Ocean absorbs nearly half of the carbon dioxide -- the key greenhouse gas linked to climate change -- that is absorbed by all the world's oceans. ""Understanding how climate change will affect carbon dioxide absorption by the Southern Ocean, especially in coastal Antarctic regions like the West Antarctic Peninsula, is critical to improving predictions of the global impacts of climate change,"" said lead author Michael Brown, an oceanography doctoral student in the Center for Ocean Observing Leadership in the Department of Marine and Coastal Sciences at the School of Environmental and Biological Sciences. The study tapped an unprecedented 25 years of oceanographic measurements in the Southern Ocean and highlights the need for more monitoring in the region. The research revealed that carbon dioxide absorption by surface waters off the West Antarctic Peninsula is linked to the stability of the upper ocean, along with the amount and type of algae present. A stable upper ocean provides algae with ideal growing conditions. During photosynthesis, algae remove carbon dioxide from the surface ocean, which in turn draws carbon dioxide out of the atmosphere. From 1993 to 2017, changes in sea ice dynamics off the West Antarctic Peninsula stabilized the upper ocean, resulting in greater algal concentrations and a shift in the mix of algal species. That's led to a nearly five-fold increase in carbon dioxide absorption during the summertime. The research also found a strong north-south difference in the trend of carbon dioxide absorption. The southern portion of the peninsula, which to date has been less impacted by climate change, experienced the most dramatic increase in carbon dioxide absorption, demonstrating the poleward progression of climate change in the region. The results also demonstrate the often counterintuitive impacts of climate change. The scientists hypothesize that upper ocean stability off the West Antarctic Peninsula may ultimately decrease in the coming decades as sea ice continues to decline. Once sea ice reaches a critically low level, there won't be enough of it to prevent wind-driven mixing of the upper ocean, or to supply a sufficient amount of stabilizing meltwater. And that could result in reduced carbon dioxide absorption in the Southern Ocean over the long run. A decrease in the ocean's ability to absorb carbon dioxide could lead to more warming worldwide by allowing more of the heat-trapping gas to remain in the atmosphere. ",Environment,0.08026378448336259
40,Science Daily,Wildfires Could Permanently Alter Alaska's Forest Composition,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112658.htm,"   Using a well-tested ecosystem model called ecosys, they predicted that by the year 2100 the relative dominance of evergreen conifer trees (black spruce) will decline by 25% and non-woody herbaceous plants such as moss and lichen will decline by 66%, while broadleaf deciduous trees (aspen) will become dominant, nearly doubling in prevalence. With such large declines, this shift in vegetation will highly likely have reverberations for the entire ecosystem and climate. ""Expansion of the deciduous broadleaf forests in a warmer climate may result in several ecological and climatic feedbacks that affect the carbon cycle of northern ecosystems,"" said Zelalem Mekonnen, a Berkeley Lab postdoctoral fellow who was first author of the study. The paper, ""Expansion of High-Latitude Deciduous Forests Driven by Interactions Between Climate Warming and Fire,"" was published today in Nature Plants. The study was funded as part of DOE's Office of Science through the Next-Generation Ecosystem Experiment -- Arctic project and included co-authors from UC Irvine, the University of Alberta, and Woods Hole Research Center. NGEE-Arctic seeks to gain a predictive understanding of the Arctic terrestrial ecosystem's feedback to climate and is a collaboration among scientists at Oak Ridge National Laboratory, Berkeley Lab, Los Alamos National Laboratory, Brookhaven National Laboratory, and the University of Alaska Fairbanks. ""We predict the forest system will remain a net sink for carbon, meaning it will absorb more carbon than it emits,"" said co-author William J. Riley, a senior scientist in Berkeley Lab's Earth & Environmental Sciences Area. ""But will it be more or less of a sink? Our next study will quantify the carbon and surface energy budgets. This study focused more on how vegetation types are expected to change."" Changes in forest cover type will affect many important ecosystem processes. For example, an increase in deciduous broadleaf trees, which lose their leaves every year, unlike evergreens, could result in more rapid microbial decomposition and increased transpiration (the loss of moisture through leaves); both of these processes introduce amplifying feedbacks to climate warming. On the other hand, higher surface reflectance may have a cooling effect when more snow is exposed because of fewer evergreen trees; what's more, deciduous trees are less flammable than evergreen trees. The researchers predicted modest effects on net carbon budgets and will analyze that further in future work. Riley added that the study included many steps to confirm that the results from ecosys were valid. ""We evaluated model performance against many current observations of forest cover and carbon cycling measurements, and against long-term changes under natural climate variation,"" he said. Combo of fire plus climate warming could alter forests in 40 years Climate change is hitting the northern latitudes especially hard due to the phenomenon of Arctic amplification, a positive feedback that causes temperatures to rise faster than the global average. While average global temperatures are projected to rise about 4 degrees Celsius by 2100 in a ""business as usual"" scenario, some recent studies are predicting much larger increases for the Arctic. The extent to which fires will increase is even more uncertain. So the researchers modeled four scenarios, from a zero increase in burn area up to a 150% increase by 2100. The scenarios were taken from published studies that accounted for factors such as warmer temperatures and increases in lightning strikes. What is known about fires are the impacts they have on the forest ecosystem. ""Fires deepen the active layer, which is the zone of soil that remains unfrozen,"" said Riley. ""That leads to an increase in soil nutrients available for plants. Increases in soil nutrients favor deciduous plants, which is one reason why we predict they will do so well under a warming climate. Higher deciduous tree cover has happened under previous climates; paleoecological studies of the last 10,000 years suggest that Alaskan forests have undergone similar shifts in dominant tree species."" Another factor that favors broadleaf deciduous over evergreen conifer trees is that their leaves decompose more rapidly, leading to more rapid carbon turnover, which determines the available nutrients in the ecosystem. ""As you get more rapid turnover, you get more deciduous plants,"" Riley said. ""It's a self-reinforcing mechanism."" Although previous studies have examined how climate change will impact boreal forests, Riley said this was the first to consider the complex interactions among plants, soil, and nutrients -- both above and below ground -- and how they evolve over time. ""This study is a more detailed and mechanistic explanation of these processes,"" he said. Other factors that favor broadleaf deciduous trees in a future warmer climate are their greater ability for post-fire seedling regeneration and their ability to grow fast and thus compete for light. ""Plants have different strategies to survive under different environmental conditions,"" Mekonnen said. The study found that both climate change and increased fire were required to produce broadleaf deciduous trees' dominance. Across the fire scenarios tested where fires increased, that shift was projected to occur around the year 2058. If warming occurred without increased fire or vice versa, the model found that evergreen conifers remained the dominant Alaskan tree type through the 21st century. Another forest component that will be affected is wildlife. ""Broadleaf deciduous trees have a large canopy which covers underlying vegetation, potentially decreasing herbaceous plant cover. Those plants, especially moss, are very important forage for wildlife,"" Mekonnen said. What's more, the modeling technique can be used to study how climate change and fire will affect other geographic areas. ""Our modeling approach is applicable to other northern regions because the fundamental mechanisms that control these dynamics are similar everywhere,"" Mekonnen said. ",Environment,0.08002291773632689
41,Science Daily,Shocking Rate of Plant Extinctions in South Africa,Science,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822141918.htm,"   According to a study published in the journal Current Biology this week, this represents a shocking 45.4% of all known plant extinctions from 10 of the world's 36 biodiversity hotspots. Biodiversity hotspots are areas that harbour exceptionally high numbers of unique species, but at the same time they are under severe threat from human disturbance. South Africa is remarkable in that, for a relatively small country, it is home to three of these hotspots. An international team of researchers, led by Prof Jaco Le Roux and Dr Heidi Hirsch, affiliated with the Centre for Invasion Biology (CIB) at Stellenbosch University (SU), analysed a comprehensive dataset of 291 plant extinctions since 1700 in ten biodiversity hotspots and six coldspots, covering about 15% of Earth's land surface. The main drivers for extinctions in South Africa were found to be agriculture (49.4%), urbanisation (38%) and invasive species (22%). Variability in predictions on the rate of plant extinctions The results of their analysis show that, since the 1990s, extinction rates for plants over the past 300 years appear to have settled at about 1.26 extinctions per year. At its peak, however, it was at least 350 times that of historical background rates during pre-human times. At this rate, they predict that, in the areas they studied, an additional 21 plant species will go extinct by 2030, 47 species by 2050 and 110 species by 2100. However, these findings stand in sharp contrast to predictions from other studies that as much as half of Earth's estimated 390,000 plant species may disappear within the remainder of this century. ""This would translate into more than 49,000 extinctions in the regions we studied over the next 80 years, which seems unlikely, bar a cataclysmic event such as an asteroid strike!"" they argue. Prof Le Roux says regional datasets provide valuable data to make general inferences around plant extinctions and the drivers underlying these extinctions. There are, however, still many regions in the world without a Red List of Plants, or with outdated lists, such as Madagascar and Hawaii. These 'hottest' of hotspots were therefore not included in their analysis. ""A lack of up-to-date lists prevents us from gaining a more complete and precise picture of what we are losing, and at exactly what rate,"" Dr Hirsch adds. They believe the only way to better understand the magnitude of the extinction crisis faced by plants, and biodiversity in general, is to urgently initiate regional or at least country-level biodiversity assessments. ""While our study suggests that modern plant extinctions are relatively low, it is important to keep in mind that plants are exceptionally good at 'hanging in there'. Some of them are among the longest living organisms on earth today and many can persist in low densities, even under prolonged periods of unfavourable environmental conditions. A recent report, for example, indicated that 431 plant species, previously thought to be extinct, have been rediscovered,"" Le Roux explains. This means that many plant species may technically not be extinct, even though they only have one or a few living individuals remaining in the wild. Claiming extinction rates for plant species therefore remains a particularly challenging exercise. ""We need comprehensive and up-to-date datasets to make informative forecasts about the future and preservation of Earth's flora,"" they emphasise. Lost plant species in South Africa's biodiversity hotspots The first recorded species to be lost to forestry in South Africa in the 1700s was a type of fountainbush that used to grow next to streams in the Tulbagh region -- Psoralea cataracta. In 2008 it was listed as extinct on the Red List of South African Plants. The next species to be confirmed extinct was one of the African daisies, Osteospermum hirsutum, last seen in 1775, followed by the honeybush, Cyclopia laxiflora, last seen around 1800. The reasons for their extinction are listed as agriculture, forestry and urbanisation. More recently in 2012, an extremely rare species of vygie, Jordaaniella anemoniflora, was declared extinct in the wild after losing its battle against sprawling urbanisation and coastal developments around Strand, Macassar and Hermanus. The Succulent Karoo has seen three confirmed plant extinctions -- a vygie, Lampranthus vanzijliae (extinct in 1921, due to agriculture and urbanisation), the legume, Leobordea magnifica (extinct in 1947 due to agriculture and grazing) and the 'knopie' Conophytum semivestitum, lost to urbanisation and mining. For the Maputuland-Pondoland-Albany corridor, twenty species have been confirmed extinct, mainly due to agriculture and utilisation, and include Adenia natalensis (1865), Barleria natalensis (1890) and more recently, Pleiospilos simulans (2007). In conclusion The researchers emphasise that biodiversity loss, together with climate change, are the biggest threats faced by humanity: ""Along with habitat destruction, the effects of climate change are expected to be particularly severe on those plants not capable of dispersing their seeds over long distances,"" they conclude. ",Environment,0.07989909750227589
42,Science Daily,Beaver Reintroduction Key to Solving Freshwater Biodiversity Crisis,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104844.htm,"   New research from the Faculty of Natural Sciences has provided further support to previous work that has shown beavers have an important impact on the variety of plant and animal life. The latest study, led by Dr Alan Law and Professor Nigel Willby, found that the number of species only found in beaver-built ponds was 50 percent higher than other wetlands in the same region. Dr Law, Lecturer in Biological and Environmental Sciences, said: ""Beavers make ponds that, at first glance, are not much different from any other pond. However, we found that the biodiversity -- predominantly water plants and beetles -- in beaver ponds was greater than and surprisingly different from that found in other wetlands in the same region. ""Our results also emphasise the importance of natural disturbance by big herbivores -- in this case, tree felling, grazing and digging of canals by beavers -- in creating habitat for species which otherwise tend to be lost. ""Reintroducing beavers where they were once native should benefit wider biodiversity and should be seen as an important and bold step towards solving the freshwater biodiversity crisis."" Beavers are one of the only animals that can profoundly engineer the environment that they live in -- using sticks to build dams across small rivers, behind which ponds form. Beavers do this to raise water levels to avoid predators, such as wolves and bears: however, numerous other plants and animals also benefit from their work. The research team surveyed water plants and beetles in 20 wetlands in a small area of southern Sweden -- 10 created by beavers and 10 that were not -- to understand whether beavers might provide a solution to the current biodiversity crisis by creating novel habitats. Professor Willby added: ""The loss of large mammals from modern landscapes is a global conservation concern. These animals are important in their own right, but our research emphasises the added biodiversity benefits that go with them. ""We are best reminded of this effect when large herbivores, such as beavers, are reintroduced to places where they have been lost."" This research follows the team's 2018 study that found that 33 percent more plant species and 26 percent more beetles were living in wetlands created by beavers, compared to those that were not. Another previous study, from 2017, showed that -- over a period of 12 years -- local plant richness in a Tayside wetland rose by 46 percent following the introduction of beavers. They created 195 metres of dams, 500 metres of canals and a hectare of ponds. ",Environment,0.07971995723192389
43,MIT News,High-precision technique stores cellular “memory” in DNA,Research,2019-08-22,-,http://news.mit.edu/2019/domino-cellular-memory-dna-0822,"  Using a technique that can precisely edit DNA bases, MIT researchers have created a way to store complex “memories” in the DNA of living cells, including human cells. The new system, known as DOMINO, can be used to record the intensity, duration, sequence, and timing of many events in the life of a cell, such as exposures to certain chemicals. This memory-storage capacity can act as the foundation of complex circuits in which one event, or series of events, triggers another event, such as the production of a fluorescent protein. “This platform gives us a way to encode memory and logic operations in cells in a scalable fashion,” says Fahim Farzadfard, a Schmidt Science Postdoctoral Fellow at MIT and the lead author of the paper. “Similar to silicon-based computers, in order to create complex forms of logic and computation, we need to have access to vast amounts of memory.” Applications for these types of complex memory circuits include tracking the changes that occur from generation to generation as cells differentiate, or creating sensors that could detect, and possibly treat, diseased cells. Timothy Lu, an MIT associate professor of electrical engineering and computer science and of biological engineering, is the senior author of the study, which appears in the Aug. 22 issue of Molecular Cell. Other authors of the paper include Harvard University graduate student Nava Gharaei, former MIT researcher Yasutomi Higashikuni, MIT graduate student Giyoung Jung, and MIT postdoc Jicong Cao. Written in DNA Several years ago, Lu’s lab developed a memory storage system based on enzymes called DNA recombinases, which can “flip” segments of DNA when a specific event occurs. However, this approach is limited in scale: It can only record one or two events, because the DNA sequences that have to be flipped are very large, and each requires a different recombinase. Lu and Farzadfard then developed a more targeted approach in which they could insert new DNA sequences into predetermined locations in the genome, but that approach only worked in bacterial cells. In 2016, they developed a memory storage system based on CRISPR, a genome-editing system that consists of a DNA-cutting enzyme called Cas9 and a short RNA strand that guides the enzyme to a specific area of the genome. This CRISPR-based process allowed the researchers to insert mutations at specific DNA locations, but it relied on the cell’s own DNA-repair machinery to generate mutations after Cas9 cut the DNA. This meant that the mutational outcomes were not always predictable, thus limiting the amount of information that could be stored. The new DOMINO system uses a variant of the CRISPR-Cas9 enzyme that makes more well-defined mutations because it directly modifies and stores bits of information in DNA bases instead of cutting DNA and waiting for cells to repair the damage. The researchers showed that they could get this system to work accurately in both human and bacterial cells. “This paper tries to overcome all the limitations of the previous ones,” Lu says. “It gets us much closer to the ultimate vision, which is to have robust, highly scalable, and defined memory systems, similar to how a hard drive would work.” To achieve this higher level of precision, the researchers attached a version of Cas9 to a recently developed “base editor” enzyme, which can convert the nucleotide cytosine to thymine without breaking the double-stranded DNA. Guide RNA strands, which direct the base editor where to make this switch, are produced only when certain inputs are present in the cell. When one of the target inputs is present, the guide RNA leads the base editor either to a stretch of DNA that the researchers added to the cell’s nucleus, or to genes found in the cell’s own genome, depending on the application. Measuring the resulting cytosine to thymine mutations allows the researchers to determine what the cell has been exposed to. “You can design the system so that each combination of the inputs gives you a unique mutational signature, and from that signature you can tell which combination of the inputs has been present,” Farzadfard says. Complex calculations The researchers used DOMINO to create circuits that perform logic calculations, including AND and OR gates, which can detect the presence of multiple inputs. They also created circuits that can record cascades of events that occur in a certain order, similar to an array of dominos falling. “This is very innovative work that enables recording and retrieving cellular information using DNA. The ability to perform sequential or logic computation and associative learning is particularly impressive,” says Wilson Wong, an associate professor of biomedical engineering at Boston University, who was not involved in the research. “This work highlights novel genetic circuits that can be achieved with CRISPR/Cas.” Most previous versions of cellular memory storage have required stored memories to be read by sequencing the DNA. However, that process destroys the cells, so no further experiments can be done on them. In this study, the researchers designed their circuits so that the final output would activate the gene for green fluorescent protein (GFP). By measuring the level of fluorescence, the researchers could estimate how many mutations had accumulated, without killing the cells. The technology could potentially be used to create mouse immune cells that produce GFP when certain signaling molecules are activated, which researchers could analyze by periodically taking blood samples from the mice. Another possible application is designing circuits that can detect gene activity linked to cancer, the researchers say. Such circuits could also be programmed to turn on genes that produce cancer-fighting molecules, allowing the system to both detect and treat the disease. “Those are applications that may be further away from real-world use but are certainly enabled by this type of technology,” Lu says. The research was funded by the National Institutes of Health, the Office of Naval Research, the National Science Foundation, the Defense Advanced Research Projects Agency, the MIT Center for Microbiome Informatics and Therapeutics, and the NSF Expeditions in Computing Program Award. ",Health,0.07908688975430989
44,Science Daily,Monster Tumbleweed: Invasive New Species Is Here to Stay,Environment,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092336.htm,"   The species, Salsola ryanii, is significantly larger than either of its parent plants, which can grow up to 6 feet tall. A new study from UC Riverside supports the theory that the new tumbleweed grows more vigorously because it is a hybrid with doubled pairs of its parents' chromosomes. Findings from the study are detailed in a new paper published in the Oxford University-produced journal AoB Plants. ""Salsola ryanii is a nasty species replacing other nasty species of tumbleweed in the U.S.,"" said study co-author Norman Ellstrand, UCR Distinguished Professor of Genetics. ""It's healthier than earlier versions, and now we know why."" Humans are diploid organisms, with one set of chromosomes donated by the mother and one set from the father. Sometimes a mother's egg contains two sets of chromosomes rather than just the one she is meant to pass on. If this egg is fertilized, the offspring would be triploid, with three sets of chromosomes. Most humans do not survive this. Plants with parents closely related enough to mate can produce triploid offspring that survive but are unable to reproduce themselves. However, a hybrid plant that manages to get two copies from the mother and two from the father will be fertile. Some species can have more than four sets of chromosomes. They can even have ""hexaploidy,"" with six sets of chromosomes. Scientists have long assumed there must be some kind of evolutionary advantage to polyploidy, the term for hybrids that have multiple sets of chromosomes, since it poses some immediate difficulties for the new hybrids. ""Typically, when something is new, and it's the only one of its kind, that's a disadvantage. There's nobody exactly like you to mate with,"" said study co-author Shana Welles, the graduate student in Ellstrand's laboratory that conducted the study as part of her Ph.D. research. She is now a postdoctoral fellow at Chapman University. The advantage to having multiple sets of chromosomes, according to the study, is that the hybrid plant grows more vigorously than either of its parents. This has been suggested as the reason polyploidy is so common in plants. However, it has not, until now, been demonstrated experimentally. Polyploidy is associated with our favorite crops; domesticated peanuts have four sets of chromosomes, and the wheat we eat has six. Though tumbleweeds are often seen as symbols of America's old West, they are also invasive plants that cause traffic accidents, damage agricultural operations, and cause millions in property damage every year. Last year, the desert town of Victorville, California, was buried in them, piling up to the second story of some homes. Currently, Salsola ryanii has a relatively small but expanding geographic range. Since the new study determined it is even more vigorous than its progenitors, which are invasive in 48 states, Welles said it is likely to continue to expand its range. Additionally, Welles said climate change could increase its territory takeover. Though this tumbleweed is an annual, it tends to grow on the later side of winter. ""It's one of the only things that's still green in late summer,"" Welles said. ""They may be well positioned to take advantage of summer rains if climate changes make those more prevalent."" Given its potential for damage, the knowledge now available about Salsola ryanii could be important for helping to suppress it, and Ellstrand believes that is what should happen before it takes over. ""An ounce of prevention is a pound of cure,"" he said. ",Environment,0.0773733753024855
45,MIT News,New method classifies brain cells based on electrical signals ,Research,2019-08-22,-,http://news.mit.edu/2019/new-method-classifies-brain-cells-based-electrical-signals-0822,"  For decades, neuroscientists have relied on a technique for reading out electrical “spikes” of brain activity in live, behaving subjects that tells them very little about the types of cells they are monitoring. In a new study, researchers at the University of Tuebingen, Germany, and MIT’s Picower Institute for Learning and Memory demonstrate a way to increase their insight by distinguishing four distinct classes of cells from that spiking information. The advance offers brain researchers the chance to better understand how different kinds of neurons are contributing to behavior, perception, and memory, and how they are malfunctioning in cases of psychiatric or neurological diseases. Much like mechanics can better understand and troubleshoot a machine by watching how each part works as it runs, neuroscientists, too, are better able to understand the brain when they can tease apart the roles different cells play while it thinks. “We know from anatomical studies that there are multiple types of cells in the brain and if they are there, they must be there for a reason,” says Earl Miller, the Picower Professor of Neuroscience in the Department of Brain and Cognitive Sciences at MIT, and co-senior author of the paper in Current Biology. “We can’t truly understand the functional circuitry of the brain until we fully understand what different roles these different cell types might play.” Miller collaborated with the Tuebingen-based team of lead author Caterina Trainito, Constantin von Nicolai, and Professor Markus Siegel, co-senior author and a former postdoc in Miller’s lab, to develop the new way to wring more neuron type information from electrophysiology measurements. Those measures track the rapid voltage changes, or spikes, that neurons exhibit as they communicate in circuits, a phenomenon essential for brain function. “Identifying different cell types will be key to understand both local and large-scale information processing in the brain,” Siegel says. Four is greater than two At best, neuroscientists have so far only been able to determine from electrophysiology whether a neuron was excitatory or inhibitory. That’s because they only analyzed the difference in the width of the spike. The typical amount of data in an electrophysiology study — spikes from a few hundred neurons — only supported that single degree of distinction, Miller says. But the new study could go farther because it derives from a dataset of recordings from nearly 2,500 neurons. Miller and Siegel gathered the data years ago at MIT from three regions in the cortex of animals who were performing experimental tasks that integrated perception and decision-making. “We recognized the uncommonly rich resource at our disposal,” Siegel says. Thus, the team decided to put the dataset through a ringer of sophisticated statistical and computational tools to analyze the waveforms of the spikes. Their analysis showed that the waveforms could actually be sorted along two dimensions: how quickly the waveform ranges between its lowest and highest voltage (“trough to peak duration”), and how quickly the voltage changes again afterward, returning from the peak to the normal level (“repolarization time”). Plotting those two factors against each other neatly sorted the cells into four distinct “clusters.” Not only were the clusters evident across the whole dataset, but individually within each of the three cortical regions, too. For the distinction to have any meaning, the four classes of cells would have to have functional differences. To test that, the researchers decided to sort the cells out based on other criteria such as their “firing rate” (how often they spike), whether they tend to fire in bursts, and how variable their intervals are between spikes — all factors in how they participate in and influence the circuits they are connected in. Indeed, the cell classes remained distinct by these measures. In yet another phase of analysis, the cell classes also remained distinguishable as the researchers watched them respond to the animals perceiving and processing visual stimulation. But in this case, they saw the cells play different roles in different regions. A class 1 cell, for example, might respond differently in one region than it did in another. “These cell types are truly different cell types that have different properties,” Miller says. “But they have different functions in different cortical areas because different cortical areas have different functions.” New research capability In the study, the authors speculate about which real neuron types their four mathematically defined classes most closely resemble, but they don’t yet offer a definitive determination. Still, Miller says the finer-grained distinctions the study draws are enough to make him want to reanalyze old neural spiking data to see what new things he can learn. One of Miller’s main research interests is the nature of working memory — our ability to hold information like directions in mind while we make use of it. His research has revealed that it is enabled by a complex interplay of brain regions and precisely timed bursts of neural activity. Now he may be able to figure out how different classes of neurons play specific roles in specific regions to endow us with this handy mental ability. And both Miller’s and Siegel’s labs are particularly interested in different brain rhythms, which are abundant in the brain and likely play a key role for orchestrating communication between neurons. The new results open a powerful new window for them to unravel which role different neuron classes play for these brain rhythms.     The U.S. National Institutes of Health, the European Research Council, the Deutsche Forschungsgemeinschaft, and the Center for Integrative Neuroscience provided funding for the study. ",Health,0.06821354514683602
46,Science Daily,An Unreported Zika Outbreak in 2017 Detected Through Travel Surveillance and Genetics,Science,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822113356.htm,"   Three years ago, a well-publicized Zika epidemic in Brazil was tied to microcephaly and other congenital abnormalities. Zika is also associated with neurologic disorders, including Guillain-Barré syndrome. However, disparities in data make it unclear whether the virus -- transmitted by the mosquito Aedes aegypti -- is still a threat in the Americas. ""As the larger Zika epidemic in the Americas appeared to be waning towards the end of 2016, we became interested in understanding whether the epidemic was truly gone or whether 'hidden' transmissions could still be occurring,"" says study co-author Scott F. Michael, a biologist at Florida Gulf Coast University. Accurately pinpointing cases of Zika can be challenging: its symptoms mimic those of other diseases; and regions with inadequate health care systems often lack reliable, inexpensive diagnostic tools. Early and rapid pathogen detection is critical in preventing outbreaks from spinning into large-scale epidemics, says co-author Kristian Andersen, an infectious disease researcher at Scripps Research. Based on travel incidence rates reported by other countries, in 2017, Cuba would have experienced 1,000 to 20,000 Zika cases, Andersen says. However, only 187 cases were reported in 2016 and none in 2017. Between June 2017 and October 2018, more than 98 percent of the travel-associated cases reported in Florida and Europe came from Cuba. The timing of this outbreak was a mystery: conditions in Cuba could have supported a large Zika outbreak in 2016. Why did Cuba's cases jump in 2017? The main resource for reporting Zika cases is the Pan-American Health Organization (PAHO), which relies on local case reporting from member countries. To check the accuracy of PAHO results, the researchers sequenced Zika virus genomes from infected travelers in Florida and detected the unreported spike in cases in 2017. The researchers determined that the delay was likely caused by a successful mosquito eradication campaign that had taken place in Cuba in 2016. ""We show that the 2017 Zika outbreak was sparked by long-lived lineages of Zika virus introduced a year prior,"" Grubaugh says. ""Our data suggest that while mosquito control in Cuba may initially have been effective at mitigating Zika virus transmission, such measures may need to be maintained to be effective."" ",Health,0.06685546069410862
47,Science Daily,Medicare Patients With Multiple Sclerosis Bear the Burden of Rising Drug Prices,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826110402.htm,"   Using Medicare claims data from 2006-2016, the researchers looked at trends in multiple sclerosis drug prices over time. Not only did they find steep increases in list prices -- the starting point before rebates, coupons or insurance kicks in -- but also in the ultimate costs to both Medicare and its recipients. ""We wanted to see how increases in list prices translated to increases in out-of-pocket spending, and we discovered that actual price increases do get passed down to patients, and that can negatively affect access,"" said study senior author Inmaculada Hernandez, Pharm.D., Ph.D., assistant professor of pharmacy at Pitt. Several drugs on the market reduce the frequency and severity of multiple sclerosis flare-ups, which can involve a variety of disabling neurological symptoms, such as vision loss, pain, fatigue and muscle weakness. From 2006-2016, the annual list prices of these drugs more than quadrupled, ballooning from about $18,000 to nearly $76,000 per patient per year. Some of the most popular drugs for treating multiple sclerosis are Copaxone, Tecfidera and Avonex, and, despite increased market competition over time, prices have been rising steadily for nearly all of them. ""One of the most significant findings was that the prices of these drugs have increased in parallel,"" said lead author Alvaro San-Juan-Rodriguez, Pharm.D., a pharmacy fellow at Pitt. ""Only a couple exceptions deviate from that general trend."" Although this trend among list prices is alarming on its own, critics have argued that since some of the cost is canceled out by manufacturer rebates and other kinds of discounts, rising list prices may not be translating into increased spending. But since Medicare claims provide a detailed cost breakdown, the researchers were able to measure changes in what Medicare Part D beneficiaries actually paid out of pocket for multiple sclerosis drugs, as well as what Medicare itself paid. What they found was that from 2006-2016, Medicare spending increased by more than tenfold, and the patients themselves saw more than a sevenfold increase in their share of the bill. ""We're not talking about patients without health insurance here,"" Hernandez said. ""We're talking about insured patients, under Medicare. Still, they are paying much more for multiple sclerosis drugs than they were 10 years ago."" Additional authors on the study include Chester Good, M.D., M.P.H., Natasha Parekh, M.D., M.S., and William Shrank, M.D., M.S.H.S., from the UPMC Health Plan; and Rock Heyman, M.D., from Pitt and UPMC. Funding was provided by the Myers Family Foundation and the National Heart, Lung, and Blood Institute (grant number K01HL142847). Hernandez disclosed fees paid to her personally by Pfizer, for services unrelated to the scope of this work. ",Health,0.06237265671466019
48,Science Daily,Breaching the Brain's Defense Causes Epilepsy,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112644.htm,"   The study found that just before an epileptic seizure, nerve cells were abnormally active but only in a localized area of the brain. Instead, glial cells showed large burst of synchronous activity that are widely dispersed across the brain. During the actual seizure, the neuronal activity increased abruptly. The functional connections between the nerve cells and glial cells became vigorous. When this happened, generalized seizure spread like a storm of electrical activity across the entire brain due to a strong increase in the level of glutamate, a chemical compound that transmits signals between neuronal cells. Glutamate was secreted by glial cells, which convert themselves from a friend to a foe. The findings indicate that epilepsy may occur not only due to anomalies in neurons, but also in glial cells. ""Our results provide a direct evidence that the interactions between glial cells and neurons change during the transition from a pre-seizure state to a generalized seizure. It will be interesting to see if this phenomenon is generalizable across different types of epilepsies,"" says Prof. Emre Yaksi. Normally, the glial cells absorb the excess glutamate that is excreted during the increased activity of the nerve cells. This study assumes that the secretion process of the glial cells that we observed in combination with their hyperactivity just before a seizure is a defence mechanism of the brain. ",Health,0.05779901945629182
49,Science Daily,Even Scientists Have Gender Stereotypes ... Which Can Hamper the Career of Women Researchers,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826110354.htm,"   Women remain underrepresented in scientific research: at the French National Centre for Scientific Research (CNRS), across all disciplines, the average percentage of female researchers is 35%. And the higher the scientific research position, the more this percentage declines. Several reasons have been cited to explain these disparities: differences in levels of motivation, self-censorship ... but is discrimination also part of the story? To find out, scientists in social and cognitive psychology studied 40 evaluation committees (1) tasked with evaluating applications for research director (2) positions at the CNRS over a period of two years. This is the first time that a research institution has carried out such a scientific study of its practices in the course of an annual nationwide competition covering the entire scientific spectrum. This study shows that, from particle physics to the social sciences, most scientists, whether male or female, associate ""science"" and ""masculine"" in their semantic memory (the memory of concepts and words). This stereotype is implicit, which is to say that most often it is not detectable at the level of discourse. And it is equivalent to that observed among the general population. Yet does this implicit stereotype have consequences on the decisions made by evaluation committees? Yes, when committees deny or minimise the existence of bias against women.(3) Here, this is the case for around half of the committees. In these committees, the stronger the implicit stereotypes, the less often women are promoted. In contrast, when committees acknowledge the possibility of bias, implicit stereotypes, however strong they may be, have no influence. Even if disparities between men and women in science have multiple causes and start at school (as the same authors have shown in other publications), this study indicates for the first time the existence of implicit gender stereotypes among male and female researchers across all disciplines -- stereotypes that can harm the careers of women scientists. Since 2019, at the instigation of the CNRS Mission for the place of women, members of evaluation committees have been invited to participate in training sessions on gender stereotypes and each committee has appointed a reference person in charge of gender equality issues. However, the authors of the study emphasise that, in order to be fully effective, this process must be accompanied by other measures aiming, on the one hand, to enlighten committee members on the exact conditions in which implicit stereotypes influence their decisions, and, on the other, to explain strategies likely to control this influence. Notes: (1) In total, 414 people participated to the study. The committees considered in this study have since come to the end of their commission. (2) A senior researcher. (3) They more often attribute gender disparities in science to the choices made by women or gender differences in ability than to the existence of discrimination or the constraints of family life. ",Society,0.056106169354853956
50,Science Daily,China's Two-Child Policy Has Led to 5.4 Million Extra Births,Science,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821185332.htm,"   It is the first study to use national data to estimate the impacts of the policy change and shows that births increased in response to the policy, but not as much as some policymakers had hoped. China's two-child policy, announced in October 2015, was enacted to reverse the nation's stagnant population growth, ageing population, and shrinking workforce. The policy targeted some 90 million women of reproductive age who already had a child, and now would be allowed to have a second child. There has been much speculation about the impact of the policy, with projections ranging from slightly over 1 million to more than 20 million annually. But, so far, studies have been limited. So a team of researchers based in China and the US set out to measure changes in births and health-related birth characteristics associated with the policy change. Using two national databases, they compared the number of births in two phases: ""baseline period"" (up to and including June 2016, 9 months after the announcement) and ""effective period"" (July 2016 to December 2017). Their findings are based on 67.8 million births in 28 out of 31 provinces of mainland China, an average of 1.41 million births per month. The researchers estimate an additional 5.4 million births as a result of the new policy during the first 18 months that it was in effect. And for the first time, the number of births to those who had given birth previously (multiparous mothers) exceeded births to first-time (primiparous) mothers. The policy was also associated with a 59% average increase in births to older mothers (35 years or older), but there was no accompanying increase in premature births. Finally, there was a slight decrease in caesarean deliveries to primiparous mothers, which might signal a favourable trend towards vaginal birth in first-time mothers in China, explain the researchers. However, they point out that many of the changes associated with the policy, including the increase in births, appeared to diminish at the end of the study period, raising questions about whether the policy's effects will be sustained. This is an observational study, so can't establish cause, and the researchers outline some limitations relating to the validity and representativeness of the data. But they say their findings clearly show that births increased in response to the policy, albeit not as much as some policymakers had hoped. Although they found no significant increased premature births, they say ""more work is needed to document and ensure the health of an increasingly older maternal population of second-time mothers in a nation where caesarean delivery rates are high."" Further research is also needed ""to develop a more nuanced understanding of the sustained impact of this historic change on the world's largest nation,"" they conclude. ",Health,0.05424554172940536
51,Science Daily,Australian Men's Life Expectancy Tops Other Men's,Science,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822094020.htm,"   The study introduces a new way of measuring life expectancy, accounting for the historical mortality conditions that today's older generations lived through. By this measure, Australian men, on average, live to 74.1. The news is good for Australian women too; the study shows they're ranked second, behind their Swiss counterparts. Dr Collin Payne co-led the study, which used data from 15 countries across Europe, North America and Asia with high life expectancies. ""Popular belief has it that Japan and the Nordic countries are doing really well in terms of health, wellbeing, and longevity. But Australia is right there,"" Dr Payne said. ""The results have a lot to do with long term stability and the fact Australia's had a high standard of living for a really, really long time. Simple things like having enough to eat, and not seeing a lot of major conflict play a part."" Dr Payne's study grouped people by year of birth, separating 'early' deaths from 'late' deaths, to come up with the age at which someone can be considered an 'above-average' survivor. ""Most measures of life expectancy are just based on mortality rates at a given time,"" Dr Payne said. ""It's basically saying if you took a hypothetical group of people and put them through the mortality rates that a country experienced in 2018, for example, they would live to an average age of 80. ""But that doesn't tell you anything about the life courses of people, as they've lived through to old age. ""Our measure takes the life course into account, including mortality rates from 50, 60, or 70 years ago. ""What matters is we're comparing a group of people who were born in the same year, and so have experienced similar conditions throughout their life."" Dr Payne says this method allows us to clearly see whether someone is reaching their cohort's life expectancy. ""For example, any Australian man who's above age 74 we know with 100 per cent certainty has outlived half of his cohort -- he's an above average survivor compared to his peers born in the same year,"" he said. ""And those figures are higher here than anywhere else that we've measured life expectancy. ""On the other hand, any man who's died before age 74 is not living up to their cohort's life expectancy."" Dr Payne says there are a number of factors which might've contributed to Australia jumping ahead in these new rankings. ""Mortality was really high in Japan in the 30s, 40s and 50s. In Australia, mortality was really low during that time,"" Dr Payne said. ""French males, for example, drop out because a lot of them died during WW2, some from direct conflict, others from childhood conditions."" Dr Payne is now hoping to get enough data to look at how rankings have changed over the last 30 or 40 years. The research has been published in the journal Population Studies. ",Health,0.05335532563513843
52,Science Daily,Hiring Committees That Don't Believe in Gender Bias Promote Fewer Women,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112653.htm,"   Opinions vary, but a new study by a UBC psychologist and researchers in France reveals that hiring committees who denied it's a problem were less likely to promote women. ""Our evidence suggests that when people recognize women might face barriers, they are more able to put aside their own biases,"" said Toni Schmader, a UBC psychology professor and Canada Research Chair in social psychology. ""We don't see any favourability for or against male or female candidates among those committees who believe they need to be vigilant to the possibility that biases could be creeping in to their decision-making."" The study was unique in that findings were based on actual decisions made by 40 hiring committees in France, charged with filling elite research positions with the National Committee for Scientific Research (CNRS) for two consecutive years. Past research in this area has relied mostly on hypothetical scenarios, such as presenting a large sample of participants with identical resumés bearing either male or female names and asking who they would hire. By contrast, the decisions made during this study had real impact on scientists' careers. With cooperation from the CNRS, the researchers were able to first measure how strongly hiring committee members associated men with science. They did this using an ""implicit association test"" that flashes words on a computer screen and measures how quickly participants are able to assign those words to a particular category. People who make a strong association between men and science have to think a bit longer, and react more slowly, when challenged to pair female-related words with science concepts. Both men and women on the hiring committees tended to show the science = male association, which is difficult to hide in such a test. ""There's research suggesting that you can document a 'think science, think male' implicit association showing up with kids as early as elementary school,"" Schmader said. ""We learn associations from what we see in our environment. If we don't see a lot of women who are role models in science, then we learn to associate science more with men than women."" These implicit associations are distinct from people's explicit beliefs about women in science. In a separate survey that asked panellists directly whether women in science careers are impacted by such things as discrimination and family constraints, some hiring committees minimized those issues. Others acknowledged them. When the researchers compared these implicit and explicit beliefs with the actual hiring outcomes, they learned that committees attuned to the barriers women face were more likely to overcome their implicit science/male associations when selecting candidates for the job. Among committees that believed ""science isn't sexist,"" those which implicitly associated science more with men promoted fewer women. The difference was especially pronounced in Year 2 of the study, when committee members would have been less conscious of the fact that their selections were being studied. The findings show that awareness and acknowledgement of the barriers women face might be key to making sure implicit biases don't affect hiring decisions. They also point to the importance of educating hiring committees about gender bias and how to guard against it, Schmader said. The study was published today in Nature Human Behaviour. ",Society,0.05141770562259055
53,Science Daily,Tech Time Not to Blame for Teens' Mental Health Problems,Science,2019-08-23,-,https://www.sciencedaily.com/releases/2019/08/190823140736.htm,"   The study tracked young adolescents on their smartphones to test whether more time spent using digital technology was linked to worse mental health outcomes. The researchers -- Candice Odgers, professor of psychological science at the University of California, Irvine; Michaeline Jensen, assistant professor of psychology at the University of North Carolina at Greensboro; Madeleine George, postdoctoral researcher at Purdue University; and Michael Russell, assistant professor of behavioral health at Pennsylvania State University -- found little evidence of longitudinal or daily linkages between digital technology use and adolescent mental health. ""It may be time for adults to stop arguing over whether smartphones and social media are good or bad for teens' mental health and start figuring out ways to best support them in both their offline and online lives,"" Odgers said. ""Contrary to the common belief that smartphones and social media are damaging adolescents' mental health, we don't see much support for the idea that time spent on phones and online is associated with increased risk for mental health problems,"" Jensen said. The study surveyed more than 2,000 youth and then intensively tracked a subsample of nearly 400 teens on their smartphones multiple times a day for two weeks. Adolescents in the study were between 10 and 15 years old and represented the economically and racially diverse population of youth attending North Carolina public schools. The researchers collected reports of mental health symptoms from the adolescents three times a day and they also reported on their daily technology usage each night. They asked whether youth who engaged more with digital technologies were more likely to experience later mental health symptoms and whether days that adolescents spent more time using digital technology for a wide range of purposes were also days when mental health problems were more common. In both cases, increased digital technology use was not related to worse mental health. When associations were observed, they were small and in the opposite direction that would be expected given all of the recent concerns about digital technology damaging adolescents' mental health. For instance, teens who reported sending more text messages over the study period actually reported feeling better (less depressed) than teens who were less frequent texters. ",Society,0.04902718091985206
54,MIT News,Two studies reveal benefits of mindfulness for middle school students,Research,2019-08-26,-,http://news.mit.edu/2019/mindfulness-mental-health-benefits-students-0826,"  Two new studies from MIT suggest that mindfulness — the practice of focusing one’s awareness on the present moment — can enhance academic performance and mental health in middle schoolers. The researchers found that more mindfulness correlates with better academic performance, fewer suspensions from school, and less stress. “By definition, mindfulness is the ability to focus attention on the present moment, as opposed to being distracted by external things or internal thoughts. If you’re focused on the teacher in front of you, or the homework in front of you, that should be good for learning,” says John Gabrieli, the Grover M. Hermann Professor in Health Sciences and Technology, a professor of brain and cognitive sciences, and a member of MIT’s McGovern Institute for Brain Research. The researchers also showed, for the first time, that mindfulness training can alter brain activity in students. Sixth-graders who received mindfulness training not only reported feeling less stressed, but their brain scans revealed reduced activation of the amygdala, a brain region that processes fear and other emotions, when they viewed images of fearful faces. Together, the findings suggest that offering mindfulness training in schools could benefit many students, says Gabrieli, who is the senior author of both studies.  “We think there is a reasonable possibility that mindfulness training would be beneficial for children as part of the daily curriculum in their classroom,” he says. “What’s also appealing about mindfulness is that there are pretty well-established ways of teaching it.” In the moment Both studies were performed at charter schools in Boston. In one of the papers, which appears today in the journal Behavioral Neuroscience, the MIT team studied about 100 sixth-graders. Half of the students received mindfulness training every day for eight weeks, while the other half took a coding class. The mindfulness exercises were designed to encourage students to pay attention to their breath, and to focus on the present moment rather than thoughts of the past or the future. Students who received the mindfulness training reported that their stress levels went down after the training, while the students in the control group did not. Students in the mindfulness training group also reported fewer negative feelings, such as sadness or anger, after the training. About 40 of the students also participated in brain imaging studies before and after the training. The researchers measured activity in the amygdala as the students looked at pictures of faces expressing different emotions. At the beginning of the study, before any training, students who reported higher stress levels showed more amygdala activity when they saw fearful faces. This is consistent with previous research showing that the amygdala can be overactive in people who experience more stress, leading them to have stronger negative reactions to adverse events. “There’s a lot of evidence that an overly strong amygdala response to negative things is associated with high stress in early childhood and risk for depression,” Gabrieli says. After the mindfulness training, students showed a smaller amygdala response when they saw the fearful faces, consistent with their reports that they felt less stressed. This suggests that mindfulness training could potentially help prevent or mitigate mood disorders linked with higher stress levels, the researchers say. Richard Davidson, a professor of psychology and psychiatry at the University of Wisconsin, says that the findings suggest there could be great benefit to implementing mindfulness training in middle schools. “This is really one of the very first rigorous studies with children of that age to demonstrate behavioral and neural benefits of a simple mindfulness training,” says Davidson, who was not involved in the study. Evaluating mindfulness In the other paper, which appeared in the journal Mind, Brain, and Education in June, the researchers did not perform any mindfulness training but used a questionnaire to evaluate mindfulness in more than 2,000 students in grades 5-8. The questionnaire was based on the Mindfulness Attention Awareness Scale, which is often used in mindfulness studies on adults. Participants are asked to rate how strongly they agree with statements such as “I rush through activities without being really attentive to them.” The researchers compared the questionnaire results with students’ grades, their scores on statewide standardized tests, their attendance rates, and the number of times they had been suspended from school. Students who showed more mindfulness tended to have better grades and test scores, as well as fewer absences and suspensions. “People had not asked that question in any quantitative sense at all, as to whether a more mindful child is more likely to fare better in school,” Gabrieli says. “This is the first paper that says there is a relationship between the two.” The researchers now plan to do a full school-year study, with a larger group of students across many schools, to examine the longer-term effects of mindfulness training. Shorter programs like the two-month training used in the Behavioral Neuroscience study would most likely not have a lasting impact, Gabrieli says. “Mindfulness is like going to the gym. If you go for a month, that’s good, but if you stop going, the effects won’t last,” he says. “It’s a form of mental exercise that needs to be sustained.” The research was funded by the Walton Family Foundation, the Poitras Center for Psychiatric Disorders Research at the McGovern Institute for Brain Research, and the National Council of Science and Technology of Mexico. Camila Caballero ’13, now a graduate student at Yale University, is the lead author of the Mind, Brain, and Education study. Caballero and MIT postdoc Clemens Bauer are lead authors of the Behavioral Neuroscience study. Additional collaborators were from the Harvard Graduate School of Education, Transforming Education, Boston Collegiate Charter School, and Calmer Choice. ",Society,0.0484352185677873
55,MIT News,A much less invasive way to monitor pressure in the brain,Research,2019-08-23,-,http://news.mit.edu/2019/less-invasive-brain-pressure-0823,"  Traumatic brain injuries, as well as infectious diseases such as meningitis, can lead to brain swelling and dangerously high pressure in the brain. If untreated, patients are at risk for brain damage, and in some cases elevated pressure can be fatal. Current techniques for measuring pressure within the brain are so invasive that the measurement is only performed in the patients at highest risk. However, that may soon change, now that a team of researchers from MIT and Boston Children’s Hospital has devised a much less invasive way to monitor intracranial pressure (ICP). “Ultimately the goal is to have a monitor at the bedside in which we only use minimally invasive or noninvasive measurements and produce estimates of ICP in real time,” says Thomas Heldt, the W. M. Keck Career Development Professor in Biomedical Engineering in MIT’s Institute of Medical Engineering and Science, an associate professor of electrical and biomedical engineering, and a principal investigator in MIT’s Research Laboratory of Electronics. In a study of patients ranging in age from 2 to 25 years, the researchers showed that their measurement is nearly as accurate as the current gold standard technique, which requires drilling a hole in the skull. Heldt is the senior author of the paper, which appears in the Aug. 23 issue of the Journal of Neurosurgery: Pediatrics. MIT research scientist Andrea Fanelli is the study’s lead author. Elevated risk Under normal conditions, ICP is between 5 and 15 millimeters of mercury (mmHg). When the brain suffers a traumatic injury or swelling caused by inflammation, pressure can go above 20 mmHg, impeding blood flow into the brain. This can lead to cell death from lack of oxygen, and in severe cases swelling pushes down on the brainstem — the area that controls breathing — and can cause the patient to lose consciousness or even stop breathing. Measuring ICP currently requires drilling a hole in the skull and inserting a catheter into the ventricular space, which contains cerebrospinal fluid. This invasive procedure is only done for patients in intensive care units who are at high risk of elevated ICP. When a patient’s brain pressure becomes dangerously high, doctors can help relieve it by draining cerebrospinal fluid through a catheter inserted into the brain. In very severe cases, they remove a piece of the skull so the brain has more room to expand, then replace it once the swelling goes down. Heldt first began working on a less invasive way to monitor ICP more than 10 years ago, along with George Verghese, the Henry Ellis Warren Professor of Electrical Engineering at MIT, and then-graduate student Faisal Kashif. The researchers published a paper in 2012 in which they developed a way to estimate ICP based on two measurements: arterial blood pressure, which is taken by inserting a catheter at the patient’s wrist, and the velocity of blood flow entering the brain, measured by holding an ultrasound probe to the patient’s temple. For that initial study, the researchers developed a mathematical model of the relationship between blood pressure, cerebral blood flow velocity, and ICP. They tested the model on data collected several years earlier from patients with traumatic brain injury at Cambridge University, with encouraging results. In their new study, the researchers wanted to improve the algorithm that they were using to estimate ICP, and also to develop methods to collect their own data from pediatric patients. They teamed up with Robert Tasker, director of the pediatric neurocritical care program at Boston Children’s Hospital and a co-author of the new paper, to identify patients for the study and help move the technology to the bedside. The system was tested only on patients whose guardians approved the procedure. Arterial blood pressure and ICP were already being measured as part of the patients’ routine monitoring, so the only additional element was the ultrasound measurement. Fanelli also devised a way to automate the data analysis so that only data segments with the highest signal-to-noise ratio were used, making the estimates of ICP more accurate. “We built a signal processing pipeline that was able to automatically detect the segments of data that we could trust versus the segments of data that were too noisy to be used for ICP estimation,” he says. “We wanted to have an automated approach that could be completely user-independent.” Expanded monitoring The ICP estimates generated by this new technique were, on average, within about 1 mmHg of the measurements taken with the invasive method. “From a clinical perspective, it was well within the limits that we would consider useful,” Tasker says. In this study, the researchers focused on patients with severe injuries because those are the patients who already had an invasive ICP measurement being done. However, a less invasive approach could allow ICP monitoring to be expanded to include patients with diseases such as meningitis and encephalitis, as well as malaria, which can all cause brain swelling. “In the past, for these conditions, we would never consider ICP monitoring. What the current research has opened up for us is the possibility that we can include these other patients and try to identify not only whether they’ve got raised ICP but some degree of magnitude to that,” Tasker says. “These findings are very encouraging and may open the way for reliable, non-invasive neuro-critical care,” says Nino Stocchetti, a professor of anesthesia and intensive care medicine at Policlinico of Milan, Italy, who was not involved in the research. “As the authors acknowledge, these results ‘indicate a promising route’ rather than being conclusive: additional work, refinements and more patients remain necessary.” The researchers are now running two additional studies, at Beth Israel Deaconess Medical Center and Boston Medical Center, to test their system in a wider range of patients, including those who have suffered strokes. In addition to helping doctors evaluate patients, the researchers hope that their technology could also help with research efforts to learn more about how elevated ICP affects the brain. “There’s been a fundamental limitation of studying intracranial pressure and its relation to a variety of conditions, simply because we didn’t have an accurate and robust way to get at the measurement noninvasively,” Heldt says. The researchers are also working on a way to measure arterial blood pressure without inserting a catheter, which would make the technology easier to deploy in any location. “This estimate could be of greatest benefit in the pediatrician’s office, the ophthalmologist’s office, the ambulance, the emergency department, so you want to have a completely noninvasive arterial blood pressure measurement,” Heldt says. “We’re working to develop that.” The research was funded by the National Institutes for Neurological Disorders and Stroke, Maxim Integrated Products, and the Boston Children’s Hospital Department of Anesthesiology, Critical Care, and Pain Medicine. ",Health,0.04702439412898664
56,Science Daily,Blood Test Detects Concussion and Subconcussive Injuries in Children and Adults,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092315.htm,"   Subconcussive injuries often show no symptoms or immediate effects, but can cause wear and tear on the brain over time with repeated injuries. The latest study, published in the journal BMJ Paediatrics Open, includes more than 700 emergency room patients -- children and adults. The study gets us closer to developing a standard blood test to spot these injuries as early as possible. ""A unique feature of this study is that it includes patients who hit their heads but have no symptoms,"" said Linda Papa, MD, lead author of the study and emergency medicine doctor at Orlando Health. ""This group is rarely -- if ever -- included in biomarker studies."" The blood test looks for two proteins (GFAP and UCH-L1) found in our brains and released into blood after an injury -- higher levels of which could indicate a concussion or subconcussive injury. Dr. Papa has been studying these biomarkers for more than a decade. Some of her previous studies have focused on athletes, but now she's expanding her research on subconcussive injuries to the general population and all age groups. Historically, people who suffer head trauma without concussion symptoms may have been classified as having ""no injury."" Plus, there are very few studies addressing the impact of subconcussive injuries following head trauma in the civilian population, as opposed to military members or athletes. ""It is estimated that up to 3.8 million concussions occur in the U.S. annually from organized and recreational sports -- and there are more than 2 million ER visits for traumatic brain injuries and concussions,"" said Papa. ""It is a significant health problem in both athletes and non-athletes."" The study looked at patients with concussions, those with head trauma without overt signs of concussion and those with body trauma without head trauma or concussion. Elevated levels of both biomarkers were found in patients with nonconcussive head trauma, potentially signaling a subconcussive brain injury. Furthermore, this blood test goes even deeper than a routine CT scan. Previous studies using the two biomarkers have focused on detecting brain lesions, but subconcussive injuries don't necessarily result in lesions -- and even the vast majority of patients with concussions tend to have a normal CT scan. ""The study includes an array of patients with different injury mechanisms, including car crashes, falls and bicycle accidents in addition to recreational and sports injuries,"" said Papa. ""It is not limited to just one group of injury types."" A number of companies are now working on developing a bench-top device for the hospital lab -- along with a point-of-care handheld device that can be used to detect subconcussive injuries in a variety of settings -- including sporting events, in the ambulance, at the scene of car crashes, in military settings or even after a simple bump to the head. ""The technology is only a year or two away,"" said Papa. ",Health,0.04536155465289154
57,Science Daily,Exercise Is Good for the Aging Brain,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826110409.htm,"   University of Iowa researchers have found that a single bout of exercise improves cognitive functions and working memory in some older people. In experiments that included physical activity, brain scans, and working memory tests, the researchers also found that participants experienced the same cognitive benefits and improved memory from a single exercise session as they did from longer, regular exercise. ""One implication of this study is you could think of the benefits day by day,"" says Michelle Voss, assistant professor in the Department of Psychological and Brain Sciences and the study's corresponding author. ""In terms of behavioral change and cognitive benefits from physical activity, you can say, 'I'm just going to be active today. I'll get a benefit.' So, you don't need to think of it like you're going to train for a marathon to get some sort of optimal peak of performance. You just could work at it day by day to gain those benefits."" Previous research has shown exercise can confer a mental boost. But the benefits vary: One person may improve cognitively and have improved memory, while another person may show little to no gain. Limited research has been done on how a single bout of physical activity may affect cognition and working memory specifically in older populations, despite evidence that some brain functions slip as people age. Voss wanted to tease out how a single session of exercise may affect older individuals. Her team enrolled 34 adults between 60 and 80 years of age who were healthy but not regularly active. Each participant rode a stationary bike on two separate occasions -- with light and then more strenuous resistance when pedaling -- for 20 minutes. Before and after each exercise session, each participant underwent a brain scan and completed a memory test. In the brain scan, the researchers examined bursts of activity in regions known to be involved in the collection and sharing of memories. In the working memory tests, each participant used a computer screen to look at a set of eight young adult faces that rotated every three seconds -- flashcard style -- and had to decide when a face seen two ""cards"" previously matched the one they were currently viewing. After a single exercise session, the researchers found in some individuals increased connectivity between the medial temporal (which surrounds the brain's memory center, the hippocampus) and the parietal cortex and prefrontal cortex, two regions involved in cognition and memory. Those same individuals also performed better on the memory tests. Other individuals showed little to no gain. The boost in cognition and memory from a single exercise session lasted only a short while for those who showed gains, the researchers found. ""The benefits can be there a lot more quickly than people think,"" Voss says. ""The hope is that a lot of people will then keep it up because those benefits to the brain are temporary. Understanding exactly how long the benefits last after a single session, and why some benefit more than others, are exciting directions for future research."" The participants also engaged in regular exercise, pedaling on a stationary bike for 50 minutes three times a week for three months. One group engaged in moderate-intensity pedaling, while another group had a mostly lighter workout in which the bike pedals moved for them. Most individuals in the moderate and lighter-intensity groups showed mental benefits, judging by the brain scans and working memory tests given at the beginning and at the end of the three-month exercise period. But the brain gains were no greater than the improvements from when they had exercised a single time. ""The result that a single session of aerobic exercise mimics the effects of 12 weeks of training on performance has important implications both practically and theoretically,"" the authors write. The researchers note their study had a small participant pool, with a homogenous population that excluded anyone with chronic health conditions or who were taking beta-blockers. To address those limitations, Voss has expanded her participant pool in a current, five-year study to confirm the initial findings and learn more about how exercise alters older people's brains. The participants are healthy older individuals who are not physically active, similar to the participants' profile in the study's results reported here. The National Institute on Aging, part of the National Institutes of Health, funded the research. ",Health,0.04493100017242812
58,Science Daily,Stable Home Lives Improve Prospects for Preemies,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104830.htm,"   Researchers at Washington University School of Medicine in St. Louis who have been trying to determine what puts such children at risk for these problems have found that their mental health may be related less to medical challenges they face after birth than to the environment the babies enter once they leave the newborn intensive care unit (NICU). In a new study, the children who were most likely to have overcome the complications of being born so early and who showed normal psychiatric and neurodevelopmental outcomes also were those with healthier, more nurturing mothers and more stable home lives. The findings are published Aug. 26 in The Journal of Child Psychology and Psychiatry. ""Home environment is what really differentiated these kids,"" said first author Rachel E. Lean, PhD, a postdoctoral research associate in child psychiatry. ""Preterm children who did the best had mothers who reported lower levels of depression and parenting stress. These children received more cognitive stimulation in the home, with parents who read to them and did other learning-type activities with their children. There also tended to be more stability in their families. That suggests to us that modifiable factors in the home life of a child could lead to positive outcomes for these very preterm infants."" The researchers evaluated 125 5-year-old children. Of them, 85 had been born at least 10 weeks before their due dates. The other 40 children in the study were born full-term, at 40 weeks' gestation. The children completed standardized tests to assess their cognitive, language and motor skills. Parents and teachers also were asked to complete checklists to help determine whether a child might have issues indicative of ADHD or autism spectrum disorder, as well as social or emotional problems or behavioral issues. It turned out the children who had been born at 30 weeks of gestation or sooner tended to fit into one of four groups. One group, representing 27% of the very preterm children, was found to be particularly resilient. ""They had cognitive, language and motor skills in the normal range, the range we would expect for children their age, and they tended not to have psychiatric issues,"" Lean said. ""About 45% of the very preterm children, although within the normal range, tended to be at the low end of normal. They were healthy, but they weren't doing quite as well as the more resilient kids in the first group."" The other two groups had clear psychiatric issues such as ADHD, autism spectrum disorder or anxiety. A group of about 13% of the very preterm kids had moderate to severe psychiatric problems. The other 15% of children, identified via surveys from teachers, displayed a combination of problems with inattention and with hyperactive and impulsive behavior. The children in those last two groups weren't markedly different from other kids in the study in terms of cognitive, language and motor skills, but they had higher rates of ADHD, autism spectrum disorder and other problems. ""The children with psychiatric problems also came from homes with mothers who experienced more ADHD symptoms, higher levels of psychosocial stress, high parenting stress, just more family dysfunction in general,"" said senior investigator Cynthia E. Rogers, MD, an associate professor of child psychiatry. ""The mothers' issues and the characteristics of the family environment were likely to be factors for children in these groups with significant impairment. In our clinical programs, we screen mothers for depression and other mental health issues while their babies still are patients in the NICU."" Rogers and Lean believe the findings may indicate good news because maternal psychiatric health and family environment are modifiable factors that can be targeted with interventions that have the potential to improve long-term outcomes for children who are born prematurely. ""Our results show that it wasn't necessarily the clinical characteristics infants faced in the NICU that put them at risk for problems later on,"" Rogers said. ""It was what happened after a baby went home from the NICU. Many people have thought that babies who are born extremely preterm will be the most impaired, but we really didn't see that in our data. What that means is in addition to focusing on babies' health in the NICU, we need also to focus on maternal and family functioning if we want to promote optimal development."" The researchers are continuing to follow the children from the study. This work was supported by the Eunice Kennedy Shriver National Institute of Child Health and Human Development, the National Institute of Neurological Disorders and Stroke and the National Institute of Mental Health of the National Institutes of Health (NIH). Grant numbers R01 HD057098, R01 MH113570, K02 NS089852, UL1 TR000448, K23-MH105179 and U54-HD087011. Additional funding was provided by the Cerebral Palsy International Research Foundation, the Dana Foundation, the Child Neurology Foundation and the Doris Duke Charitable Foundation. ",Health,0.04341515880969083
59,MIT News,Study links certain metabolites to stem cell function in the intestine,Research,2019-08-22,-,http://news.mit.edu/2019/ketones-stem-cell-intestine-0822,"  MIT biologists have discovered an unexpected effect of a ketogenic, or fat-rich, diet: They showed that high levels of ketone bodies, molecules produced by the breakdown of fat, help the intestine to maintain a large pool of adult stem cells, which are crucial for keeping the intestinal lining healthy. The researchers also found that intestinal stem cells produce unusually high levels of ketone bodies even in the absence of a high-fat diet. These ketone bodies activate a well-known signaling pathway called Notch, which has previously been shown to help regulate stem cell differentiation. “Ketone bodies are one of the first examples of how a metabolite instructs stem cell fate in the intestine,” says Omer Yilmaz, the Eisen and Chang Career Development Associate Professor of Biology and a member of MIT’s Koch Institute for Integrative Cancer Research. “These ketone bodies, which are normally thought to play a critical role in energy maintenance during times of nutritional stress, engage the Notch pathway to enhance stem cell function. Changes in ketone body levels in different nutritional states or diets enable stem cells to adapt to different physiologies.” In a study of mice, the researchers found that a ketogenic diet gave intestinal stem cells a regenerative boost that made them better able to recover from damage to the intestinal lining, compared to the stem cells of mice on a regular diet. Yilmaz is the senior author of the study, which appears in the Aug. 22 issue of Cell. MIT postdoc Chia-Wei Cheng is the paper’s lead author. An unexpected role Adult stem cells, which can differentiate into many different cell types, are found in tissues throughout the body. These stem cells are particularly important in the intestine because the intestinal lining is replaced every few days. Yilmaz’ lab has previously shown that fasting enhances stem cell function in aged mice, and that a high-fat diet can stimulate rapid growth of stem cell populations in the intestine. In this study, the research team wanted to study the possible role of metabolism in the function of intestinal stem cells. By analyzing gene expression data, Cheng discovered that several enzymes involved in the production of ketone bodies are more abundant in intestinal stem cells than in other types of cells. When a very high-fat diet is consumed, cells use these enzymes to break down fat into ketone bodies, which the body can use for fuel in the absence of carbohydrates. However, because these enzymes are so active in intestinal stem cells, these cells have unusually high ketone body levels even when a normal diet is consumed. To their surprise, the researchers found that the ketones stimulate the Notch signaling pathway, which is known to be critical for regulating stem cell functions such as regenerating damaged tissue. “Intestinal stem cells can generate ketone bodies by themselves, and use them to sustain their own stemness through fine-tuning a hardwired developmental pathway that controls cell lineage and fate,” Cheng says. In mice, the researchers showed that a ketogenic diet enhanced this effect, and mice on such a diet were better able to regenerate new intestinal tissue. When the researchers fed the mice a high-sugar diet, they saw the opposite effect: Ketone production and stem cell function both declined. Stem cell function The study helps to answer some questions raised by Yilmaz’ previous work showing that both fasting and high-fat diets enhance intestinal stem cell function. The new findings suggest that stimulating ketogenesis through any kind of diet that limits carbohydrate intake helps promote stem cell proliferation. “Ketone bodies become highly induced in the intestine during periods of food deprivation and play an important role in the process of preserving and enhancing stem cell activity,” Yilmaz says. “When food isn’t readily available, it might be that the intestine needs to preserve stem cell function so that when nutrients become replete, you have a pool of very active stem cells that can go on to repopulate the cells of the intestine.” The findings suggest that a ketogenic diet, which would drive ketone body production in the intestine, might be helpful for repairing damage to the intestinal lining, which can occur in cancer patients receiving radiation or chemotherapy treatments, Yilmaz says. The researchers now plan to study whether adult stem cells in other types of tissue use ketone bodies to regulate their function. Another key question is whether ketone-induced stem cell activity could be linked to cancer development, because there is evidence that some tumors in the intestines and other tissues arise from stem cells. “If an intervention drives stem cell proliferation, a population of cells that serve as the origin of some tumors, could such an intervention possibly elevate cancer risk? That’s something we want to understand,” Yilmaz says. “What role do these ketone bodies play in the early steps of tumor formation, and can driving this pathway too much, either through diet or small molecule mimetics, impact cancer formation? We just don’t know the answer to those questions.” The research was funded by the National Institutes of Health, a V Foundation V Scholar Award, a Sidney Kimmel Scholar Award, a Pew-Stewart Trust Scholar Award, the MIT Stem Cell Initiative, the Koch Institute Frontier Research Program through the Kathy and Curt Marble Cancer Research Fund, the Koch Institute Dana Farber/Harvard Cancer Center Bridge Project, and the American Federation of Aging Research. ",Health,0.04226217252942871
60,Science Daily,Simple Blood Test Unmasks Concussions Absent on CT Scans,Health,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092334.htm,"   In a study led by UC San Francisco, researchers tracked 450 patients with suspected traumatic brain injury (TBI) -- which includes concussion or mild TBI -- who had been admitted to one of 18 level 1 trauma centers throughout the nation. The patients, whose injuries were mainly attributed to traffic accidents or falls, all had normal CT scans, according to the study publishing in The Lancet Neurology on Aug. 23, 2019. Within 24 hours of their accidents, the patients had their blood drawn to measure for glial fibrillary acidic protein, a marker correlating to TBI. The study used a device by Abbott Laboratories called i-STAT™ Alinity™, a handheld portable blood analyzer, currently unavailable in the U.S., that produces test results in minutes. The researchers later confirmed the blood test results against MRI, a more sensitive and expensive scan that is not as widely available as CT but offers a more definitive diagnosis of TBI. They found that 120 of these 450 patients (27 percent) had an MRI that was positive for TBI. Patients with TBI Are Not Even Getting a Diagnosis ""Our earlier research has shown that even in the best trauma centers, patients with TBI are not getting the care they need,"" said Geoffrey Manley, MD, PhD, senior author of the study, professor of neurosurgery at UCSF and a member of the Weill Institutes for Neurosciences. ""Now we know that many of these patients with TBI are not even getting a diagnosis."" Manley is also the principal investigator of TRACK TBI, which has analyzed clinical data on more than 3,300 patients and comparison participants, and previously has linked concussion with major depression, post-traumatic stress disorder and cognitive deficits. Work by other UCSF faculty has found correlations between TBI and Parkinson's disease and TBI and dementia. To assess the accuracy of the blood test, researchers compared the results of the patients whose CT-negative TBIs were confirmed by MRI, with a group of healthy participants as well as a cohort of patients with orthopedic injuries. They found that the average protein value of the blood samples of patients with positive MRIs was 31.6 times higher than those with orthopedic injuries and nearly 52 times that of the healthy participants. The protein was elevated even in the patients with normal MRIs, suggesting that the test may be sensitive to injury undetectable by MRI, the researchers noted. In the future, the blood test may help clinicians decide who can safely avoid a CT scan, with the advantage of not exposing patients to radiation from a CT, said first author John Yue, MD, of the UCSF Department of Neurological Surgery. Additionally, the blood test may be a useful tool for those patients in trauma centers and emergency departments, whose symptoms may be altered by substance use, he said. ""Patients with concussion may present as confused and disoriented, and may repeat themselves -- symptoms that are similar in people with intoxication. With the blood test, we may be able to discern whether their symptoms are primarily due to brain injury and treat accordingly."" The blood test may also clarify diagnosis in patients with co-existing conditions or those who take medications that may impact speech and behavior, said Yue. ""These blood-based biomarkers are the next step in the evolution of diagnosing and treating TBI,"" said Manley. ""We are finding that not only are they more sensitive than CT in identifying TBI, but they may be more accurate than the current standard of MRI."" The study follows an earlier TRACK-TBI pilot study that found approximately 30 percent of concussion patients with negative CTs and positive MRIs had disability three months post-injury. ",Health,0.0412983052165743
