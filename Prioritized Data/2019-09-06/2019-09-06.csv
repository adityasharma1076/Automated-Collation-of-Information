,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,MIT News,Creating new opportunities from nanoscale materials,Electronics and Technology,2019-09-05,-,http://news.mit.edu/2019/creating-new-opportunities-nanoscale-materials-0905,"  A hundred years ago, “2d” meant a two-penny, or 1-inch, nail. Today, “2-D” encompasses a broad range of atomically thin flat materials, many with exotic properties not found in the bulk equivalents of the same materials, with graphene — the single-atom-thick form of carbon — perhaps the most prominent. While many researchers at MIT and elsewhere are exploring two-dimensional materials and their special properties, Frances M. Ross, the Ellen Swallow Richards Professor in Materials Science and Engineering, is interested in what happens when these 2-D materials and ordinary 3-D materials come together. “We’re interested in the interface between a 2-D material and a 3-D material because every 2-D material that you want to use in an application, such as an electronic device, still has to talk to the outside world, which is three-dimensional,” Ross says. “We’re at an interesting time because there are immense developments in instrumentation for electron microscopy, and there is great interest in materials with very precisely controlled structures and properties, and these two things cross in a fascinating way,” says Ross.  “The opportunities are very exciting,” Ross says. “We’re going to be really improving the characterization capabilities here at MIT.” Ross specializes in examining how nanoscale materials grow and react in both gases and liquid media, by recording movies using electron microscopy. Microscopy of reactions in liquids is particularly useful for understanding the mechanisms of electrochemical reactions that govern the performance of catalysts, batteries, fuel cells, and other important technologies. “In the case of liquid phase microscopy, you can also look at corrosion where things dissolve away, while in gases you can look at how individual crystals grow or how materials react with, say, oxygen,” she says. Ross joined the Department of Materials Science and Engineering (DMSE) faculty last year, moving from the nanoscale materials analysis department at the IBM Thomas J. Watson Research Center. “I learned a tremendous amount from my IBM colleagues and hope to extend our research in material design and growth in new directions,” she says. Recording movies During a recent visit to her lab, Ross explained an experimental setup donated to MIT by IBM. An ultra-high vacuum evaporator system arrived first, to be attached later directly onto a specially designed transmission electron microscope. “This gives powerful possibilities,” Ross explains. “We can put a sample in the vacuum, clean it, do all sorts of things to it such as heating and adding other materials, then transfer it under vacuum into the microscope, where we can do more experiments while we record images. So we can, for example, deposit silicon or germanium, or evaporate metals, while the sample is in the microscope and the electron beam is shining through it, and we are recording a movie of the process.” While waiting this spring for the transmission electron microscope to be set up, members of Ross’ seven-member research group, including materials science and engineering postdoc Shu Fen Tan and graduate student Kate Reidy, made and studied a variety of self-assembled structures. The evaporator system was housed temporarily on the fifth-level prototyping space of MIT.nano while Ross’s lab was being readied in Building 13. “MIT.nano had the resources and space; we were happy to be able to help,” says Anna Osherov, MIT.nano assistant director of user services. “All of us are interested in this grand challenge of materials science, which is: ‘How do you make a material with the properties you want and, in particular, how do you use nanoscale dimensions to tweak the properties, and create new properties, that you can’t get from bulk materials?’” Ross says. Using the ultra-high vacuum system, graduate student Kate Reidy formed structures of gold and niobium on several 2-D materials. “Gold loves to grow into little triangles,” Ross notes. “We’ve been talking to people in physics and materials science about which combinations of materials are the most important to them in terms of controlling the structures and the interfaces between the components in order to give some improvement in the properties of the material,” she notes. Shu Fen Tan synthesized nickel-platinum nanoparticles and examined them using another technique, liquid cell electron microscopy. She could arrange for only the nickel to dissolve, leaving behind spiky skeletons of platinum. “Inside the liquid cell, we are able to see this whole process at high spatial and temporal resolutions,” Tan says. She explains that platinum is a noble metal and less reactive than nickel, so under the right conditions the nickel participates in an electrochemical dissolution reaction and the platinum is left behind. Platinum is a well-known catalyst in organic chemistry and fuel cell materials, Tan notes, but it is also expensive, so finding combinations with less-expensive materials such as nickel is desirable. “This is an example of the range of materials reactions you can image in the electron microscope using the liquid cell technique,” Ross says. “You can grow materials; you can etch them away; you can look at, for example, bubble formation and fluid motion.” A particularly important application of this technique is to study cycling of battery materials. “Obviously, I can’t put an AA battery in here, but you could set up the important materials inside this very small liquid cell and then you can cycle it back and forth and ask, if I charge and discharge it 10 times, what happens? It does not work just as well as before — how does it fail?” Ross asks. “Some kind of failure analysis and all the intermediate stages of charging and discharging can be observed in the liquid cell.” “Microscopy experiments where you see every step of a reaction give you a much better chance of understanding what’s going on,” Ross says. Moiré patterns Graduate student Reidy is interested in how to control the growth of gold on 2-D materials such as graphene, tungsten diselenide, and molybdenum disulfide. When she deposited gold on “dirty” graphene, blobs of gold collected around the impurities. But when Reidy grew gold on graphene that had been heated and cleaned of impurities, she found perfect triangles of gold. Depositing gold on both the top and bottom sides of clean graphene, Reidy saw in the microscope features known as moiré patterns, which are caused when the overlapping crystal structures are out of alignment. The gold triangles may be useful as photonic and plasmonic structures. “We think this could be important for a lot of applications, and it is always interesting for us to see what happens,” Reidy says. She is planning to extend her clean growth method to form 3-D metal crystals on stacked 2-D materials with various rotation angles and other mixed-layer structures. Reidy is interested in the properties of graphene and hexagonal boron nitride (hBN), as well as two materials that are semiconducting in their 2-D single-layer form, molybdenum disulfide (MoS2) and tungsten diselenide (WSe2). “One aspect that’s very interesting in the 2-D materials community is the contacts between 2-D materials and 3-D metals,” Reidy says. “If they want to make a semiconducting device or a device with graphene, the contact could be ohmic for the graphene case or a Schottky contact for the semiconducting case, and the interface between these materials is really, really important.” “You can also imagine devices using the graphene just as a spacer layer between two other materials,” Ross adds. For device makers, Reidy says it is sometimes important to have a 3-D material grow with its atomic arrangement aligned perfectly with the atomic arrangement in the 2-D layer beneath. This is called epitaxial growth. Describing an image of gold grown together with silver on graphene, Reidy explains, “We found that silver doesn’t grow epitaxially, it doesn’t make those perfect single crystals on graphene that we wanted to make, but by first depositing the gold and then depositing silver around it, we can almost force silver to go into an epitaxial shape because it wants to conform to what its gold neighbors are doing.” Electron microscope images can also show imperfections in a crystal such as rippling or bending, Reidy notes. “One of the great things about electron microscopy is that it is very sensitive to changes in the arrangement of the atoms,” Ross says. “You could have a perfect crystal and it would all look the same shade of gray, but if you have a local change in the structure, even a subtle change, electron microscopy can pick it up. Even if the change is just within the top few layers of atoms without affecting the rest of the material beneath, the image will show distinctive features that allow us to work out what’s going on.” Reidy also is exploring the possibilities of combining niobium — a metal that is superconducting at low temperatures — with a 2-D topological insulator, bismuth telluride. Topological insulators have fascinating properties whose discovery resulted in the Nobel Prize in Physics in 2016. “If you deposit niobium on top of bismuth telluride, with a very good interface, you can make superconducting junctions. We’ve been looking into niobium deposition, and rather than triangles we see structures that are more dendritic looking,” Reidy says. Dendritic structures look like the frost patterns formed on the inside of windows in winter, or the feathery patterns of some ferns. Changing the temperature and other conditions during the deposition of niobium can change the patterns that the material takes. All the researchers are eager for new electron microscopes to arrive at MIT.nano to give further insights into the behavior of these materials. “Many things will happen within the next year, things are ramping up already, and I have great people to work with. One new microscope is being installed now in MIT.nano and another will arrive next year. The whole community will see the benefits of improved microscopy characterization capabilities here,” Ross says. MIT.nano’s Osherov notes that two cryogenic transmission electron microscopes (cryo-TEM) are installed and running. “Our goal is to establish a unique microscopy-centered community. We encourage and hope to facilitate a cross-pollination between the cryo-EM researchers, primarily focused on biological applications and ‘soft’ material, as well as other research communities across campus,” she says. The latest addition of a scanning transmission electron microscope with enhanced analytical capabilities (ultrahigh energy resolution monochromator, 4-D STEM detector, Super-X EDS detector, tomography, and several in situ holders) brought in by John Chipman Associate Professor of Materials Science and Engineering James M. LeBeau, once installed, will substantially enhance the microscopy capabilities of the MIT campus. “We consider Professor Ross to be an immense resource for advising us in how to shape the in situ approach to measurements using the advanced instrumentation that will be shared and available to all the researchers within the MIT community and beyond,” Osherov says. Little drinking straws “Sometimes you know more or less what you are going to see during a growth experiment, but very often there’s something that you don’t expect,” Ross says. She shows an example of zinc oxide nanowires that were grown using a germanium catalyst. Some of the long crystals have a hole through their centers, creating structures which are like little drinking straws, circular outside but with a hexagonally shaped interior. “This is a single crystal of zinc oxide, and the interesting question for us is why do the experimental conditions create these facets inside, while the outside is smooth?” Ross asks. “Metal oxide nanostructures have so many different applications, and each new structure can show different properties. In particular, by going to the nanoscale you get access to a diverse set of properties.” “Ultimately, we’d like to develop techniques for growing well-defined structures out of metal oxides, especially if we can control the composition at each location on the structure,” Ross says. A key to this approach is self-assembly, where the material builds itself into the structure you want without having to individually tweak each component. “Self-assembly works very well for certain materials but the problem is that there’s always some uncertainty, some randomness or fluctuations. There’s poor control over the exact structures that you get. So the idea is to try to understand self-assembly well enough to be able to control it and get the properties that you want,” Ross says. “We have to understand how the atoms end up where they are, then use that self-assembly ability of atoms to make a structure we want. The way to understand how things self-assemble is to watch them do it, and that requires movies with high spatial resolution and good time resolution,” Ross explains. Electron microscopy can be used to acquire structural and compositional information and can even measure strain fields or electric and magnetic fields. “Imagine recording all of these things, but in a movie where you are also controlling how materials grow within the microscope. Once you have made a movie of something happening, you analyze all the steps of the growth process and use that to understand which physical principles were the key ones that determined how the structure nucleated and evolved and ended up the way it does.” Future directions Ross hopes to bring in a unique high-resolution, high vacuum TEM with capabilities to image materials growth and other dynamic processes. She intends to develop new capabilities for both water-based and gas-based environments. This custom microscope is still in the planning stages but will be situated in one of the rooms in the Imaging Suite in MIT.nano. “Professor Ross is a pioneer in this field,” Osherov says. “The majority of TEM studies to-date have been static, rather than dynamic. With static measurements you are observing a sample at one particular snapshot in time, so you don’t gain any information about how it was formed. Using dynamic measurements, you can look at the atoms hopping from state to state until they find the final position. The ability to observe self-assembling processes and growth in real time provides valuable mechanistic insights. We’re looking forward to bringing these advanced capabilities to MIT.nano.” she says. “Once a certain technique is disseminated to the public, it brings attention,” Osherov says. “When results are published, researchers expand their vision of experimental design based on available state-of-the-art capabilities, leading to many new experiments that will be focused on dynamic applications.” Rooms in MIT.nano feature the quietest space on the MIT campus, designed to reduce vibrations and electromagnetic interference to as low a level as possible. “There is space available for Professor Ross to continue her research and to develop it further,” Osherov says. “The ability of in situ monitoring the formation of matter and interfaces will find applications in multiple fields across campus, and lead to a further push of the conventional electron microscopy limits.” ",Electronics and Technology,0.1618873712978666
1,Science Daily,Time Saving Software in an Age of Ever-Expanding Data,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145448.htm,"   It is hard to get people excited about research software says Eliza Grames, a PhD candidate in ecology and evolutionary biology. Yet, the software she has developed is exciting and to understand why, it is important to put yourself into the shoes of a researcher. Before embarking on a new research project, a thorough and exhaustive review of existing literature must be done to make sure the new project is novel. Researchers can also explore the entire body of previously published data on a subject to answer a new question using that same data. This is a daunting task, especially considering that millions of new research articles are published each year. Where does one even begin to explore all of that data? ""Each new study contributes more to what we know about a topic, adding nuance and complexity that helps improve our understanding of the natural world. To make sense of this wealth of evidence and get closer to a complete picture of the world, researchers are increasingly turning to systematic review methods as a way to synthesize this information,"" says Grames. ""It is important to find all of the relevant information and to not find too much of it,"" says Grames. The way to perform this search is through something called a systematic review, which Grames says started in the fields of medicine and public health, where keeping current with research can be a question of life or death. ""In those fields, there is an established system with Medical Subject Headers where articles get tagged with keywords associated with the work, but ecology does not have that."" Other fields of research across the scientific spectrum were in the same boat. The project sprang out of need. In her own process of reviewing, Grames noted she would miss articles and key terms and was interested in finding out how to identify those missing terms on the first try. ""As we were working on this software, we realized there was a much faster way to do the reviews than how others were doing them,"" says Grames, ""The traditional way was mostly going through papers and pulling out a term and then reading the rest of the article to identify more terms to use."" Even with fairly specific key words, Grames notes the average systematic review in her field of conservation biology initially yields about 10,000 research papers for bigger projects. It is important to retrieve relevant information, without also retrieving too much irrelevant information. ""Each year, the amount of data just keeps increasing. There are some systematic reviews that if you look at the amount of time they would have taken just three years ago, they would take about 300 days to perform. If the same reviews were done today, they would take about 350 days because the number of publications just keeps going up and up."" Grames says it took about a month or two to hash out ideas for the software, then she spent a summer writing and fixing the code. The result is an open-source software package called litsearchr. How it works, says Grames, is that a user will input their best faith effort of putting together a search into a few databases. ""The keywords should be fairly relevant to retrieve articles that are entered into the algorithm to extract all of the potential keywords, which are then put into a network. The original keywords are at the center of the network and are the most well-connected."" Grames says the time required to develop a search strategy has been decreased by 90%. Presented with the most relevant articles, researchers then have significantly fewer papers to parse through manually. This review stage itself is partially automated now too, adds Grames. Litsearchr is part of a collaborative effort by researchers, called metaverse, where the goal is to link several software packages together so researchers can perform their research from start to finish in the same coding language, R. ""Researchers can develop their systematic reviews, import data, and there is even a package that can write up the results section for the systematic review,"" says Grames. Grames and her team set up the software so that it could be used by anyone, whether they can code or not, using templates ready to upload information to. There is also a detailed step-by-step video to take users through the process. By keeping the software open source, Grames says debugging and editing is improved because users can point out details that need attention, ""Every time I get an email, it is so exciting. It is nice to have it open because people can let me know when there is a typo."" The software is currently being used by researchers in scientific fields such as nutritional science and psychology and for a massive undertaking of screening all papers pertaining to insect populations across the globe. Grames says it is nice to have the software in place to be able to take on such a big project. ""There is no way we could do this project without the level of automation we get using litsearchr. I built this out of a need from another project, but this software is making it possible to do even bigger analyses than before."" ",Computer Science,0.15544414853981056
2,MIT News,Exotic physics phenomenon is observed for first time,Research,2019-09-05,-,http://news.mit.edu/2019/aharonov-bohm-effect-physics-observed-0905,"  An exotic physical phenomenon, involving optical waves, synthetic magnetic fields, and time reversal, has been directly observed for the first time, following decades of attempts. The new finding could lead to realizations of what are known as topological phases, and eventually to advances toward fault-tolerant quantum computers, the researchers say. The new finding involves the non-Abelian Aharonov-Bohm Effect and is reported today in the journal Science by MIT graduate student Yi Yang, MIT visiting scholar Chao Peng (a professor at Peking University), MIT graduate student Di Zhu, Professor Hrvoje Buljan at University of Zagreb in Croatia, Francis Wright Davis Professor of Physics John Joannopoulos at MIT, Professor Bo Zhen at the University of Pennsylvania, and MIT professor of physics Marin Soljačić. The finding relates to gauge fields, which describe transformations that particles undergo. Gauge fields fall into two classes, known as Abelian and non-Abelian. The Aharonov-Bohm Effect, named after the theorists who predicted it in 1959, confirmed that gauge fields — beyond being a pure mathematical aid — have physical consequences. But the observations only worked in Abelian systems, or those in which gauge fields are commutative — that is, they take place the same way both forward and backward in time.  In 1975, Tai-Tsun Wu and Chen-Ning Yang generalized the effect to the non-Abelian regime as a thought experiment. Nevertheless, it remained unclear whether it would even be possible to ever observe the effect in a non-Abelian system. Physicists lacked ways of creating the effect in the lab, and also lacked ways of detecting the effect even if it could be produced. Now, both of those puzzles have been solved, and the observations carried out successfully. The effect has to do with one of the strange and counterintuitive aspects of modern physics, the fact that virtually all fundamental physical phenomena are time-invariant. That means that the details of the way particles and forces interact can run either forward or backward in time, and a movie of how the events unfold can be run in either direction, so there’s no way to tell which is the real version. But a few exotic phenomena violate this time symmetry. Creating the Abelian version of the Aharonov-Bohm effects requires breaking the time-reversal symmetry, a challenging task in itself, Soljačić says. But to achieve the non-Abelian version of the effect requires breaking this time-reversal multiple times, and in different ways, making it an even greater challenge. To produce the effect, the researchers use photon polarization. Then, they produced two different kinds of time-reversal breaking. They used fiber optics to produce two types of gauge fields that affected the geometric phases of the optical waves, first by sending them through a crystal biased by powerful magnetic fields, and second by modulating them with time-varying electrical signals, both of which break the time-reversal symmetry. They were then able to produce interference patterns that revealed the differences in how the light was affected when sent through the fiber-optic system in opposite directions, clockwise or counterclockwise. Without the breaking of time-reversal invariance, the beams should have been identical, but instead, their interference patterns revealed specific sets of differences as predicted, demonstrating the details of the elusive effect. The original, Abelian version of the Aharonov-Bohm effect “has been observed with a series of experimental efforts, but the non-Abelian effect has not been observed until now,” Yang says. The finding “allows us to do many things,” he says, opening the door to a wide variety of potential experiments, including classical and quantum physical regimes, to explore variations of the effect. The experimental approach devised by this team “might inspire the realization of exotic topological phases in quantum simulations using photons, polaritons, quantum gases, and superconducting qubits,” Soljačić says. For photonics itself, this could be useful in a variety of optoelectronic applications, he says. In addition, the non-Abelian gauge fields that the group was able to synthesize produced a non-Abelian Berry phase, and “combined with interactions, it may potentially one day serve as a platform for fault-tolerant topological quantum computation,” he says. At this point, the experiment is primarily of interest for fundamental physics research, with the aim of gaining a better understanding of some basic underpinnings of modern physical theory. The many possible practical applications “will require additional breakthroughs going forward,” Soljačić says. For one thing, for quantum computation, the experiment would need to be scaled up from one single device to likely a whole lattice of them. And instead of the beams of laser light used in their experiment, it would require working with a source of single individual photons. But even in its present form, the system could be used to explore questions in topological physics, which is a very active area of current research, Soljačić says. “The non-Abelian Berry phase is a theoretical gem that is the doorway to understanding many intriguing ideas in contemporary physics,” says Ashvin Vishwanath, a professor of physics at Harvard University, who was not associated with this work. “I am glad to see it getting the experimental attention it deserves in the current work, which reports a well-controlled and characterized realization. I expect this work to stimulate progress both directly as a building block for more complex architectures, and also indirectly in inspiring other realizations.” ",Electronics and Technology,0.14869567959133276
3,Science Daily,Using Nature to Produce a Revolutionary Optical Material,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905124512.htm,"   The work, published in the journal Nature Communications, also describes a superior manner of telecom switching without the use of electronics; instead, they use an all-optical method that could improve the speed and capacity of internet communications. That could remove a roadblock in moving from 4GLTE to 5G networks. The team reported that a material created using tellurium nanorods -- produced by naturally occurring bacteria -- is an effective nonlinear optical material, capable of protecting electronic devices against high-intensity bursts of light, including those emitted by inexpensive household lasers targeted at aircraft, drones or other critical systems. The researchers describe the material and its performance as a material of choice for next-generation optoelectronic and photonic devices. Seamus Curran, a physics professor at the University of Houston and one of the paper's authors, said while most optical materials are chemically synthesized, using a biologically-based nanomaterial proved less expensive and less toxic. ""We found a cheaper, easier, simpler way to manufacture the material,"" he said. ""We let Mother Nature do it."" The new findings grew out of earlier work by Curran and his team, working in collaboration with Werner J. Blau of Trinity College Dublin and Ron Oremland with the U.S. Geological Survey. Curran initially synthesized the nanocomposites to examine their potential in the photonics world. He holds a U.S. and international series of patents for that work. The researchers noted that using bacteria to create the nanocrystals suggests an environmentally friendly route of synthesis, while generating impressive results. ""Nonlinear optical measurements of this material reveal the strong saturable absorption and nonlinear optical extinctions induced by Mie scattering overbroad temporal and wavelength ranges,"" they wrote. ""In both cases, Te [tellurium] particles exhibit superior optical nonlinearity compared to graphene."" Light at very high intensity, such as that emitted by a laser, can have unpredictable polarizing effects on certain materials, Curran said, and physicists have been searching for suitable nonlinear materials that can withstand the effects. One goal, he said, is a material that can effectively reduce the light intensity, allowing for a device to be developed that could prevent damage by that light. The researchers used the nanocomposite, made up of biologically generated elemental tellurium nanocrystals and a polymer to build an electro-optic switch -- an electrical device used to modulate beams of light -- that is immune to damage from a laser, he said. Oremland noted that the current work grew out of 30 years of basic research, stemming from their initial discovery of selenite-respiring bacteria and the fact that the bacteria form discrete packets of elemental selenium. ""From there, it was a step down the Periodic Table to learn that the same could be done with tellurium oxyanions,"" he said. ""The fact that tellurium had potential application in the realm of nanophotonics came as a serendipitous surprise."" Blau said the biologically generated tellurium nanorods are especially suitable for photonic device applications in the mid-infrared range. ""This wavelength region is becoming a hot technological topic as it is useful for biomedical, environmental and security-related sensing, as well as laser processing and for opening up new windows for fiber optical and free-space communications."" Work will continue to expand the material's potential for use in all-optical telecom switches, which Curran said is critical in expanding broadband capacity. ""We need a massive investment in optical fiber,"" he said. ""We need greater bandwidth and switching speeds. We need all-optical switches to do that."" ",Electronics and Technology,0.13476698522663821
4,Science Daily,Future of LEDs Gets Boost from Verification of Localization States in InGaN Quantum Wells,Matter & Energy,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904135642.htm,"   In the Journal of Applied Physics, from AIP Publishing, researchers in China report an InGaN LED structure with high luminescence efficiency and what is believed to be the first direct observation of transition carriers between different localization states within InGaN. The localization states were confirmed by temperature-dependent photoluminescence and excitation power-dependent photoluminescence. Localization states theory is commonly used to explain the high luminescence efficiency gained via the large number of dislocations within InGaN materials. Localization states are the energy minima states believed to exist within the InGaN quantum well region (discrete energy values), but a direct observation of localization states was elusive until now. ""Based primarily on indium content fluctuations, we explored the 'energy minima' that remain within the InGaN quantum well region,"" said Yangfeng Li, the paper's lead author and a now postdoctoral fellow at the Hong Kong University of Science and Technology. ""Such energy minima will capture the charge carriers -- electrons and holes -- and prevent them from being captured by defects (dislocations). This means that the emission efficiency is less affected by the large number of defects."" The group's direct observation of localization states is an important discovery for the future of LEDs, because it verifies their existence, which was a long-standing open scientific question. ""Segregation of indium may be one of the reasons causing localization states,"" said Li. ""Due to the existence of localization states, the charge carriers will mainly be captured in the localization states rather than by nonradiative recombination defects. This improves the high luminescence efficiency of light-emitting devices."" Based on the group's electroluminescence spectra, ""the InGaN sample with stronger localization states provides more than a twofold enhancement of the light-output at the same current-injection conditions as samples of weaker localization states,"" Li said. The researchers' work can serve as a reference about the emission properties of InGaN materials for use in manufacturing LEDs and laser diodes. They plan to continue to explore gallium nitride-related materials and devices ""not only to gain a better understanding of their localizations but also the properties of InGaN quantum dots, which are semiconductor particles with potential applications in solar cells and electronics,"" Li said. ""We hope that other researchers will also conduct in-depth theoretical studies of localization states."" ",Electronics and Technology,0.1326670785664116
5,Science Daily,Silicon as a Semiconductor: Silicon Carbide Would Be Much More Efficient,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111650.htm,"   Energy consumption is growing across the globe; electric power is being relied upon more and more, and sustainable energy supplies such as wind and solar power are becoming increasingly important. Electric power, however, is often generated a long distance away from the consumer. Efficient distribution and transport systems are thus just as crucial as transformer stations and power converters that turn the generated direct current into alternating current. Huge savings are possible Modern power electronics must be able to handle large currents and high voltages. Current transistors made of semiconductor materials for field-effect transistors are now mainly based on silicon technology. Significant physical and chemical advantages, however, arise from the use of SiC over silicon: in addition to a much higher heat resistance, this material provides significantly better energy efficiency, which could lead to massive savings. It is known that these advantages are significantly compromised by defects at the interface between silicon carbide and the insulating material silicon dioxide. This damage is based on tiny, irregular clusters of carbon rings bound in the crystal lattice, as experimentally demonstrated by researchers led by Professor Thomas Jung at the Swiss Nanoscience Institute and Department of Physics from the University of Basel and the Paul Scherrer Institute. Using atomic force microscope analysis and Raman spectroscopy, they showed that the defects are generated in the vicinity of the interface by the oxidation process. Experimentally confirmed The interfering carbon clusters, which are only a few nanometers in size, are formed during the oxidation process of silicon carbide to silicon dioxide under high temperatures. ""If we change certain parameters during oxidation, we can influence the occurrence of the defects,"" says doctoral student Dipanwita Dutta. For example, a nitrous oxide atmosphere in the heating process leads to significantly fewer carbon clusters. The experimental results were confirmed by the team led by Professor Stefan Gödecker at the Department of Physics and Swiss Nanoscience Institute from the University of Basel. Computer simulations confirmed the structural and chemical changes induced by graphitic carbon atoms as observed experimentally. Beyond experiments, atomistic insight has been gained in the generation of the defects and their impact on the electron flow in the semiconductor material. Better use of electricity ""Our studies provide important insight to drive the onward development of field-effect transistors based on silicon carbide. Therefore we expect to provide a significant contribution to the more effective use of electrical power,"" comments Jung. The work was initiated as part of the Nano Argovia program for applied research projects. ",Matter & Energy,0.11823714217952738
6,IEEE,Soft Self-Healing Materials for Robots That Cannot Be Destroyed,Robotics,2019-09-06,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed,"      If there’s one thing we know about robots, it’s that they break. They break, like, literally all the time. The software breaks. The hardware breaks. The bits that you think could never, ever, ever possibly break end up breaking just when you need them not to break the most, and then you have to try to explain what happened to your advisor who’s been standing there watching your robot fail and then stay up all night fixing the thing that seriously was not supposed to break. While most of this is just a fundamental characteristic of robots that can’t be helped, the European Commission is funding a project called SHERO (Self HEaling soft RObotics) to try and solve at least some of those physical robot breaking problems through the use of structural materials that can autonomously heal themselves over and over again.  SHERO is a three year, €3 million collaboration between Vrije Universiteit Brussel, University of Cambridge, École Supérieure de Physique et de Chimie Industrielles de la ville de Paris (ESPCI-Paris), and Swiss Federal Laboratories for Materials Science and Technology (Empa). As the name SHERO suggests, the goal of the project is to develop soft materials that can completely recover from the kinds of damage that robots are likely to suffer in day to day operations, as well as the occasional more extreme accident.  Most materials, especially soft materials, are fixable somehow, whether it’s with super glue or duct tape. But fixing things involves a human first identifying when they’re broken, and then performing a potentially skill, labor, time, and money intensive task. SHERO’s soft materials will, eventually, make this entire process autonomous, allowing robots to self-identify damage and initiate healing on their own. What these self-healing materials can do is really pretty amazing. The researchers are actually developing two different types—the first one heals itself when there’s an application of heat, either internally or externally, which gives some control over when and how the healing process starts. For example, if the robot is handling stuff that’s dirty, you’d want to get it cleaned up before healing it so that dirt doesn’t become embedded in the material. This could mean that the robot either takes itself to a heating station, or it could activate some kind of embedded heating mechanism to be more self-sufficient. The second kind of self-healing material is autonomous, in that it will heal itself at room temperature without any additional input, and is probably more suitable for relatively minor scrapes and cracks. Here are some numbers about how well the healing works: Autonomous self-healing polymers do not require heat. They can heal damage at room temperature. Developing soft robotic systems from autonomous self-healing polymers excludes the need of additional heating devices… The healing however takes some time. The healing efficiency after 3 days, 7 days and 14 days is respectively 62 percent, 91 percent and 97 percent.  This material was used to develop a healable soft pneumatic hand. Relevant large cuts can be healed entirely without the need of external heat stimulus. Depending on the size of the damage and even more on the location of damage, the healing takes only seconds or up to a week. Damage on locations on the actuator that are subjected to very small stresses during actuation was healed instantaneously. Larger damages, like cutting the actuator completely in half, took 7 days to heal. But even this severe damage could be healed completely without the need of any external stimulus. Both of these materials can be mixed together, and their mechanical properties can be customized so that the structure that they’re a part of can be tuned to move in different ways. The researchers also plan on introducing flexible conductive sensors into the material, which will help sense damage as well as providing position feedback for control systems. A lot of development will happen over the next few years, and for more details, we spoke with Bram Vanderborght at Vrije Universiteit in Brussels. IEEE Spectrum: How easy or difficult or expensive is it to produce these materials? Will they add significant cost to robotic grippers? Bram Vanderborght: They are definitely more expensive materials, but it’s also a matter of size of production. At the moment, we’ve made a few kilograms of the material (enough to make several demonstrators), and the price already dropped significantly from when we ordered 100 grams of the material in the first phase of the project. So probably the cost of the gripper will be higher [than a regular gripper], but you won’t need to replace the gripper as often as other grippers that need to be replaced due to wear, so it can be an advantage. Moreover due to the method of 3D printing the material, the surface is smoother and airtight (so no post-processing is required to make it airtight). Also, the smooth surface is better to avoid contamination for food handling, for example.  In commercial or industrial applications, gradual fatigue seems to be a more common issue than more abrupt trauma like cuts. How well does the self-healing work to improve durability over long periods of time? We did not test for gradual fatigue over very long times. But both macroscopic and microscopic damage can be healed. So hopefully it can provide an answer here as well.   How much does the self-healing capability restrict the material properties? What are the limits for softness or hardness or smoothness or other characteristics of the material? Typically the mechanical properties of networked polymers are much better than thermoplastics. Our material is a networked polymer but in which the crosslinks are reversible. We can change quite a lot of parameters in the design of the materials. So we can develop very stiff (fracture strain at 1.24 percent) and very elastic materials (fracture strain at 450 percent). The big advantage that our material has is we can mix it to have intermediate properties. Moreover, at the interface of the materials with different mechanical properties, we have the same chemical bonds, so the interface is perfect. While other materials, they may need to glue it, which gives local stresses and a weak spot. When the material heals itself, is it less structurally sound in that spot? Can it heal damage that happens to the same spot over and over again? In theory we can heal it an infinite amount of times. When the wound is not perfectly aligned, of course in that spot it will become weaker. Also too high temperatures lead to irreversible bonds, and impurities lead to weak spots. Besides grippers and skins, what other potential robotics applications would this technology be useful for? Most of self healing materials available now are used for coatings. What we are developing are structural components, therefore the mechanical properties of the material need to be good for such applications. So maybe part of the skeleton of the robot can be developed with such materials to make it lighter, since can be designed for regular repair. And for exceptional loads, it breaks and can be repaired like our human body. [ SHERO Project ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11117129308485446
7,MIT News,Taking the next giant leaps,Space & Time,2019-09-05,-,http://news.mit.edu/2019/taking-next-giant-leaps-0905,"  In July, the world celebrated the 50th anniversary of the historic Apollo 11 moon landing. MIT played an enormous role in that accomplishment, helping to usher in a new age of space exploration. Now MIT faculty, staff, and students are working toward the next great advances — ones that could propel humans back to the moon, and to parts still unknown.   “I am hard-pressed to think of another event that brought the world together in such a collective way as the Apollo moon landing,” says Daniel Hastings, the Cecil and Ida Green Education Professor and head of the Department of Aeronautics and Astronautics (AeroAstro). “Since the spring, we have been celebrating the role MIT played in getting us there and reflecting on how far technology has come in the past five decades.”  “Our community continues to build on the incredible legacy of Apollo,” Hastings adds. Some aspects of future of space exploration, he notes, will follow from lessons learned. Others will come from newly developed technologies that were unimaginable in the 1960s. And still others will arise from novel collaborations that will fuel the next phases of research and discovery.  “This is a tremendously exciting time to think about the future of space exploration,” Hastings says. “And MIT is leading the way.” Sticking the landing Making a safe landing — anywhere — can be a life-or-death situation. On Earth, thanks to a network of global positioning satellites and a range of ground-based systems, pilots have instantaneous access to real-time data on every aspect of a landing environment. The moon, however, is not home to any of this precision navigation technology, making it rife with potential danger.  NASA’s recent decision to return to moon has made this a more pressing challenge — and one that MIT has risen to before. The former MIT Instrumentation Lab (now the independent Draper) developed the guidance systems that enabled Neil Armstrong and Buzz Aldrin to land safely on the moon, and that were used on all Apollo spacecraft. This system relied on inertial navigation, which integrates acceleration and velocity measurements from electronic sensors on the vehicle and a digital computer to determine the spacecraft’s location. It was a remarkable achievement — the first time that humans traveled in a vehicle controlled by a computer.   Today, working in MIT’s Aerospace Controls Lab with Jonathan How, the Richard Cockburn Maclaurin Professor of Aeronautics and Astronautics, graduate student Lena Downes — who is also co-advised by Ted Steiner at Draper — is developing a camera-based navigation system that can sense the terrain beneath the landing vehicle and use that information to update the location estimation. “If we want to explore a crater to determine its age or origin,” Downes explains, “we will need to avoid landing on the more highly-sloped rim of the crater. Since lunar landings can have errors as high as several kilometers, we can’t plan to land too closely to the edge.”  Downes’s research on crater detection involves processing images using convolutional neural networks and traditional computer vision methods. The images are combined with other data, such as previous measurements and known crater location information, enabling increased precision vehicle location estimation. “When we return to the moon, we want to visit more interesting locations, but the problem is that more interesting can often mean more hazardous,” says Downes. “Terrain-relative navigation will allow us to explore these locations more safely.” “Make it, don’t take it” NASA also has its sights set on Mars — and with that objective comes a very different challenge: What if something breaks? Given that the estimated travel time to Mars is between 150 and 300 days, there is a relatively high chance that something will break or malfunction during flight. (Just ask Jim Lovell or Fred Haise, whose spacecraft needed serious repairs only 55 hours and 54 minutes into the Apollo 13 mission.) Matthew Moraguez, a graduate student in Professor Olivier L. de Weck’s Engineering Systems Lab, wants to empower astronauts to manufacture whatever they need, whenever they need it. (“On the fly,” you could say). “In-space manufacturing (ISM) — where astronauts can carry out the fabrication, assembly, and integration of components — could revolutionize this paradigm,” says Moraguez. “Since components wouldn’t be limited by launch-related design constraints, ISM could reduce the cost and improve the performance of existing space systems while also enabling entirely new capabilities.” Historically, a key challenge facing ISM is correctly pairing the components with manufacturing processes needed to produce them. Moraguez approached this problem by first defining the constraints created by a stressful launch environment, which can limit the size and weight of a payload. He then itemized the challenges that could potentially be alleviated by ISM and developed cost-estimating relationships and performance models to determine the exact break-even point at which ISM surpasses the current approach.  Moraguez points to Made in Space, an additive manufacturing facility that is currently in use on the International Space Station. The facility produces tools and other materials as needed, reducing both the cost and the wait time of replenishing supplies from Earth. Moraguez is now developing physics-based manufacturing models that will determine the size, weight, and power required for the next generation of ISM equipment. “We have been able to evaluate the commercial viability of ISM across a wide range of application areas,” says Moraguez. “Armed with this framework, we aim to determine the best components to produce with ISM and their appropriate manufacturing processes. We want to develop the technology to a point where it truly revolutionizes the future of spaceflight. Ultimately, it could allow humans to travel further into deep space for longer durations than ever before,” he says.  Partnering with industry The MIT Instrumentation Lab was awarded the first contract for the Apollo program in 1961. In one brief paragraph on a Western Union telegram, the lab was charged with developing the program’s guidance and control system. Today the future of space exploration depends as much as ever on deep collaborations.  Boeing is a longstanding corporate partner of MIT, supporting such efforts as the Wright Brother’s Wind Tunnel renovation and the New Engineering Education Transformation (NEET) program, which focuses on modern industry and real-world projects in support of MIT’s educational mission. In 2020, Boeing is slated to open the Aerospace and Autonomy Center in Kendall Square, which will focus on advancing enabling technologies for autonomous aircraft. Just last spring the Institute announced a new relationship with Blue Origin in which it will begin planning and developing new payloads for missions to the moon. These new science experiments, rovers, power systems, and more will hitch a ride to the moon via Blue Moon, Blue Origin’s flexible lunar lander.  Working with IBM, MIT researchers are exploring the potential uses of artificial intelligence in space research. This year, IBM’s AI Research Week (Sept. 16-20) will feature an event, co-hosted with AeroAstro, in which researchers will pitch ideas for projects related to AI and the International Space Station. “We are currently in an exciting new era marked by the development and growth of entrepreneurial private enterprises driving space exploration,” says Hastings. “This will lead to new and transformative ways for human beings to travel to space, to create new profit-making ventures in space for the world’s economy, and, of course, lowering the barrier of access to space so many other countries can join this exciting new enterprise.” ",Space & Time,0.10092420426196314
8,Science Daily,Closing in on Elusive Particles,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905161426.htm,"   While the Standard Model of Particle Physics has remained mostly unchanged since its initial conception, experimental observations for neutrinos have forced the neutrino part of the theory to be reconsidered in its entirety. Neutrino oscillation was the first observation inconsistent with the predictions and proves that neutrinos have non-zero masses, a property that contradicts the Standard Model. In 2015, this discovery was rewarded with the Nobel Prize. Are neutrinos their own antiparticles? Additionally, there is the longstanding conjecture that neutrinos are so-called Majorana particles: Unlike all other constituents of matter, neutrinos might be their own antiparticles. This would also help explain why there is so much more matter than antimatter in the Universe. The GERDA experiment is designed to scrutinize the Majorana hypothesis by searching for the neutrinoless double beta decay of the germanium isotope 76-Ge: Two neutrons inside a 76-Ge nucleus simultaneously transform into two protons with the emission of two electrons. This decay is forbidden in the Standard Model because the two antineutrinos -- the balancing antimatter -- are missing. The Technical University of Munich (TUM) has been a key partner of the GERDA project (GERmanium Detector Array) for many years. Prof. Stefan Schönert, who heads the TUM research group, is the speaker of the new LEGEND project. The GERDA experiment achieves extreme levels of sensitivity GERDA is the first experiment to reach exceptionally low levels of background noise and has now surpassed the half-life sensitivity for decay of 10^26 years. In other words: GERDA proves that the process has a half-life of at least 10^26 years, or 10,000,000,000,000,000 times the age of the Universe. Physicists know that neutrinos are at least 100,000 times lighter than electrons, the next heaviest particles. What mass they have exactly, however, is still unknown and another important research topic. In the standard interpretation, the half-life of the neutrinoless double beta decay is related to a special variant of the neutrino mass called the Majorana mass. Based the new GERDA limit and those from other experiments, this mass must be at least a million times smaller than that of an electron, or in the terms of physicists, less than 0.07 to 0.16 eV/c^2. Consistent with other experiments Also other experiments limit the neutrino mass: the Planck mission provides a limit on another variant of the neutrino mass: The sum of the masses of all known neutrino types is less than 0.12 to 0.66 eV/c^2. The tritium decay experiment KATRIN at the Karlsruhe Institute of Technology (KIT) is set-up to measure the neutrino mass with a sensitivity of about 0.2 eV/c^2 in the coming years. These masses are not directly comparable, but they provide a cross check on the paradigm that neutrinos are Majorana particles. So far, no discrepancy has been observed. From GERDA to LEGEND During the reported data collection period, GERDA operated detectors with a total mass of 35.6 kg of 76-Ge. Now, a newly formed international collaboration, LEGEND, will increase this mass to 200 kg of 76-Ge until 2021 and further reduce the background noise. The aim is to achieve a sensitivity of 10^27 years within the next five years. ",Space & Time,0.10070515222181549
9,Science Daily,Role of Earthquake Motions in Triggering a 'Surprise' Tsunami,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905090931.htm,"   The tsunami was as surprising to scientists as it was devastating to communities in Sulawesi. It occurred near an active plate boundary, where earthquakes are common. Surprisingly, the earthquake caused a major tsunami, although it primarily offset the ground horizontally -- normally, large-scale tsunamis are typically caused by vertical motions. Researchers were at a loss -- what happened? How was the water displaced to create this tsunami: by landslides, faulting, or both? Satellite data of the surface rupture suggests relatively straight, smooth faults, but do not cover areas offshore, such as the critical Palu Bay. Researchers wondered -- what is the shape of the faults beneath Palu Bay and is this important for generating the tsunami? This earthquake was extremely fast. Could rupture speed have amplified the tsunami? Using a supercomputer operated by the Leibniz Supercomputing Centre, a member of the Gauss Centre for Supercomputing, the team showed that the earthquake-induced movement of the seafloor beneath Palu Bay itself could have generated the tsunami, meaning the contribution of landslides is not required to explain the tsunami's main features. The team suggests an extremely fast rupture on a straight, tilted fault within the bay. In their model, slip is mostly lateral, but also downward along the fault, resulting in anywhere from 0.8 metres to 2.8 metres vertical seafloor change that averaged 1.5 metres across the area studied. Critical to generating this tsunami source are the tilted fault geometry and the combination of lateral and extensional strains exerted on the region by complex tectonics. The scientists come to this conclusion using a cutting-edge, physics-based earthquake-tsunami model. The earthquake model, based on earthquake physics, differs from conventional data-driven earthquake models, which fit observations with high accuracy at the cost of potential incompatibility with real-world physics. It instead incorporates models of the complex physical processes occurring at and off of the fault, allowing researchers to produce a realistic scenario compatible both with earthquake physics and regional tectonics. The researchers evaluated the earthquake-tsunami scenario against multiple available datasets. Sustained supershear rupture velocity, or when the earthquake front moves faster than the seismic waves near the slipping faults, is required to match simulation to observations. The modeled tsunami wave amplitudes match the available wave measurements and the modeled inundation elevation (defined as the sum of the ground elevation and the maximum water height) qualitatively match field observations. This approach offers a rapid, physics-based evaluation of the earthquake-tsunami interactions during this puzzling sequence of events. ""Finding that earthquake displacements probably played a critical role generating the Palu tsunami is as surprising as the very fast movements during the earthquake itself,"" said Thomas Ulrich, PhD student at Ludwig Maximilian University of Munich and lead author of the paper. ""We hope that our study will launch a much closer look on the tectonic settings and earthquake physics potentially favouring localized tsunamis in similar fault systems worldwide."" ",Environment,0.097902806051581
10,MIT News,Cleaning up hydrogen peroxide production,Computer Science,2019-09-05,-,http://news.mit.edu/2019/solugen-hydrogen-peroxide-0905,"  The most common process for making hydrogen peroxide begins with a highly toxic, flammable working solution that is combined with hydrogen, filtered, combined with oxygen, mixed in water, and then concentrated to extremely high levels for shipping. The transportation process is equally convoluted. Most of the massive chemical plants that make hydrogen peroxide are located in Russia and China. For big markets like the U.S. oil and gas industry, concentrated hydrogen peroxide is usually freighted to America, diluted, then shipped via rail or truck to places like western Texas, where a service company buys it and pumps it for the customer. All of this complexity masks the fact that hydrogen peroxide is structurally simple. In fact, a large group of specialized proteins, called enzymes, have long been known to work with hydrogen peroxide in various biological systems. But translating that knowledge into a more natural way to create hydrogen peroxide has proven difficult — until recently. For the past few years, the startup Solugen, which was co-founded by an MIT alumnus, has been producing hydrogen peroxide by combining genetically modified enzymes with organic compounds like plant sugars. The reaction creates bio-based hydrogen peroxide as well as organic acids, and the company says this method is cheaper, safer, and far less toxic than traditional processes. Solugen currently has two pilot facilities in Texas that produce more than 10 tons of hydrogen peroxide per day, with a much larger site opening next summer. The technology has the potential to decarbonize the production of an extremely common chemical used for a host of consumer and industrial applications. Science companies like Solugen are often started by researchers who have spent years studying a specific problem. Their success often hinges on securing government grants or corporate partnerships. But Solugen has a much more colorful history. The company can attribute its success to research into pancreatic cancer, a Facebook group of float spa enthusiasts, a fruitful splurge at Home Depot, and the emergence of several fields that make Solugen’s solution possible. Getting by with help from Facebook friends Solugen co-founder Gaurab Chakrabarti was in medical school studying pancreatic cancer in 2015 when he discovered an enzyme in cancer cells that could function in extremely high concentrations of hydrogen peroxide. The enzyme required another expensive chemical to be useful in reactions, so Chakrabarti partnered with Sean Hunt SM ’13 PhD ’16, whom he’d befriended while attending medical school with Hunt’ wife. Hunt was studying more traditional chemical processing methods for his PhD when Chakrabarti showed him the enzyme. “My background is not in biotech, so I’m kind of the recovering biotech skeptic,” Hunt says. “I learned about enzymes in school, and everyone knew how active and selective they were, but they were just so unstable and hard to manufacture.” Using computational protein design methods, Hunt and Chakrabarti were able to genetically modify the enzyme to make it produce hydrogen peroxide at room temperature when combined with cheap organic compounds like sugar. Soon after, the founders were finalists in the 2016 MIT $100K pitch competition, earning $10,000. But they still weren’t sure the technology was worth pursuing. Then they were contacted by a Facebook group of float spa enthusiasts. Float spas suspend people in salty waters while shutting out all noise and light to help them achieve sensory deprivation. Hydrogen peroxide is used to keep float spa waters clean. “There’s about 400 float spas in the U.S., and they’re all on one Facebook group, and one owner saw our MIT $100K pitch video and shared it to the Facebook group,” Hunt explains. “That’s really what made us continue Solugen that summer. Because we were contacted by these float spa owners saying, ‘This is how much we pay for peroxide. If you guys can make it, we’ll buy it.’” Emboldened, the founders rented cheap lab space in Dallas and sent one of their early enzyme designs to a protein manufacturer in China. Then Hunt spent $7,000 at Home Depot to create a pilot reactor he describes as “this little PVC bubble column.” Running out of money, the founders bought 55 gallon drums of sugar and ran them through the reactor with their enzyme, watching triumphantly as organic acids and hydrogen peroxide came out the other end. The founders began selling all the peroxide they could produce, sometimes sleeping on the floor to keep the reactor running through the night. By December of 2016, they were making $10,000 a month selling pails of peroxide to the float spa community. The company used its PVC bubble reactor until the summer of 2017, when they built a fully automated reactor capable of producing 10 times more hydrogen peroxide. That’s when they moved into the oil and gas industry. A big, toxic problem As companies pump oil and gas out of the ground, they generate large amounts of contaminated salt water that needs to be treated or disposed of. Billions of gallons of such water are produced every day in the U.S. alone. Hydrogen peroxide is commonly used in the treatment process, but the traditional methods for creating hydrogen peroxide, in addition to being dangerous, leave a huge carbon footprint associated with the constant venting of the working solution. “What I really love about this is it’s a true environmental crisis that I think we’re making a big difference on,” Hunt says, noting other chemicals used to treat wastewater are extremely toxic and don’t biodegrade like hydrogen peroxide does. Solugen’s current production facilities ship concentrated forms of hydrogen peroxide, but the founders plan on building “minimills” next to oil and gas plants that don’t require concentration and dilution, to further reduce costs and improve sustainability. “When we were building these things out, we realized that because we’re doing all this chemistry with enzymes where it’s room temperature, in water, and low pressure, it’s very safe, and as a result we can build these small plants,” Hunt says. “That’s really exciting for us. … For instance, you can sell hydrogen peroxide for $2 a gallon. It costs $1.50 a gallon just to ship it to the customer. The freight is almost the price of the chemical. And in some instances, it’s more than the chemical itself.” Solugen’s solution is also intriguing because it couldn’t have existed until recently. To make its proprietary enzymes, the company is leveraging fairly new methods for computational protein design and genetic engineering. It also relies on an industry of protein contract manufacturers that can produce large amounts of engineered enzymes for far cheaper than what would have been possible even five years ago. Looking forward, Hunt says Solugen’s infrastructure could be used to co-produce hundreds of different organic acids by changing the enzymes and compounds being mixed. One of the co-products he’s most excited about is acetic acid, which is used to make vinegar. Acetic acid is also used in the production of important materials like polyester fiber and plastic. “Hydrogen peroxide and acetic acid are fundamental building blocks for our economy,” Hunt says. “We see Solugen as a platform [for other solutions]. In the long term, that’s what really excites us.” ",Matter & Energy,0.0945229068041643
11,Science Daily,People Can See Beauty in Complex Mathematics,Health,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905090944.htm,"   The study, published in science journal Cognition, showed people even agreed on what made such abstract mathematical arguments beautiful. The findings may have implications for teaching schoolchildren, who may not be entirely convinced that there is beauty in mathematics. The similarities between mathematics and music have long been noted but the study co-authors, Yale mathematician Stefan Steinerberger and University of Bath psychologist Dr. Samuel G.B.Johnson, wanted to add art to the mix to see if there was something universal at play in the people judge aesthetics and beauty -- be they in art, music or abstract mathematics. The research was sparked when Steinerberger, while teaching his students, likened a mathematical proof to a 'really good Schubert sonata' -- but couldn't put his finger on why. He approached Johnson, assistant professor of marketing at the University of Bath School of Management, who was completing his Ph.D. in psychology at Yale. Johnson designed an experiment to test his question of whether people share the same aesthetic sensibilities about maths that they do about art or music -- and if this would hold true for an average person, not just a career mathematician. For the study, they chose four mathematical proof, four landscape paintings, and four classical piano pieces. None of the participants was a mathematician. The mathematical proofs used were: the sum of an infinite geometric series, Gauss's summation trick for positive integers, the Pigeonhole principle, and a geometric proof of a Faulhaber formula. A mathematical proof is an argument which convinces people something is true. The piano pieces were Schubert's Moment Musical No. 4, D 780 (Op. 94), Bach's Fugue from Toccata in E Minor (BWV 914), Beethoven's Diabelli Variations (Op. 120) and Shostakovich's Prelude in D-flat major (Op.87 No. 15). The landscape paintings were Looking Down Yosemite Valley, California by Albert Bierstadt; A Storm in the Rocky Mountains, Mt. Rosalie by Albert Bierstadt; The Hay Wain by John Constable; and The Heart of the Andes by Frederic Edwin Church. Johnson divided the study into three parts. The first task required a sample of individuals to match the four maths proofs to the four landscape paintings based on how aesthetically similar they found them. The second task required a different group of people to compare the four maths proofs to the four piano sonatas. Finally, the third asked another sample group to rate each of the four works of art and mathematical arguments for nine different criteria -- seriousness, universality, profundity, novelty, clarity, simplicity, elegance, intricacy, and sophistication. Participants in the third group agreed with each other about how elegant, profound, clear, etc., each of the mathematical arguments and paintings was. But Steinerberger and Johnson were most impressed that these ratings could be used to predict how similar participants in the first group believed that each argument and painting were to each other. This finding suggests that perceived correspondences between maths and art really have to do with their underlying beauty. Overall, the results showed there was considerable consensus in comparing mathematical arguments to artworks. And there was some consensus in judging the similarity of classical piano music and mathematics. ""Laypeople not only had similar intuitions about the beauty of math as they did about the beauty of art but also had similar intuitions about beauty as each other. In other words, there was consensus about what makes something beautiful, regardless of modality,"" Johnson said. However, it was not clear whether the results would be the same with different music. ""I'd like to see our study done again but with different pieces of music, different proofs, different artwork,"" said Steinerberger. ""We demonstrated this phenomenon, but we don't know the limits of it. Where does it stop existing? Does it have to be classical music? Do the paintings have to be of the natural world, which is highly aesthetic?"" Both Steinerberger and Johnson believe the research may have implications for maths education, especially at the secondary-school level. ""There might be opportunities to make the more abstract, more formal aspects of mathematics more accessible and more exciting to students at that age,"" said Johnson, ""And that might be useful in terms of encouraging more people to enter the field of mathematics."" ",Society,0.0933176498979799
12,Science Daily,Underwater Soundscapes Reveal Differences in Marine Environments,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904153958.htm,"   Using underwater acoustic monitors, researchers listened in on Stellwagen Bank National Marine Sanctuary off the coast of Boston; Glacier Bay National Park and Preserve in Alaska; National Park of American Samoa; and Buck Island Reef National Monument in the Virgin Islands. They found that the ambient sounds varied widely across the sites and were driven by differences in animal vocalization rates, human activity and weather. The findings demonstrate that sound monitoring is an effective tool for assessing conditions and monitoring changes, said Samara Haver, a doctoral candidate in the College of Agricultural Sciences at OSU and the study's lead author. ""This is a relatively economical way for us to get a ton of information about the environment,"" said Haver, who studies marine acoustics and works out of the Cooperative Institute for Marine Resources Studies, a partnership between OSU and the National Oceanic and Atmospheric Administration at the Hatfield Marine Science Center in Newport. ""Documenting current and potentially changing conditions in the ocean soundscape can provide important information for managing the ocean environment."" The findings were published recently in the journal Frontiers in Marine Science. Co-authors include Robert Dziak, an acoustics scientist with NOAA who holds a courtesy appointment in OSU's College of Earth, Ocean, and Atmospheric Sciences; and other researchers from OSU, NOAA, Cornell University and the National Park Service. Passive acoustic monitoring is seen as a cost-effective and low-impact method for monitoring the marine environment. The researchers' goal was to test how effective acoustic monitoring would be for long-term assessment of underwater conditions. ""Ocean noise levels have been identified as a potential measure for effectiveness of conservation efforts, but until now comparing sound across different locations has been challenging,"" Haver said. ""Using equipment that was calibrated across all of the sites, we were able to compare the sound environments of these diverse areas in the ocean."" The researchers collected low frequency, passive acoustic recordings from each of the locations between 2014 and 2018. They compared ambient sounds as well as sounds of humpback whales, a species commonly found in all four locations. The inclusion of the humpback whale sounds -- mostly songs associated with mating in the southern waters, and feeding or social calls in the northern waters -- gives researchers a way to compare the sounds of biological resources across all the soundscapes, Haver said. The researchers found that ambient sound levels varied across all four study sites and sound levels were driven by differences in animal vocalization rates, human activity and weather. The highest sound levels were found in Stellwagen Bank during the winter/spring, driven by higher animal sound rates, vessel activity and high wind speeds. The lowest sound levels were found in Glacier Bay in the summer. ""Generally, the Atlantic areas were louder, especially around Stellwagen, than the Pacific sites,"" Haver said. ""That makes sense, as there is generally more human-made sound activity in the Atlantic. There also was a lot of vessel noise in the Caribbean."" The researchers also were able to hear how sound in the ocean changes before, during and after hurricanes and other severe storms; the monitoring equipment captured Hurricanes Maria and Irma in the Virgin Islands and Tropical Cyclone Winston in American Samoa. Ultimately, the study provides a baseline for these four regions and can be used for comparison over time. Documenting current and potentially changing conditions in the ocean soundscape can provide important information for managing the ocean environment, particularly in and around areas that have been designated as protected, Haver said. ",Environment,0.08721582874633822
13,Science Daily,Solutions to Urban Heat Differ Between Tropical and Drier Climes,Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904135649.htm,"   Understanding a city's heat island effect is critical for developing strategies to reduce energy use and stave off dangerously high temperatures, said Elie Bou-Zeid, one of the study's authors and a professor in Princeton's Department of Civil and Environmental Engineering. The analysis suggests that cooling cities by planting more vegetation may be more effective in drier regions than in wetter ones. Using summer temperature data from more than 30,000 world cities, Bou-Zeid worked with colleagues at ETH Zurich and Duke University to develop a new model for urban heat islands. The novelty of the model is that it uses population and precipitation as proxies for a complex array of factors involving climate, environment and urban engineering. One of the model's benefits is its simplicity. Although it cannot capture details of individual cities, it can give planners a quick and broadly accurate view of possible solutions and their effects on a city's temperature. ""There are a few cities -- New York, London, Baltimore -- that are studied intensively, and we don't know very much about a large range of other cities,"" said Bou-Zeid. ""With a reduced model that only needs information on precipitation and population, we are hoping to provide a simple framework that can give guidance to any city"" in planning heat mitigation efforts. The heat island effect, defined in the study as the surface temperature difference between urban and rural areas, is somewhat larger for cities with higher populations. One key reason is that these cities tend to have larger areas as well as more high-rise buildings that do not dissipate heat as effectively as lower structures. The researchers also found that the heat island effect increases as a city's average annual precipitation increases, since its surroundings become greener and cooler, but only up to a point. Beyond a precipitation level of about 39 inches (100 centimeters) per year -- similar to that of Washington, D.C. -- a city's temperature boost does not rise much above 2 degrees Fahrenheit (1.25 degrees Celsius). This has implications for approaches to cooling wetter cities. Although planting vegetation can lower city temperatures through evapotranspiration, there are limits to this approach. Southeast Asian cities such as Singapore have high precipitation and large areas of green cover, but have strong urban heat island effects because nearby rainforests inevitably contain far more plant life than the city. On the other hand, drier cities such as Phoenix can be even cooler than surrounding areas in the summer if irrigation is used to grow plants in the city. ""In places that are already wet and vegetated, adding more vegetation is not going to help,"" explained Bou-Zeid. Lowering summer temperatures in these cities will require different solutions, such as increasing shading or ventilation, or building with novel materials. Still, all cities can reap other benefits from green areas, such as improved air quality and opportunities for recreation. ""Our results show that there is no one-size-fits-all solution to reduce city-scale warming,"" said Gabriele Manoli, a research fellow at ETH Zurich and the study's lead author. ""The efficiency of heat mitigation strategies varies across geographic regions, and any effort aimed at greening and cooling world cities should be put in the context of local hydro-climatic conditions."" Given that urban areas will face the combined effects of global climate change and population growth, these results can provide guidance for the climate-sensitive design of future cities. Bou-Zeid and his colleagues are working to extend their model to examine seasonal variations in the urban heat island effect. Their framework could also be used to create more tailored models for specific regions of the world. ",Environment,0.08622432844371904
14,Science Daily,How Your Brain Remembers Motor Sequences,Health,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828100539.htm,"   Contrary to the common assumption, the researchers found that overlapping regions in the premotor and parietal cortices represent the sequences in multiple levels of motor hierarchy (e.g., chunks of a few finger movements, or chunks of a few chunks), whereas the individual finger movements (i.e., the lowest level in the hierarchy) were uniquely represented in the primary motor cortex. These results uncovered the first detailed map of cortical sequence representation in the human brain. The results may also provide some clue for locating new candidate brain areas as signal sources for motor BCI application or developing more sophisticated algorithm to reconstruct complex motor behavior. The results were published online as Yokoi and Diedrichsen ""Neural Organization of Hierarchical Motor Sequence Representations in the Human Neocortex"" in Neuron on July 22, 2019. Achievements The best way to remember/produce long and complex motor sequences is to divide them into several smaller pieces recursively. For example, a musical piece may be remembered as a sequence of smaller chunks, with each chunk representing a group of often co-occurring notes. Such hierarchical organization has long been thought to underlie our control of motor sequences from the highly skillful actions, like playing music, to daily behavior, like making a cup of tea. Yet, very little is known about how these hierarchies are implemented in our brain. In a new study published in a journal Neuron, Atsushi Yokoi, Center for Information and Neural Networks (CiNet), NICT, and Jörn Diedrichsen, Brain and Mind Institute, Western Univ., provide the first direct evidence of how hierarchically organized sequences are represented through the population activity across the human cerebral cortex. The researchers measured the fine-grained fMRI activity patterns, while human participants produced 8 different remembered sequences of 11 finger presses. ""Remembering 8 different sequences of 11 finger presses is a tough task, so you will definitely need to organize them hierarchically,"" says Diedrichsen, the study's senior author and a Western Research Chair for Motor Control and Computational Neuroscience at the Western University, Canada. ""To study a hierarchy, you would really need the sequences to have this much of complexity. And currently it's very hard to train animals to learn such sequences,"" added Yokoi, the study's lead author who is a former postdoctoral researcher at the Diedrichsen's group since both were at the Institute of Cognitive Neuroscience, University College London in UK, and now a Researcher at the CiNet, NICT, Japan. Through a series of careful behavioural analyses, the researchers could show that participants encoded the sequences in terms of a three-level hierarchy; (1) individual finger presses, (2) chunks consisting of two or three finger presses, and (3) entire sequences consisting of four chunks. They could then characterize the fMRI activity patterns with respect to these hierarchies using machine learning techniques. As expected, the patterns in primary motor cortex, the area that controls finger movements, seemed to only depend on each individual finger moved, independent of it positioning in the sequence. Activity in higher-order motor areas, such as premotor and parietal cortices, clearly could be shown to encode the sequential context at the level of chunks or entire sequence. Thus, in contrast to primary motor cortex, these areas ""know"" what was played before and what comes after the ongoing finger press. For the first time the study now allowed insights into the organization of these higher-order representations. Surprisingly, different levels of sequence information overlapped greatly. An unsupervised clustering approach further subdivided these areas into distinct clusters, each had a different mixing ratio of the representations, just like how one's iPhone storage is used. These results uncovered the first detailed map of cortical sequence representation in the human brain. Study's impact One common assumption in the cognitive neuroscience has been that each level in the functional hierarchy would mirror the anatomical hierarchy, from the higher, association cortices (e.g., premotor or parietal cortices) down to the primary sensorimotor cortices. The mysterious coexistence of a clear anatomical separation (i.e., individual finger vs. other representations) and an overlap (i.e., chunk and sequence representations) sheds new light on the classical question of the correspondence between functional and anatomical hierarchies. ""It can be said the brain represents motor sequences in partly hierarchical, yet partly flat ways."" ""Although its functional role is still unclear, the anatomical overlap between chunk and sequence representations may suggest these representations in upper movement hierarchy may influence with each other to support flexible sequence production. This needs to be tested in the future study,"" Yokoi concluded. Future prospects The study also suggests possible loci from which we can record brain signal to control neural prosthetics to make fluent movement sequences in potential BCI applications. The researchers also hope that it could also contribute in developing a new decoding algorithm that effectively combines the information in different hierarchies to reproduce movements. The study was conducted under an international collaboration between NICT (Japan), UCL (UK), and Western University (Canada). ",Health,0.08086860571225399
15,Science Daily,"From the Tropics to the Boreal, Temperature Drives Ecosystem Functioning",Environment,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165229.htm,"   ""Temperature influences many ecological processes and has been used to explain patterns of biodiversity for over a century; however, we still don't have a clear understanding of how temperature influences the functioning of ecosystems,"" Buzzard said. But by measuring and comparing the traits of diverse species to understand how they function in their environment across a range of temperatures, the team uncovered how temperature influences an ecosystem. They found that temperature drives coordinated shifts in the functional traits between plants and microbes that influence ecosystems, according to Buzzard, who is the lead author on the paper published in Nature Ecology and Evolution on Aug. 19. ""The work represents an unprecedented monitoring of soils and forests from hot tropical forests to the cold boreal forests and fills important gaps in our ecological understanding of how organisms within different levels of an ecosystem's food chain are linked via temperature,"" Enquist said of the project that began in 2011. ""The work involved much field work in remote locations, lab work associated with analyzing soil microbial DNA and computer analyses using large datasets."" As an example, bacteria within certain communities have genes tailored by evolution for cycling the nutrients that are naturally available within their ecosystem. The team saw a shift in the genes tied to nutrient cycling for bacteria as temperatures differed across sites. ""As you increase the latitude -- so, cooler temperatures -- we have a nitrogen limitation. We expect that to influence the structuring of these communities, both plants and microbes,"" Enquist said. In tropical forests, on the other hand, trees quickly grow and shed very broad leaves. That means these ""throw-away leaves,"" as Buzzard put it, constantly fall to the floor for microbes to consume. The research team saw a lower abundance of genes in the local microbes for processing carbon. In forests with pine trees that sprout and drop dense, narrow leaves, the local bacteria had different functional traits: They have a greater abundance of carbon-cycling genes to handle the complex-difficult to access large pools of carbon that is available in temperate regions. It's like eating lettuce (tropics) verse eating bark (temperate regions), according to Buzzard. ""We can use this understanding to make predictions about how we expect soil microbial communities to function as climate changes,"" Buzzard said. ""If temperature drives the observed shift in plant and bacterial functioning, ecosystems subjected to climate warming should also experience directional shifts in functional diversity and biogeochemistry."" That shift might happen too quickly for ecosystems to adapt. She also added that diversity is not solely driven by temperature. There could be other constraining factors that could be teased out in another study. Next, Buzzard said the team will install more sites to collect more data. They also want to monitor how growth rates in plants vary across ecosystems with differing temperatures. This means much more time in the field, but that's no problem for Buzzard. She spent at least six months a year for the first three years of the study in the field: ""I really enjoyed being in the field. There are long days. They're hard, but you get to go see these amazing places and have unique interactions with the wildlife."" ",Environment,0.08001983826905491
16,Science Daily,Diversity Increases Ecosystem Stability,Environment,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905094056.htm,"   As the researchers state, there is increasing scientific evidence of positive relationships between the diversity of tree species and ecosystem functioning. However most studies on this relationship to date have used either data from forests where the influence of biodiversity cannot be separated from other factors, or from young planted experiments, which do not provide data on longer periods of time. Therefore, the Freiburg research team analyzed data from the Sardinilla experiment which was planted in Panama in 2001. This experiment covers 22 plots planted with one, two, three or five native tree species. Since these grow at different rates, the plots with a greater variety of species also have a greater structural diversity with regard to the height and diameter of the trees. Annual data on the size and height of the trees, which are seen as indicators of the productivity and stability of the ecosystem, come from the period 2006 to 2016. The study concludes that mixtures of two and three tree species have on average a 25 to 30 per cent higher productivity than monocultures, and those with five species even 50 percent higher. The differences during a severe dry period caused by the tropical climate phenomenon El Niño were especially pronounced. This indicates that forests with a greater diversity of tree species are not only more productive, but also more stable and resilient under drought stress -- the researchers believe this is a particularly important finding in view of global climate change. In the context of initiatives that aim to reduce atmospheric CO2 with extensive reforestation, these results indicate that to store the same amount of CO2 in biomass, far less space is needed with mixed-species forests. According to the team, these results offer new insights into the dynamics of tropical plantation forests and emphasize the importance of analyses that cover a longer development period, since they contribute to a better understanding of the connections between the diversity, productivity and stability of ecosystems. The study is based on Florian Schnabel's master thesis, for which he will be receiving the Hansjürg-Steinlin prize, a University of Freiburg award for new talent, in October at the start of the 2019/20 academic year. Florian Schnabel is now a PhD student involved in the TreeDì project at the German Centre for Integrative Biodiversity Research (iDiv) in Leipzig. ",Environment,0.07943097923028078
17,Science Daily,"Plant Research Could Benefit Wastewater Treatment, Biofuels and Antibiotics",Environment,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905080134.htm,"   The study is in the journal Proceedings of the National Academy of Sciences. The researchers used a new DNA sequencing approach to study the genome of Spirodela polyrhiza, one of 37 species of duckweed, which are small, fast-growing aquatic plants found worldwide. The scientists discovered how the immune system of Spirodela polyrhiza adapts to a polluted environment in a way that differs from land plants. They identified the species' powerful genes that protect against a wide range of harmful microbes and pests, including waterborne fungi and bacteria. The study could help lead to the use of duckweed strains for bioreactors that recycle wastes, and to make drugs and other products, treat agricultural and industrial wastewater and make biofuels such as ethanol for automobiles. Duckweed could also be used to generate electricity. ""The new gene sequencing approach is a major step forward for the analysis of entire genomes in plants and could lead to many societal benefits,"" said co-author Joachim Messing, Distinguished University Professor and director of the Waksman Institute of Microbiology at Rutgers University-New Brunswick. Duckweed can also serve as protein- and mineral-rich food for people, farmed fish, chickens and livestock, especially in developing countries, according to Eric Lam, a Distinguished Professor in Rutgers' School of Environmental and Biological Sciences who was not part of this study. Lam's lab is at the vanguard of duckweed farming research and development. His team houses the world's largest collection of duckweed species and their 900-plus strains. The lead author was in Messing's laboratory and now has her own laboratory at Shanghai Jiao Tong University in China. Scientists at the Chinese Academy of Sciences and Chinese Academy of Agricultural Sciences contributed to the study. ",Environment,0.07821459630426911
18,Science Daily,Mathematical Model Provides New Support for Environmental Taxes,Science,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904141300.htm,"   A worldwide ""green development"" movement calls for reducing pollution and increasing resource utilization efficiency without hindering economic expansion. Many governments have proposed or imposed environmental taxes, such as taxes on carbon emissions, to promote environmentally friendly economic practices. However, few studies have rigorously quantified the effects of environmental taxes on the interconnected factors involved in green development. To help clarify the impact of environmental taxation, Fan and colleagues developed and validated a mathematical model that reflects the closely integrated relationships between environmental taxes, economic growth, pollution emissions, and utilization of resources, such as water and fossil fuels. Then they applied the model to real-world data in order to analyze the effects of environmental taxes on green development in China. The analysis suggests that environmental taxes can indeed help to stimulate economic growth, decrease emissions, and improve resource utilization. The researchers explored several different scenarios, finding that the beneficial effects of an environmental tax are enhanced by advanced technology, elevated consumer awareness, and -- especially -- firm government control. The authors suggest that their model could be applied to explore the effects of environmental taxes in other countries beyond China. Researchers may also seek to modify the model for application to different industries or economic sectors, as opposed to countries or regions. The model could potentially be improved by identification and incorporation of more sophisticated mathematical relationships between the various green development factors. ",Environment,0.0763637000002547
19,Science Daily,Groundwater Studies Can Be Tainted by 'Survivor Bias',Environment,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905080043.htm,"   Researchers at the University of Waterloo uncovered the problem while examining a discrepancy between scientific data and anecdotal evidence in southern India. Reports on thousands of wells and satellite images taken between 1996 and 2016 suggested groundwater levels were rising, good news in an area where it is vitally important for agriculture. At the same time, however, fieldworkers were hearing more stories from farmers about wells running dry, suggesting levels were actually declining. Researchers solved the apparent paradox by first obtaining census data that backed up the anecdotal evidence. It showed, for example, that more farmers were digging expensive deep wells in the hard-rock aquifer. ""If indeed groundwater levels are going up, why would farmers choose to pay more and dig deeper wells?"" asked Nandita Basu, a civil and environmental engineering professor. ""It didn't make sense."" Researchers then examined the well data and found that those with missing water level data were often excluded from analysis because they were considered unreliable. When the excluded wells were added back into the mix, the results confirmed the evidence from farmers that groundwater levels were decreasing, not increasing. ""They were systematically picking the wells with a lot of data and potentially ignoring the wells that were going dry because they had incomplete data,"" said Tejasvi Hora, an engineering PhD student who led the research. The culprit was identified as something called 'survivor bias,' a statistical phenomenon that results in the exclusion of negative data. When wells ran dry, there were no water levels to report. That created gaps in reports for those wells, and their incomplete data was then discarded as inferior to the complete data from good wells that hadn't run dry. Basu, also a professor of earth and environmental sciences and a member of the Water Institute at Waterloo, said the lesson from southern India is applicable anywhere in the world that groundwater levels are monitored and analyzed. ""Our main point is that bad data is good data,"" she said. ""When you have wells with a lot of missing data points, that is telling you something important. Take notice of it."" ""Whenever you're focusing only on complete data, you should take a step back and ask if there is a reason for the incomplete data, a systematic bias in your data source,"" Hora said. ",Environment,0.07564895934891284
20,Science Daily,"Hunter-Gatherers Agree on What Is Moral, but Not Who Is Moral",Health,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905080039.htm,"   The research appears in the journal Social Psychological and Personality Science. Interviewing Hadza hunter-gatherers to rank the people they live with on a number of traits, such as who has a good heart, who shares the most, and who works the hardest, the team found that Hadza agreed on how important generosity and hard work was to moral character, but disagreed on who most exemplified these traits. ""They disagreed on who among them had the most moral character,"" says Kristopher Smith, a Penn postdoctoral fellow and lead paper author. Smith and Coren Apicella, an associate professor of Psychology, had 94 judges rank their campmates on global character and relevant character traits for a total of 824 observations. The Hadza live in small nomadic groups and move from group to group so that the social structure of each group frequently changes. Scientists suspect this nomadic way of living gives insight into the origins of human cooperation. Although the researchers note that the Hadza are a modern people, living in modern times, their way of living is more similar to how human ancestors lived than in western societies. ""It's not that the Hadza do not have a concept of morality or don't care about it. They agreed on what traits contributed to moral character. But they cannot agree on who exemplifies it,"" says Smith. Previous work by Apicella and Smith looking at generosity found that this trait changed depending on the group dynamic. A generous group led members to be more generous, and a group that didn't share much led to others not sharing as well. It may be this mobility and changing group dynamics that drive their current findings as well, as Apicella notes that ""the changing of groups and behavior may make it difficult for individuals to track and agree on moral reputations."" From the 1,000-foot morality perspective, ""this suggests that, for the Hadza, there is little consistent moral behavior across situations, and that reputation may have played a smaller role in the evolution of morality,"" says Smith. ""Understanding how moral psychology differs across cultures and in different social systems may provide insight into ways to improve our interactions with one another, and maybe overcome moral disagreements in our society,"" summarizes Smith. ",Society,0.06394903521030552
21,Science Daily,Report Cards on Women in STEM Fields Finds Much Room for Improvement,Health,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111636.htm,"   ""The data suggest that we are making headway,"" says Reshma Jagsi, a radiation oncologist and director of the Center for Bioethics and Social Sciences in Medicine at the University of Michigan and one of the corresponding authors. ""That said, there are still many institutions that have few women in senior-most faculty positions. There also remains quite a bit of room for improvement in certain areas, including the representation of women in certain roles, such as speaking at scientific meetings."" The researchers obtained their data through the use of institutional report cards that were collected when individual researchers applied for grants from NYSCF. The report cards were part of a 2014 NYSCF project that put forward a number of strategies aimed at helping to achieve gender parity in science, technology, engineering, and math (STEM). Of the 1,287 report cards that were submitted, 741 provided complete information for a given year, and some included multiple years. Overall, the data in the paper represent 541 institutions in 38 countries in North America (72%) and Europe (18%). The investigators found that although women made up more than half of the population among undergraduate, graduate, and post-graduate students, the picture became different as seniority increased. Women made up 42% of assistant professors, 34% of associate professors, and 23% of full professors. These rates varied greatly by institution: At about one-third of the institutions surveyed, women made up less than 10% of tenured faculty recruits. ""We expected to find that women would be better represented at more junior ranks compared with senior ranks,"" Jagsi says. ""But I found it noteworthy that there were regional differences. For example, institutions in Europe come closer to achieving gender parity."" The researchers say their findings suggest that the primary issue is not recruiting women into STEM roles but retaining them and promoting them into more influential positions. They also point out the important part that funding organizations can play. ""Funding organizations are in a unique position to require institutional leaders to pay attention to equity, diversity, and inclusion within their organizations,"" Jagsi says. ""By requiring these report cards, they can promote actions that help all scientists thrive. We hope that other funding bodies, like the NIH, will adopt a similar report card."" The next phase of IWISE will focus on highlighting best practices undertaken by institutions. This will provide comparative data and allow the researchers to monitor progress over time. The researchers will also look at other factors that may influence the recruitment and retention of women scientists, such as the presence of women in top leadership roles, the rates at which tenured women stay in their positions, and equity in salaries across gender, race, and ethnicity. ""For my own work, I plan to begin to focus more on issues of intersectionality,"" Jagsi concludes. ""A particularly understudied area involves the career experiences in women with other minority identities, such as race. Further research is needed to understand the challenges these women face."" ",Society,0.05758641179561391
22,Science Daily,The Future of Mind Control,Health,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905094048.htm,"   In a recent perspective titled ""Precision Electronic Medicine,"" published in Nature Biotechnology, Patel, a faculty member at the Harvard Medical School and Massachusetts General Hospital, and Lieber, the Joshua and Beth Friedman University Professor, argue that neurotechnology is on the cusp of a major renaissance. Throughout history, scientists have blurred discipline lines to tackle problems larger than their individual fields. The Human Genome Project, for example, convened international teams of scientists to map human genes faster than otherwise possible. ""The next frontier is really the merging of human cognition with machines,"" Patel said. He and Lieber see mesh electronics as the foundation for those machines, a way to design personalized electronic treatment for just about anything related to the brain. ""Everything manifests in the brain fundamentally. Everything. All your thoughts, your perceptions, any type of disease,"" Patel said. Scientists can pinpoint the general areas of the brain where decision-making, learning, and emotions originate, but tracing behaviors to specific neurons is still a challenge. Right now, when the brain's complex circuitry starts to misbehave or degrade due to psychiatric illnesses like addiction or Obsessive-Compulsive Disorder, neurodegenerative diseases like Parkinson's or Alzheimer's, or even natural aging, patients have only two options for medical intervention: drugs or, when those fail, implanted electrodes. Drugs like L-dopa can quiet the tremors that prevent someone with Parkinson's from performing simple tasks like dressing and eating. But because drugs affect more than just their target, even common L-dopa side effects can be severe, ranging from nausea to depression to abnormal heart rhythms. When drugs no longer work, FDA-approved electrodes can provide relief through Deep Brain Stimulation. Like a pace maker, a battery pack set beneath the clavicle sends automated electrical pulses to two brain implants. Lieber said each electrode ""looks like a pencil. It's big."" During implantation, Parkinson's patients are awake, so surgeons can calibrate the electrical pulses. Dial the electricity up, and the tremors calm. ""Almost instantly, you can see the person regain control of their limbs,"" Patel said. ""It blows my mind."" But, like with L-dopa, the large electrodes stimulate more than their intended targets, causing sometimes severe side effects like speech impediments. And, over time, the brain's immune system treats the stiff implants as foreign objects: Neural immune cells (glia cells) engulf the perceived invader, displacing or even killing neurons and reducing the device's ability to maintain treatment. In contrast, Lieber's mesh electronics provoke almost no immune response. With close, long-term proximity to the same neurons, the implants can collect robust data on how individual neurons communicate over time or, in the case of neurological disorders, fail to communicate. Eventually, such technology could track how specific neural subtypes talk, too, all of which could lead to a cleaner, more precise map of the brain's communication network. With higher resolution targets, future electrodes can act with greater precision, eliminating unwanted side effects. If that happens, Patel said, they could be tuned to treat any neurological disorder. And, unlike current electrodes, Lieber's have already demonstrated a valuable trick of their own: They encourage neural migration, potentially guiding newborn neurons to damaged areas, like pockets created by stroke. ""The potential for it is outstanding,"" Patel said. ""In my own mind, I see this at the level of what started with the transistor or telecommunications."" The potential reaches beyond therapeutics: Adaptive electrodes could provide heightened control over prosthetic or even paralyzed limbs. In time, they could act like neural substitutes, replacing damaged circuitry to re-establish broken communication networks and recalibrate based on live feedback. ""If you could actually interact in a precise and long-term way and also provide feedback information,"" Lieber said, ""you could really communicate with the brain in the same way that the brain is communicating within itself."" A few major technology companies are also eager to champion brain-machine interfaces. Some, like Elon Musk's Neuralink, which plans to give paralyzed patients the power to work computers with their minds, are focused on assistive applications. Others have broader plans: Facebook wants people to text by imaging the words, and Brian Johnson's Kernel hopes to enhance cognitive abilities. During his postdoctoral studies, Patel saw how just a short pulse of electricity -- no more than 500 milliseconds of stimulation -- could control a person's ability to make a safe or impulsive decision. After a little zap, subjects who almost always chose the risky bet, instead went with the safe option. ""You would have no idea that it's happened,"" Patel said. ""You're unaware of it. It's beyond your conscious awareness."" Such power demands intense ethical scrutiny. For people struggling to combat addiction or obsessive-compulsive disorder, an external pulse regulator could significantly improve their quality of life. But, companies that operate those regulators could access their client's most personal data -- their thoughts. And, if enhanced learning and memory are for sale, who gets to buy a better brain? ""One does need to be a little careful about the ethics involved if you're trying to make a superhuman,"" Lieber said. ""Being able to help people is much more important to me at this time."" Mesh electronics still have several major challenges to overcome: scaling up the number of implanted electrodes, processing the data flood those implants deliver, and feeding that information back into the system to enable live recalibration. ""I always joke in talks that I'm doing this because my memory has gotten a little worse than it used to be,"" Lieber said. ""That's natural aging. But does it have to be that way? What if you could correct it?"" If he and Patel succeed in galvanizing researchers around mesh electronics, the question might not be if but when. ",Health,0.05349737372739567
23,Science Daily,Having an Elder Brother Is Associated With Slower Language Development,Health,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905124522.htm,"   What is even more surprising is that apparently only elder brothers impact the language skills of their younger siblings, as a study conducted by a research team from the CNRS, the AP-HP, the EHESS, the ENS and the INSERM has just shown. The study finds that children who have grown up with an elder sister have identical development to children with no elder sibling. More than 1000 children have been followed from birth to five and a half years old in the mother-child cohort EDEN.* Their language skills were evaluated at 2, 3 and 5.5 years old by tests measuring several aspects of language, such as vocabulary, syntax and verbal reasoning. Children who have an elder brother had on average a two-month delay in language development compared with children with an elder sister. The scientists propose two hypotheses that may explain this result. The first is that elder sisters, in being more willing to talk to their younger siblings than brothers, may compensate for their parents being less available. Another hypothesis would be that elder sisters compete less than elder brothers for parental attention. Though this study cannot separate these two hypotheses, it does show that early language development in a younger sibling tends to be slower when the elder is a boy. For their next project, the scientists want to examine the impact of culture (specifically geographical origin) on these results. *- The EDEN cohort recruited families between 2003 and 2006 in the CHU in Nancy and Poitiers. ",Society,0.0514174875927001
24,Science Daily,"'Mental Rigidity' at Root of Intense Political Partisanship on Both Left and Right, Study Finds",Science,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829081401.htm,"   This ""mental rigidity"" makes it harder for people to change their ways of thinking or adapt to new environments, say researchers. Importantly, mental rigidity was found in those with the most fervent beliefs and affiliations on both the left and right of the political divide. The study of over 700 US citizens, conducted by scientists from the University of Cambridge, is the largest -- and first for over 20 years -- to investigate whether the more politically ""extreme"" have a certain ""type of mind"" through the use of objective psychological testing. The findings suggest that the basic mental processes governing our ability to switch between different concepts and tasks are linked to the intensity with which we attach ourselves to political doctrines -- regardless of the ideology. ""Relative to political moderates, participants who indicated extreme attachment to either the Democratic or Republican Party exhibited mental rigidity on multiple objective neuropsychological tests,"" said Dr Leor Zmigrod, a Cambridge Gates Scholar and lead author of the study, now published in the Journal of Experimental Psychology. ""While political animosity often appears to be driven by emotion, we find that the way people unconsciously process neutral stimuli seems to play an important role in how they process ideological arguments."" ""Those with lower cognitive flexibility see the world in more black-and-white terms, and struggle with new and different perspectives. The more inflexible mind may be especially susceptible to the clarity, certainty, and safety frequently offered by strong loyalty to collective ideologies,"" she said. The research is the latest in a series of studies from Zmigrod and her Cambridge colleagues, Dr Jason Rentfrow and Professor Trevor Robbins, on the relationship between ideology and cognitive flexibility. Their previous work over the last 18 months has suggested that mental rigidity is linked to more extreme attitudes with regards to religiosity, nationalism, and a willingness to endorse violence and sacrifice one's life for an ideological group. For the latest study, the Cambridge team recruited 743 men and women of various ages and educational backgrounds from across the political spectrum through the Amazon Mechanical Turk platform. Participants completed three psychological tests online: a word association game, a card-sorting test -- where colours, shapes and numbers are matched according to shifting rules -- and an exercise in which participants have a two-minute window to imagine possible uses for everyday objects. ""These are established and standardized cognitive tests which quantify how well individuals adapt to changing environments and how flexibly their minds process words and concepts,"" said Zmigrod. The participants were also asked to score their feelings towards various divisive social and economic issues -- from abortion and marriage to welfare -- and the extent of ""overlap"" between their personal identity and the US Republican and Democrat parties. Zmigrod and colleagues found that ""partisan extremity"" -- the intensity of participants' attachment to their favoured political party -- was a strong predictor of rigidity in all three cognitive tests. They also found that self-described Independents displayed greater cognitive flexibility compared to both Democrats and Republicans. Other cognitive traits, such as originality or fluency of thought, were not related to heightened political partisanship, which researchers argue suggests the unique contribution of cognitive inflexibility. ""In the context of today's highly divided politics, it is important we work to understand the psychological underpinnings of dogmatism and strict ideological adherence,"" said Zmigrod. ""The aim of this research is not to draw false equivalences between different, and sometimes opposing, ideologies. We want to highlight the common psychological factors that shape how people come to hold extreme views and identities,"" said Zmigrod. ""Past studies have shown that it is possible to cultivate cognitive flexibility through training and education. Our findings raise the question of whether heightening our cognitive flexibility might help build more tolerant societies, and even develop antidotes to radicalization."" ""While the conservatism and liberalism of our beliefs may at times divide us, our capacity to think about the world flexibly and adaptively can unite us,"" she added. ",Society,0.050363482454813965
25,Science Daily,Parental Burnout Can Lead to Harmful Outcomes for Parent and Child,Society,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828080538.htm,"   ""In the current cultural context, there is a lot of pressure on parents,"" says lead researcher Moïra Mikolajczak of UCLouvain. ""But being a perfect parent is impossible and attempting to be one can lead to exhaustion. Our research suggests that whatever allows parents to recharge their batteries, to avoid exhaustion, is good for children."" Mikolajczak and coauthors James J. Gross of Stanford University and Isabelle Roskam of UCLouvain became interested in the issue through their clinical encounters with good parents who, as a result of their exhaustion, had become the opposite of what they were trying to be. Although previous research had explored the causes of parental burnout, relatively little was known about its consequences. The researchers decided to directly examine the outcomes associated with parental burnout in two studies that followed parents over time. In the first study, Mikolajczak and colleagues recruited parents through social networks, schools, pediatricians, and other sources to participate in research on ""parental well-being and exhaustion."" The parents, mostly French-speaking adults in Belgium, completed three batches of online surveys spaced about 5.5 months apart. The surveys included a 22-item measure of parental burnout that gauged parents' emotional exhaustion, emotional distancing, and feelings of inefficacy; a six-item measure that gauged their thoughts about escaping their family; a 17-item measure that gauged the degree to which they neglected their childrens' physical, educational and emotional needs; and a 15-item measure that gauged their tendency to engage in verbal, physical, or psychological violence. Because many of the questions asked about sensitive topics, the researchers also measured participants' tendency to choose the most socially desirable responses when confronted with probing questions. A total of 2,068 parents participated in the first survey, with 557 still participating at the third survey. Participants' data revealed a strong association between burnout and the three variables -- escape ideation, parental neglect, and parental violence -- at each of the three time points. Parental burnout at the first and second survey was associated with later parental neglect, parental violence, and escape ideation. The researchers found that parental burnout and parental neglect had a circular relationship: Parental burnout led to increased parental neglect, which led to increased burnout, and so on. Parental violence appeared to be a clear consequence of burnout. Importantly, all of these patterns held even when the researchers took participants' tendency toward socially desirable responding into account. A second online study with mostly English-speaking parents in the UK produced similar findings. Together, the data suggest that parental burnout is likely the cause of escape ideation, parental neglect, and parental violence. ""We were a bit surprised by the irony of the results,"" says Mikolajczak. ""If you want to do the right thing too much, you can end up doing the wrong thing. Too much pressure on parents can lead them to exhaustion which can have damaging consequences for the parent and for the children."" Additional studies are needed to confirm and extend these findings with broader samples and measures. Nonetheless, the robust pattern of results suggests that there are important lessons to be learned from these findings, the researchers say. ""Parents need to know that self-care is good for the child and that when they feel severely exhausted, they should seek help. Health and child services professionals need to be informed about parental burnout so that they can accurately diagnose it and provide parents with the most appropriate care. And those engaged in policy and public health need to help raise awareness and lift the taboo on parental burnout, which will encourage parents to seek the help they need,"" Mikolajczak concludes. ",Society,0.04977767878158072
26,Science Daily,Mindfulness for Middle School Students,Society,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826153630.htm,"   ""By definition, mindfulness is the ability to focus attention on the present moment, as opposed to being distracted by external things or internal thoughts. If you're focused on the teacher in front of you, or the homework in front of you, that should be good for learning,"" says John Gabrieli, the Grover M. Hermann Professor in Health Sciences and Technology, a professor of brain and cognitive sciences, and a member of MIT's McGovern Institute for Brain Research. The researchers also showed, for the first time, that mindfulness training can alter brain activity in students. Sixth-graders who received mindfulness training not only reported feeling less stressed, but their brain scans revealed reduced activation of the amygdala, a brain region that processes fear and other emotions, when they viewed images of fearful faces. Together, the findings suggest that offering mindfulness training in schools could benefit many students, says Gabrieli, who is the senior author of both studies. ""We think there is a reasonable possibility that mindfulness training would be beneficial for children as part of the daily curriculum in their classroom,"" he says. ""What's also appealing about mindfulness is that there are pretty well-established ways of teaching it."" In the moment Both studies were performed at charter schools in Boston. In one of the papers, which appears today in the journal Behavioral Neuroscience, the MIT team studied about 100 sixth-graders. Half of the students received mindfulness training every day for eight weeks, while the other half took a coding class. The mindfulness exercises were designed to encourage students to pay attention to their breath, and to focus on the present moment rather than thoughts of the past or the future. Students who received the mindfulness training reported that their stress levels went down after the training, while the students in the control group did not. Students in the mindfulness training group also reported fewer negative feelings, such as sadness or anger, after the training. About 40 of the students also participated in brain imaging studies before and after the training. The researchers measured activity in the amygdala as the students looked at pictures of faces expressing different emotions. At the beginning of the study, before any training, students who reported higher stress levels showed more amygdala activity when they saw fearful faces. This is consistent with previous research showing that the amygdala can be overactive in people who experience more stress, leading them to have stronger negative reactions to adverse events. ""There's a lot of evidence that an overly strong amygdala response to negative things is associated with high stress in early childhood and risk for depression,"" Gabrieli says. After the mindfulness training, students showed a smaller amygdala response when they saw the fearful faces, consistent with their reports that they felt less stressed. This suggests that mindfulness training could potentially help prevent or mitigate mood disorders linked with higher stress levels, the researchers say. Evaluating mindfulness In the other paper, which appeared in the journal Mind, Brain, and Education in June, the researchers did not perform any mindfulness training but used a questionnaire to evaluate mindfulness in more than 2,000 students in grades 5-8. The questionnaire was based on the Mindfulness Attention Awareness Scale, which is often used in mindfulness studies on adults. Participants are asked to rate how strongly they agree with statements such as ""I rush through activities without being really attentive to them."" The researchers compared the questionnaire results with students' grades, their scores on statewide standardized tests, their attendance rates, and the number of times they had been suspended from school. Students who showed more mindfulness tended to have better grades and test scores, as well as fewer absences and suspensions. ""People had not asked that question in any quantitative sense at all, as to whether a more mindful child is more likely to fare better in school,"" Gabrieli says. ""This is the first paper that says there is a relationship between the two."" The researchers now plan to do a full school-year study, with a larger group of students across many schools, to examine the longer-term effects of mindfulness training. Shorter programs like the two-month training used in the Behavioral Neuroscience study would most likely not have a lasting impact, Gabrieli says. ""Mindfulness is like going to the gym. If you go for a month, that's good, but if you stop going, the effects won't last,"" he says. ""It's a form of mental exercise that needs to be sustained."" ",Society,0.049005922474658285
27,Science Daily,Patients in the US and Canada Are Likely to Receive Opioids After Surgery,Health,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165221.htm,"   To compare international opioid prescribing rates after surgery, researchers analyzed data on four frequently performed procedures: surgery to remove the appendix, surgery to remove the gallbladder, a minimally invasive procedure to treat a torn meniscus cartilage in the knee and a procedure to remove a breast lump. Within seven days of discharge, about 75 percent of the patients in the United States and Canada filled an opioid prescription, compared to just 11 percent of the patients in Sweden. By the one-month mark, nearly half of U.S patients had received high-dose opioid prescriptions (i.e., prescriptions totaling more than 200 MME) -- nearly double the rate in Canada (25 percent) and nine times higher than the rate in Sweden (5 percent). ""Our findings reveal stark differences in prescribing practices across the three countries and suggest real opportunities to encourage more judicious use of opioids before and after surgery for patients in the United States and Canada,"" said the study's corresponding author Mark D. Neuman, MD, an associate professor of Anesthesiology and Critical Care and Chair of the Penn Medicine Opioid Task Force. ""While innovative strategies, like enhanced recovery protocols, have helped to reduce the number of prescribed opioids, it's clear that we need to continue to identify ways to improve prescribing practices in the United States and Canada."" Opioids, such as codeine, tramadol and morphine, are routinely prescribed for postoperative pain management in many countries. However, recent research suggests that overprescribing opioid medications for short-term pain may be widespread in the United States. The excessive prescribing can increase the risk of drug diversion, new long-term opioid use and the development of opioid use disorder. In the last decade, opioid overdose deaths have significantly increased in countries across the world, including the United States and Canada -- which have the highest opioid use per capita in the world. While the use of opioids varies in countries worldwide, there has been little research -- until now -- that characterizes the international disparities in opioid use for specific indications, such as pain relief after surgery. In their analysis, researchers examined data from more than 220,000 cases -- ranging from 2013 to 2016 -- to identify differences in the percentage of opioid prescriptions filled within seven and 30 days of the procedures, as well as the quantity and types of opioids dispensed. They specifically sought patients who shared similar characteristics, including age and medical history, and who had not received an opioid in the 90 days prior to the surgery. Researchers found that at least 65 percent of patients in the United States and Canada filled an opioid prescription in the first seven days after each procedure. In Sweden, the prescribing rate didn't exceed 20 percent for any of the procedures. Meanwhile, the average dosage of the initial prescription in the United States was 247 MME -much higher than the dosage dispensed in Sweden (197) and Canada (169). In addition to the disparities in prescribing rate and dosage, researchers also identified a significant variation in the types of opioid medication prescribed. For example, codeine and tramadol accounted for 58 percent of the postoperative prescriptions dispensed in Canada and 45 percent of the prescriptions in Sweden, but just 7 percent of prescriptions in the United States. In the United States, hydrocodone and oxycodone were the most commonly dispensed opioid medications. ""Our findings point to systematic differences in practitioners' approaches to opioid prescribing, public attitudes regarding the role of opioids in treating pain and broader structural factors related to drug marketing and regulation,"" said Dr. Karim Ladha, a clinician-scientist at the Li Ka Shing Knowledge Institute of St. Michael's Hospital and co-author of the study. The work was supported, in part, by a grant from the National Institutes of Health (1-R01-DA042299) and ICES. ",Health,0.04441075284504198
28,Science Daily,Snack Tax May Be More Effective Than a Sugary Drink Tax to Tackle Obesity,Science,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904194447.htm,"   The researchers say this option ""is worthy of further research and consideration as part of an integrated approach to tackling obesity."" Obesity rates are increasing across the world. In the UK, obesity is estimated to affect around 1 in every 4 adults and around 1 in every 5 children aged 10 to 11, with higher rates among those living in more deprived areas. The use of taxes to lower sugar and energy intake have mainly focused on sugar sweetened drinks. But in the UK, high sugar snacks, such as biscuits, cakes, chocolates and sweets make up more free sugar and energy intake than sugary drinks. Reducing purchases of high sugar snacks therefore has the potential to make a greater impact on population health than reducing the purchase of sugary drinks. To explore this in more detail, researchers used economic modelling to assess the impact of a 20% price increase on high sugar snack foods in the UK. Modelling was based on food purchase data for 36,324 UK households and National Diet and Nutrition Survey data for 2,544 adults. Results were grouped by household income and body mass index (BMI) to estimate changes in weight and prevalence of obesity over one year. The results suggest that for all income groups combined, increasing the price of biscuits, cakes, chocolates, and sweets by 20% would reduce annual average energy intake by around 8,900 calories, leading to an average weight loss of 1.3 kg over one year. In contrast, a similar price increase on sugary drinks would result in an average weight loss of just 203 g over one year. What's more, the model predicts that the impact of the price increase would be largest in low income households with the highest rates of obesity, suggesting that taxing high sugar snacks could help to reduce health inequalities driven by diet related diseases, say the researchers. They point to some possible study limitations, such as the relatively short, one-year, time-frame over which weight changes were modelled, but say findings were based on information from high quality databases and remained largely unchanged after varying some key assumptions. As such, they say that a 20% price increase in high sugar snacks, ""has the potential to reduce overall energy purchased among all body mass index and income groups in the UK, leading to an estimated population level reduction in obesity prevalence of 2.7 percentage points after the first year."" ""The results also suggest that price increases in high sugar snacks could also make an important contribution to reducing health inequalities driven by diet related disease,"" they conclude. There is a strong rationale for using fiscal policy to improve diet and health, but caution is needed, say researchers in a linked editorial. For example, they point out that substitution and displacement effects in response to food tax and subsidy policies are complicated and difficult to predict, while product reformulation in response to consumer demand can also have unintended consequences. They also argue that fiscal policies aimed at reducing sugar, salt, and saturated fat intake ""might be useful, but they fail to incentivise the consumption of healthy foods."" Ultimately, tackling obesity and diet related disease ""requires close scrutiny of the social determinants of food environments and a systemic, sustained group of initiatives aimed at reducing health inequalities,"" they conclude. ",Health,0.044255577618273714
29,Science Daily,Gut Bacteria May Be Linked to High Blood Pressure and Depression,Health,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905161408.htm,"   ""People are 'meta-organisms' made up of roughly equal numbers of human cells and bacteria. Gut bacteria ecology interacts with our bodily physiology and brains, which may steer some people towards developing high blood pressure and depression,"" said Bruce R. Stevens, Ph.D., lead author of the study and professor of physiology & functional genomics, medicine and psychiatry at the University of Florida College of Medicine in Gainesville, Florida. ""In the future, health professionals may target your gut in order to prevent, diagnose and selectively treat different forms of high blood pressure."" Stevens said there's potential for this research to uncover treatment approaches that could improve outcomes in people with treatment-resistant hypertension. Nearly 20 percent of patients with high blood pressure don't respond well to treatment, even with multiple medications. The researchers isolated DNA (deoxyribonucleic acid, the carrier of genetic information) from gut bacteria obtained from the stool samples of 105 volunteers. They used a new technique involving artificial-intelligence software to analyze the bacteria, which revealed four distinct types of bacterial genes and signature molecules. Surprisingly, the investigators discovered unique patterns of bacteria from people with 1) high blood pressure plus depression; 2) high blood pressure without depression; 3) depression with healthy blood pressure; or 4) healthy subjects without depression or high blood pressure. Stevens said the results suggest different medical mechanisms of high blood pressure that correlate with signature molecules produced by gut bacteria. These molecules are thought to impact the cardiovascular system, metabolism, hormones and the nervous system. ""We believe we have uncovered new forms of high blood pressure: 'Depressive Hypertension' (high blood pressure with depression), which may be a completely different disease than 'Non-Depressive Hypertension' (high blood pressure without depression), which are each different from 'Non-Hypertensive Depression,'"" Stevens said. ",Health,0.04249580175178675
30,Science Daily,Brain Circuit Connects Feeding and Mood in Response to Stress,Health,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904213722.htm,"   ""This study was initiated by first author Dr. Na Qu, a psychiatrist of Wuhan Mental Health Center, China, when she was visiting my lab,"" said corresponding author Dr. Yong Xu, associate professor of pediatrics and of molecular and cellular biology at Baylor College of Medicine. Qu, a practicing psychiatrist who also conducts basic brain research, was interested in investigating whether there was a neurological basis for the association between depression and other psychiatric disorders and alterations in metabolism, such as obesity or lack of appetite, she had observed in a number of her patients. Xu, Qu and their colleagues worked with a mouse model of depression induced by chronic stress and observed that depressed animals ate less and lost weight. Then, they applied a number of experimental techniques to identify the neuronal circuits that changed activity when the animals were depressed. ""We found that POMC neurons in the hypothalamus, which are essential for regulating body weight and feeding behavior, extend physical connections into another region of the brain that has numerous dopamine neurons that are implicated in the regulation of mood,"" said Xu, who also is a researcher at the USDA/ARS Children's Nutrition Research Center at Baylor and Texas Children's Hospital. ""We know that a decrease in dopamine may trigger depression."" In addition to the physical connection between the feeding and the mood centers of the brain, the researchers also discovered that when they triggered depression in mice, the POMC neurons were activated and this led to inhibition of the dopamine neurons. Interestingly, when the researchers inhibited the neuronal circuit connecting the feeding and the mood centers, the animals ate more, gained weight and looked less depressed. ""We have discovered that a form of chronic stress triggered a neuronal circuit that starts in a population of cells that are known to regulate metabolism and feeding behavior and ends in a group of neurons that are famous for their regulation of mood,"" Xu said. ""Stress-triggered activation of the feeding center led to inhibition of dopamine-producing neurons in the mood center."" Although more research is needed, Xu, Qu and their colleagues propose that their findings provide a new biological basis that may explain some of the connections between mood alterations and changes in metabolism observed in people, and may provide solutions in the future. ""Our findings only explain one scenario, when depression is associated with poor appetite. But in other cases depression has been linked to overeating. We are interested in investigating this second association between mood and eating behavior to identify the neuronal circuits that may explain that response,"" Xu said. ",Health,0.04135731408731748
