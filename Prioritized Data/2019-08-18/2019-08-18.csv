,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,Science Daily,"Router Guest Networks Lack Adequate Security, Experts Say",Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815081258.htm,"   According to Adar Ovadya, a master's student in BGU's Department of Software and Information Systems Engineering, ""all of the routers we surveyed regardless of brand or price point were vulnerable to at least some cross-network communication once we used specially crafted network packets. A hardware-based solution seems to be the safest approach to guaranteeing isolation between secure and non-secure network devices."" The BGU research was presented at the 13th USENIX Workshop on Offensive Technologies (WOOT) in Santa Clara this week. Most routers sold today offer consumers two or more network options -- one for the family, which may connect all the sensitive smart home and IoT devices and computers, and the other for visitors or less sensitive data. In an organization, data traffic sent may include mission-critical business documents, control data for industrial systems, or private medical information. Less sensitive data may include multimedia streams or environmental sensor readings. Network separation and network isolation are important components of the security policy of many organizations if not mandated as standard practice, for example, in hospitals. The goal of these policies is to prevent network intrusions and information leakage by separating sensitive network segments from other segments of the organizational network, and indeed from the general internet. In the paper, the researchers demonstrated the existence of different levels of cross-router covert channels which can be combined and exploited to either control a malicious implant, or to exfiltrate or steal the data. In some instances, these can be patched as a simple software bug, but more pervasive covert cross-channel communication is impossible to prevent, unless the data streams are separated on different hardware. The USENIX Workshop on Offensive Technologies (WOOT) aims to present a broad picture of offense and its contributions, bringing together researchers and practitioners in all areas of computer security. WOOT provides a forum for high-quality, peer-reviewed work discussing tools and techniques for attack. All vulnerabilities were previously disclosed to the manufacturers. This research was supported by Israel Science Foundation grants 702/16 and 703/16. Adar Ovadya is co-supervised by Dr. Yossi Oren, a senior lecturer in BGU's Department of Software and Information Systems Engineering and head of the Implementation Security and Side-Channel Attacks Lab at Cyber@BGU, and Dr. Niv Gilboa, from BGU's Department of Communication Systems Engineering. Also contributing to the research were BGU graduate student Rom Ogen and undergraduate student Yakov Mallah. ",Computer Science,0.17977298645195508
1,ACM,Trilce Estrada Wins 2019 ACM SIGHPC Emerging Woman Leader in Technical Computing Award,ACM,2019-08-06,-,https://cmns.umd.edu/news-events/features/4470,"    Seeing How Computers 'Think' Helps Humans Stump Machines     University of MarylandAugust 6, 2019   University of Maryland (UMD) researchers have developed a technique for reliably generating questions that challenge computers, and mirror the complexity of human language via human-computer collaboration. The researchers invented an interface that shows words the computer uses as a basis for its guesses as a person types a question, then edits the question to exploit weaknesses. The researchers compiled a dataset of 1,213 questions that humans can easily answer, yet are beyond the capabilities of the best modern computer-answering systems. UMD's Jordan Boyd-Graber said tests on the dataset ""will reveal if a computer language system is actually reading and doing the same sorts of processing that humans are able to do."" A computer that masters these questions will be better enabled to understand language than any current system, and the dataset also could be used to train improved machine learning algorithms.                 ",Computer Science,0.17693232636357764
2,Stanford,Atomically thin heat shield protects electronics,Science,2019-08-18,-,https://news.stanford.edu/2019/08/16/atomically-thin-heat-shield-protects-electronics/,"   Excess heat given off by smartphones, laptops and other electronic devices can be annoying, but beyond that it contributes to malfunctions and, in extreme cases, can even cause lithium batteries to explode.  This greatly magnified image shows four layers of atomically thin materials that form a heat-shield just two to three nanometers thick, or roughly 50,000 times thinner than a sheet of paper. (Image credit: National Institute of Standards and Technology)  To guard against such ills, engineers often insert glass, plastic or even layers of air as insulation to prevent heat-generating components like microprocessors from causing damage or discomforting users. Now, Stanford researchers have shown that a few layers of atomically thin materials, stacked like sheets of paper atop hot spots, can provide the same insulation as a sheet of glass 100 times thicker. In the near term, thinner heat shields will enable engineers to make electronic devices even more compact than those we have today, said Eric Pop, professor of electrical engineering and senior author of a paper published Aug. 16 in Science Advances. “We’re looking at the heat in electronic devices in an entirely new way,” Pop said. Detecting sound as heat The heat we feel from smartphones or laptops is actually an inaudible form of high-frequency sound. If that seems crazy, consider the underlying physics. Electricity flows through wires as a stream of electrons. As these electrons move, they collide with the atoms of the materials through which they pass. With each such collision an electron causes an atom to vibrate, and the more current flows, the more collisions occur, until electrons are beating on atoms like so many hammers on so many bells – except that this cacophony of vibrations moves through the solid material at frequencies far above the threshold of hearing, generating energy that we feel as heat. Thinking about heat as a form of sound inspired the Stanford researchers to borrow some principles from the physical world. From his days as a radio DJ at Stanford’s KZSU 90.1 FM, Pop knew that music recording studios are quiet thanks to thick glass windows that block the exterior sound. A similar principle applies to the heat shields in today’s electronics. If better insulation were their only concern, the researchers could simply borrow the music studio principle and thicken their heat barriers. But that would frustrate efforts to make electronics thinner. Their solution was to borrow a trick from homeowners, who install multi-paned windows – usually, layers of air between sheets of glass with varying thickness – to make interiors warmer and quieter. “We adapted that idea by creating an insulator that used several layers of atomically thin materials instead of a thick mass of glass,” said postdoctoral scholar Sam Vaziri, the lead author on the paper. Atomically thin materials are a relatively recent discovery. It was only 15 years ago that scientists were able to isolate some materials into such thin layers. The first example discovered was graphene, which is a single layer of carbon atoms and, ever since it was found, scientists have been looking for, and experimenting with, other sheet-like materials. The Stanford team used a layer of graphene and three other sheet-like materials – each three atoms thick – to create a four-layered insulator just 10 atoms deep. Despite its thinness, the insulator is effective because the atomic heat vibrations are dampened and lose much of their energy as they pass through each layer. To make nanoscale heat shields practical, the researchers will have to find some mass production technique to spray or otherwise deposit atom-thin layers of materials onto electronic components during manufacturing. But behind the immediate goal of developing thinner insulators looms a larger ambition: Scientists hope to one day control the vibrational energy inside materials the way they now control electricity and light. As they come to understand the heat in solid objects as a form of sound, a new field of phononics is emerging, a name taken from the Greek root word behind telephone, phonograph and phonetics. “As engineers, we know quite a lot about how to control electricity, and we’re getting better with light, but we’re just starting to understand how to manipulate the high-frequency sound that manifests itself as heat at the atomic scale,” Pop said. Eric Pop is an affiliate of the Precourt Institute for Energy. Stanford authors include former postdoctoral scholars Eilam Yalon and Miguel Muñoz Rojo, and graduate students Connor McClellan, Connor Bailey, Kirby Smithe, Alexander Gabourie, Victoria Chen, Sanchit Deshmukh and Saurabh Suryavanshi. Other authors are from Theiss Research and the National Institute of Standards and Technology. This research was supported by the Stanford Nanofabrication Facility, the Stanford Nano Shared Facilities, the National Science Foundation, the Semiconductor Research Corporation, the Defense Advanced Research Projects Agency, the Air Force Office of Scientific Research, the Stanford SystemX Alliance, the Knut and Alice Wallenberg Foundation, the Stanford Graduate Fellowship program and the National Institute of Standards and Technology. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Electronics and Technology,0.17149668539278956
3,Science Daily,Nylon as a Building Block for Transparent Electronic Devices?,Computers & Math,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191445.htm,"   Besides the use in textiles, it was discovered that some nylons also exhibit so called ""ferroelectric properties."" This means that positive and negative electric charges can be separated and this state can be maintained. The ferroelectric materials are used in sensors, actuators, memories and energy harvesting devices. The advantage in using polymers is that they can be liquified using adequate solvents and therefore processed from solution at low cost to form flexible thin-films which are suitable for electronic devices such as capacitors, transistors and diodes. This makes ferroelectric polymers a viable choice for integration with e-textiles. Although nylon polymers have found over the years significant commercial applications in fabrics and fibers, their application in electronic devices was hindered because it was impossible to create high quality thin films of ferroelectric nylons by solution processing. Scientists at the MPI-P, in collaboration with researchers from the Johannes Gutenberg University of Mainz and Lodz University of Technology, have now solved this forty year old problem, and developed a method to fabricate ferroelectric nylon thin-film capacitors by dissolving nylon in a mixture of trifluoroacetic acid and acetone and solidifying it again in vacuum. They were able to realize thin nylon films that are typically only a few 100 nanometers thick, several 100 times thinner than human hair. ""Using this method, we have produced extremely smooth thin-films. This is very important because it prevents electrical break down of for example capacitors and destroying the electronic circuits. At the same time, the smoothness allows for having transparent thin-films and eventually transparent electronic devices,"" says Dr. Kamal Asadi, group leader at the MPI-P. By using their newly developed method, the group around Kamal Asadi was able to produce high performance nylon capacitors. The scientists subjected the prototypes of the capacitors to extended stress cycles and demonstrated robustness of ferroelectric nylons under millions of operation cycles. The thin nylon films could become an important component for use in flexible electronics in the future and find applications in bendable electronic devices or for electronics in clothing. These new findings pave the way towards multi-functional fabrics that serve as cloth for covering our body and at the same time can generate electricity from our body movement. ",Electronics and Technology,0.1698865869557979
4,Science Daily,New 3D Interconnection Technology for Future Wearable Bioelectronics,Computers & Math,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815081308.htm,"   Though they were supposed to feel like a second skin of the wearer, it has been technically impossible to devise ""wearable"" devices that are comfortable to bend and stretch and also keep good data recording capabilities on soft and curved skin. Wearable smart devices gather a person's bio measurements by connecting electrodes to the surface of the skin. Inside the device are 3D-shaped electrode wirings (i.e. interconnects) that transmit electrical signals. To date, not only can the wirings only be formed on a hard surface, but also the components of such interconnects delicate and hardly-stretchable metals such as gold, copper, and aluminum. In a paper published today in the journal Nano Letters, the joint research team led by Prof. Jang-Ung Park at the Center for Nanomedicine within the Institute for Basic Science (IBS) in Daejeon, South Korea, and Prof. Chang Young Lee at the Ulsan National Institute of Science and Technology (UNIST) in Ulsan, South Korea reported fully-transformable electrode materials that also feature a high electric conductivity. Notably, this novel composite is super-thin, 5 micrometers in diameter, which is half of the width of conventional wire bonding. By enabling ever-slimmer 3D interconnects, this study can help to revolutionize the physical appearance of smart gadgets, in addition to reinforcing their technical functions. The research team used liquid metals (LM) as the main substrate since LMs are highly stretchable and have relatively high conductivities similar to solid metals. To improve the mechanical stability of the metal liquid, carbon nanotubes (CNT) were dispersed uniformly. ""To have a uniform and homogeneous dispersion of CNTs in liquid metal, we selected platinum (Pt), for having a strong affinity to both CNT and LM, as the mixer and it worked,"" said Young-Geun Park, the first author of the study. This study also demonstrated a new interconnection technology that can form a highly conductive 3D structure at room temperature: For having a high conductivity, the new system does not require any heating or compressing process. Also the soft and stretchable nature of the new electrode makes it easy to come through the nozzle in a fine diameter. The research team used a nozzle for the direct printing of various 3D patterning structure. Park explains, ""Forming high-conductivity 3D interconnections at room temperature is an essential technology that enables the use of various flexible electronic materials. The wire bonding technology used in existing electronic devices forms interconnects using heat, pressure, or ultrasonic waves that can damage soft, skin-like devices. They have been a great challenge in the manufacturing process of high-performance electronic devices."" He noted that the pointed nozzle also allows reshaping of the preprinted pattern into various 3D structure, thus having an electrode work like a ""switch"" to turn on and off power. Using the direct printing method, the high-resolution 3D printing of this composite forms free-standing, wire-like interconnects. This new stretchable 3D electrical interconnections specifically consist of super-thin wires, as fine as 5 micrometers. Previous studies on stretchable metals have only been able to present wire lines of several hundred micrometers in diameter. The new system is even thinner than the interconnect of conventional wire bonding. Professor Jang-Ung Park, the corresponding author of the study noted, ""We may soon be able to say goodbye to those bulky skin-based interfaces as this freely-transformable, super-thin 3D interconnection technology will come as a big breakthrough to the industry's efforts to produce ever compact and slim gadgets."" Blurring the boundary between the human body and electric devices, this new technology will facilitate the production of more integrated and higher-performing semiconductor components for use in existing computers and smartphones, as well as for flexible and stretchable electronic devices."" ",Electronics and Technology,0.16521577076675462
5,Science Daily,Optofluidic Chip With Nanopore 'Smart Gate' Developed for Single Molecule Analysis,Computers & Math,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816075535.htm,"   In a paper published August 16 in Nature Communications, the researchers reported using the device to control the delivery of individual biomolecules -- including ribosomes, DNA, and proteins -- into a fluid-filled channel on the chip. They also showed that the device can be used to sort different types of molecules, enabling selective analysis of target molecules from a mixture. The capabilities of the programmable nanopore-optofluidic device point the way toward a novel research tool for high-throughput single-molecule analysis on a chip, said Holger Schmidt, the Kapany Professor of Optoelectronics at UC Santa Cruz and corresponding author of the paper. ""We can bring a single molecule into a fluidic channel where it can then be analyzed using integrated optical waveguides or other techniques,"" Schmidt said. ""The idea is to introduce a particle or molecule, hold it in the channel for analysis, then discard the particle, and easily and rapidly repeat the process to develop robust statistics of many single-molecule experiments."" The new device builds on previous work by Schmidt's lab and his collaborator Aaron Hawkins' group at Brigham Young University to develop optofluidic chip technology combining microfluidics (tiny channels for handling liquid samples on a chip) with integrated optics for optical analysis of single molecules. The addition of nanopores allows controlled delivery of molecules into the channel, as well as the opportunity to analyze the electrical signal produced as a molecule passes through the pore. This latest work was led by first author Mahmudur Rahman, a graduate student in Schmidt's lab at UC Santa Cruz. Nanopore technology has been successfully used in DNA sequencing applications, and Schmidt and other researchers have been exploring new ways to exploit the information in the signals produced as molecules or particles translocate through a nanopore. With the feedback control system (a microcontroller and solid-state relay) in the new device, real-time analysis of the current turns the nanopore into a ""smart gate"" that can be programmed by the user to deliver molecules into the channel in a predetermined manner. The gate can be closed as soon as a single molecule (or any number set by the user) has passed through, and opened again after a set time. ""The use of nanopores as 'smart gates' is a key step toward a single-molecule analysis system that is user-friendly and can work at high throughput,"" Schmidt said. ""It allows user-programmable control over the number of molecules that are being delivered to a fluidic channel for further analysis or processing, selective gating of different types of single molecules, and the ability to deliver single molecules into a chip at record rates of many hundreds per minute."" Using bacterial (70S) ribosomes, the researchers demonstrated controlled delivery of more than 500 ribosomes per minute. Coauthor Harry Noller, the Sinsheimer Professor of Molecular Biology at UC Santa Cruz, has done pioneering research on the structure and function of ribosomes, the molecular machines that synthesize proteins in all living cells, and has been collaborating with Schmidt's group since 2006. The researchers also used a mixture of DNA and ribosomes to show the device's capacity to selectively activate the gating function for a target molecule (in this case, DNA). This can enable, for example, fluorescence experiments on a controlled number of target molecules, while unlabeled particles are ignored and discarded. Selective gating could also be used for purification or sorting of different particles downstream from the nanopore, based on the signals as the particles pass through the nanopore, Schmidt said. The programmable system allows flexibility for a wide range of potential applications, he said. ",Electronics and Technology,0.16318427684269418
6,Science Daily,Wireless Sensors That Stick to the Skin to Track Our Health,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816092424.htm,"   Now, Stanford engineers have developed a way to detect physiological signals emanating from the skin with sensors that stick like band-aids and beam wireless readings to a receiver clipped onto clothing. To demonstrate this wearable technology, the researchers stuck sensors to the wrist and abdomen of one test subject to monitor the person's pulse and respiration by detecting how their skin stretched and contracted with each heartbeat or breath. Likewise, stickers on the person's elbows and knees tracked arm and leg motions by gauging the minute tightening or relaxation of the skin each time the corresponding muscle flexed. Zhenan Bao, the chemical engineering professor whose lab described the system in an Aug. 15 article in Nature Electronics, thinks this wearable technology, which they call BodyNet, will first be used in medical settings such as monitoring patients with sleep disorders or heart conditions. Her lab is already trying to develop new stickers to sense sweat and other secretions to track variables such as body temperature and stress. Her ultimate goal is to create an array of wireless sensors that stick to the skin and work in conjunction with smart clothing to more accurately track a wider variety of health indicators than the smart phones or watches consumers use today. ""We think one day it will be possible to create a full-body skin-sensor array to collect physiological data without interfering with a person's normal behavior,"" said Bao, who is also the K.K. Lee Professor in the School of Engineering. Stretchable, comfortable, functional Postdoctoral scholars Simiao Niu and Naoji Matsuhisa led the 14-person team that spent three years designing the sensors. Their goal was to develop a technology that would be comfortable to wear and have no batteries or rigid circuits to prevent the stickers from stretching and contracting with the skin. Their eventual design met these parameters with a variation of the RFID -- radiofrequency identification -- technology used to control keyless entry to locked rooms. When a person holds an ID card up to an RFID receiver, an antenna in the ID card harvests a tiny bit of RFID energy from the receiver and uses this to generate a code that it then beams back to the receiver. The BodyNet sticker is similar to the ID card: It has an antenna that harvests a bit of the incoming RFID energy from a receiver on the clothing to power its sensors. It then takes readings from the skin and beams them back to the nearby receiver. But to make the wireless sticker work, the researchers had to create an antenna that could stretch and bend like skin. They did this by screen-printing metallic ink on a rubber sticker. However, whenever the antenna bent or stretched, those movements made its signal too weak and unstable to be useful. To get around this problem, the Stanford researchers developed a new type of RFID system that could beam strong and accurate signals to the receiver despite constant fluctuations. The battery-powered receiver then uses Bluetooth to periodically upload data from the stickers to a smartphone, computer or other permanent storage system. The initial version of the stickers relied on tiny motion sensors to take respiration and pulse readings. The researchers are now studying how to integrate sweat, temperature and other sensors into their antenna systems. To move their technology beyond clinical applications and into consumer-friendly devices, the researchers need to overcome another challenge -- keeping the sensor and receiver close to each other. In their experiments, the researchers clipped a receiver on clothing just above each sensor. One-to-one pairings of sensors and receivers would be fine in medical monitoring, but to create a BodyNet that someone could wear while exercising, antennas would have to be woven into clothing to receive and transmit signals no matter where a person sticks a sensor. ",Electronics and Technology,0.15863327635776098
7,Science Daily,Attackers Could Be Listening to What You Type,Health,2019-08-14,-,https://www.sciencedaily.com/releases/2019/08/190814105223.htm,"   Researchers from SMU's Darwin Deason Institute for Cybersecurity found that acoustic signals, or sound waves, produced when we type on a computer keyboard can successfully be picked up by a smartphone. The sounds intercepted by the phone can then be processed, allowing a skilled hacker to decipher which keys were struck and what they were typing. The researchers were able to decode much of what was being typed using common keyboards and smartphones -- even in a noisy conference room filled with the sounds of other people typing and having conversations. ""We were able to pick up what people are typing at a 41 percent word accuracy rate. And we can extend that out -- above 41 percent -- if we look at, say, the top 10 words of what we think it might be,"" said Eric C. Larson, one of the two lead authors and an assistant professor in SMU Lyle School's Department of Computer Science. The study was published in the June edition of the journal Interactive, Mobile, Wearable and Ubiquitous Technologies. Co-authors of the study are Tyler Giallanza, Travis Siems, Elena Sharp, Erik Gabrielsen and Ian Johnson -- all current or former students at the Deason Institute. It might take only a couple of seconds to obtain information on what you're typing, noted lead author Mitch Thornton, director of SMU's Deason Institute and professor of electrical and computer engineering. ""Based on what we found, I think smartphone makers are going to have to go back to the drawing board and make sure they are enhancing the privacy with which people have access to these sensors in a smartphone,"" Larson said. The researchers wanted to create a scenario that would mimic what might happen in real life. So they arranged several people in a conference room, talking to each other and taking notes on a laptop. Placed on the same table as their laptop or computer, were as many as eight mobile phones, kept anywhere from three inches to several feet feet away from the computer, Thornton said. Study participants were not given a script of what to say when they were talking, and were allowed to use shorthand or full sentences when typing. They were also allowed to either correct typewritten errors or leave them, as they saw fit. ""We were looking at security holes that might exist when you have these 'always-on' sensing devices -- that being your smartphone,"" Larson said. ""We wanted to understand if what you're typing on your laptop, or any keyboard for that matter, could be sensed by just those mobile phones that are sitting on the same table. ""The answer was a definite, ""Yes."" But just how does it work? ""There are many kinds of sensors in smartphones that cause the phone to know its orientation and to detect when it is sitting still on a table or being carried in someone's pocket. Some sensors require the user to give permission to turn them on, but many of them are always turned on,"" Thornton explained. ""We used sensors that are always turned on, so all we had to do was develop a new app that processed the sensor output to predict the key that was pressed by a typist."" There are some caveats, though. ""An attacker would need to know the material type of the table,"" Larson said, because different tables create different sound waves when you type. For instance, a wooden table like the kind used in this study sounds different than someone typing on a metal tabletop. Larson said, ""An attacker would also need a way of knowing there are multiple phones on the table and how to sample from them."" A successful interception of this sort could potentially be very scary, Thornton noted, because ""there's no way to know if you're being hacked this way."" The Deason Institute is part of SMU's Lyle School of Engineering, and its mission is to to advance the science, policy, application and education of cyber security through basic and problem-driven, interdisciplinary research. ",Computer Science,0.15854306253066233
8,Stanford,Wireless sensors stick to skin and track health,Science,2019-08-18,-,https://news.stanford.edu/2019/08/16/wireless-sensors-stick-skin-track-health/,"   We tend to take our skin’s protective function for granted, ignoring its other roles in signaling subtleties like a fluttering heart or a flush of embarrassment.  Using metallic ink, researchers screen-print an antenna and sensor onto a stretchable sticker designed to adhere to skin and track pulse and other health indicators, and beam these readings to a receiver on a person’s clothing. (Image credit: Bao Lab)  Now, Stanford engineers have developed a way to detect physiological signals emanating from the skin with sensors that stick like band-aids and beam wireless readings to a receiver clipped onto clothing. To demonstrate this wearable technology, the researchers stuck sensors to the wrist and abdomen of one test subject to monitor the person’s pulse and respiration by detecting how their skin stretched and contracted with each heartbeat or breath. Likewise, stickers on the person’s elbows and knees tracked arm and leg motions by gauging the minute tightening or relaxation of the skin each time the corresponding muscle flexed. Zhenan Bao, the chemical engineering professor whose lab described the system in an Aug. 15 article in Nature Electronics, thinks this wearable technology, which they call BodyNet, will first be used in medical settings such as monitoring patients with sleep disorders or heart conditions. Her lab is already trying to develop new stickers to sense sweat and other secretions to track variables such as body temperature and stress. Her ultimate goal is to create an array of wireless sensors that stick to the skin and work in conjunction with smart clothing to more accurately track a wider variety of health indicators than the smart phones or watches consumers use today. “We think one day it will be possible to create a full-body skin-sensor array to collect physiological data without interfering with a person’s normal behavior,” said Bao, who is also the K.K. Lee Professor in the School of Engineering. Stretchable, comfortable, functional Postdoctoral scholars Simiao Niu and Naoji Matsuhisa led the 14-person team that spent three years designing the sensors. Their goal was to develop a technology that would be comfortable to wear and have no batteries or rigid circuits to prevent the stickers from stretching and contracting with the skin. Their eventual design met these parameters with a variation of the RFID – radiofrequency identification – technology used to control keyless entry to locked rooms. When a person holds an ID card up to an RFID receiver, an antenna in the ID card harvests a tiny bit of RFID energy from the receiver and uses this to generate a code that it then beams back to the receiver.  The rubber sticker attached to the wrist can bend and stretch as the person’s skin moves, beaming pulse readings to a receiver clipped to the person’s clothing. (Image credit: Bao Lab)  The BodyNet sticker is similar to the ID card: It has an antenna that harvests a bit of the incoming RFID energy from a receiver on the clothing to power its sensors. It then takes readings from the skin and beams them back to the nearby receiver. But to make the wireless sticker work, the researchers had to create an antenna that could stretch and bend like skin. They did this by screen-printing metallic ink on a rubber sticker. However, whenever the antenna bent or stretched, those movements made its signal too weak and unstable to be useful. To get around this problem, the Stanford researchers developed a new type of RFID system that could beam strong and accurate signals to the receiver despite constant fluctuations. The battery-powered receiver then uses Bluetooth to periodically upload data from the stickers to a smartphone, computer or other permanent storage system. The initial version of the stickers relied on tiny motion sensors to take respiration and pulse readings. The researchers are now studying how to integrate sweat, temperature and other sensors into their antenna systems. To move their technology beyond clinical applications and into consumer-friendly devices, the researchers need to overcome another challenge – keeping the sensor and receiver close to each other. In their experiments, the researchers clipped a receiver on clothing just above each sensor. One-to-one pairings of sensors and receivers would be fine in medical monitoring, but to create a BodyNet that someone could wear while exercising, antennas would have to be woven into clothing to receive and transmit signals no matter where a person sticks a sensor. Bao is also a senior fellow of the Precourt Institute for Energy, a member of Stanford Bio-X, a faculty fellow of Stanford ChEM-H, an affiliate of the Stanford Woods Institute for the Environment and a member of the Wu Tsai Neurosciences Institute. Other Stanford co-authors are Jeffrey B.-H. Tok, research scientist; Ada Poon, associate professor of electrical engineering; William Burnett, adjunct professor of mechanical engineering; postdoctoral scholars Yuanwen Jiang and Jinxing Li; graduate student Jiechen Wang; and former visiting scholar Youngjun Yun and former postdoctoral scholars Sihong Wang, Xuzhou Yan and Levent Beker. Researchers from Singapore’s Nanyang Technological University also co-authored the study. This research was supported by Samsung Electronics; the Singapore Agency for Science, Technology and Research; the Japan Society for the Promotion of Science; and the Stanford Precision Health and Integrated Diagnosis Center. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Electronics and Technology,0.15829822278684325
9,Science Daily,Tiny Lensless Endoscope Captures 3D Images of Objects Smaller Than a Cell,Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815180631.htm,"   As a minimally invasive tool for imaging features inside living tissues, the extremely thin endoscope could enable a variety of research and medical applications. The research will be presented at the Frontiers in Optics + Laser Science (FIO + LS) conference, held September 15-19 in Washington, D.C., U.S.A. According to Juergen W. Czarske, Director and C4-Professor at TU Dresden, Germany and lead author on the paper: ""The lensless fiber endoscope is approximately the size of a needle, allowing it to have minimally invasive access and high-contrast imaging as well as stimulation with a robust calibration against bending or twisting of the fiber."" The endoscope is likely to be especially useful for optogenetics -- research approaches that use light to stimulate cellular activity. It also could prove useful for monitoring cells and tissues during medical procedures as well as for technical inspections. A self-calibrating system Conventional endoscopes use cameras and lights to capture images inside the body. In recent years researchers have developed alternative ways to capture images through optical fibers, eliminating the need for bulky cameras and other bulky components, allowing for significantly thinner endoscopes. Despite their promise, however, these technologies suffer from limitations such as an inability to tolerate temperature fluctuations or bending and twisting of the fiber. A major hurdle to making these technologies practical is that they require complicated calibration processes, in many cases while the fiber is collecting images. To address this, the researchers added a thin glass plate, just 150 microns thick, to the tip of a coherent fiber bundle, a type of optical fiber that is commonly used in endoscopy applications. The coherent fiber bundle used in the experiment was about 350 microns wide and consisted of 10,000 cores. When the central fiber core is illuminated, it emits a beam that is reflected back into the fiber bundle and serves as a virtual guide star for measuring how the light is being transmitted, known as the optical transfer function. The optical transfer function provides crucial data the system uses to calibrate itself on the fly. Keeping the view in focus A key component of the new setup is a spatial light modulator, which is used to manipulate the direction of the light and enable remote focusing. The spatial light modulator compensates the optical transfer function and images onto the fiber bundle. The back-reflected light from the fiber bundle is captured on the camera and superposed with a reference wave to measure the light's phase. The position of the virtual guide star determines the instrument's focus, with a minimal focus diameter of approximately one micron. The researchers used an adaptive lens and a 2D galvometer mirror to shift the focus and enable scanning at different depths. Demonstrating 3D imaging The team tested their device by using it to image a 3D specimen under a 140-micron thick cover slip. Scanning the image plane in 13 steps over 400 microns with an image rate of 4 cycles per second, the device successfully imaged particles at the top and bottom of the 3D specimen. However, its focus deteriorated as the galvometer mirror's angle increased. The researchers suggest future work could address this limitation. In addition, using a galvometer scanner with a higher frame rate could allow faster image acquisition. ""The novel approach enables both real-time calibration and imaging with minimal invasiveness, important for in-situ 3D imaging, lab-on-a-chip-based mechanical cell manipulation, deep tissue in vivo optogenetics, and key-hole technical inspections,"" said Czarske. ",Electronics and Technology,0.15783245445602428
10,Science Daily,Stronger Graphene Oxide 'Paper' Made With Weaker Units,Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815081305.htm,"   Wrong -- at least when working with ""flakes"" of graphene oxide (GO). A new study from Northwestern University researchers shows that better GO ""paper"" can be made by mixing strong, solid GO flakes with weak, porous GO flakes. The finding will aid the production of higher quality GO materials, and it sheds light on a general problem in materials engineering: how to build a nano-scale material into a macroscopic material without losing its desirable properties. ""To put it in human terms, collaboration is very important,"" said Jiaxing Huang, Northwestern Engineering professor of materials science and engineering, who led the study. ""Excellent players can still make a bad team if they don't work well together. Here, we add some seemingly weaker players and they strengthen the whole team."" The research was a four-way collaboration. In addition to Huang's, three other groups participated, led by Horacio Espinosa, professor of mechanical engineering at the McCormick School of Engineering; SonBinh Nguyen, professor of chemistry at Northwestern; and Tae Hee Han, a former postdoc researcher at the University who's now a professor of organic and nano engineering at Hanyang University, South Korea. The study was published today in Nature Communications. High-tech paper GO is a derivative of graphite that can be used to make the two-dimensional, super material graphene. Since GO is easier to make, scientists study it as a model material. It generally comes as a dispersion of tiny flakes in water. From one end to the other, each flake is smaller than the width of a human hair and only one nanometer thick. When a solution of GO flakes is poured onto a filter and the water removed, a thin ""paper"" is formed, usually a few inches in diameter with a thickness less than or equal to 40 micrometers. Intermolecular forces hold the flakes together, nothing more. Strength from weakness Scientists can make strong GO in single layers but layering the flakes into a paper form doesn't work too well. While testing the effect of holes on the strength of GO flakes, Huang and his collaborators discovered a solution. Using a mixture of ammonia and hydrogen peroxide, the researchers chemically ""etched"" holes in the GO flakes. Flakes left soaking for one to three hours were drastically weaker than un-etched flakes. After five hours of soaking, flakes became so weak they couldn't be measured. Then, the team found something surprising: Paper made from the weakened flakes was stronger than expected. At the single layer level, one-hour-etched porous flakes, for example, were 70 percent weaker than solid flakes, but paper made from those flakes was only 10 percent weaker than paper made from solid flakes. Things got even more interesting when the team mixed solid and porous flakes together, Huang said. Instead of weakening the paper made solely from solid flakes, the addition of 10 or 25 percent of the weakest flakes strengthened it by about 95 and 70 percent, respectively. Effective connection If GO sheets can be likened to aluminum foil, Huang said, making a GO paper is just like stacking the foil up to make a thick aluminum slab. If you start with large sheets of aluminum foil, chances are good that many will wrinkle, impeding tight packing between sheets. On the other hand, smaller sheets don't wrinkle as easily. They pack together well but create tight stacks that don't integrate well with other tight stacks, creating voids within GO paper where it can easily break. ""Weak flakes warp to fill in those voids, which improves the distribution of forces throughout the material,"" Huang said. ""It's a reminder that the strength of individual units is only part of the equation; effective connection and stress distribution is equally important."" This finding will be directly applicable to other two-dimensional materials, like graphene, Huang said, and will also lead to the design of higher quality GO products. He hopes to test it out on GO fibers next. The Office of Naval Research (ONRN000141612838) supported this work. ",Electronics and Technology,0.15737131020951323
11,Science Daily,Newfound Superconductor Material Could Be the 'Silicon of Quantum Computers',Computers & Math,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815140843.htm,"   Newly discovered properties in the compound uranium ditelluride, or UTe2, show that it could prove highly resistant to one of the nemeses of quantum computer development -- the difficulty with making such a computer's memory storage switches, called qubits, function long enough to finish a computation before losing the delicate physical relationship that allows them to operate as a group. This relationship, called quantum coherence, is hard to maintain because of disturbances from the surrounding world. The compound's unusual and strong resistance to magnetic fields makes it a rare bird among superconducting (SC) materials, which offer distinct advantages for qubit design, chiefly their resistance to the errors that can easily creep into quantum computation. UTe2's exceptional behaviors could make it attractive to the nascent quantum computer industry, according to the research team's Nick Butch. ""This is potentially the silicon of the quantum information age,"" said Butch, a physicist at the NIST Center for Neutron Research (NCNR). ""You could use uranium ditelluride to build the qubits of an efficient quantum computer."" Research results from the team, which also includes scientists from the University of Maryland and Ames Laboratory, appear today in the journal Science. Their paper details UTe2's uncommon properties, which are interesting from the perspectives of both technological application and fundamental science. One of these is the unusual way the electrons that conduct electricity through UTe2 partner up. In copper wire or some other ordinary conductor, electrons travel as individual particles, but in all SCs they form what are called Cooper pairs. The electromagnetic interactions that cause these pairings are responsible for the material's superconductivity. The explanation for this kind of superconductivity is named BCS theory after the three scientists who uncovered the pairings (and shared the Nobel Prize for doing so). What's specifically important to this Cooper pairing is a property that all electrons have. Known as quantum ""spin,"" it makes electrons behave as if they each have a little bar magnet running through them. In most SCs, the paired electrons have their quantum spins oriented in a single way -- one electron's points upward, while its partner points down. This opposed pairing is called a spin singlet. A small number of known superconductors, though, are nonconformists, and UTe2 looks to be among them. Their Cooper pairs can have their spins oriented in one of three combinations, making them spin triplets. These combinations allow for the Cooper-pair spins to be oriented in parallel rather than in opposition. Most spin-triplet SCs are predicted to be ""topological"" SCs as well, with a highly useful property in which the superconductivity would occur on the surface of the material and would remain superconducting even in the face of external disturbances. ""These parallel spin pairs could help the computer remain functional,"" Butch said. ""It can't spontaneously crash because of quantum fluctuations."" All quantum computers up until this point have needed a way to correct the errors that creep in from their surroundings. SCs have long been understood to have general advantages as the basis for quantum computer components, and several recent commercial advances in quantum computer development have involved circuits made from superconductors. A topological SC's properties -- which a quantum computer might employ -- would have the added advantage of not needing quantum error correction. ""We want a topological SC because it would give you error-free qubits. They could have very long lifetimes,"" Butch said. ""Topological SCs are an alternate route to quantum computing because they would protect the qubit from the environment."" The team stumbled upon UTe2 while exploring uranium-based magnets, whose electronic properties can be tuned as desired by changing their chemistry, pressure or magnetic field -- a useful feature to have when you want customizable materials. (None of these parameters are based on radioactivity. The material contains ""depleted uranium,"" which is only slightly radioactive. Qubits made from UTe2 would be tiny, and they could easily be shielded from their environment by the rest of the computer.) The team did not expect the compound to possess the properties they discovered. ""UTe2 had first been created back in the 1970s, and even fairly recent research articles described it as unremarkable,"" Butch said. ""We happened to make some UTe2 while we were synthesizing related materials, so we tested it at lower temperatures to see if perhaps some phenomenon might have been overlooked. We quickly realized that we had something very special on our hands.""?? The NIST team started exploring UTe2 with specialized tools at both the NCNR and the University of Maryland. They saw that it became superconducting at low temperatures (below -271.5 degrees Celsius, or 1.6 kelvin). Its superconducting properties resembled those of rare superconductors that are also simultaneously ferromagnetic -- acting like low-temperature permanent magnets. Yet, curiously, UTe2 is itself not ferromagnetic. ""That makes UTe2 fundamentally new for that reason alone,"" Butch said. It is also highly resistant to magnetic fields. Typically a field will destroy superconductivity, but depending on the direction in which the field is applied, UTe2 can withstand fields as high as 35 tesla. This is 3,500 times as strong as a typical refrigerator magnet, and many times more than most low-temperature topological SCs can endure. While the team has not yet proved conclusively that UTe2 is a topological SC, Butch says this unusual resistance to strong magnetic fields means that it must be a spin-triplet SC, and therefore it is likely a topological SC as well. This resistance also might help scientists understand the nature of UTe2 and perhaps superconductivity itself. ""Exploring it further might give us insight into what stabilizes these parallel-spin SCs,"" he said. ""A major goal of SC research is to be able to understand superconductivity well enough that we know where to look for undiscovered SC materials. Right now we can't do that. What about them is essential? We are hoping this material will tell us more."" ",Electronics and Technology,0.15656268652421979
12,Stanford,nil,Science,2019-08-18,-,https://engineering.stanford.edu/magazine/article/qa-how-catalytic-converters-cars-go-bad-and-why-it-matters,"  Modern cars rely on catalytic converters to remove carbon monoxide, hydrocarbons and other harmful chemicals from exhaust emissions. To do so they rely on costly metals that have special chemical properties that diminish in effectiveness over time. Assistant professor Matteo Cargnello and doctoral candidate Emmett Goodman recently led a team that has proposed a new way to reduce the cost and extend the lifespan of these materials, solving a problem that has vexed automotive engineers for years. In the process, Cargnello and colleagues have done something remarkable: made a breakthrough in a mature field where change comes slowly, if at all. What about catalytic converters needs to be improved? A new catalytic converter can cost $1,000 or more, making it among the most expensive individual parts on any car. They are costly because they use expensive metals such as palladium to promote the chemical reactions that cleanse the exhaust. Palladium costs about $50 a gram — more than gold — and each catalytic converter contains about 5 grams of it. Metals like palladium are catalysts — a special class of materials that speed up chemical reactions but don’t chemically change themselves. In theory, catalysts can be used over and over, indefinitely. In practice, however, the performance of catalysts degrades over time. To compensate, we are forced to use more of these expensive metals up front, adding to the cost. Our goal is to better understand the causes of this degradation and how to counteract it. Why do catalysts go bad? Ideally, catalysts should be designed to have the greatest surface area possible to promote the greatest number of chemical reactions. So, manufacturers typically spread many small particles over the surface of a new catalytic converter. From past research we know that, over time, the metal atoms begin to move, forming larger and larger particles that offer less surface area, and thus become less effective. We call this clumping process “sintering.” To counteract sintering, manufacturers use excessive amounts of metal so that the converter will meet emissions standards for the 10- or 15-year lifespan of a car. Our team has discovered that sintering isn’t the only cause of deactivation. In fact, this new deactivation mechanism turns out to be quite the opposite of sintering. Under some circumstances, instead of particles getting larger, they decompose into smaller particles and eventually become single atoms that are essentially inactive. This is a new understanding we believe no one has presented before, and it prompted us to look for an entirely new way to maximize the lifespan and performance of the metals in catalytic converters. What can we do to make catalysts last longer? Our research suggests that if we carefully control both the size and the spacing of metal particles, palladium particles will neither sinter into large clumps nor decompose into single atoms. Previously, many people in the catalysis community thought that if you want to make particles stable, you had to keep them as far apart as possible to prevent migration of the particles. We confounded this notion by bringing together a collaborative team that studied degradation in a new way. Aaron Johnston-Peck from the National Institute of Standards and Technology used advanced microscopy to help visualize the presence of the single atoms. Simon Bare from the SLAC National Accelerator Laboratory used X-ray techniques to prove that catalytic materials start as particles and end up as single atoms. To put these experimental results in a theoretical framework, we worked with Frank Abild-Pedersen from the SUNCAT Center for Interface Science and Catalysis and SLAC, and Philipp Plessow from the Karlsruhe Institute of Technology in Germany. They had the computational resources to help us simulate the deactivation mechanism at the atomic scale. In the end, we’ve provided a scientific basis that could make it possible to maintain pollution reduction while using less precious metal and lowering the costs of catalytic converters. If automotive engineers ultimately confirm and implement these findings, it would be a huge win for consumers in the long run. This work was supported by the U.S. Department of Energy, the SUNCAT Center for Interface Science and Catalysis, the Stanford Synchrotron Radiation Lightsource and the SLAC National Accelerator Laboratory. Matteo Cargnello, assistant professor of chemical engineering and, by courtesy, of materials science and engineering. ",Electronics and Technology,0.15616491631683857
13,Science Daily,Wearable Sensors Detect What's in Your Sweat,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191428.htm,"   They hope that one day, monitoring perspiration could bypass the need for more invasive procedures like blood draws, and provide real-time updates on health problems such as dehydration or fatigue. In a paper appearing today (Friday, August 16) in Science Advances, the team describes a new sensor design that can be rapidly manufactured using a ""roll-to-roll"" processing technique that essentially prints the sensors onto a sheet of plastic like words on a newspaper. They used the sensors to monitor the sweat rate, and the electrolytes and metabolites in sweat, from volunteers who were exercising, and others who were experiencing chemically induced perspiration. ""The goal of the project is not just to make the sensors but start to do many subject studies and see what sweat tells us -- I always say 'decoding' sweat composition,"" said Ali Javey, a professor of electrical engineering and computer science at UC Berkeley and senior author on the paper. ""For that we need sensors that are reliable, reproducible, and that we can fabricate to scale so that we can put multiple sensors in different spots of the body and put them on many subjects,"" said Javey, who also serves as a faculty scientist at Lawrence Berkeley National Laboratory. The new sensors contain a spiraling microscopic tube, or microfluidic, that wicks sweat from the skin. By tracking how fast the sweat moves through the microfluidic, the sensors can report how much a person is sweating, or their sweat rate. The microfluidics are also outfitted with chemical sensors that can detect concentrations of electrolytes like potassium and sodium, and metabolites like glucose. Javey and his team worked with researchers at the VTT Technical Research Center of Finland to develop a way to quickly manufacture the sensor patches in a roll-to-roll processing technique similar to screen printing. ""Roll-to-roll processing enables high-volume production of disposable patches at low cost,"" Jussi Hiltunen of VTT said. ""Academic groups gain significant benefit from roll-to-roll technology when the number of test devices is not limiting the research. Additionally, up-scaled fabrication demonstrates the potential to apply the sweat-sensing concept in practical applications."" To better understand what sweat can say about the real-time health of the human body, the researchers first placed the sweat sensors on different spots on volunteers' bodies -- including the forehead, forearm, underarm and upper back -- and measured their sweat rates and the sodium and potassium levels in their sweat while they rode on an exercise bike. They found that local sweat rate could indicate the body's overall liquid loss during exercise, meaning that tracking sweat rate might be a way to give athletes a heads up when they may be pushing themselves too hard. ""Traditionally what people have done is they would collect sweat from the body for a certain amount of time and then analyze it,"" said Hnin Yin Yin Nyein, a graduate student in materials science and engineering at UC Berkeley and one of the lead authors on the paper. ""So you couldn't really see the dynamic changes very well with good resolution. Using these wearable devices we can now continuously collect data from different parts of the body, for example to understand how the local sweat loss can estimate whole-body fluid loss."" They also used the sensors to compare sweat glucose levels and blood glucose levels in healthy and diabetic patients, finding that a single sweat glucose measurement cannot necessarily indicate a person's blood glucose level. ""There's been a lot of hope that non-invasive sweat tests could replace blood-based measurements for diagnosing and monitoring diabetes, but we've shown that there isn't a simple, universal correlation between sweat and blood glucose levels,"" said Mallika Bariya, a graduate student in materials science and engineering at UC Berkeley and the other lead author on the paper. ""This is important for the community to know, so that going forward we focus on investigating individualized or multi-parameter correlations."" This work was supported by the NSF Nanomanufacturing Systems for Mobile Computing and Mobile Energy Technologies (NASCENT), the Berkeley Sensor and Actuator Center (BSAC), and the Bakar fellowship. ",Electronics and Technology,0.14492876359410373
14,Science Daily,Superconductors: Unraveling the Stripe Order Mystery,Matter & Energy,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191423.htm,"   Now researchers at the University of Illinois at Urbana-Champaign, collaborating with scientists at the SLAC National Accelerator Laboratory, have shed new light on how these disparate states can exist adjacent to one another. Illinois Physics post-doctoral researcher Matteo Mitrano, Professor Peter Abbamonte, and their team applied a new x-ray scattering technique, time-resolved resonant soft x-ray scattering, taking advantage of the state-of-the-art equipment at SLAC. This method enabled the scientists to probe the striped charge order phase with an unprecedented energy resolution. This is the first time this has been done at an energy scale relevant to superconductivity. The scientists measured the fluctuations of charge order in a prototypical copper-oxide superconductor, La2?xBaxCuO4 (LBCO) and found the fluctuations had an energy that matched the material's superconducting critical temperature, implying that superconductivity in this material -- and by extrapolation, in the cuprates -- may be mediated by charge-order fluctuations. The researchers further demonstrated that, if the charge order melts, the electrons in the system will reform the striped areas of charge order within tens of picoseconds. As it turns out, this process obeys a universal scaling law. To understand what they were seeing in their experiment, Mitrano and Abbamonte turned to Illinois Physics Professor Nigel Goldenfeld and his graduate student Minhui Zhu, who were able to apply theoretical methods borrowed from soft condensed matter physics to describe the formation of the striped patterns. These findings were published on August 16, 2019, in the online journal Science Advances. Cuprates have stripes The significance of this mystery can be understood within the context of research in high-temperature superconductors (HTS), specifically the cuprates -- layered materials that contain copper complexes. The cuprates, some of the first discovered HTS, have significantly higher critical temperatures than ""ordinary"" superconductors (e.g., aluminum and lead superconductors have a critical temperature below 10 K). In the 1980s, LBCO, a cuprate, was found to have a superconducting critical temperature of 35 K (-396°F), a discovery for which Bednorz and Müller won the Nobel Prize. That discovery precipitated a flood of research into the cuprates. In time, scientists found experimental evidence of inhomogeneities in LBCO and similar materials: insulating and metallic phases that were coexisting. In 1998, Illinois Physics Professor Eduardo Fradkin, Stanford Professor Steven Kivelson, and others proposed that Mott insulators -- materials that ought to conduct under conventional band theory but insulate due to repulsion between electrons -- are able to host stripes of charge order and superconductivity. La2CuO4, the parent compound of LBCO, is an example of a Mott insulator. As Ba is added to that compound, replacing some La atoms, stripes form due to the spontaneous organization of holes -- vacancies of electrons that act like positive charges. Still, other questions regarding the behavior of the stripes remained. Are the areas of charge order immobile? Do they fluctuate? ""The conventional belief is that if you add these doped holes, they add a static phase which is bad for superconductivity -- you freeze the holes, and the material cannot carry electricity,"" Mitrano comments. ""If they are dynamic -- if they fluctuate -- then there are ways in which the holes could aid high-temperature superconductivity."" Probing the fluctuations in LBCO To understand what exactly the stripes are doing, Mitrano and Abbamonte conceived of an experiment to melt the charge order and observe the process of its reformation in LBCO. Mitrano and Abbamonte reimagined a measurement technique called resonant inelastic x-ray scattering, adding a time-dependent protocol to observe how the charge order recovers over a duration of 40 picoseconds. The team shot a laser at the LBCO sample, imparting extra energy into the electrons to melt the charge order and introduce electronic homogeneity. ""We used a novel type of spectrometer developed for ultra-fast sources, because we are doing experiments in which our laser pulses are extremely short,"" Mitrano explains. ""We performed our measurements at the Linac Coherent Light Source at SLAC, a flagship in this field of investigation. Our measurements are two orders of magnitude more sensitive in energy than what can be done at any other conventional scattering facility."" Abbamonte adds, ""What is innovative here is using time-domain scattering to study collective excitations at the sub-meV energy scale. This technique was demonstrated previously for phonons. Here, we have shown the same approach can be applied to excitations in the valence band."" Hints of a mechanism for superconductivity The first significant result of this experiment is that the charge order does in fact fluctuate, moving with an energy that almost matches the energy established by the critical temperature of LBCO. This suggests that Josephson coupling may be crucial for superconductivity. The idea behind the Josephson effect, discovered by Brian Josephson in 1962, is that two superconductors can be connected via a weak link, typically an insulator or a normal metal. In this type of system, superconducting electrons can leak from the two superconductors into the weak link, generating within it a current of superconducting electrons. Josephson coupling provides a possible explanation for the coupling between superconductivity and striped regions of charge order, wherein the stripes fluctuate such that superconductivity leaks into the areas of charge order, the weak links. Obeying universal scaling laws of pattern formation  After melting the charge order, Mitrano and Abbamonte measured the recovery of the stripes as they evolved in time. As the charge order approached its full recovery, it followed an unexpected time dependence. This result was nothing like what the researchers had encountered in the past. What could possibly explain this? The answer is borrowed from the field of soft condensed matter physics, and more specifically from a scaling law theory Goldenfeld had developed two decades prior to describe pattern formation in liquids and polymers. Goldenfeld and Zhu demonstrated the stripes in LBCO recover according to a universal, dynamic, self-similar scaling law. Goldenfeld explains, ""By the mid-1990s, scientists had an understanding of how uniform systems approach equilibrium, but how about stripe systems? I worked on this question about 20 years ago, looking at the patterns that emerge when a fluid is heated from below, such as the hexagonal spots of circulating, upwelling white flecks in hot miso soup. Under some circumstances these systems form stripes of circulating fluid, not spots, analogous to the stripe patterns of electrons in the cuprate superconductors. And when the pattern is forming, it follows a universal scaling law. This is exactly what we see in LBCO as it reforms its stripes of charge order."" Through their calculations, Goldenfeld and Zhu were able to elucidate the process of time-dependent pattern reformation in Mitrano and Abbamonte's experiment. The stripes reform with a logarithmic time dependence -- a very slow process. Adherence to the scaling law in LBCO further implies that it contains topological defects, or irregularities in its lattice structure. This is the second significant result from this experiment. Zhu comments, ""It was exciting to be a part of this collaborative research, working with solid-state physicists, but applying techniques from soft condensed matter to analyze a problem in a strongly correlated system, like high-temperature superconductivity. I not only contributed my calculations, but also picked up new knowledge from my colleagues with different backgrounds, and in this way gained new perspectives on physical problems, as well as new ways of scientific thinking."" In future research, Mitrano, Abbamonte, and Goldenfeld plan to further probe the physics of charge order fluctuations with the goal of completely melting the charge order in LBCO to observe the physics of stripe formation. They also plan similar experiments with other cuprates, including yttrium barium copper oxide compounds, better known as YBCO. Goldenfeld sees this and future experiments as ones that could catalyze new research in HTS: ""What we learned in the 20 years since Eduardo Fradkin and Steven Kivelson's work on the periodic modulation of charge is that we should think about the HTS as electronic liquid crystals,"" he states. ""We're now starting to apply the soft condensed matter physics of liquid crystals to HTS to understand why the superconducting phase exists in these materials."" ",Electronics and Technology,0.13625892317099816
15,Science Daily,Researchers Demonstrate Three-Dimensional Quantum Hall Effect for the First Time,Matter & Energy,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816101008.htm,"   The Hall effect, a fundamental technique for material characterization, is formed when a magnetic field deflects the flow of electrons sideways and leads to a voltage drop across the transverse direction. In 1980, a surprising observation was made when measuring the Hall effect for a two-dimensional (2D) electron gas trapped in a semiconductor structure -- the measured Hall resistivity showed a series of completely flat plateau, quantized to values with a remarkable accuracy of one part in 10 billion. This became known as the QHE. QHE has since revolutionized our fundamental understanding of condensed matter physics, generating a vast field of physics research. Many new emerging topics, such as topological materials, can also be traced back to it. Soon after its discovery, researchers pursued the possibility of generalizing QHE from 2D systems to three dimensions (3D). Bertrand Halperin predicted that such a generalized effect, called the 3D QHE, is indeed possible in a seminal paper published in 1987. From theoretical analysis, he gave signatures for 3D QHE and pointed out that enhanced interactions between the electrons under a magnetic field can be the key to drive a metal material into the 3D QHE state. 30 years have passed since Halperin's prediction and while there have been continuing efforts in trying to realize 3D QHE in experiment, clear evidence has been elusive due to the stringent conditions required for 3D QHE -- the material needs to be very pure, have high mobility, and low carrier density. SUTD's experimental collaborator, the Southern University of Science and Technology (SUSTech) in China, has been working on a unique material known as ZrTe5 since 2014. This material is able to satisfy the required conditions and exhibit the signatures of 3D QHE. In the research paper published in Nature, the researchers show that when the material is cooled to very low temperature while under a moderate magnetic field, its longitudinal resistivity drops to zero, indicating that the material transforms from a metal to an insulator. This is due to the electronic interactions where the electrons redistribute themselves and form a periodic density wave along the magnetic field direction (as illustrated in the image) called the charge density wave. ""This change would usually freeze the electron motion and the material becomes insulating, disallowing the electron to flow through the interior of the material. However, using this unique material, the electrons can move through the surfaces, giving a Hall resistivity quantized by the wavelength of the charge density wave,"" explained co-author Professor Zhang Liyuan from SUSTech. This in turn proves the first demonstration of the long speculating 3D QHE, pushing the celebrated QHE from 2D to 3D. ""We can expect that the discovery of 3D QHE will lead to new breakthroughs in our knowledge of physics and provide a cornucopia of new physical effects. This new knowledge, in one way or another, will also provide us new opportunities for practical technological development,"" said co-author, Assistant Professor Yang Shengyuan from SUTD. ",Electronics and Technology,0.13520612997940004
16,Science Daily,Best of Both Worlds: Asteroids and Massive Mergers,Space & Time,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816092427.htm,"   The University of Arizona's Steward Observatory has partnered with the Catalina Sky Survey, which searches for near-Earth asteroids from atop Mount Lemmon, in an effort dubbed Searches after Gravitational Waves Using ARizona Observatories, or SAGUARO, to find optical counterparts to massive mergers. ""Catalina Sky Survey has all of this infrastructure for their asteroid survey. So we have deployed additional software to take gravitational wave alerts from LIGO (the Laser Interferometer Gravitational-Wave Observatory) and the Virgo interferometer then notify the survey to search an area of sky most likely to contain the optical counterpart,"" said Michael Lundquist, postdoctoral research associate and lead author on the study published today in the Astrophysical Journal Letters. ""Essentially, instead of searching the next section of sky that we would normally, we go off and observe some other area that has a higher probability of containing an optical counterpart of a gravitational wave event,"" said Eric Christensen, Catalina Sky Survey director and Lunar and Planetary Laboratory senior staff scientist. ""The main idea is we can run this system while still maintaining the asteroid search."" The ongoing campaign began in April, and in that month alone, the team was notified of three massive collisions. Because it is difficult to tell the precise location from which the gravitational wave originated, locating optical counterparts can be difficult. According to Lundquist, two strategies are being employed. In the first, teams with small telescopes target galaxies that are at the right approximate distance, according to the gravitational wave signal. Catalina Sky Survey, on the other hand, utilizes a 60-inch telescope with a wide field of view to scan large swaths of sky in 30 minutes. Three alerts, on April 9, 25 and 26, triggered the team's software to search nearly 20,000 objects. Machine learning software then trimmed down the total number of potential optical counterparts to five. The first gravitational wave event was a merger of two black holes, Lundquist said. ""There are some people who think you can get an optical counterpart to those, but it's definitely inconclusive,"" he said. The second event was a merger of two neutron stars, the incredibly dense core of a collapsed giant star. The third is thought to be a merger between a neutron star and a black hole, Lundquist said. While no teams confirmed optical counterparts, the UA team did find several supernovae. They also used the Large Binocular Telescope Observatory to spectroscopically classify one promising target from another group. It was determined to be a supernova and not associated with the gravitational wave event. ""We also found a near-Earth object in the search field on April 25,"" Christensen said. ""That proves right there we can do both things at the same time."" They were able to do this because Catalina Sky Survey has observations of the same swaths of sky going back many years. Many other groups don't have easy access to past photos for comparison, offering the UA team a leg up. ""We have really nice references,"" Lundquist said. ""We subtract the new image from the old image and use that difference to look for anything new in the sky."" ""The process Michael described,"" Christensen said, ""starting with a large number of candidate detections and filtering down to whatever the true detections are, is very familiar. We do that with near-Earth objects, as well."" The team is planning on deploying a second telescope in the hunt for optical counterparts: Catalina Sky Survey's 0.7-meter Schmidt telescope. While the telescope is smaller than the 60-inch telescope, it has an even wider field of view, which allows astronomers to quickly search an even larger chunk of sky. They've also improved their machine learning software to filter out stars that regularly change in brightness. ""Catalina Sky Survey takes hundreds of thousands of images of the sky every year, from multiple telescopes. Our survey telescopes image the entire visible nighttime sky several times per month, then we are looking for one kind of narrow slice of the pie,"" Christensen said. ""So, we've been willing to share the data with whoever wants to use it."" ",Space & Time,0.12013238134687648
17,Science Daily,Fluorescent Glow May Reveal Hidden Life in the Cosmos,Space & Time,2019-08-13,-,https://www.sciencedaily.com/releases/2019/08/190813144510.htm,"   ""Biofluorescent Worlds II: Biological Fluorescence Induced by Stellar UV Flares, a New Temporal Biosignature,"" was published in Monthly Notices of the Royal Astronomical Society. ""This is a completely novel way to search for life in the universe. Just imagine an alien world glowing softly in a powerful telescope,"" said lead author Jack O'Malley-James, a researcher at Cornell's Carl Sagan Institute. ""On Earth, there are some undersea coral that use biofluorescence to render the sun's harmful ultraviolet radiation into harmless visible wavelengths, creating a beautiful radiance. Maybe such life forms can exist on other worlds too, leaving us a telltale sign to spot them,"" said co-author Lisa Kaltenegger, associate professor of astronomy and director of the Carl Sagan Institute Astronomers generally agree that a large fraction of exoplanets -- planets beyond our solar system -- reside in the habitable zone of M-type stars, the most plentiful kinds of stars in the universe. M-type stars frequently flare, and when those ultraviolet flares strike their planets, biofluorescence could paint these worlds in beautiful colors. The next generation of Earth- or space-based telescopes can detect the glowing exoplanets, if they exist in the cosmos. Ultraviolet rays can get absorbed into longer, safer wavelengths through a process called ""photoprotective biofluorescence,"" and that mechanism leaves a specific sign for which astronomers can search. ""Such biofluorescence could expose hidden biospheres on new worlds through their temporary glow, when a flare from a star hits the planet,"" said Kaltenegger. The astronomers used emission characteristics of common coral fluorescent pigments from Earth to create model spectra and colors for planets orbiting active M stars to mimic the strength of the signal and whether it could be detected for life. In 2016, astronomers found a rocky exoplanet named Proxima b -- a potentially habitable world orbiting the active M star Proxima Centauri, Earth's closest star beyond the sun -- that might qualify as a target. Proxima b is also one of the most optimal far-future travel destinations. ""These biotic kinds of exoplanets are very good targets in our search for exoplanets, and these luminescent wonders are among our best bets for finding life on exoplanets,"" O'Malley-James said. Large, land-based telescopes that are being developed now for 10 to 20 years into the future may be able to spot this glow. ""It is a great target for the next generation of big telescopes, which can catch enough light from small planets to analyze it for signs of life, like the Extremely Large Telescope in Chile,"" Kaltenegger said. ",Space & Time,0.11994899033960439
18,Science Daily,James Webb Space Telescope Could Begin Learning About TRAPPIST-1 Atmospheres in a Year,Space & Time,2019-08-13,-,https://www.sciencedaily.com/releases/2019/08/190813170727.htm,"   The study, led by Jacob Lustig-Yaeger, a UW doctoral student in astronomy, finds that the James Webb telescope, set to launch in 2021, might be able to learn key information about the atmospheres of the TRAPPIST-1 worlds even in its first year of operation, unless -- as an old song goes -- clouds get in the way. ""The Webb telescope has been built, and we have an idea how it will operate,"" said Lustig-Yaeger. ""We used computer modeling to determine the most efficient way to use the telescope to answer the most basic question we'll want to ask, which is: Are there even atmospheres on these planets, or not?"" His paper, ""The Detectability and Characterization of the TRAPPIST-1 Exoplanet Atmospheres with JWST,"" was published online in June in the Astronomical Journal. The TRAPPIST-1 system, 39 light-years -- or about 235 trillion miles -- away in the constellation of Aquarius, interests astronomers because of its seven orbiting rocky, or Earth-like, planets. Three of these worlds are in the star's habitable zone -- that swath of space around a star that is just right to allow liquid water on the surface of a rocky planet, thus giving life a chance. The star, TRAPPIST-1, was much hotter when it formed than it is now, which would have subjected all seven planets to ocean, ice and atmospheric loss in the past. ""There is a big question in the field right now whether these planets even have atmospheres, especially the innermost planets,"" Lustig-Yaeger said. ""Once we have confirmed that there are atmospheres, then what can we learn about each planet's atmosphere -- the molecules that make it up?"" Given the way he suggests the James Webb Space Telescope might search, it could learn a lot in fairly short time, this paper finds. Astronomers detect exoplanets when they pass in front of or ""transit"" their host star, resulting in a measurable dimming of starlight. Planets closer to their star transit more frequently and so are somewhat easier to study. When a planet transits its star, a bit of the star's light passes through the planet's atmosphere, with which astronomers can learn about the molecular composition of the atmosphere. Lustig-Yaeger said astronomers can see tiny differences in the planet's size when they look in different colors, or wavelengths, of light. ""This happens because the gases in the planet's atmosphere absorb light only at very specific colors. Since each gas has a unique 'spectral fingerprint,' we can identify them and begin to piece together the composition of the exoplanet's atmosphere."" Lustig-Yaeger said the team's modeling indicates that the James Webb telescope, using a versatile onboard tool called the Near-Infrared Spectrograph, could detect the atmospheres of all seven TRAPPIST-1 planets in 10 or fewer transits -- if they have cloud-free atmospheres. And of course we don't know whether or not they have clouds. If the TRAPPIST-1 planets have thick, globally enshrouding clouds like Venus does, detecting atmospheres might take up to 30 transits. ""But that is still an achievable goal,"" he said. ""It means that even in the case of realistic high-altitude clouds, the James Webb telescope will still be capable of detecting the presence of atmospheres -- which before our paper was not known."" Many rocky exoplanets have been discovered in recent years, but astronomers have not yet detected their atmospheres. The modeling in this study, Lustig-Yaeger said, ""demonstrates that, for this TRAPPIST-1 system, detecting terrestrial exoplanet atmospheres is on the horizon with the James Webb Space Telescope -- perhaps well within its primary five-year mission."" The team found that the Webb telescope may be able to detect signs that the TRAPPIST-1 planets lost large amounts of water in the past, when the star was much hotter. This could leave instances where abiotically produced oxygen -- not representative of life -- fills an exoplanet atmosphere, which could give a sort of ""false positive"" for life. If this is the case with TRAPPIST-1 planets, the Webb telescope may be able to detect those as well. Lustig-Yaeger's co-authors, both with the UW, are astronomy professor Victoria Meadows, who is also principal investigator for the UW-based Virtual Planetary Laboratory; and astronomy doctoral student Andrew Lincowski. The work follows, in part, on previous work by Lincowski modeling possible climates for the seven TRAPPIST-1 worlds. ""By doing this study, we have looked at: What are the best-case scenarios for the James Webb Space Telescope? What is it going to be capable of doing? Because there are definitely going to be more Earth-sized planets found before it launches in 2021."" The research was funded by a grant from the NASA Astrobiology Program's Virtual Planetary Laboratory team, as part of the Nexus for Exoplanet System Science (NExSS) research coordination network. Lustig-Yaeger added: ""It's hard to conceive in theory of a planetary system better suited for James Webb than TRAPPIST-1."" ",Space & Time,0.11991011116976934
19,Science Daily,Near-Earth Asteroid 2006 QV89 Not a Threat for Next Century,Space & Time,2019-08-13,-,https://www.sciencedaily.com/releases/2019/08/190813101956.htm,"   Observations of the near-Earth asteroid 2006 QV89 made on August 11 with the Canada-France-Hawaii Telescope (CFHT) have ruled out any potential future impact threat to Earth by this asteroid for the next century. 2006 QV89 was discovered on August 29, 2006, with a telescope in Arizona, and observations were only possible through September 8, 2006, when the asteroid became unobservable from telescopes on Earth. The orbit determined from these limited observations had significant uncertainty, and it was not possible to rule out the low probability of the asteroid impacting Earth in the future, possibly as early as 2019. Last month, observations with the European Southern Observatory's Very Large Telescope (VLT) in Chile did not find the asteroid where it would have appeared if it was on a trajectory that would impact Earth this September. This ruled out an impact in 2019, but an impact for 2020 remained a possibility, along with nearly two dozen more over the next hundred years, with eight of those in the next decade. ""There is a big difference between knowing where a hazardous asteroid isn't, and knowing where it is,"" said David Tholen, astronomer at the University of Hawai'i's Institute for Astronomy, who led the effort to recover 2006 QV89. This summer provided the first clear opportunity to recover the asteroid since its discovery, but the uncertainty in its position on the sky spanned roughly 30 degrees (60 times the diameter of the moon) in mid-July, growing even larger as the asteroid approached Earth. ""That made the use of a large telescope with a wide-field camera absolutely essential,"" noted Tholen. Only a fraction of that uncertainty region had been imaged with CFHT on July 14, but operations at the existing telescopes were suspended on July 16, due to the protest on Maunakea. ""We found at least a dozen asteroids in the July 14 data that fell close to the region where 2006 QV89 could have been, but the suspension of operations prevented us from confirming which, if any, of those objects was 2006 QV89,"" said Tholen. With access to the Maunakea telescopes blocked, Tholen enlisted the aid of Marco Micheli of the European Space Agency's NEO Coordination Centre in Frascati, Italy. Micheli is a UH graduate who led the effort to rule out the 2019 impact scenario with ESO's VLT. He pointed a telescope in Spain at the position for the best of the candidate objects, but after two hours of data collection, the object at the predicted position could not be convincingly distinguished from electronic noise in the data. It came as a great relief to learn that CFHT would resume operations last weekend. ""Our highest priority target for Saturday night was the best 2006 QV89 candidate, and despite some thin cirrus clouds and a lot of moonlight, we needed only four minutes of data to obtain proof that we had found the right object,"" said Tholen. The International Astronomical Union's Minor Planet Center announced the recovery to the world on Sunday, and the impact monitoring services at the Jet Propulsion Laboratory and the University of Pisa/SpaceDys in Italy immediately began crunching the numbers to update the impact predictions. A little over an hour later, Davide Farnocchia of Center of Near-Earth Object Studies at NASA's Jet Propulsion Laboratory in Pasadena reported that all the impact scenarios for the next century had been eliminated. ""This result is only one example of the telescopes on Maunakea protecting Earth by observing and studying the asteroids that enter Earth's neighborhood,"" said Kelly Fast, manager of the Near Earth Object Observations Program in NASA's Planetary Defense Coordination Office, which supported the observations. Much in the same way that meteorologists use weather satellite imagery to track hurricanes to determine whether they represent a hazard to people and property, astronomers use telescopes to track asteroids near Earth to determine whether they represent an impact hazard. ""A different asteroid, 2019 NX5, got away from us while the Maunakea telescopes were shuttered, which is regrettable,"" Tholen said. ""We are relieved that we were able to catch 2006 QV89 before our window closed. We are even more relieved that it won't impact the Earth."" ",Space & Time,0.11984898364550352
20,Science Daily,Finding a Cosmic Fog Within Shattered Intergalactic Pancakes,Space & Time,2019-08-13,-,https://www.sciencedaily.com/releases/2019/08/190813113452.htm,"   They learned intriguing new details about the dynamics of baryons, the collection of subatomic particles (including protons and neutrons) that accounts for much of the visible matter in the universe. Most baryons reside in the intergalactic medium (IGM), which is the space in-between galaxies where matter is neither bound to nor tugged upon by surrounding systems. In a new study, Yale postdoctoral associate Nir Mandelker and professor Frank C. van den Bosch report on the most detailed simulation ever of a large patch of the IGM. For the first time, they were able to see how cold, dense gas clouds in the IGM organize themselves and react within much larger ""sheets"" or ""pancakes"" of matter in the vastness of space. The findings appear in the Astrophysical Journal Letters. Researchers have tried for years to piece together the structures and properties of the IGM -- in part to test the standard model of Big Bang cosmology, which predicts that 80%-90% of baryons are in the IGM, but also to investigate the IGM's crucial role as the universe's fuel source. ""The reason galaxies are able to form stars continuously is because fresh gas flows into galaxies from the IGM,"" said Mandelker, lead author of the study. ""It is clear that galaxies would run out of gas in very short order if they didn't accrete fresh gas from the IGM."" Yet detecting the IGM's gas has been supremely difficult. Unlike galaxies, which shine brightly in starlight, gas in the IGM is almost never luminous enough to detect directly. Instead, it has to be studied indirectly, through the absorption of background light. Such absorption studies allow researchers to learn about the density and chemical composition of gas clouds; in particular, they're able to find out if star formation in nearby galaxies has polluted the gas with metals (elements heavier than helium). With its new simulation, the Yale team learned quite a lot -- including new properties of those aforementioned sheets of baryons. ""These are flattened distributions of matter, known as 'pancakes,' that extend across many millions of light years across,"" said van den Bosch. ""We found that rather than being smoothly distributed, the gas in these pancakes shatters into what resembles a 'cosmic fog' made up of tiny, discrete clouds of relatively cold and dense gas."" Such dense clouds of gas had been thought to form only in areas of space close to galaxies, where the gas is naturally denser. But the new simulation shows that they also can condense out of the low-density IGM. The researchers said the phenomenon occurs naturally, as the result of an instability triggered by the efficient cooling of the gas. Another aspect of this cosmic fog, based on the Yale simulation, is that it is pristine; it is too far away from any galaxy to be polluted with metals. According to Mandelker, this is significant because it explains recent, puzzling observations of dense, metal-free clouds at large distances from galaxies. Astronomers could not explain this phenomenon, but the new simulation suggests their presence may simply be the outcome of a natural process. ""Our work highlights the importance of properly resolving the properties of gas in the IGM, which is often neglected in favor of better resolving the central galaxies,"" Mandelker said. ""It has been very difficult to understand how the gas in the IGM could possibly become so dense and optically thick, especially when previous generations of cosmological simulations did not reveal any such dense gas in the IGM."" Mandelker is a Tschira Postdoctoral Fellow in a collaborative program in astrophysics operated by Yale and the Heidelberg Institute for Theoretical Studies (HITS). The principal investigators of the program are van den Bosch and Volker Springel of HITS, who is a co-author of the new study. Freeke van de Voort of the Max Planck Institute for Astrophysics also is a co-author of the study. The research was supported by the Klauss Tschira Foundation through the HITS Yale Program in Astrophysics and a grant from NASA. ",Space & Time,0.11893726868772099
21,Stanford,Making physics more inclusive,Science,2019-08-18,-,https://news.stanford.edu/2019/08/14/making-physics-inclusive/,"   It was no surprise when, in 2016, a Stanford University survey of undergraduates revealed physics as among the least diverse departments at the university – also the case for physics as an academic field nationally. But, deeper analysis of the survey responses revealed a telling and crucial difference between the answers from incoming students and those on their way to graduation.  Students in the spring break program “Physics Outreach through Inclusive Science Education” (POISE) during a trip to Monterey, where they visited the Monterey Bay Aquarium. (Image credit: Courtesy of Josie Meyer)  “Many students from all backgrounds and identities come to Stanford excited about physics, and this interest does not strongly depend on race or gender. But we lose a larger number of Black, Latinx and Native students, as well as women of all races, in the first two years of undergraduate study,” said Risa Wechsler, a professor of physics and of particle physics and astrophysics at Stanford University. “A lot of that is due to the lack of community and overall climate. People from underrepresented groups often do not feel welcome in physics classes.” For Wechsler, who is also director of the Kavli Institute for Particle Astrophysics and Cosmology, this issue is personal. Even she experienced doubts about her place in physics early in her studies and career. To address this problem head on, Wechsler and six other physics faculty members formed the Equity and Inclusion Committee, which also includes students, research staff and postdoctoral scholars. While the committee was formulating a strategic plan, students formed the group Physics Undergraduate Women and Gender Minorities at Stanford (PUWMAS) and Wechsler joined Heising-Simons Foundation’s Physics and Astronomy Leadership Council. That position, which aims to help the foundation diversify physics in the United States, provided her with grant money that she could use to fund local projects. The fact that these efforts are helmed and helped along by so many people, representing various areas of the Physics Department, reflects a larger trend in the field. “In the last five to eight years, there’s been a growing awareness about the importance of identity within the physics community,” said Lauren Tompkins, assistant professor of physics and member of the Equity and Inclusion Committee. “Your identity affects your experience as a physicist and even the physics that you do. If we can acknowledge and understand that, it makes us better physicists.” Groups, classes and a book club PUWMAS was one of the earliest results of this current push for more inclusive physics at Stanford. Undergraduates Deepti Kannan, Nicel Mohamed-Hinds and Manisha Patel founded the group with the mission to “promote diversity in physics by uniting and uplifting minority voices” and “provide opportunities for personal, academic and career development.” In service of these missions, the group organizes social events, a lunchtime speaker series, a mentorship program and professional development workshops on topics such as CV/resume writing, negotiation and interviewing.   Other inclusive efforts Physics 41E: The same as Physics 41: Mechanics, which is a required course for physics majors, but with added support. Students from underrepresented groups often don’t have the same level of preparation from high school as their majority peers. The difference in preparation is large enough that it may lead students to drop out of the major but small enough that the kind of support offered by this course can be enough to keep them in. Physics 94SI: Diverse Perspectives in Physics: A seminar course, also initiated by Meyer and Patel, where physics faculty members from diverse backgrounds share the story of their lives and careers. This course meets over lunch and includes Q&A sessions after each presentation. Rising Stars in Physics: Stanford and MIT co-sponsor this workshop that brings together top early career women in physics and astronomy who are interested in careers in academia. The two-day workshop includes research presentations, panels on issues relevant to academic careers and informal networking opportunities.  “Developing this kind of group involves risk and took incredible effort but it has paid off,” said Wechsler. “It’s only two years old and I feel like they have transformed the atmosphere for undergrads in the department.” In 2018, PUWMAS won an Office of Student Engagement Campus Impact Award. Upon their graduation this year, the founders received the department’s first-ever Departmental Service Award for their efforts. Amidst the success of PUWMAS, co-founder Patel and inaugural director of diversity and education Josie Meyer, BS ’19, brought Wechsler and Tompkins another idea for further development, focused on the fact that students who leave the department often do so between winter and spring quarter. Led by Meyer and Patel, and advised by Tompkins and Wechsler, the team created a pair of initiatives. Physics 93SI: Beyond the Laboratory: Physics, Identity and Society is a new student-taught course that explores issues of diversity and culture in physics directly, drawing directly from disciplines as varied as history, anthropology and critical race theory. Physics Outreach through Inclusive Science Education (POISE) is an optional extension of the course where students spend spring break developing an hour-long workshop for high school students while learning and applying lessons about inclusion in science. “We want the students to see that they can pursue their love of physics and still make a social impact in the world,” said Meyer. “One reason for leaving the department frequently cited by underrepresented minorities is that they feel physics is not relevant to their communities, and I hope POISE changes that.” POISE was offered for the first time in 2019, supported by Wechsler’s Heising-Simons funding. It included field trips to Bay Area museums, labs, companies and Pinnacles National Park. At the end, students taught workshops about the physics of sound and music, spacetime and the universe, and the physics of earthquakes to students at San Lorenzo High School. One unintended advantage was that all of the POISE students were in their first or last years of undergraduate study, which created natural opportunities for mentorship and access to perspectives from students who were in high school last year. “Having a science class where students were asked to reflect about their experience allowed them to bring their whole selves to their work in a way that was empowering,” said Tompkins, who is the faculty advisor for 93SI, POISE, PUWMAS and the Stanford University Physics Society. In a broader effort, the Equity and Inclusion Committee also created the Equity and Inclusion reading group, which has met twice a quarter since early 2018 and is open to anyone interested in equity in STEM. Each meeting features a speaker from the department talking about themes related to the latest reading. Just the beginning Now, Meyer, Tompkins and Wechsler are assessing how the first year of POISE went and what it might look like in 2021. They have also been interested in efforts centered on incoming potential physics students. Ideally, students would know right away about the community the department is building and improving – and reach out to the department about what more they might do.  POISE students at San Lorenzo High School, where they presented their teaching projects. (Image credit: Courtesy of Josie Meyer)  “We want to have the best people in the world involved in this research, but also I really believe that it is a human right to understand how the universe came to be,” said Wechsler. “So, we need to think very deliberately about having a scientific community that includes the whole world’s population. And we’re not there.” Meyer graduated in June but stayed over the summer as the department’s Equity and Inclusion intern. Some of her work so far has included developing training for future POISE leaders, constructing a “Tools for Practical Allyship” workshop series and developing a midterm assessment for progress on the Equity and Inclusion Strategic Plan. “Students should feel empowered to create positive change within the Stanford physics department and for the rest of their careers,” she said. “Over the past few years, undergraduate students have led the way toward a more inclusive climate in the physics community and I hope we can inspire the next generation of leaders to do the same.” To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Others,0.1182369351555216
22,Science Daily,Better Food and Faster Analysis of Blood Tests,Computers & Math,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815113740.htm,"   ""The new interpretive method of gas chromatographic analysis can make this type of analysis accessible to many more, which means that better and cheaper decisions can be made in a number of areas in society,"" says Professor Rasmus Bro, Department of Food Science at the University of Copenhagen (UCPH FOOD), who is one of the researchers behind the new interpretive method. The method can also be used to measure flavour and aroma in gastronomy and to examine blood samples taken in the hospital. ""Gas chromatography is one of the most widely used analytical methods and it provides a chemical profile that can reveal thousands of things. The analysis shows most of the chemical components in a sample of biological material in a particular pattern that can then be interpreted in terms of the specific things you want to examine. You could say that you take a chemical ""fingerprint"" of the material,"" says Rasmus Bro. Gas chromatography is generally important when talking about safety and improving the quality of global food production. And monitoring by means of measurements and artificial intelligence is one of the topics at a new major food conference in Copenhagen later this month, Food Day 2019, under the heading ""The role of online monitoring and artificial intelligence in sustainable food production."" Use of gas chromatographic analysis in the food industry It is quite expensive to interpret the analyses, as it requires highly specialised workforce. ""There is a great deal of manual work behind the interpretation of many gas chromatographic analyses and in some cases it takes several weeks to get the results from the measurements. With this research, we show that some of the most time consuming tasks can be done automatically by a computer,"" says PhD student Anne Bech Risum, who is also one of the researchers behind the method of interpretation. The computer can make a number of decisions that usually require a chemist. ""The interpretation consumes a great deal of work because you work through the data bit by bit. A computer can do this much more efficiently and reproducibly,"" explains Anne Bech Risum. Gas chromatography using mass spectrometry (GC-MS) is widely used in the food industry. For example, all large companies that work with fermentation will use this method of analysis to measure how the microorganisms in a fermentation develop and affect the final product. ""If you, for example, produce a cheese, the taste and aroma develop differently depending on the microbiological culture you add and how you treat the cheese during production. Gas chromatography can be used to measure the chemical elements that together form the aroma profile of the cheese. So if you, for example, want a more fruity or nutty aroma, you could try to change the production and then measure whether you have formed more of the chemicals behind the desired flavour profile,"" says Anne Bech Risum. The method of interpretation could also help give smaller food companies access to highly advanced analytical methods that can help companies with product optimisation, quality assurance and raw material identification. ",Matter & Energy,0.11786678782308223
23,Science Daily,Data Assimilation Method Offers Improved Hurricane Forecasting,Computers & Math,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815113737.htm,"   Now a new approach developed at Penn State's Center for Advanced Data Assimilation and Predictability Techniques can forecast the intensity and trajectory of Hurricane Harvey, according to researchers at Penn State and the National Oceanographic and Atmospheric Administration. The approach used data from the GOES-16 satellite, coupled with Penn State's all-sky radiance method, which more accurately modeled Hurricane Harvey. The data is called ""all-sky"" because it captures data in all weather conditions, including clouds and rain. The work, led by Fuqing Zhang, distinguished professor of meteorology and atmospheric science at Penn State, now deceased, is the first time GOES-16 satellite data was used to forecast hurricanes. Hurricane Harvey was the first major hurricane captured by GOES-16, which became fully operational in 2017. Zhang died in July not long after being diagnosed with cancer. When he discussed the research in June, Zhang said, ""This is still experimental. We have demonstrated that we can improve the track, position, intensity and structure of this particular event. We still need to study all other hurricane events with new satellite data but this gives us a lot of promises for the future of hurricane forecasting."" Zhang added that this study, published in the Bulletin of the American Meteorological Society, suggested that all-sky radiance data could greatly benefit hurricane forecasting in general. In this proof-of-concept study, researchers used hindcasting -- using data collected during the event, but analyzing it afterwards. This allowed researchers to hone in on the most telling data and further refine the model. The process for creating operational-ready models often takes several years. It begins with hindcasted models before those models are tested alongside existing models to see if any improvements occurred. Because weather forecasting saves lives, the models undergo strict procedures and testing before implementation. The all-sky radiance approach was paired with a model developed at the National Center for Atmospheric Research with help from members of Penn State's Department of Meteorology and Atmospheric Science. When running the model for a 24-hour period, researchers found assimilating all-sky radiance data better reproduced cloud intensity and patterns when contrasted with the current model. That led to more accurate forecasting in both the eye of the storm and peripherals. Research shows common inaccuracies in forecasting hurricane intensity and structure days in advance primarily come from poor hurricane vortex generation. Better predicting the eyewall and secondary circulations of a storm could lead to more accurate hurricane prediction, Zhang said. ""We will continue to test our satellite data assimilation system with more hurricanes to see if this method works well with other severe weather events,"" said Xingchao Chen, an assistant research professor at Penn State who was involved in this research. ""In addition to all-sky infrared radiances, we are beginning to look at microwave radiances, which effectively penetrate cloudy regions."" When researchers contrasted images created using models with and without all-sky radiance data included, it not only showed a stark improvement over operational models, it created images nearly identical to actual satellite images during the storm. The operational models that failed to predict the rapid intensification of Harvey included the National Oceanic and Atmospheric Administration's (NOAA) regional-scale Hurricane Weather Research and Forecasting Model, NOAA's Global Forecast System, and the integrated forecast system by the European Center for Medium Range Weather Forecasts. ""That's the beauty of assimilating the GOES-16 satellite,"" Zhang said. ""It looks almost identical to the actual observation. The use of all-sky radiance doesn't just improve existing models. It makes a huge difference."" Masashi Minamide, Robert G. Nystrom and Xingchao Chen, all of Penn State, and Shian-Jiann Lin and Lucas M. Harris, of NOAA, contributed to this research. This research was funded by NASA, the National Science Foundation and NOAA. ",Space & Time,0.11683857529554283
24,Science Daily,Pores for Thought: Ion Channel Study Beckons First Whole-Brain Simulation,Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815081303.htm,"   For the first time, researchers at the EPFL's Blue Brain Project have mapped the behavior of the largest family of these voltage-gated ion channels: Kv channels. Published in Frontiers in Cellular Neuroscience, and freely available online as raw data, their pioneering work will power virtual drug discovery -- and, they hope, the first whole-brain simulation. Data for a digital brain Thousands of studies have probed the behavior of Kv channels, in various cells, by measuring the movement of ions across tiny patches of cell membrane at controlled voltages. In the early years of the Blue Brain Project, neuroscientist Dr. Rajnish Ranjan was tasked with modelling the behavior of Kv channels, based on these studies, for use in its brain simulations. ""To my surprise, despite 30 years of research none of the raw data was available,"" recalls Ranjan, lead author of the new study. Even the published, processed data reflected a Wild West of study protocols, with inconsistent and incompatible results. Many Kv channels were hardly or never studied, and crucially, across the board there was a lack of studies near body temperature. ""Near body temperature, fatty cell membranes soften and slip away from recording apparatus. So, virtually all studies were performed at room temperature,"" Ranjan explains. No data -- no channel models -- no brain simulation. If the Blue Brain team were to succeed, they would need to record their own Kv channel data -- thirty years' worth of it. Bringing ion channel research in from the cold Fortunately, the Blue Brain team includes an ion channel-recording robot. By automating recordings, this allowed them to overcome the high failure rate near body temperature with sheer speed and volume of attempts. The result is the first ever map of the behavior of all Kv channels -- or any family of ion channels, for that matter. The map by turns reconciles, reinforces and refutes the last thirty years of Kv channel studies. ""Under standardized conditions and with large sample sizes, the behavior of Kv channels is largely consistent across cell lines and species. And as expected, quantitatively, Kv channels activate and inactivate faster at 35°C than at 25°C or 15°C. ""The big new finding was that Kv channels behave qualitatively very differently from 15°C, to 25°C to 35°C."" For instance, some channels inactivate only at higher temperature, so were previously wrongly taken as non-inactivating despite thousands of studies at room temperature. Others display a new type of delayed inactivation. Even more surprising, some channels change their behavior seemingly at random -- which though unexplained, could underlie some of the inconsistencies between previous studies. ""The qualitative differences we found at 35°C clearly demand further tests,"" says Ranjan. In particular, the team's method should be applied to systematically study Kv channel behavior in the presence of known modulators -- like protein signals, chaperones and anchors -- or genetic channel variants. Beyond this, work is already underway at Blue Brain to map the behavior of other voltage-gated ion channels, including ones that allow Na+, or any positive ion to cross the cell membrane. Ultimately, they will require all of these to build a digital copy of the brain. Channelpedia The importance of these findings is more immediate than Blue Brain's ambitious whole-brain simulations, however. The study data will enable researchers everywhere to develop their own improved models for Kv channels -- useful, for example, in drug discovery. ""The data and methods we have established to map the behavior of Kv channels can be used to systematically screen drug candidates for potentially positive or deleterious effects on channel behavior,"" suggests Ranjan. To this end, the study team has provided open access to their million-plus Kv channel recordings from over 9,000 cells, and a growing dataset for other channels. These are publicly available for download in a dedicated, wiki-like platform called Channelpedia. ""Anyone can access Channelpedia online and contribute,"" Ranjan emphasizes. ""We encourage other labs to share their ion channel data to expand and refine this resource."" The models the team provides, based on its systematic data collections, make it possible to interpret any channel recording in the context of physiological temperature. ""The temperature-activity relationships established in our models allow recordings at any temperature to be converted to physiological temperature,"" explains Ranjan. ""This is a really key contribution of our work, since channel recordings at physiological temperature remain difficult and future experiments will likely continue to be performed at lower temperatures."" ",Electronics and Technology,0.11616604340008606
25,Science Daily,A Novel Cellular Process to Engulf Nano-Sized Materials,Matter & Energy,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816123239.htm,"   Traditional methods include linking them to short fragments of proteins called peptides, which are structural components of cells and tissues, hormones, toxins, antibiotics and enzymes. These peptides, by interacting with cells, will lead nanomaterial into cells. The impact of these interactions on other cellular activities remains poorly understood, plus this peptide coupling introduces additional complexity in nanomaterial manufacturing, and may change their functionality as well. In a study published in Nature Communications, University of Minnesota researchers discovered a novel cellular process that can engulf nanomaterial without direct peptide functionalization, and its activity is regulated by Cysteine surrounding the cells. The research team termed this cellular process of engulfing bystander NPs as 'bystander uptake.' ""By simply mixing two types of nano-sized material, we discover a novel cellular process that offers an easy solution for nanomaterial entry into cells,"" said Hongbo Pang, corresponding author, an assistant professor in the College of Pharmacy and a member of the Masonic Cancer Center. ""Moreover, it opens up a new avenue of cell biology that interconnects several fundamental elements of living cells. Further understanding of this process will aid in both cell biology and nanotechnology development."" The study revealed the following unique properties: the bystander uptake only allows the cells to engulf nano-sized materials, but not other substances surrounding the cells (e.g. fluids); the activity of this bystander uptake is stimulated by the existence of one of 20 natural amino acids, Cysteine, surrounding the cells.These phenomena have been validated with a wide variety of cells, nanoparticles (aka nanomaterials), and under various physiological conditions. The study findings included: co-administration with TAT-NP, a peptide and nanomaterial fusion, enables cells to engulf nano-sized materials in a bystander manner; this bystander uptake is specific to nanomaterial, but not other substances surrounding the cells; cysteine in the cell culture medium greatly stimulates the activity of this bystander uptake. ",Health,0.1069107918847305
26,Science Daily,'Invisible Ink' on Antique Nile Papyrus Revealed by Multiple Methods,Matter & Energy,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816092420.htm,"   For more than a century, numerous metal crates and cardboard boxes have sat in storage at the Egyptian Museum and Papyrus Collection Berlin, all of which were excavated by Otto Rubensohn from 1906 to 1908 from an island called Elephantine on the River Nile in the south of Egypt, near the city of Aswan. Eighty percent of the texts on the papyrus in these containers have yet to be studied, and this can hardly be done using conventional methods anymore. Thousands of years ago, the Egyptians would carefully roll up or fold together letters, contracts and amulets to a tiny size so that they would take up the least possible space. In order to read them, the papyri would have to be just as carefully unfolded again. ""Today, however, much of this papyrus has aged considerably, so the valuable texts can easily crumble if we try to unfold or unroll them,"" Prof. Dr. Heinz-Eberhard Mahnke of Helmholtz-Zentrum Berlin and Freie Universität Berlin describes the greatest obstacle facing the Egyptologists, who are eager to unearth the scientific treasures waiting in the boxes and crates in the Berlin Egyptian Museum. Testing the fragile papyrus with nondestructive methods The physicist at Helmholtz-Zentrum Berlin knew from many years of research how to analyse the fragile papyrus without destroying it: shining a beam of X-ray light on the specimen causes the atoms in the papyrus to become excited and send back X-rays of their own, much like an echo. Because the respective elements exhibit different X-ray fluorescence behaviour, the researchers can distinguish the atoms in the sample by the energy of the radiation they return. The scientists therefore long ago developed laboratory equipment that uses this X-ray fluorescence to analyse sensitive specimens without destroying them. Scholars in ancient Egypt typically wrote with a black soot ink made from charred pieces of wood or bone and which consisted mainly of elemental carbon. ""For certain purposes, however, the ancient Egyptians also used coloured inks containing elements such as iron, copper, mercury or lead,"" Heinz-Eberhard Mahnke explains. If the ancient Egyptian scribes had used such a ""metal ink"" to inscribe the part that now appears blank on the Elephantine papyrus, then X-ray fluorescence should be able to reveal traces of those metals. Indeed, using the equipment in their laboratory, the researchers were able to detect lead in the blank patch of papyrus. Revealing sharper details at BESSY II with ""absorption edge radiography"" In fact, they even managed to discern characters, albeit as a blurry image. To capture a much sharper image, they studied it with X-ray radiography at BESSY II, where the synchrotron radiation illuminates the specimen with many X-ray photons of high coherence. Using ""absorption edge radiography"" at the BAMline station of BESSY II, they were able to increase the brightness of this technique for the sample studied, and thus better distinguish the characters written on the papyrus from the structure of the ancient paper. So far, it has not been possible to translate the character, but it could conceivably depict a deity. Composition of the invisible ink resolved in the Rathgen laboratory The analysis at BESSY II did not identify the kind of leaded ink the ancient scribes used to write these characters on the papyrus. Only by using a ""Fourier-transform infrared spectrometer"" could the scientists of the Rathgen Research Laboratory Berlin finally identify the substance as lead carboxylate, which is in fact colourless. But why would the ancient scribe have wanted to write on the papyrus with this kind of ""invisible ink""? ""We suspect the characters may originally have been written in bright minium (red lead) or perhaps coal-black galena (lead glance),"" says Heinz-Eberhard Mahnke, summarising the researchers' deliberations. If such inks are exposed to sunlight for too long, the energy of the light can trigger chemical reactions that alter the colours. Even many modern dyes similarly fade over time in the bright sunlight. It is therefore easily conceivable that, over thousands of years, the bright red minium or jet black galena would transform into the invisible lead carboxylate, only to mystify researchers as a conspicuously blank space on the papyrus fragment. Method developed to study folded papyri without contact With their investigation, Dr. Tobias Arlt of Technische Universität Berlin, Prof. Dr. Heinz-Eberhard Mahnke and their colleagues have pushed the door wide open for future studies to decipher texts even on finely folded or rolled papyri from the Egyptian Museum without having to unfold them and risk destroying the precious finds. The researchers namely developed a new technique for virtually opening the valuable papyri on the computer without ever touching them. The Elephantine project funded by the European Research Council, ERC, and headed by Prof. Dr. Verena Lepper (Stiftung Preußischer Kulturbesitz-Staatliche Museen zu Berlin) is thus well on its way to studying many more of the hidden treasures in the collection of papyrus in Berlin and other parts of the world, and thus to learning more about Ancient Egypt. ",Matter & Energy,0.09739380626659022
27,Science Daily,AI Used to Test Evolution's Oldest Mathematical Model,Matter & Energy,2019-08-14,-,https://www.sciencedaily.com/releases/2019/08/190814140500.htm,"   The researchers, from the University of Cambridge, the University of Essex, the Tokyo Institute of Technology and the Natural History Museum London used their machine learning algorithm to test whether butterfly species can co-evolve similar wing patterns for mutual benefit. This phenomenon, known as Müllerian mimicry, is considered evolutionary biology's oldest mathematical model and was put forward less than two decades after Darwin's theory of evolution by natural selection. The algorithm was trained to quantify variation between different subspecies of Heliconius butterflies, from subtle differences in the size, shape, number, position and colour of wing pattern features, to broad differences in major pattern groups. This is the first fully automated, objective method to successfully measure overall visual similarity, which by extension can be used to test how species use wing pattern evolution as a means of protection. The results are reported in the journal Science Advances. The researchers found that different butterfly species act both as model and as mimic, 'borrowing' features from each other and even generating new patterns. ""We can now apply AI in new fields to make discoveries which simply weren't possible before,"" said lead author Dr Jennifer Hoyal Cuthill from Cambridge's Department of Earth Sciences. ""We wanted to test Müller's theory in the real world: did these species converge on each other's wing patterns and if so how much? We haven't been able to test mimicry across this evolutionary system before because of the difficulty in quantifying how similar two butterflies are."" Müllerian mimicry theory is named after German naturalist Fritz Müller, who first proposed the concept in 1878, less than two decades after Charles Darwin published On the Origin of Species in 1859. Müller's theory proposed that species mimic each other for mutual benefit. This is also an important case study for the phenomenon of evolutionary convergence, in which the same features evolve again and again in different species. For example, Müller's theory predicts that two equally bad-tasting or toxic butterfly populations in the same location will come to resemble each other because both will benefit by 'sharing' the loss of some individuals to predators learning how bad they taste. This provides protection through cooperation and mutualism. It contrasts with Batesian mimicry, which proposes that harmless species mimic harmful ones to protect themselves. Heliconius butterflies are well-known mimics, and are considered a classic example of Müllerian mimicry. They are widespread across tropical and sub-tropical areas in the Americas. There are more than 30 different recognisable pattern types within the two species that the study focused on, and each pattern type contains a pair of mimic subspecies. However, since previous studies of wing patterns had to be done manually, it hadn't been possible to do large-scale or in-depth analysis of how these butterflies are mimicking each other. ""Machine learning is allowing us to enter a new phenomic age, in which we are able to analyse biological phenotypes -- what species actually look like -- at a scale comparable to genomic data,"" said Hoyal Cuthill, who also holds positions at the Tokyo Institute of Technology and University of Essex. The researchers used more than 2,400 photographs of Heliconius butterflies from the collections of the Natural History Museum, representing 38 subspecies, to train their algorithm, called 'ButterflyNet'. ButterflyNet was trained to classify the photographs, first by subspecies, and then to quantify similarity between the various wing patterns and colours. It plotted the different images in a multidimensional space, with more similar butterflies closer together and less similar butterflies further apart. ""We found that these butterfly species borrow from each other, which validates Müller's hypothesis of mutual co-evolution,"" said Hoyal Cuthill. ""In fact, the convergence is so strong that mimics from different species are more similar than members of the same species."" The researchers also found that Müllerian mimicry can generate entirely new patterns by combining features from different lineages. ""Intuitively, you would expect that there would be fewer wing patterns where species are mimicking each other, but we see exactly the opposite, which has been an evolutionary mystery,"" said Hoyal Cuthill. ""Our analysis has shown that mutual co-evolution can actually increase the diversity of patterns that we see, explaining how evolutionary convergence can create new pattern feature combinations and add to biological diversity. ""By harnessing AI, we discovered a new mechanism by which mimicry can produce evolutionary novelty. Counterintuitively, mimicry itself can generate new patterns through the exchange of features between species which mimic each other. Thanks to AI, we are now able to quantify the remarkable diversity of life to make new scientific discoveries like this: it might open up whole new avenues of research in the natural world."" ",Matter & Energy,0.09642033519957977
28,Science Daily,Warmer Winters Are Changing the Makeup of Water in Black Sea,Environment,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815120650.htm,"   A new study published in AGU's Journal of Geophysical Research: Oceans analyzing water temperatures, density and salinity in the Black Sea from 2005 to 2019 finds warming winter weather is warming the middle water layer of the Black Sea, known as the cold intermediate layer, which exists between the oxygen-free bottom layer of the sea and the oxygenated top layer of water. This warming is causing the cold intermediate layer to mix with the other two layers of water, according to the new research. This intermediate layer has fluctuated in the past, but in the last 14 years its core temperature has warmed 0.7 degrees Celsius (1.26 degrees Fahrenheit). The blending of the cold intermediate layer with the other layers of water could enable the water masses from the deeper layers of the sea to eventually infiltrate the top layer, which would have unknown impacts on the sea's marine life. The new study suggests climate change is causing the intermediate layer to warm and change, but natural fluctuations could also be playing a role, according to the study's authors. Studying changes in smaller water bodies like the Black Sea shows scientists how larger bodies of water might evolve in the future. The new study suggests what might happen to Earth's oceans as the climate continues to warm, according to the researchers. Water masses, which exist in bodies of water around the globe, influence Earth's climate and move nutrients around the world. Changes in oceanic masses' composition could reshape global currents, affecting the planet's climate and ecosystems. It is difficult to study massive water masses in the oceans so scientists use regional water masses like those in the Black Sea to determine how climate change could be affecting oceanic masses. ""We want to at least know what could happen under different global climate change scenarios,"" said Emil Stanev, a physical oceanographer at Helmholtz-Zentrum Geesthacht Center for Materials and Coastal Research in Geesthacht, Germany, and lead author of the new study. Changing water masses The Black Sea lies between the Balkans and Eastern Europe and receives water from many major European rivers. It also receives and loses water through the Bosporus Strait, which links the Black Sea to the Mediterranean. The Black Sea's water stratification comes from the mixing of different water sources, creating water masses within the sea. Water masses have distinct temperatures, salinities, and densities, usually identified by their horizontal and vertical positions in bodies of water. The Black Sea's cold intermediate water mass's depth varies depending on its distance from the shore. The mass separates low-saline surface water from the high-saline bottom water. Each of the sea's water layers hosts specific organisms suited to its oceanographic conditions. Scientists previously studied the Black Sea's cold intermediate layer, but they had not analyzed how it has changed over time. ""They didn't pay too much attention to the evolution of the water masses,"" Stanev said. In the new study, Stanev and his colleagues charted the evolution of the Black Sea's cold intermediate water mass for 14 years, comparing its progression with the region's climate trends. They used battery-powered floats to measure the temperature, density and salinity from the sea surface down to 1000 meters (3281 feet) at various points throughout the seasons. They then compared the float data to surface air temperatures to see if there was a correlation between warmer winters and changes in the cold intermediate water mass's temperature and salinity. They found winter weather fluctuations changed the temperature and salinity of the cold intermediate layer, but the density of the water mass remained almost the same. The Black Sea's cold intermediate layer became warmer, allowing its edges to blend with the top and bottom layers of the sea. If this trend continues, it could potentially change the stratification of the sea, according to the study's authors. Restructuring the layers could bring sulfides, corrosive and noxious chemicals at the bottom of the sea, up to the surface, impacting marine wildlife and tourism. Climate change might be warming the sea, but natural variability could also be responsible, according to James Murray, a chemical oceanographer at the University of Washington, who was not connected with the new study. Past research both by Murray and other scientists has shown the Black Sea's water layers have cycled through warm and cool periods since the 1950s. However, the Black Sea's cold intermediate layer has never been this warm, Murray added. Both Stanev and Murray agree that more research on the evolution of the Black Sea's layers is necessary. Continuing to study the Black Sea's cold intermediate layer and its fluctuations will indicate whether climate change is behind the layer's gradual disappearance. ",Environment,0.09402488630440856
29,Science Daily,Composition of Fossil Insect Eyes Surprises Researchers,Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815101550.htm,"   ""We were surprised by what we found because we were not looking for, or expecting it,"" says Johan Lindgren, an Associate Professor at the Department of Geology, Lund University, and lead author of the study published this week in the journal Nature. The researchers went on to examine the eyes of living crane-flies, and found additional evidence for eumelanin in the modern species as well. By comparing the fossilized eyes with optic tissues from living crane-flies, the researchers were able to look closer at how the fossilization process has affected the conservation of compound eyes across geological time. The fossilized eyes further possessed calcified ommatidial lenses, and Johan Lindgren believes that this mineral has replaced the original chitinous material. This, in turn, led the researchers to conclude that another widely held hypothesis may need to be reconsidered. Previous research has suggested that trilobites -- an exceedingly well-known group of extinct seagoing arthropods -- had mineralized lenses in life. ""The general view has been that trilobites had lenses made from single calcium carbonate crystals. However, they were probably much more similar to modern arthropods in that their eyes were primarily organic,"" says Johan Lindgren. Compound eyes are found in arthropods, such as insects and crustaceans, and are the most common visual organ seen in the animal kingdom. They are made up of multiple tiny and light-sensitive ommatidia, and the perceived image is a combination of inputs from these individual units. ",Matter & Energy,0.093848279002463
30,Science Daily,How Media Around the World Frame Climate Change News,Science,2019-08-13,-,https://www.sciencedaily.com/releases/2019/08/190813130431.htm,"   While richer countries tend to frame climate change coverage as a political issue, poorer countries more often frame it as an international issue that the world at large needs to address. ""Media can tell people what to think about. At the same time, framing can have an effect on how people think about certain issues,"" said Hong Vu, assistant professor of journalism at KU and the study's lead author. ""Not only can framing have an impact on how an issue is perceived but on whether and how policy is made on the issue. With big data, machine-learning techniques, we were able to analyze a large amount of media climate change coverage from 45 countries and territories from 2011 to 2015."" Vu and co-authors Yuchen Liu, graduate student at KU; and Duc Vinh Tran of Hanoi University of Science and Technology published their findings in the journal Global Environmental Change. They analyzed over 37,000 articles and considered national factors such as economic development, weather and energy consumption. They reviewed headlines from nationally circulated publications of varying political ideologies that contained the keywords ""greenhouse gas,"" ""climate change"" and/or ""global warming,"" or the local language equivalent. The most consistent predictor of how the issue was framed was a nation's gross domestic product per capita. ""We showed that the issue is more politicized in richer countries. In poorer countries, it was framed more as an international issue,"" Vu said. ""Which makes sense, as poorer countries don't have the resources that richer countries do to fight it."" Even when richer countries framed the issue as one they could address with their more plentiful resources, it was often also framed as a political issue and would focus on debate or argument about political approaches as opposed to proposing policy solutions. Media from richer countries also focused more on the science of climate change. When climate change was framed as an economic issue, it was in countries that had the most severe climates and those that have experienced the most adverse consequences of climate change and natural disasters, loss of life and property, and economic effects. In terms of social progress framing, richer countries framed the issue in terms of energy policy and use. Those that emit the most carbon dioxide framed content in terms of energy issues, while poorer countries and those that had experienced the most severe climates focused more on natural impact. The study also used independent nation-level variables from several databases, including the World Bank, the Center for Research on the Epidemiology of Disasters, the Global Carbon Atlas Project and Freedom House, all nongovernment organizations working in development or on climate change. The authors argue that the international relations frame being the most widely used reflects the fact that climate change is a problem every nation needs to address. Economic effects being second most popular reflects that fighting climate change will have impacts on every economy and that when natural disasters and climate change were discussed, they were nearly always brought forth in an economic sense. They also contend that richer countries framing the issue as political reflects that climate change skeptics in those nations gaining more media prominence and the efforts of multiple groups trying to politicize the issue, influence media agendas and policymaking. The study helps add to the understanding of media influence on climate change coverage, Vu said. Future work will address questions of framing the topic, if it's done on local, national or global levels, if communicators suggest solutions, if such solutions are attributed to individuals, businesses or governments and efficacy of proposed solutions. Three decades of communications on the topic show there is not a sense of immediacy in covering the problem and influencing policy. ""As communications researchers we want to know why, if climate change entered public discussion more than 30 years ago and we've been covering it as a global problem since, why can't we slow the warming climate down,"" Vu said. ""If we want the public to have better awareness of climate change, we need to have media imparting it in an immediate sense. By looking at how they have portrayed it, we can better understand how to improve it, and hopefully make it a priority that is reflected in policy."" ",Environment,0.09342587497269667
31,Science Daily,"Oil and Water: Better Monitoring Needed to Secure Vital Groundwater Supplies, Researchers Say",Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815180627.htm,"   ""There's a critical need for long-term -- years to decades -- monitoring for potential contamination of drinking water resources not only from fracking, but also from conventional oil and gas production,"" McIntosh said. Fracking, also known as high-volume hydraulic fracturing (HVHF), is the practice of injecting liquid under high pressure into petroleum-bearing rock formations to produce fractures or cracks that allow recovery of oil and natural gas. But it's a smaller player in the underground water scene. ""The amount of water injected and produced for conventional oil and gas production exceeds that associated with fracking and unconventional production by well over a factor of ten,"" McIntosh said. While groundwater use varies by region in both countries, about 30 per cent of Canadians and more than 45 per cent of Americans depend on the resource for their municipal, domestic, and agricultural needs. In more arid regions of Canada and the United States , surface freshwater supplies are similarly important. The researchers' work appears in an issue paper published in the journal Groundwater. It looks at the relatively new practice of fracking alongside established activities such as enhanced oil recovery (EOR) and saltwater disposal that have been around almost as long as the oil industry itself. As its name implies, EOR involves injecting water into petroleum-bearing rock formations to ""push"" the oil and gas toward extraction wells to enhance production. Saltwater is produced as a by-product of petroleum production, and while it is used for EOR, any excess must be disposed of, typically by injecting it into depleted formations. There are regulations governing the petroleum industry with regard to groundwater, but information about what is happening underground varies by province and state. Some jurisdictions keep excellent data while for others it's virtually nonexistent. Despite this, Ferguson said they can make some observations. ""I think the general conclusions about water use and potential for contamination are correct, but the details are fuzzy in some areas,"" said Ferguson, a researcher with USask's Global Institute for Water Security. ""Alberta probably has better records than most areas and the Alberta Energy Regulator has produced similar numbers to ours for that region. We saw similar trends for other oil and gas producing regions but we need better reporting, record keeping and monitoring."" Both fracking and conventional practices can affect groundwater and surface water used to make up volumes when there isn't enough from other sources to continue petroleum production. McIntosh and Ferguson looked at how much water was and is being injected underground by petroleum industry activities, how they change pressures and water movement underground, and how these practices could contaminate groundwater supplies. They also compared fracking with the older practices. What they found is that there is likely more water now in the petroleum-bearing formations than there was to begin with due to EOR, and this has changed the behaviour of liquids underground. The concern is that this increases the likelihood of contaminated water making its way into aquifers -- the freshwater-bearing formations on which so many communities depend. Potential effects can be felt far from petroleum-producing regions. For example, previous studies show that operation of disposal wells can cause seismic activity detectable from more than 90 kilometres away. Conventional activities, while they use lower pressures, take place over longer periods of time and may allow contamination over greater distances. Another wild card is that there are thousands of active, dormant, and even abandoned wells across North America. Some of these are leaky or improperly decommissioned, providing possible pathways for contamination from petroleum-producing formations into freshwater aquifers. While there is some effort to deal with this problem through organizations such as Alberta's Orphan Well Association, there is little consensus as to the size of the problem. Ferguson said depending which source is cited -- highly speculative based on per-well decommissioning costs -- the price tag ranges from a few billion to a few hundred billion dollars. ""We haven't done enough site investigations and monitoring of groundwater to know what the liability really looks like,"" he said. ""My guess is that some wells probably should be left as is and others are going to need more work to address migration of brines and hydrocarbons from leaks that are decades old."" ",Environment,0.09305027389602391
32,Science Daily,Discovery of a Bottleneck Relief in Photosynthesis May Have a Major Impact on Food Crops,Environment,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816075543.htm,"   ""We tested the effect of increasing the production of the Rieske FeS protein, and found it increases photosynthesis by 10 percent,"" said lead researcher Dr Maria Ermakova from the ARC Centre of Excellence for Translational Photosynthesis (CoETP). ""The Rieske FeS protein belongs to a complex which is like a hose through which electrons flow, so the energy can be used by the carbon engine of the plant. By overexpressing this protein, we have discovered how to release the pressure of the hose, so more electrons can flow, accelerating the photosynthetic process,"" said Dr Ermakova, who works at The Australian National University (ANU) Centre Node. Dr Ermakova, the lead author of the paper published this week in the journal Communications Biology, said that this is the first time that scientists have generated more of the Rieske FeS protein inside plants that use the C4 photosynthesis pathway. Until now, the majority of efforts to improve photosynthesis have been done in species that use C3 photosynthesis, such as wheat and rice, but not a lot has been done in enhancing C4 photosynthesis. This is despite the fact that C4 crop species -- like maize and sorghum -- play a key role in world agriculture, and are already some of the most productive crops in the world. ""These results demonstrate that changing the rate of electron transport enhances photosynthesis in the C4 model species, Setaria viridis, a close relative of maize and sorghum. It is an important proof of concept that helps us enormously to understand more about how C4 photosynthesis works,"" said CoETP's Deputy Director Professor Susanne von Caemmerer, one of the co-authors of this study. The Rieske protein is particularly important in environments with high radiance, where C4 plants grow. Previous research has shown that overexpressing the Rieske protein in C3 plants improves photosynthesis, but more research was needed in C4 plants. ""It is really exciting, as we are now ready to transform this into sorghum and test the effect it has on biomass in a food crop,"" Professor von Caemmerer says. The research is the result of an international collaboration with researchers from the University of Essex in the UK, who are part of the Realizing Increased Photosynthetic Efficiency (RIPE) project. ""This is a great example that we need international collaborations to solve the complex challenges faced in trying to improve crop production,"" said University of Essex researcher Patricia Lopez-Calcagno, who was involved in producing some of the essential genetic components for the plant transformation. ""In the last 30 years, we have learnt a lot about how C4 plants work by making them worse -- by breaking them as part of the process of discovery. However, this is the first example in which we have actually improved the plants,"" says Professor Robert Furbank, Director of the ARC Centre of Excellence for Translational Photosynthesis and one of the authors of the study. ""Our next steps are to assemble the whole protein FeS complex, which has many other components. There is a lot more to do and lots of things about this protein complex we still don't understand. We have reached 10 percent enhancement by overexpressing the Rieske FeS component, but we know we can do better than that,"" says Professor Furbank. This research has been funded by the ARC Centre of Excellence for Translational Photosynthesis, which aims to improve the process of photosynthesis to increase the production of major food crops such as sorghum, wheat and rice. ",Environment,0.09178049248889929
33,Science Daily,Early Species Developed Much Faster Than Previously Thought,Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815180633.htm,"   She and fellow researchers have narrowed in a specific time during an era known as the Ordovician Radiation, showing that new species actually developed rapidly during a much shorter time frame than previously thought. The Great Biodiversification Event where many new species developed, they argue, happened during the Darriwilian Stage about 465 million years ago. Their research, ""Coordinated biotic and abiotic change during the Great Ordovician Biodiversification Event: Darriwilian assembly of early Paleozoic building blocks,"" was published in Palaeogeography, Palaeoclimatology, Palaeoecology as part of a special issue they are editing on the Great Ordovician Biodiversification Event. New datasets have allowed them to show that what previously looked like species development widespread over time and geography was actually a diversification pulse. Picture a world before the continents as we know them, when most of the land mass was south of the equator, with only small continents and islands in the vast oceans above the tropics. Then picture ice caps forming over the southern pole. As the ice caps form, the ocean recedes and local, isolated environments form around islands and in seas perched atop continents. In those shallow marine environments, new species develop. Then picture the ice caps melting and the oceans rising again, with those new species riding the waves of global diversification to populate new regions. The cycle then repeats producing waves of new species and new dispersals. Lighting the Spark of Diversification The early evolution of animal life on Earth is a complex and fascinating subject. The Cambrian Explosion (between about 540 to 510 million years ago) produced a stunning array of body plans, but very few separate species of each, notes Stigall. But nearly 40 million years later, during the Ordovician Period, this situation changed, with a rapid radiation of species and genera during the Great Ordovician Biodiversification Event. The triggers of the GOBE and processes that promoted diversification have been subject to much debate, but most geoscientists haven't fully considered how changes like global cooling or increased oxygenation would foster increased diversification. A recent review paper by Stigall and an international team of collaborators attempts to provide clarity on these issues. For this study, Stigall teamed up with Cole Edwards (Appalachian State University), a sedimentary geochemist, and fellow paleontologists Christian Mac Ørum Rasmussen (University of Copenhagen) and Rebecca Freeman (University of Kentucky) to analyze how changes to the physical earth system during the Ordovician could have promoted this rapid increase in diversity. In their paper, Stigall and colleagues demonstrate that the main pulse of diversification during the GOBE is temporally restricted and occurred in the Middle Ordovician Darriwilian Stage (about 465 million years ago). Many changes to the physical earth system, including oceanic cooling, increased nutrient availability, and increased atmospheric oxygen accumulate in the interval leading up to the Darriwilian. These physical changes were necessary building blocks, but on their own were not enough to light the spark of diversification. The missing ingredient was a method to alternately connect and isolate populations of species through cycles of vicariance and dispersal. That spark finally occurs in the Darriwilian Stage when ice caps form over the south pole of the Ordovician Earth. The waxing and waning of these ice sheets caused sea level to rise and fall (similar to the Pleistocene), which provided the alternate connection and disconnection needed to facilitate rapid diversity accumulation. Stigall and her collaborators compared this to the assembly of building blocks required to pass a threshold. ",Matter & Energy,0.09154056836348111
34,Science Daily,Tiny GPS Backpacks Uncover the Secret Life of Desert Bats,Environment,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816101004.htm,"   Wildlife tracking has revolutionized the study of animal movement and their behavior. Yet, tracking small, flying animals such as desert bats remained challenging. Now a new generation of miniaturized satellite-based tags is allowing unique insights into the life of these mysterious mammals. Researchers used 1 g GPS devices to reconstruct the movements of yellow-winged bats, one of two false vampire bats occurring in Africa and one of the few desert bats large enough to carrying this innovative technology. ""GPS tags have seen up to now a limited use with insectivorous bats due to weight constraints and low success in data collection -- we achieved great results in tracking such a light species,"" says Irene Conenna, a PhD candidate at the University of Helsinki and the lead author of the study. Future under the changing climate? ""Bats are some of the most successful desert mammals. Powered flight allows them to efficiently track scarce resources and their nocturnal lifestyle buffers them from the baking sun. However, they still struggle to find enough resources during the drier periods of the year,"" says Ricardo Rocha, one of the co-authors of the paper. The study was conducted in Sibiloi National Park, Northern Kenya, along the shores of Lake Turkana, the world's largest desert lake. Researchers placed GPS loggers in 29 bats, 15 in the rainy season and 14 in the dry and, for one week. Their whereabouts were recorded every 30 to 60 minutes every night. This revealed that during dry periods bats used larger home ranges and had extended activity periods, potentially to compensate for a shortage in food resources. Bats comprise roughly one fifth of all mammal species and deserts are home to over 150 bat species. They display wide variation in morphology, foraging behavior, and habitat use, making them an excellent indicator group for assessing how species respond to changes in their habitats. ""The responses exhibited by bats offer important insights into the responses of other taxonomic groups,"" explains Conenna. ""These new miniaturized satellite-based tags now allow us to better understand how increased aridity affects bats foraging efficiency, leading us one step forward to understanding limits in aridity tolerance and impacts of climate change,"" adds Conenna. Deserts around the world are getting warmer and as they warm desert creatures need to cope with even harsher conditions. ""Understanding how animals cope with seasonal changes is key to understand how they might react to the challenges in the horizon. New technological devices, such as miniaturized satellite-based loggers, go a long way to help us in this task.,"" adds Mar Cabeza, senior author of the study, University of Helsinki. ",Environment,0.09136903282725467
35,Stanford,Gauging trees’ potential to slow global warming,Science,2019-08-18,-,https://news.stanford.edu/2019/08/12/gauging-trees-potential-slow-global-warming/,"   Like the eponymous character in Shel Silverstein’s classic children’s tale, trees are generous with their gifts, cleaning the air we breathe and slowing the ravages of global warming by absorbing about a quarter of all human-caused carbon dioxide emissions. But this generosity likely can’t last forever in the face of unabated fossil fuel consumption and deforestation. Scientists have long wondered whether trees and plants could reach a breaking point and no longer adequately absorb carbon dioxide.  Trees such as these in Sequoia National Park will continue to absorb carbon dioxide at generous rates through at least the end of the century, a new study finds. (Image credit: Kolibrik/Pixabay)  An international team led by scientists at Stanford University and the Autonomous University of Barcelona finds reason to hope trees will continue to suck up carbon dioxide at generous rates through at least the end of the century. However, the study published Aug. 12 in Nature Climate Change warns that trees can only absorb a fraction of carbon dioxide in the atmosphere and their ability to do so beyond 2100 is unclear. “Keeping fossil fuels in the ground is the best way to limit further warming,” said study lead author César Terrer, a postdoctoral scholar in Earth system science in Stanford’s School of Earth, Energy & Environmental Sciences. “But stopping deforestation and preserving forests so they can grow more is our next-best solution.” Weighing carbon dioxide Carbon dioxide – the dominant greenhouse gas warming the earth – is food for trees and plants. Combined with nutrients like nitrogen and phosphorus, it helps trees grow and thrive. But as carbon dioxide concentrations rise, trees will need extra nitrogen and phosphorus to balance their diet. The question of how much extra carbon dioxide trees can take up, given limitations of these other nutrients, is a critical uncertainty in predicting global warming.  Potential increase in plant biomass in the U.S. for carbon dioxide levels expected in 2100. (Image credit: César Terrer)  “Planting or restoring trees is like putting money in the bank,” said co-author Rob Jackson, the Michelle and Kevin Douglas Provostial Professor in Earth System Science at Stanford. “Extra growth from carbon dioxide is the interest we gain on our balance. We need to know how high the interest rate will be on our carbon investment.” Several individual experiments, such as fumigating forests with elevated levels of carbon dioxide and growing plants in gas-filled chambers, have provided critical data but no definitive answer globally. To more accurately predict the capacity of trees and plants to sequester carbon dioxide in the future, the researchers synthesized data from all elevated carbon dioxide experiments conducted so far – in grassland, shrubland, cropland and forest systems – including ones the researchers directed. Using statistical methods, machine-learning, models and satellite data, they quantified how much soil nutrients and climate factors limit the ability of plants and trees to absorb extra carbon dioxide. Based on global datasets of soil nutrients, they also mapped the potential of carbon dioxide to increase the amount and size of plants in the future, when atmospheric concentrations of the gas could double. Their results show that carbon dioxide levels expected by the end of the century should increase plant biomass by 12 percent, enabling plants and trees to store more carbon dioxide – an amount equivalent to six years of current fossil fuel emissions. The study highlights important partnerships trees forge with soil microbes and fungi to help them take up the extra nitrogen and phosphorus they need to balance their additional carbon dioxide intake. It also emphasizes the critical role of tropical forests, such as those in the Amazon, Congo and Indonesia, as regions with the greatest potential to store additional carbon. “We have already witnessed indiscriminate logging in pristine tropical forests, which are the largest reservoirs of biomass in the planet,” said Terrer, who also has a secondary affiliation with the Institut de Ciència i Tecnologia Ambientals, Universitat Autònoma de Barcelona. “We stand to lose a tremendously important tool to limit global warming.” To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. This paper is a contribution to the Global Carbon Project, which Jackson chairs. Jackson is also senior fellow at the Stanford Woods Institute for the Environment and the Precourt Institute for Energy. Co-authors also include Chris Field, the Perry L. McCarty Director of the Stanford Woods Institute for the Environment, the Melvin and Joan Lane Professor for Interdisciplinary Environmental Studies, professor of Earth system science and of biology and senior fellow at the Precourt Institute for Energy; and researchers at the International Institute for Applied Systems, Imperial College London, Macquarie University, Tsinghua University, University of California at Berkeley, Lawrence Berkeley National Laboratory, University of Vienna, University of Antwerp, California Institute of Technology, University of California at Los Angeles, University of Minnesota, Western Sydney University, the Ecological and Forestry Applications Research Centre, Northern Arizona University, Leiden University, James Cook University, University of Idaho, Peking University, Chinese Academy of Sciences, AgResearch, University of Tasmania, United States Department of Agriculture, University of New South Wales, Justus Liebig University of Giessen, University College Dublin, the Smithsonian Tropical Research Institute, Maastricht University, Utrecht University, Wageningen University, Tokyo University of Agriculture and Technology and Hokkaido University. The research was funded by the Natural Environment Research Council (U.K.), the Spanish Ministry of Science, the European Research Council, the Fund for Scientific Research – Flanders (Belgium),the U.S. Department of Energy, NASA, the California Institute of Technology and the Netherlands Organization for Scientific Research.  ",Environment,0.09073141941363476
36,Science Daily,Could Biological Clocks in Plants Set the Time for Crop Spraying?,Environment,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816075540.htm,"   Just like human jet lag, plants have body clocks that are crucial for their life in a world that has day and night. Plant biological clocks make a crucial contribution to their growth and the responses of crops to their fluctuating environments. In a new paper, published today [Friday 16 August] in the journal Nature Communications, the researchers found that the death of plant tissue and slow-down in growth resulting from the herbicide glyphosate depends upon the time that the herbicide is applied and also the biological clock. Crucially, the biological clock also led to a daily change in the minimum amount of herbicide that is needed to affect the plant, so less herbicide was needed at certain times of day. This provides an opportunity to reduce the quantity of herbicides used, saving farmers time, money and reducing environmental impacts. In medicine, ""chronotherapy"" considers the body clock when deciding the best time to give a medicine or treatment. This new research suggests that a similar approach could be adopted for future agricultural practice, with crop treatments being applied at times that are most appropriate for certain species of weed or crop. By employing a form of agricultural chronotherapy might have a future role in the sustainable intensification of agriculture required to feed the growing population. ",Environment,0.09031578138240498
37,Science Daily,"Extinct Caribbean Bird Yields DNA After 2,500 Years in Watery Grave",Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815143212.htm,"   Studies of ancient DNA from tropical birds have faced two formidable obstacles. Organic material quickly degrades when exposed to heat, light and oxygen. And birds' lightweight, hollow bones break easily, accelerating the decay of the DNA within. But the dark, oxygen-free depths of a 100-foot blue hole known as Sawmill Sink provided ideal preservation conditions for the bones of Caracara creightoni, a species of large carrion-eating falcon that disappeared soon after humans arrived in the Bahamas about 1,000 years ago. Florida Museum of Natural History postdoctoral researcher Jessica Oswald extracted and sequenced genetic material from a 2,500-year-old C. creightoni femur from the blue hole. Because ancient DNA is often fragmented or missing, Oswald had modest expectations for what she would find -- maybe one or two genes. But instead, the bone yielded 98.7% of the bird's mitochondrial genome, the set of DNA that most living things inherit only from their mothers. ""I was super excited. I would have been happy to get that amount of coverage from a fresh specimen,"" said Oswald, lead author of a study describing the work and also a postdoctoral researcher at the University of Nevada, Reno. ""Getting DNA from an extinct bird in the tropics is significant because it hasn't been successful in many cases or even tried."" The mitochondrial genome showed that C. creightoni is closely related to the two remaining caracara species alive today: the crested caracara, Caracara cheriway, and the southern caracara, Caracara plancus. The three species last shared a common ancestor between 1.2 and 0.4 million years ago. At least six species of caracara once cleaned carcasses and picked off small prey in the Caribbean. But the retreat of glaciers 15,000 years ago and the resulting rise in sea levels triggered extinctions of many birds, said David Steadman, Florida Museum curator of ornithology. C. creightoni managed to survive the sweeping climatic changes, but the arrival of people on the islands ultimately heralded the species' demise, as the tortoises, crocodiles, iguanas and rodents that the caracara depended on for food swiftly disappeared. ""This species would still be flying around if it weren't for humans,"" Steadman said. ""We're using ancient DNA to study what should be modern biodiversity."" Today, the islands host only a fraction of the wildlife that once flourished in the scrubland, forests and water. But blue holes like Sawmill Sink can offer a portal into the past. Researchers have collected more than 10,000 fossils from the sinkhole, representing nearly 100 species, including crocodiles, tortoises, iguanas, snakes, bats and more than 60 species of birds. Sawmill Sink's rich store of fossils was discovered by cave diver Brian Kakuk in 2005 in his quest for horizontal passages in the limestone. The hole was not a popular diving spot: Thirty feet below the surface lay a 20-foot-thick layer of saturated hydrogen sulfide, an opaque mass that not only smells of rotten egg, but also reacts with the freshwater above it to form sulfuric acid, which causes severe chemical burns. After multiple attempts, Kakuk, outfitted with a rebreather system and extra skin protection, punched through the hydrogen sulfide. His lamp lit up dozens of skulls and bones on the blue hole's floor. Soon after, Kakuk and fellow cave diver Nancy Albury began an organized diving program in Sawmill Sink. ""This was found by someone who recognized what it was and never moved anything until it was all done right,"" Steadman said. Though the hydrogen sulfide layer presented a foul problem for divers, it provided excellent insulation for the fossils below, blocking UV light and oxygen from reaching the lower layer of water. Among the crocodile skulls and tortoise shells were the C. creightoni bones, including an intact skull. ""For birds, having an entire head of an extinct species from a fossil site is pretty mind-blowing,"" Oswald said. ""Because all the material from the blue hole is beautifully preserved, we thought at least some DNA would probably be there."" Since 2017, Oswald has been revitalizing the museum's ancient DNA laboratory, testing methods and developing best practices for extracting and analyzing DNA from fossils and objects that are hundreds to millions of years old. Ancient DNA is a challenging medium because it's in the process of degradation. Sometimes only a minute quantity of an animal's original DNA -- or no DNA at all -- remains after bacteria, fungi, light, oxygen, heat and other environmental factors have broken down an organism. ""With ancient DNA, you take what you can get and see what works,"" Oswald said. ""Every bone has been subjected to slightly different conditions, even relative to other ones from the same site."" To maximize her chance of salvaging genetic material, Oswald cleans a bone, freezes it with liquid nitrogen and then pulverizes it into powder with a rubber mallet. ""It's pretty fun,"" she said. While previous studies required large amounts of bone, Oswald's caracara work showed ancient DNA could be successfully recovered at a smaller scale. ""This puts an exclamation point on what's possible with ancient DNA,"" said Robert Guralnick, Florida Museum curator of bioinformatics. ""We have new techniques for looking at the context of evolution and extinction. Beyond the caracara, it's cool that we have an ancient DNA lab that's going to deliver ways to look at questions not only from the paleontological perspective, but also at the beginnings of a human-dominated planet."" Steadman, who has spent decades researching modern and extinct biodiversity in the Caribbean, said some questions can only be answered with ancient DNA. ""By understanding species that weren't able to withstand human presence, it helps us better appreciate what we have left -- and not just appreciate it, but understand that when these species evolved, there were a lot more things running and flying around than we have today."" Other co-authors are Julia Allen of the University of Nevada, Reno; Kelsey Witt of the University of California, Merced; Ryan Folk of the Florida Museum and Nancy Albury of the National Museum of the Bahamas. ",Matter & Energy,0.09022397721630408
38,Science Daily,Are Siri and Alexa Making Us Ruder?,Computers & Math,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815120648.htm,"   ""Hey Siri, is the way we talk to you making humans less polite?"" OK, OK, they didn't ask Siri. Or Alexa. Instead they asked 274 people, and after surveying and observing those people, they found some good news: Artificially-intelligent digital assistants are not making adult humans ruder to other humans. Yet. ""Worried parents and news outlets alike have fretted about how the personification of digital assistants affects our politeness, yet we have found little reason to worry about adults becoming ruder as a result of ordering around Siri or Alexa,"" said James Gaskin, associate professor of information systems at BYU. ""In other words, there is no need for adults to say ""please"" and ""thank you"" when using a digital assistant."" Gaskin and lead author Nathan Burton actually expected to find the opposite -- that the way people treat AIs would make a difference in their life and interpersonal interactions. According to their assessment, digital assistants in their current form are not personified enough by adult users to affect human-to-human interactions. But that may not be the case with children. Parental concerns have already prompted both Google and Amazon to make adjustments to their digital assistants, with both now offering features that thank and compliment children when they make requests politely. Gaskin and Burton did not study children, but assessed young adults, who generally have already formed their behavioral habits. The researchers believe that if they repeated the study with kids, they would find different results. They also say that as artificial intelligence becomes more anthropomorphic in form, such as the new Vector Robot -- which has expressive eyes, a moving head and arm-like parts -- the effects on human interactions will increase because people will be more likely to perceive the robots as having and understanding emotion. ""The Vector Robot appears to do a good job of embodying a digital assistant in a way that is easily personifiable,"" Burton said. ""If we did the same type of study using a Vector Robot, I believe we would have found a much stronger effect on human interactions."" The research is being presented this week at the Americas Conference on Information Systems. ",Society,0.09003742745111716
39,Science Daily,Bloodsucker Discovered: First North American Medicinal Leech Described in Over 40 Years,Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815140848.htm,"   An international team of museum scientists led by Anna Phillips, the museum's curator of parasitic worms, describe the new species in the Aug. 15 issue of the Journal of Parasitology. ""We found a new species of medicinal leech less than 50 miles from the National Museum of Natural History -- one of the world's largest libraries of biodiversity,"" Phillips said. ""A discovery like this makes clear just how much diversity is out there remaining to be discovered and documented, even right under scientists' noses."" Leeches are parasitic worms, many of which feed on the blood of their hosts. In the 1700s and 1800s, physicians used leeches to treat a wide range of ailments, believing that by ridding a patient's body of bad blood, the parasites could cure headaches, fevers and other conditions. Any leech that readily feeds on humans is considered a medicinal leech, although in North America most leeches used for bloodletting were imported from Europe, leaving native species relatively undisturbed. Phillips and her colleagues have been exploring the diversity of medicinal leeches in North America for years. When she returned to the museum from a 2015 field expedition with several orange-spotted, olive-green leech specimens she had collected from a Maryland swamp, she and her team assumed they belonged to a familiar species called M. decora, a leech that is thought to live throughout a large swath of the northern United States. But DNA sequencing revealed otherwise. Examining the specimens' genomes at key regions used for species identification, Ricardo Salas-Montiel, a graduate student at the National Autonomous University of Mexico, found significant differences from the DNA of M. decora. The molecular discrepancy was surprising, but when the scientists took a closer look at the newly collected leeches, they found a physical difference that distinguished them from M. decora, as well. Like M. decora, the new leeches have multiple reproductive pores along the bottom of their bodies, known as gonopores and accessory pores. In the new leeches, the gonorpores and accessory pores were located in a different position relative to each other. A subsequent field outing led to the team finding more leeches from South Carolina that shared the same accessory pore positioning. ""Then we sequenced [their DNA], and they all came out more closely related to the leeches we had found in Maryland than to anything else known to science,"" Phillips said. Phillips quickly retrieved dozens of North American leeches stored in the Smithsonian's parasite collection and examined their accessory pores. ""All of a sudden, I started finding these things everywhere,"" she said. Leeches with the unique pore positioning had been found in locations from northern Georgia to Long Island and preserved in the museum's collection for years. The oldest, Phillips said, dated back to 1937. From there, Phillips expanded her search, scouring parasite collections at the North Carolina Museum of Natural Sciences and the Virginia Museum of Natural History, pinpointing additional places where the leech had been found in the past. She and her team also found fresh specimens in Georgia and North Carolina and used DNA sequencing to confirm their close relationship to the others. Each specimen added to the team's understanding of the leech's history in the region and its geographical range. Their molecular, geographical and morphological data suggest the M. mimicus occupies a sliver of the eastern United States that is nestled between the ranges of two other medicinal leech species, Phillips said. The historical record from the museums' collections, with specimens spanning 63 years, adds another critical layer of information, confirming that the species was not recently introduced to the area and does not represent a newly evolved species. ""It's been here this whole time,"" she said. ""We just hadn't looked at it in this new way."" Funding for this research was provided by the Smithsonian, the National Autonomous University of Mexico, the Natural Sciences and Engineering Research Council of Canada, the Royal Ontario Museum and the Olle Engkvist Byggmästare Foundation in Sweden. ",Matter & Energy,0.08997358590637577
40,Science Daily,Humans Migrated to Mongolia Much Earlier Than Previously Believed,Matter & Energy,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191447.htm,"   The site also points to a new location for where modern humans may have first encountered their mysterious cousins, the now extinct Denisovans, said Nicolas Zwyns, an associate professor of anthropology and lead author of the study. Zwyns led excavations from 2011 to 2016 at the Tolbor-16 site along the Tolbor River in the Northern Hangai Mountains between Siberia and northern Mongolia. The excavations yielded thousands of stone artifacts, with 826 stone artifacts associated with the oldest human occupation at the site. With long and regular blades, the tools resemble those found at other sites in Siberia and Northwest China -- indicating a large-scale dispersal of humans across the region, Zwyns said. ""These objects existed before, in Siberia, but not to such a degree of standardization,"" Zwyns said. ""The most intriguing (aspect) is that they are produced in a complicated yet systematic way -- and that seems to be the signature of a human group that shares a common technical and cultural background."" That technology, known in the region as the Initial Upper Palaeolithic, led the researchers to rule out Neanderthals or Denisovans as the site's occupants. ""Although we found no human remains at the site, the dates we obtained match the age of the earliest Homo sapiens found in Siberia,"" Zwyns said. ""After carefully considering other options, we suggest that this change in technology illustrates movements of Homo sapiens in the region."" Their findings were published online in an article in Scientific Reports. The age of the site -- determined by luminescence dating on the sediment and radiocarbon dating of animal bones found near the tools -- is about 10,000 years earlier than the fossil of a human skullcap from Mongolia, and roughly 15,000 years after modern humans left Africa. Evidence of soil development (grass and other organic matter) associated with the stone tools suggests that the climate for a period became warmer and wetter, making the normally cold and dry region more hospitable to grazing animals and humans. Preliminary analysis identifies bone fragments at the site as large (wild cattle or bison) and medium size bovids (wild sheep, goat) and horses, which frequented the open steppe, forests and tundra during the Pleistocene -- another sign of human occupation at the site. The dates for the stone tools also match the age estimates obtained from genetic data for the earliest encounter between Homo sapiens and the Denisovans. ""Although we don't know yet where the meeting happened, it seems that the Denisovans passed along genes that will later help Homo sapiens settling down in high altitude and to survive hypoxia on the Tibetan Plateau,"" Zwyns said. ""From this point of view, the site of Tolbor-16 is an important archaeological link connecting Siberia with Northwest China on a route where Homo sapiens had multiple possibilities to meet local populations such as the Denisovans."" Co-authors of the paper include UC Davis anthropology graduate students Roshanne Bakhtiary and Kevin Smith, former graduate student Joshua Noyer, and undergraduate alumna Aurora Allshouse, now a graduate student at Harvard University. Other members of the team included colleagues from universities and institutes in South Carolina, the United Kingdom, Mongolia, Germany, Belgium and Russia. ",Matter & Energy,0.08992281475957027
41,Science Daily,"Climate Change 'Disrupts' Local Plant Diversity, Study Reveals",Environment,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815113724.htm,"   Researchers have discovered that the numbers of plant species recorded by botanists have increased in locations where the climate has changed most rapidly, and especially in relatively cold parts of the world. Human activity has been responsible for substantial declines in biodiversity at the global level, to such an extent that there are calls to describe the modern epoch as 'The Anthropocene'. But although the total number of plant species on the planet may be in decline, the average number of plant species found locally -- the so-called local or alpha diversity of a site -- seems to be stable, or even increasing in places. Scientists at the University of York think that the 'disruption' of these local plant communities by rapid climate change, especially changes in rainfall, may be allowing new species in and fuelling these local diversity increases. Lead author, Dr Andrew Suggitt from the University of York's Department of Biology, said: ""We used a large dataset of over 200 studies in which botanists had counted the number of plant species present in survey plots situated all around the world. ""We tested for the influence of climate change alongside other well-known drivers of diversity change, finding that the local differences in climate, and exposure to climate change, were responsible for a substantial part of the change in plant species numbers found in these surveys."" ""Our models suggest that typical rates of climate change in cooler regions of the world are driving an increase in local species richness of 5% per decade. This is really quite a large number if it continues for 13 decades or more, given that humans have already been changing the climate for over half a century, and climate change is set to continue until the year 2100, at least. What we are observing has substantial implications for future ecosystems."" Co-author Professor Chris Thomas added: ""This does not mean that the botanical world gets a clean bill of health. We are living in 'The Anthropocene' epoch, and some plant species have become globally extinct. Many, many more are endangered. ""However, there is a disconnect between what is happening at that global level and the average change to plant diversity that can be observed in, say, a one metre square plot of ground. ""The effect of climate change may not be as dramatic as a meadow being turned into a car park, or a forest being cut down, but it's a pervasive effect that is already evident over vast areas of the Earth's land surface. ""For example, warmth-loving bee orchids (Ophrys apifera) have started arriving at a much wider variety of sites across the north of England, taking advantage of the changing climate. ""The data we have analysed tells us that colonists are tending to arrive faster than incumbents disappear, giving rise to slight increases in plant diversity in places where the climate is changing the most."" Dr Suggitt added: ""The recent global assessment report by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services* highlighted worrying declines in plants and animals around the world. ""But it also highlighted the sore need for greater clarity over how climate change is shuffling the deck of plant species found in particular locations -- especially in under-sampled areas such as the tropics, Africa and Asia. ""We hope our study opens the door to a fuller understanding of how climate change is affecting plant communities, and what this means for the conservation of nature and its contribution to people."" ",Environment,0.08988713058039587
42,Science Daily,In the Shadow of the Dinosaurs,Matter & Energy,2019-08-14,-,https://www.sciencedaily.com/releases/2019/08/190814081157.htm,"   Clevosaurus hadroprodon was a small animal, similar in size with common house geckos. It belongs to the Sphenodontia, a group of lepidosaurs (which also includes snakes, lizards and amphisbaenians), that was very diverse and widespread during the Mesozoic era (the ""Age of Dinosaurs""), but today has only one remaining living species in New Zealand. Clevosaurus hadroprodon is the oldest member of the Clevosauridae, a group of small sphenodonts that were the first globally distributed lepidosaurs with fossils from the Late Triassic and Early Jurassic of North America, Europe, Asia, Africa and South America. The dentition of Clevosaurus hadroprodon is an unexpected mix of primitive and derived teeth. It is the oldest occurrence of the typical fully acrodont dentition (teeth fused to the top of the jaw bones) of sphenodontians, but most of its teeth are relatively simple and blade-like, which differs from other, only slightly younger Clevosaurus species that possess well-developed medial-posteromedial (side-to-side) expansions of the teeth for complex grinding. ""However, Clevosaurus hadroprodon also possess a large, blunt, tusk-like tooth in the first tooth position of the both premaxilla (upper jaw) and of dentary (lower jaw). This feature is typically observed only in later sphenodontian lineages"" says Annie Schmaltz Hsiou, Associate Professor at the University of São Paulo and head of the study. The name ""hadroprodon"" is Greek for ""larger first tooth"" in reference to these tusk-like teeth. ""Clevosaurus hadroprodon is an important discovery because it combines a relatively primitive sphenodontian-type tooth row with the presence of massive tusk-like teeth that were possibly not for feeding, but rather used for mate competition or defense. If correct, this means that non-feeding dental specializations predated changes in the sphenodontian dentition related to feeding strategies. This is a very exciting discovery."" says co-author Randall Nydam, Professor at Midwestern University (US). In addition to its unique dentition, the authors stress that Clevosaurus hadroprodon also adds to the growing evidence that the early diversification of sphenodontians occurred in the widely separated regions of Gondwana destined to become South American and India. This illustrates the importance of the role of the Gondwanan lepidosaur fauna in our growing understanding of the earliest stages of sphenodontian evolution and the global biogeographic distribution of lepidosaurs. ",Matter & Energy,0.08984266155388845
43,Science Daily,Discovery Could Pave the Way for Disease-Resistant Rice Crops,Environment,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815113723.htm,"   Magnaporthe oryzae, the fungus that leads to rice blast disease, creates lesions on rice plants that reduce the yield and quality of grain. The fungus causes a loss of up to a third of the global rice harvest, roughly enough to feed more than 60 million people each year. Various strategies to ward off the fungus have been employed, but a sustainable approach has not yet been developed. Cost and environmental concerns have limited the success of toxic fungicides. And a phenomena called linkage drag, where undesirable genes are transferred along with desired ones, has made it difficult for breeders to produce varieties of rice that exhibit improved disease resistance but still produce grain at a desired rate. Gene-editing technologies could eventually be used to precisely insert genes in rice plants, overcoming the issue of linkage drag, but first, genes that boost rice immunity need to be identified or engineered. A team of researchers in Japan and the U.K. report in the Journal of Biological Chemistry that a particular rice immune receptor -- from a class of receptors that typically recognize only single pathogenic proteins -- pulls double duty by triggering immune reactions in response to two separate fungal proteins. The genes that encode this receptor could become a template for engineering new receptors that can each detect multiple fungal proteins, and thereby improve disease resistance in rice crops. Rice blast fungus deploys a multitude of proteins, known as effectors, inside of rice cells. In response, rice plants have evolved genes encoding nucleotide binding-leucine-rich repeat proteins, or NLRs, which are intracellular immune receptors that bait specific fungal effectors. After an NLR receptor's specific fungal effector binds to the bait, signaling pathways are initiated that cause cell death. ""(The cells) die in a very localized area so the rest of the plant is able to survive. It's almost like sacrificing your finger to save the rest of your body,"" said Mark Banfield, professor and group leader at John Innes Centre in Norwich, England, and senior author of the study. After learning from previous work that the fungal effectors AVR-Pia and AVR-Pik have similar structures, the researchers sought to find out whether any rice NLRs known to bind to one of these effectors could perhaps also bind to the other, Banfield said. The scientists introduced different combinations of rice NLRs and fungal effectors into tobacco (a model system for studying plant immunity) and also used rice plants to show if any unusual pairs could come together and elicit immune responses. An AVR-Pik-binding rice NLR called Pikp triggered cell-death in response to AVR-Pik as expected, but surprisingly, the experiments showed that plants expressing this NLR also partially reacted to AVR-Pia. The authors took a close look at the unexpected pairing using X-ray crystallography and noticed that the rice NLR possessed two separate docking sites for AVR-Pia and AVR-Pik. In its current form, Pikp causes meager immune reactions after binding AVR-Pia, however, the receptor's DNA could be modified to improve its affinity for mismatched effectors, Banfield said. ""If we can find a way to harness that capability, we could produce a super NLR that's able to bind multiple pathogen effectors,"" Banfield said. As an ultimate endgame, gene-editing technologies could be used to insert enhanced versions of NLRs -- like Pikp -- into plants, Banfield said, which could tip the scale in favor of rice crops in the face of rice blast disease. This work was supported by Biotechnology and Biological Sciences Research Council, grant numbers BB/P012574, BB/M02198X; the ERC (proposal 743165), the John Innes Foundation, the Gatsby Charitable Foundation and JSPS KAKENHI 15H05779 and 18K05657. Other authors on this study include Freya A. Varden, Hiromasa Saitoh, Kae Yoshino, Marina Franceschetti, Sophien Kamoun and Ryohei Terauchi. ",Environment,0.08944307118825529
44,Science Daily,Human Impacts on Oceans Nearly Doubled in Recent Decade,Science,2019-08-13,-,https://www.sciencedaily.com/releases/2019/08/190813101940.htm,"   Published in the journal Scientific Reports, the study assessed for the first time where the combined impacts that humans are having on oceans -- from nutrient pollution to overfishing -- are changing and how quickly. In nearly 60% of the ocean, the cumulative impacts are increasing significantly and, in many places, at a pace that appears to be accelerating. ""That creates even more urgency to solve these problems,"" said lead author Ben Halpern, director of NCEAS and a professor at UC Santa Barbara's Bren School of Environmental Science & Management. Climate change is a key factor driving the increase across the world, as seas warm, acidify and rise. On top of that, commercial fishing, runoff from land-based pollution and shipping are intensifying progressively each year in many areas of the ocean. ""It's a multifactor problem that we need to solve. We can't just fix one thing if we want to slow and eventually stop the rate of increase in cumulative impacts,"" said Halpern. The study also projected the impacts one decade into the future, based on the rate of change in the recent past, finding that they could double again if the pace of change continues unchecked. The assessment provides a holistic perspective of where and how much human activities shape ocean change -- for better or worse -- which is essential to policy and planning. ""If you don't pay attention to the big picture, you miss the actual story,"" said Halpern. ""The bigger picture is critical if you want to make smart management decisions -- where are you going to get your biggest bang for your buck."" Regions of particular concern include Australia, Western Africa, the Eastern Caribbean islands and the Middle East, among others. Coastal habitats such as mangroves, coral reefs and seagrasses are among the hardest-hit ecosystems. There is an upside to the story, however. The authors did find ""success stories"" around every continent, areas where impacts have declined, such as the seas of South Korea, Japan, the United Kingdom and Denmark, all of which have seen significant decreases in commercial fishing and pollution. These declines suggest that policies and other actions to improve ocean conditions are making a difference -- although, the analysis does not attribute specific actions to those declines. ""We can improve things. The solutions are known and within our grasp. We just need the social and political will to take action,"" said Halpern. To assess the pace of change, the authors leveraged two previous and similar assessments conducted by several of the same team members and others in 2008 and 2013, which provided first glimpses into the full, cumulative extent of humanity's impacts on oceans. ""Previously, we had a good measure of the magnitude of human impacts, but not a clear picture of how they are changing,"" said co-author Melanie Frazier, a data scientist at NCEAS. Frazier was surprised to see in the data how dramatically ocean temperatures have increased in a relatively short period of time. ""You don't need fancy statistics to see how rapidly ocean temperature is changing and understand the magnitude of the problem,"" said Frazier. ""I think this study, along with many others, highlights the importance of a concerted global effort to control climate change."" ",Environment,0.08877954846571753
45,Science Daily,Dinosaur Brains from Baby to Adult,Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815101537.htm,"   Psittacosaurus was a very common dinosaur in the Early Cretaceous period -- 125 million years ago -- that lived in eastern Asia, especially north-east China. Hundreds of samples have been collected which show it was a beaked plant-eater, an early representative of the Ceratopsia, which had later relatives with great neck frills and face horns, such as Triceratops. The babies hatched out as tiny, hamster-sized beasts and grew to two metres long as adults. As they grew, the brain changed in shape, from being crammed into the back of the head, behind the huge eyes in the hatchling, to being longer, and extending under the skull roof in the adults. The braincase also shows evidence for a change in posture as the animals grew. There is good evidence from the relative lengths of the arms and legs, that baby Psittacosaurus scurried about on all fours, but by the age of two or three, they switched to a bipedal posture, standing up on their elongate hind legs and using their arms to grab plant food. Claire Bullar from the University of Bristol's School of Earth Sciences led the new research which has been published this week in PeerJ. She said: ""I was excited to see that the orientation of the semi-circular canals changes to show this posture switch. ""The semi-circular canals are the structures inside our ears that help us keep balance, and the so-called horizontal semi-circular canal should be just that -- horizontal -- when the animal is standing in its normal posture. ""This is just what we see, with the head of Psittacosaurus pointing down and forwards when it was a baby -- just right for moving on all-fours. Then, in the teen or adult, we see the head points exactly forwards, and not downwards, just right for a biped."" Co-supervisor Dr Qi Zhao from the Institute of Vertebrate Palaeontology and Palaeoanthropology (IVPP) in Beijing, where the specimens are housed, added: ""It's great to see our idea of posture shift confirmed, and in such a clear-cut way, from the orientation of the horizontal ear canal. ""It's also amazing to see the results of high-quality CT scanning in Beijing and the technical work by Claire to get the best 3D models from these scan data."" Professor Michael Ryan of Carleton University, Ottawa, Canada, another collaborator, said: ""This posture shift during growth from quadruped to biped is unusual for dinosaurs, or indeed any animal. Among dinosaurs, it's more usual to go the other way, to start out as a bipedal baby, and then go down on all fours as you get really huge. ""Of course, adult Psittacosaurus were not so huge, and the shift maybe reflects different modes of life: the babies were small and vulnerable and so probably hid in the undergrowth, whereas bipedalism allowed the adults to run faster and escape their predators."" Professor Michael Benton, also from the University of Bristol's School of Earth Sciences and another collaborator, added: ""This is a great example of classic, thorough anatomical work, but also an excellent example of international collaboration. ""The Bristol Palaeobiology Research Group has a long-standing collaboration with IVPP, and this enables the mix of excellent specimens and excellent research. ""Who would have imagined we could reconstruct posture of dinosaurs from baby to adult, and with multiple lines of evidence to confirm we got it right."" ",Matter & Energy,0.0883132081914962
46,Science Daily,Ancient Feces Reveal How 'Marsh Diet' Left Bronze Age Fen Folk Infected With Parasites,Matter & Energy,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815212756.htm,"   The Bronze Age settlement at Must Farm, located near what is now the fenland city of Peterborough, consisted of wooden houses built on stilts above the water. Wooden causeways connected islands in the marsh, and dugout canoes were used to travel along water channels. The village burnt down in a catastrophic fire around 3,000 years ago, with artefacts from the houses preserved in mud below the waterline, including food, cloth, and jewellery. The site has been called ""Britain's Pompeii."" Also preserved in the surrounding mud were waterlogged ""coprolites"" -- pieces of human faeces -- that have now been collected and analysed by archaeologists at the University of Cambridge. They used microscopy techniques to detect ancient parasite eggs within the faeces and surrounding sediment. Very little is known about the intestinal diseases of Bronze Age Britain. The one previous study, of a farming village in Somerset, found evidence of roundworm and whipworm: parasites spread through contamination of food by human faeces. The ancient excrement of the Anglian marshes tells a different story. ""We have found the earliest evidence for fish tapeworm, Echinostoma worm, and giant kidney worm in Britain,"" said study lead author Dr Piers Mitchell of Cambridge's Department of Archaeology. ""These parasites are spread by eating raw aquatic animals such as fish, amphibians and molluscs. Living over slow-moving water may have protected the inhabitants from some parasites, but put them at risk of others if they ate fish or frogs."" Disposal of human and animal waste into the water around the settlement likely prevented direct faecal pollution of the fenlanders' food, and so prevented infection from roundworm -- the eggs of which have been found at Bronze Age sites across Europe. However, water in the fens would have been quite stagnant, due in part to thick reed beds, leaving waste accumulating in the surrounding channels. Researchers say this likely provided fertile ground for other parasites to infect local wildlife, which -- if eaten raw or poorly cooked -- then spread to village residents. ""The dumping of excrement into the freshwater channel in which the settlement was built, and consumption of aquatic organisms from the surrounding area, created an ideal nexus for infection with various species of intestinal parasite,"" said study first author Marissa Ledger, also from Cambridge's Department of Archaeology. Fish tapeworms can reach 10m in length, and live coiled up in the intestines. Heavy infection can lead to anemia. Giant kidney worms can reach up to a metre in length. They gradually destroy the organ as they become larger, leading to kidney failure. Echinostoma worms are much smaller, up to 1cm in length. Heavy infection can lead to inflammation of the intestinal lining. ""As writing was only introduced to Britain centuries later with the Romans, these people were unable to record what happened to them during their lives. This research enables us for the first time to clearly understand the infectious diseases experienced by prehistoric people living in the Fens,"" said Ledger. The Cambridge team worked with colleagues at the University of Bristol's Organic Chemistry Unit to determine whether coprolites excavated from around the houses were human or animal. While some were human, others were from dogs. ""Both humans and dogs were infected by similar parasitic worms, which suggests the humans were sharing their food or leftovers with their dogs,"" said Ledger. Other parasites that infect animals were also found at the site, including pig whipworm and Capillaria worm. It is thought that they originated from the butchery and consumption of the intestines of farmed or hunted animals, but probably did not cause humans any harm. The researchers compared their latest data with previous studies on ancient parasites from both the Bronze Age and Neolithic. Must Farm tallies with the trend of fewer parasite species found at Bronze Age compared with Neolithic sites. ""Our study fits with the broader pattern of a shrinking of the parasite ecosystem through time,"" said Mitchell. ""Changes in diet, sanitation and human-animal relationships over millennia have affected rates of parasitic infection."" Although he points out that infections from the fish tapeworm found at Must Farm have seen a recent resurgence due to the popularity of sushi, smoked salmon and ceviche. ""We now need to study other sites in prehistoric Britain where people lived different lifestyles, to help us understand how our ancestors' way of life affected their risk of developing infectious diseases,"" added Mitchell. The Must Farm site is an exceptionally well-preserved settlement dating to 900-800 BC (the Late Bronze Age). The site was first discovered in 1999. The Cambridge Archaeological Unit carried out a major excavation between 2015 and 2016, funded by Historic England and Forterra Building Products Ltd. ",Matter & Energy,0.08790631570222193
47,Science Daily,Research Bias May Leave Some Primates at Risk,Science,2019-08-14,-,https://www.sciencedaily.com/releases/2019/08/190814090000.htm,"   The study, which appeared in Evolutionary Anthropology, examined more than 29,000 research articles published between 2011 and 2015 to determine which primate species and locations were most studied and how that focus affects both conservation efforts and risk for species extinction. ""With nearly a third of primate species listed as critically endangered and 60% of all primate species classified as threatened with extinction by the International Union for Conservation of Nature, the window of opportunity for conserving these mammals is quickly closing,"" said the study's co-author, Allison McNamara, a Ph.D. student in anthropology at UT Austin. ""To protect these species, we have to understand their biology, ecology, life history, behavior and evolutionary flexibility."" The researchers found that more than half of the 504 primate species were left out of the research literature. Of the 240 species studied, 13% of the research was on chimpanzees, compared to 3% of research that focused on the next most prevalent species, the Japanese Macaque. Furthermore, 18% of published research concerned species considered critically endangered, data deficient or non-evaluated by the IUNC. ""There are a large number of primate species and populations that are being ignored, which has the potential to misrepresent primate patterns and influence primatological and anthropological theoretical frameworks,"" said the study's lead author, Michelle Bezanson, an anthropologist at Santa Clara University. ""Additionally, not studying certain species makes it impossible to know what risks they face in their habitat, so that scientists can help inform conservation efforts."" In examining the published works, McNamara and Bezanson were most surprised to find that there were 31 primate habitat countries where no research was being conducted, including Guatemala, Trinidad, Zimbabwe, Sudan, Afghanistan and Singapore. Most of the field work was conducted in national parks and protected areas of mainland Africa (35.6%), followed by the North and South Americas (29.2%), Asia (25.1%) and Madagascar (9.9%). ""The far-reaching benefits of scientific research are missing in some countries,"" McNamara said. ""Active research can benefit communities in more ways than primate and environmental conservation, including increasing financial support for local and national economies, raising awareness of the region's natural history, sparking community involvement in scientific projects, and creating opportunities for educational programs in the region."" The researchers also noted that most published primate research focused on topics other than conservation. However, 17.6% of publications were conservation focused, addressing anthropogenic influences on habitat, population status/density, population health, genetic diversity and enthnoprimatological approaches to human/wildlife interactions. ""The findings in our paper can help researchers identify the holes in our knowledge about primates so that we may begin to fill in the gaps and frame research questions appropriately, especially in the context of the current extinction crisis of primates,"" McNamara said. ""It is imperative that field researchers pay careful attention to the conservation and ethical implications of their research from conception to publication, and that these implications be included in their published work so that our scientific community can track the progress of such efforts."" ",Environment,0.08788984966264217
48,Stanford,Stanford launches major effort to harness the microbiome to treat disease,Science,2019-08-18,-,https://news.stanford.edu/2019/08/13/stanford-launches-major-effort-harness-microbiome-treat-disease/,"   Stanford is launching a major new effort to harness the communities of microbes inhabiting our bodies – known as the microbiome – in developing new therapies for debilitating diseases. The Stanford Microbiome Therapies Initiative (MITI), a joint initiative between Stanford ChEM-H and the Department of Bioengineering, is backed by a $10 million gift from philanthropists Marc and Lynne Benioff and a $7 million gift from Mark and Debra Leslie.  Michael Fischbach will lead the Stanford Microbiome Therapies Initiative, a joint initiative between ChEM-H and the Department of Bioengineering. (Image credit: L.A. Cicero)  The initiative has an ambitious goal of building and manipulating the microbiome to create new therapies and test them in early-stage human clinical trials. “Microbiome science has great potential for advancing our understanding and treatment of human disease,” said Stanford President Marc Tessier-Lavigne. “Stanford faculty are studying the microbes that inhabit our bodies in health and disease and developing platforms to generate new therapies. This body of work creates a foundation for the Stanford Microbiome Therapies Initiative, which will foster interdisciplinary collaborations across the university to spark discoveries that will benefit patients. I’m thankful to Marc and Lynne Benioff for seeing the potential of this promising field and making a generous gift to inspire other philanthropists, to the deans of the Schools of Medicine and Engineering for their leadership in developing this new initiative, and to Mark and Debra Leslie for joining the Benioffs with their generous support.” The Benioffs announced their lead gift to Stanford along with funding for the University of California, San Francisco to create the UCSF Benioff Center for Microbiome Medicine, which aims to radically rethink the role of the microbiome in early life and develop new interventions to prevent childhood diseases. These gifts further energize the Bay Area’s thriving microbiome research community and leverage the collaborative research in this realm already taking place at the Chan Zuckerberg Biohub, a research institute affiliated with Stanford, UCSF and UC Berkeley. “Lynne and I are honored to support the cutting-edge research of two of the world’s leading universities as they pioneer a new era of microbiome research, science and therapies,” Marc Benioff said. “With a deeper understanding of the human microbiome, our generation can unlock new treatments that impact lives around the world.” Collaborative team MITI is led by Michael Fischbach, associate professor of bioengineering. MITI leverages Stanford’s extensive expertise in microbiome research, the strengths and proximity of Stanford’s Schools of Medicine and Engineering and the interdisciplinary endeavors of Stanford ChEM-H (Chemistry, Engineering and Medicine for Human Health).   Q&A:Michael Fischbach wants to build a microbiome from the ground up. So what do we need to get there? Fischbach explains why the microbiome is worth studying, what his lab is doing to understand the complex communities, and how the people and mission of ChEM-H drew him to Stanford. Read more  Fischbach, who is also the Stanford MAC3 Paul and Mildred Berg Faculty Scholar and the Willard R. and Inez Kerr Bell Faculty Scholar in the School of Engineering, was recruited to Stanford in 2017 through a collaboration between Stanford ChEM-H and the Department of Bioengineering. He brought together a small group of Stanford faculty who were already working to better understand the microbiome. Fischbach proposed forming MITI (pronounced “mighty”) to focus that group’s efforts on manipulating microbial communities – both their composition and their genetics – and engineering those communities into therapies to address a range of diseases. “This initiative is a perfect reflection of the ChEM-H vision of bringing together chemistry, engineering and medicine to revolutionize therapeutic development and to improve human health,” said Carolyn Bertozzi, professor of chemistry and the Baker Family Co-Director of Stanford ChEM-H. “Since its inception, ChEM-H has had a strong interest in microbiome science and medicine and we were thrilled to succeed, in partnership with the Department of Bioengineering, in recruiting Michael Fischbach to ChEM-H to lead a targeted, pioneering initiative in this area.” Bertozzi is also Anne T. and Robert M. Bass Professor in the School of Humanities and Sciences. The initiative draws on a culture at Stanford of working across disciplines to tackle major issues with imaginative solutions, said Kathryn Moler, vice provost and dean of research and the Sapp Family University Fellow in Undergraduate Education. “This initiative couldn’t happen without the involvement and support of the deans of the schools of Medicine and Engineering and a willingness of faculty across campus to step outside their traditional domains and creatively work together to accelerate actionable discoveries and make a tangible impact in human health,” she said. “The team has an ambitious goal and I’m thankful ChEM-H is able to provide a natural home for this initiative to flourish.” Initial funding       Science & Technology Exploring the microbiome frontier This collection of stories about scientists at Stanford reveals the many ways they are learning how the microbiome influences health and how to harness the microbiome to treat disease and improve our overall well-being.    The lead gift from Marc and Lynne Benioff, and the funds provided by Mark and Debra Leslie, will enable the Stanford MITI team to initiate the work needed to construct, manipulate and characterize novel microbial therapies for a range of human diseases. Stanford will seek additional philanthropic support in order to bring promising new therapies to early-stage human clinical trials. Marc Benioff is the chairman, co-chief executive officer and founder of Salesforce. Lynne Benioff is a Distinguished Director of the Board of Overseers of the University of California San Francisco Foundation and serves on the board of directors of the UCSF Benioff Children’s Hospitals, along with several other organizations. Mark Leslie is founder and managing general partner at Leslie Ventures, a private investment company, and a lecturer in management at the Stanford Graduate School of Business. Debra Leslie is a director of the Leslie Family Foundation, whose mission is to positively impact lives through economic development, health care and education, and to support Jewish community life. Focus on therapies   MITI leadership Executive Committee: Carolyn Bertozzi, professor of chemistry; Michael Fischbach, associate professor of bioengineering; Justin Sonnenburg, professor of microbiology & immunology Advisory Committee: Jennifer Doudna, professor of biochemistry and molecular biology, UC Berkeley; Steve Quake, professor of bioengineering and applied physics; Christopher Walsh, emeritus professor, Harvard Medical School Core members: Ami Bhatt, assistant professor of medicine and genetics; Dylan Dodd, assistant professor of pathology and microbiology & immunology; Michael Fischbach, associate professor of bioengineering; KC Huang, professor of bioengineering and microbiology & immunology; David Relman, professor of medicine and microbiology & immunology; Elizabeth Sattely, associate professor of chemical engineering; Justin Sonnenburg, professor of microbiology & immunology  Much of the microbiome research currently underway focuses on sequencing and cataloging communities of microbes in our gut and on our skin – work that has led to discoveries about the role of the microbiome in diseases like inflammatory bowel disease, liver disease, autoimmune disease and cancer. Fischbach said little has been done until now to precisely manipulate those communities or their genomes to explore possibilities for new treatments. Instead, current therapy utilizes human stool: an undefined community of microbes with unknown modes of action and variable therapeutic outcomes. “This groundbreaking research at the intersection of engineering and medicine is precisely what we envisioned when we joined forces with Stanford ChEM-H to recruit Michael Fischbach,” said Jennifer Cochran, who is the Shriram Chair of the Department of Bioengineering. “Achieving this ambitious goal will draw on Stanford’s expertise in engineering novel solutions and experience translating research into new therapies to be tested in humans. I appreciate the support of the schools of Medicine and Engineering and Stanford ChEM-H for their valuable teamwork in forming this new initiative.” The initiative will launch with seven faculty from across engineering and medicine, plus executive and advisory committees to provide expert guidance and is based in part on pioneering work from Alice Cheng, a clinical instructor in gastroenterology at Stanford Medicine. ",Health,0.08641646091786334
49,Science Daily,Research Suggests Glyphosate Lowers pH of Dicamba Spray Mixtures Below Acceptable Levels,Environment,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815125146.htm,"   The study titled, ""Spray Mixture pH as Affected by Dicamba, Glyphosate and Spray Additives,"" was authored by Tom Mueller and Larry Steckel, both professors in the UT Department of Plant Sciences. Their research consisted of four experiments which examined the effect of different components on spray mixture pH, including formulations of dicamba (XtendiMax and Engenia), glyphosate (Roundup PowerMax II and Cornerstone Plus), ammonium sulfate (AMS), and several pH modifiers. According to their data, the addition of glyphosate to XtendiMax and Engenia always decreased the measured pH by anywhere from 1.0 to 2.1 pH units. Averaged across water sources, the pH levels for XtendiMax + Roundup PowerMax were 4.8, while the levels for Engenia + Roundup PowerMax were 4.6. Conversely, when no glyphosate was added to these dicamba formulations the final pH was always above 5.0. ""If pH is the main driver for dicamba volatility, then the substantial pH changes we're seeing from the addition of glyphosate could have profound effects on volatility, as well as herbicide efficacy,"" says Mueller. Many products containing glyphosate are presently approved to be mixed with dicamba before spray applications. Combining herbicides with different modes of action is a common practice among crop producers to control a wider range of weed species. However, based on Mueller and Steckel's research, UT weed experts are discouraging the addition of glyphosate to XtendiMax and Engenia. ""Based on this research, we believe glyphosate in the tank mix could be a culprit in why we're seeing some of the drift in the fields these past three years,"" says Steckel. Dicamba drift has been a hot-button issue in the agricultural community since new and expanded uses for this herbicide were approved in 2017. Off-target dicamba movement, occurring either through physical drift or volatility, has been blamed for the damage of millions of acres of crops, trees and ornamental plants. In an effort to decrease the potential for dicamba drift through volatilization, the Environmental Protection Agency (EPA) added language to the 2019-2020 labels advising applicators to maintain a spray mixture pH above 5.0 while avoiding the addition of products that would further decrease pH. According to Steckel, the lower the spray mixture pH, the higher the probability that dicamba can dissociate to the acid form, which is the most volatile form of the herbicide. Another experiment in the UT Institute of Agriculture study evaluated spray mixture pH levels when AMS was added to dicamba formulations. AMS is commonly added to spray mixtures containing glyphosate to improve weed control effectiveness. However, new-generation dicamba labels have always forbidden the addition of AMS due to volatility concerns. Interestingly, researchers found that while the addition of AMS did always lower dicamba spray mixture pH levels in this study, it did not lower pH levels as much as the addition of glyphosate. ""That was somewhat surprising given the attention AMS has received as a dicamba volatility enhancer,"" says Mueller. ""Now, there could be some other aspects of ammonium sulfate besides pH that are enhancing volatility, but if pH is the main driver, our data would suggest that adding glyphosate, which is currently an approved tank mix partner, would increase dicamba volatility more than the prohibited AMS."" One proposed solution for addressing low pH spray mixtures is adding a pH modifier to the mix. Mueller and Steckel tested three pH modifers: Chempro CP-70, Novus K 20-0-6, and SoyScience. All three products raised the pH levels of the respective mixtures of XtendiMax + Roundup PowerMax and Engenia + Roundup PowerMax above 5.0. ""That is certainly promising,"" says Steckel, ""and the addition of a pH modifier could decrease the probability of dicamba leaving a treated field via volatility. A follow-up study really needs to test that theory. Another question raised by the results of this research -- if you artificially raise the pH are you going to lose the weed control from glyphosate? That topic also needs more research."" Until more research is done, UT weed scientists are recommending leaving glyphosate out of the dicamba spray mixture and adding a graminicide, like Clethodim, for control of grass weed species. ""Glyphosate is an important herbicide with many uses. Despite the continued evolution of glyphosate-resistant weeds, farmers would be lost without glyphosate, as it still provides excellent and economical control of many troublesome weed species,"" says Steckel. ""It just doesn't belong in a tank mix with dicamba."" ",Environment,0.08424853408286705
50,Science Daily,Shedding Light on How the Human Eye Perceives Brightness,Health,2019-08-18,-,https://www.sciencedaily.com/releases/2019/08/190818101642.htm,"   The findings could contribute to more effective therapies for complications that relate to the eye. They can also serve as the basis for developing lighting and display systems. The research was published in Scientific Reports on May 20th, 2019. The back of the human eye is lined with the retina, a layer of various types of cells, called photoreceptors, that respond to different amounts of light. The cells that process a lot of light are called cones and those that process lower levels of light are named rods. Up until recently, researchers have thought that when light struck the retina, rods and cones were the only two kinds of cells that react. Recent discoveries have revealed an entirely new type of cells, called intrinsically photosensitive retinal ganglion cells (ipRGCs). Unlike rods and cones, ipRGCs contain melanopsin, a photopigment that is sensitive to light. While it has been established that ipRGCs are involved in keeping the brain's internal clock in sync with changes in daylight, their importance in the detection of the amount of light had not yet been well understood. ""Until now, the role of retinal melanopsin cells and how they contribute to the perception of the brightness of light have been unclear,"" said Katsunori Okajima, a professor at the Faculty of Environment and Information Sciences, Yokohama National University and one of the authors of the study. ""We've found that melanopsin plays a crucial role on the human ability to see how well-lit the environment is. These findings are redefining the conventional system of light detection that so far has only taken into consideration two variables, namely brightness and the amount of incoming light. Our results suggest that brightness perception should rely on a third variable -- the intensity of a stimulus that targets melanopsin."" In the study, the authors showed how cones and melanopsin combine to allow the perception of brightness. In order to better assess the contribution of melanopsin to the detection of light, the melanopsin's signals were isolated from cones and rods. This separation allowed for more accurate observation of the melanopsin signal alone. Visual stimuli were carefully designed and positioned in order to specifically stimulate the light-sensitive chemical. Also, the researchers used tracking software to measure study participants' pupil diameters under each visual stimulus. This served as a way to determine the relationship between brightness perception and the actual visual stimulus intensity on the retina. The researchers were able to show that the varying brightness levels of an image that was perceived is a sum of the melanopsin response and the response that is generated by the cones. The former is a linear readout and the latter is not. The results also show that melanopsin is not a minor contributor in brightness perception. Rather, it is a crucial player in brightness perception. This work was supported by the Japan Society for the Promotion of Science Grants-in-Aid for Scientific Research (Grant Numbers 15H05926 and 18H04111). ",Health,0.08305016222539856
51,Science Daily,Genetic Redundancy Aids Competition Among Bacteria in Symbiosis With Squid,Environment,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815180629.htm,"   A paper describing the research, which provides insight into the molecular mechanisms of competition among bacteria in a microbiome, appears online in the Journal of Bacteriology. ""Many organisms, including humans, acquire bacteria from their environment,"" said Tim Miyashiro, assistant professor of biochemistry and molecular biology at Penn State and the leader of the research team. ""These bacteria can contribute to functions within the host organism, like how our gut bacteria help us digest food. We are interested in the dynamic interactions among bacterial cells and between bacteria and their host to better understand these mutually beneficial symbiotic relationships."" Cells of a bioluminescent species of marine bacteria, Vibrio fisheri, take up residence in the light organ of a newly hatched bobtail squid. At night, the bacteria produce a blue glow that researchers believe obscures the squid's silhouette and helps protect it from predators. The light organ has pockets, or crypts, in the squid's skin that provide nutrients and a safe environment for the bacteria. ""When the squid hatches, it doesn't yet have any bacteria in its light organ,"" said Miyashiro. ""Bacteria in the environment quickly colonize the squid's light organ. Each crypt can be occupied by several different strains initially. Some of these different bacterial strains can coexist, but others -- bacteria that use the T6SS system -- kill cells of other strains."" In the lab, researchers can control which bacterial strains are present in the environment of the hatching squid and study their interactions. A strain of V. fisheri referred to as FQ-A001 uses the T6SS system. If this strain occupies a crypt with cells of a strain that doesn't use the T6SS system, it can kill those cells and completely take over the crypt. The T6SS system, which is encoded in the genome of many bacteria as a cluster of genes, works almost like a hypodermic needle that can inject toxins into neighboring cells. ""We disabled a gene in the T6SS cluster called hcp to study its role in FQ-A001's ability to kill other bacteria"" said Miyashiro. ""The hcp gene codes for a protein that makes up part of the structure of the T6SS system and also is one of the molecules secreted by the system to kill other cells. We were therefore surprised that the T6SS system still functioned in these bacteria."" The researchers then located another copy of the hcp gene outside of the T6SS gene cluster in the genome of FQ-A001 that coded for an identical protein. Disabling this copy of the gene, called hcp1, also had no impact on the bacteria's ability to kill other cells. But when the researchers disabled both copies of the hcp gene, the T6SS system could no longer function and the FQ-A001 bacteria could coexist with other strains in the light organ of a bobtail squid. The researchers could also rescue T6SS function by inducing Hcp expression in FQ-A001 bacteria with both of its natural copies of hcp disabled. ""The two copies of the hcp gene seem to be completely functionally redundant,"" said Miyashiro. ""Knowing this helps us better understand the molecular mechanisms that underlie the establishment of the symbiotic relationships between bacteria and their hosts."" ",Environment,0.08290805300206197
52,Science Daily,How E. Coli Knows How to Cause the Worst Possible Infection,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816101002.htm,"   The new discovery shows just how the foodborne pathogen knows where and when to begin colonizing the colon on its way to making you sick. By recognizing the low-oxygen environment of the large intestine, the dangerous bacterium gives itself the best odds of establishing a robust infection -- one that is punishing for the host. ""Bacterial pathogens typically colonize a specific tissue in the host. Therefore, as part of their infection strategies, bacterial pathogens precisely time deployment of proteins and toxins to these specific colonization niches in the human host. This allows the pathogens to save energy and avoid detection by our immune systems and ultimately cause disease,"" said researcher Melissa Kendall, PhD, of UVA's Department of Microbiology, Immunology and Cancer Biology. ""By knowing how bacterial pathogens sense where they are in the body, we may one day be able to prevent E. coli, as well as other pathogens, from knowing where it is inside a human host and allow it to pass through the body without causing an infection."" A Bacterial Goldilocks E. coli naturally lives in our colons, and most strains do us no harm. But there are several strains that can cause cramps, diarrhea, vomiting, even kidney failure and death. Children are at particular risk. As such, E. coli outbreaks appear periodically in the news. In July, for example, people in several states were sickened by E. coli linked to ground bison meat. Kendall and graduate student Elizabeth M. Melson have shed important light on how harmful E. coli infections establish themselves in the body. The researchers outlined a process the bacteria use to detect low oxygen levels in the large intestine and then produce proteins that allow E. coli to attach to host cells and establish infection. Oxygen actually diffuses from the intestinal tissue into the gut, and there are comparably higher levels in the small intestine than the large. E. coli specifically waits until it has reached the-low oxygen large intestine before striking. E. coli's vital asset is a small form of RNA that activates particular genes when oxygen levels are low enough, the researchers reveal. It's at this point that the infection really gets established. Thanks to this natural sensing process, the bacteria are able to establish infection and begin to manufacture harmful Shiga toxins. The researchers believe that other bacterial pathogens, such as Shigella and Salmonella, likely employ a similar control mechanism, though more work needs to be done to establish that. ""If scientists can figure how to block oxygen sensing, we may be able to prevent E. coli from making proteins that allow it to stick to our guts,"" Kendall said. ""This may be an effective strategy to limit infection, and because we are not targeting growth or survival, E. coli may not develop drug resistance -- it just doesn't know where it is."" ",Health,0.07792787457524294
53,Science Daily,Researchers Refine Guidelines for Pediatric Brain Injuries,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191417.htm,"   The Harborview Injury Prevention and Research Center (HIPRC) developed a protocol for a standardized response to these events. The guidelines, released earlier this year, are used at Harborview Medical Center, the region's Level I pediatric trauma center where about 120 pediatric brain-trauma patients receive care each year. But there are no guidelines on whether a noninvasive method of measuring carbon dioxide from patients' exhalations, known as end-tidal capnography, is as effective as drawing blood through a child's artery. In a study published Aug. 16 in JAMA, HIPRC researchers answered that clinical question: Measuring the carbon dioxide level through an artery is still the most accurate diagnostic for pediatric brain trauma. ""To maintain appropriate carbon dioxide levels, we still need to use the gold-standard approach,"" said Dr. Jen-Ting Yang, a fellow physician in anesthesiology and pain medicine at the University of Washington School of Medicine. Sampling carbon dioxide through the artery is considered the gold standard, but end-tidal carbon dioxide is widely used in clinical practice because placing an arterial line in a child is challenging and complications are not uncommon. In a study of the two ways to detect carbon dioxide, Yang and colleagues found low agreement between using an arterial line and end-tidal capnography. The researchers analyzed the cases of 137 children under age 18 who were admitted to Harborview's Pediatric Intensive Care Unit between 2011 and 2017. Among these children, researchers had 445 paired data points from both diagnostic methods. Overall, just 42 percent of paired data agreed (187 data points out of 445). The agreement was even lower during the first eight hours after admission, and with development of pediatric acute respiratory distress syndrome. Monica Vavilala, HIPRC director and a UW professor of anesthesiology and pain medicine, said given how commonly clinicians use end-tidal derived carbon dioxide levels to adjust ventilation parameters, researchers were hoping that end-tidal capnography would be just as effective. But, she said, she is glad they were able to identify the preferred method. ""We've been able to use the evidence from patients with brain trauma to provide a better roadmap for what is agreed-upon as good quality care,"" she said. ",Health,0.07613481682222813
54,Science Daily,Males of a Feather Flock Together,Health,2019-08-14,-,https://www.sciencedaily.com/releases/2019/08/190814105227.htm,"   Social bonds in animals are defined as stable, equal and cooperative relationships, comparable to human friendships. Such bromance among unrelated adult males has been described in a few species. A close relationship with another male can be advantageous, as it promises support in critical situations, such as aggressive conflicts with other group mates. Personality homophily, i.e. the tendency to like others if they are similar, has been described both in humans and in a few species of animals. The advantage is obvious: The more similar our counterpart is to us, the better we can predict his reactions. This creates trust. But what are the characteristics that should be particularly similar for a relationship to succeed? Within the framework of the Research Training Group ""Understanding Social Relationships"" of the German Primate Centre and the University of Göttingen, the team around PhD student Anja Ebenau obtained data on 24 free-living male Assamese macaques in the Phu Khieo Wildlife Sanctuary in Thailand over a period of almost two years. In close cooperation with the psychologists Lars Penke and Christoph von Borell, the individual personality of the males was described from detailed quantitative behavior protocols and questionnaires like those used in human psychology. The analyses demonstrate that the similarity of two males in the emergent personality dimensions gregariousness, aggressiveness, sociability, vigilance and confidence could be determined. It was found that the stronger the bond between two males, the more similar the animals were in terms of gregariousness. Notably, it did not matter whether the individuals are highly gregarious or not, they only had to be similar: Two rather solitary animals that avoid others can be just as close friends as two socially very central individuals. In order to exclude the possibility that it is not the other way around, i.e. that friends become more and more similar in their personality over time, the characteristics of monkeys were examined before and after they had migrated to a new group and found new social partners there. It turned out that the personality of the animals remained rather stable, i.e. did not change with a new friend. ""We assume that personality homophily is a general biological principle that is deeply rooted in the evolution of humans and animals,"" said Oliver Schülke, scientist in the Department of Behavioral Ecology at the University of Göttingen and head of the study. Individuals whose friends have a similar character have an advantage. ""One reason might be that similar personalities also have similar needs, understand each other particularly well, communicate effectively and are therefore more successful cooperation partners,"" said Schülke. A follow-up study will investigate whether coalitions of similar personalities are actually more successful in in conflicts with other males and thus defend a high rank of dominance for a longer period of time. ",Society,0.07162744948159872
55,Science Daily,"From the Tiny Testes of Flies, New Insight Into How Genes Arise",Environment,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191421.htm,"   Using fruit flies, a Rockefeller team has gained key insight into how nature's attempts at innovation play out during the development of sperm. In research described August 16 in eLife, they mapped the presence of mutations to DNA at the single-cell level, and the activity of new genes arising from such changes. ""Our work offers an unprecedented perspective on a process that enables living things to adapt and evolve, and that ultimately contributes to the diversity of life on Earth,"" says assistant professor Li Zhao, who led the research. High stakes In recent years, studies in animals from flies to humans have turned up a number of young genes that originated in the testes. These and other discoveries suggest the testes rank among the most productive sites in the body -- male or female -- for genetic innovation. This mass production of genetic novelties comes with significant risks, however. In humans, for example, a father's sperm acquires two to three times more new mutations than do a mother's eggs in the course of normal development, leaving the sperm riddled with genetic mistakes. In some cases, such mistakes may harm his offspring, or even derail the prospect of fatherhood altogether. In other words, the male stands to lose the one thing that matters in the game of evolution: the opportunity to propagate his gene pool into the next generation. But whatever the potential downside genetic experimentation has for individual males and their offspring, the dynamics of reproduction nevertheless encourage it. Potential fathers face intense pressure to attract females and fend off competitors. Any advantage, such as brighter plumage or hardier sperm, for example, can make all the difference. At the molecular level, this pressure drives an abundance of new genes within the testes. Scientists think that if these newcomers contribute to males' ability to father healthy offspring, they rapidly acquire a fixed place in the genome and may even go on to contribute elsewhere in the body. Searching cell by cell Looking a little closer, however, the picture gets blurry. Scientists haven't yet understood the dynamics by which genetic innovation occurs within the precursor cells from which sperm develops. To find out more, Zhao and researchers in her lab tagged individual cells from fly testes, then identified and decoded the RNA sequences each contained. This approach allowed them to see how the activity of specific genes changed throughout the developmental stages. Within the RNA sequences isolated from stem cells and five intermediary cell types, the researchers examined innovation from two perspectives: that of mutations and that of genes. Mutations known as substitutions, in which one letter of DNA's code is swapped for another, are most abundant early on in the development of sperm, then decrease, they found. The sperm cells' DNA-repair machinery follows a similar pattern -- it is most active early on, then tapers off -- which makes sense, according to Zhao, since the machinery is responsible for fixing errors like these. Starting from scratch Within the RNA sequences, Zhao's team hunted for a particular type of young gene -- one that arises from scratch rather than through duplication of an existing gene. For Zhao, these so-called de novo genes, which originate from sequences that originally did not code for protein, are the most interesting new genes from an evolutionary perspective. Her team found no less than 184 de novo genes, drawn from a set they had previously identified. When they examined these de novo genes, the scientists uncovered complex patterns, with certain genes showing up primarily in certain cell types, but not in others. About 15 percent of these genes appeared early on, including in the stem cell stage -- which is surprising, Zhao says, because scientists previously thought that new genes rarely show up at the start of development as this phase is tightly controlled. The most active period for de novo genes occurred midstream, however, in the so-called spermatocyte phase of developing sperm. The scientists are now interested in understanding what purpose, if any, de novo genes serve when they first arise. And although it's possible that some essentially fire at random, making no particular contribution, Zhao suspects that in many cases, these new genes play roles in the maturation of sperm cells. ""Precisely what these de novo genes are doing to move development along is an exciting open question,"" Zhao says. ",Health,0.07058111585633325
56,Science Daily,Premature Mortality Is Partly Predicted by City Neighborhood,Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815081310.htm,"   First, the anticipated: authors Luckrezia Awuor, an Environmental Applied Science and Management graduate student, and Stephanie Melles, Assistant Professor in the Department of Chemistry and Biology in the Faculty of Science, determined that premature mortality in City of Toronto neighbourhoods was predicted by a combination of unhealthy environments and embedded socioeconomic imbalances. ""It's an ongoing concern that neighbourhoods with fewer trees, lower uptake rates in cancer screening programs, higher levels of pollution and lower total income levels best predict increased mortality rates,"" says Melles. ""It's also the case that visible minorities and Indigenous peoples are most at risk of living in neighbourhoods with higher premature mortality rates. Though we expected these results, they emphasize a persistent issue of social injustice."" Put another way, residents of wealthy neighbourhoods have lower rates of premature mortality owing in part to greater tree cover and higher rates of cancer screening. Neighbourhoods with reduced cancer screening rates, more pollution and fewer trees -- also often situated close to industrial areas -- tend to be lower income. In addition, walkable neighbourhoods in closer proximity to industrial pollution had higher rates of premature mortality despite higher access to health providers. Interestingly, higher levels of traffic-related ultrafine particulates and industrial carcinogens and non-carcinogens did not always correlate to higher rates of premature mortality. This is where the surprises came in. Larger suburbs with higher pollution levels than downtown neighbourhoods showed a decreased rate of premature mortality by about 17 deaths per 100,000. This decrease is equivalent to smoking 125 fewer cigarettes per year. ""This contradicts other published findings,"" says Melles. ""Generally speaking, greater pollution correlates with greater premature mortality. In this case, the suburban neighbourhoods also had less walkability, which may minimize the health impact. It's also possible that the differences between where a person lives and where they work play a role in their overall exposure to pollutants. We have other hypotheses that were beyond the scope of our study. But we were surprised."" Also surprising: some Toronto neighbourhoods with major highways connecting the city and those along the shoreline with good tree cover, extensive green spaces and no greater measured pollution than other downtown areas showed above average rates of premature mortality. The area that contains the Rouge National Park was one of them. ""We have some unexplained variations in the outcomes,"" says Awuor. ""This tells us that we are missing a variable we haven't yet identified. Why do shoreline neighbourhoods fare worse than others? Is there some additional exposure to undetermined air or water pollutants that cross Lake Ontario? The higher rates of premature mortality are correct, but the explanations aren't there."" The authors point to one reason they can't yet pinpoint precise explanations for what they call ""residual neighbourhood patterns,"" which are those below or above expected rates of premature mortality: an insufficient number of air quality monitoring stations. With only four stations in the city, it's not possible to fully understand where and when citizens are being exposed to carcinogens and ultrafine particulates. ""We need to collect more air quality data to create a more accurate picture of exposure,"" says Awuor. ""We also need more extensive environmental policies for better tree cover and greener spaces. And we need new approaches to promoting cancer screening programs in lower income neighbourhoods. These three advances would improve the lifespan of all Toronto residents and, in particular, visible minorities and Indigenous people, who tend to live in the least green and most polluted neighbourhoods."" ",Health,0.06952564936916282
57,Stanford,Race influences professional investors’ judgments,Science,2019-08-18,-,https://news.stanford.edu/2019/08/12/race-influences-professional-investors-judgments/,"   When a black-led venture capital firm has an impressive track record, it encounters more bias from professional investors, according to new research by Stanford scholars.  Psychology Professor Jennifer Eberhardt is lead author of a new study on how race influences professional investors’ judgments. (Image credit: Nana Kofi Nti)  In a new study led by Stanford psychologist Jennifer L. Eberhardt, in collaboration with the private investment firm Illumen Capital, researchers found that when venture capital funds are managed by a person of color with strong credentials, professional investors judge them more harshly than their white counterparts with identical credentials. These findings, set to be published Aug. 12 in Proceedings of the National Academy of Sciences, suggest that funds led by people of color might paradoxically encounter more obstacles after they have proved themselves to be strong performers, said Eberhardt, who is the Morris M. Doyle Centennial Professor in the School of Humanities and Sciences. “It’s not simply a pipeline problem,” Eberhardt said. “African Americans who are most qualified, those with the best track record, are getting blocked the most.” Co-authors on the paper include Hazel Rose Markus, the Davis-Brack Professor in the Behavioral Sciences; Sarah Lyons-Padilla, a research scientist at SPARQ, a program in the Psychology Department at the School of Humanities and Sciences co-directed by Eberhardt and Markus; and Ashby Monk, executive director of the Stanford Global Projects Center. Examining disparities in investment Professional investors – people who manage money for governments, nonprofits and companies – do not appear to be hiring or investing in professional fund managers with diverse backgrounds, the researchers said, noting previous research that found that fewer than 1.3 percent of the $69.1 trillion in global assets under the four major asset classes – mutual funds, hedge funds, real estate and private equity – are managed by women and people of color. “Given the power and influence of asset allocators – professional investors – it is critical to understand how they deploy capital and make investment decisions,” Monk said. “In today’s market, investments flow through professional money managers before taking root in companies and projects. As such, if asset allocators set incorrect or biased incentives, the entire capitalist system will reflect and reinforce these biases.” To identify any potential racial bias beyond a potential pipeline problem, the researchers asked 180 asset allocators to evaluate four venture capital funds that were led by either black or white men. The professional investors were presented with four one-page summaries of the funds’ credentials and performance history. There were two versions with a black male managing partner leading either a stronger or a weaker team, and two versions with a white male managing partner leading either a stronger or a weaker team. The researchers then asked the investors to rate each firm’s overall performance, investment skills, competency, social fit and their expectations of how much the fund could raise. They also asked the investors how likely they were to take a meeting with the team and begin the investment process. How do investors judge venture capital funds? The researchers found that professional investors rated the stronger white-male-led team marginally higher than the stronger-quality black-male-led team. But when a firm’s credentials were weaker, investors favored the black-led, racially diverse team. However, the professional investors expressed that they were not likely to invest in either of the weaker teams, diverse or otherwise, the researchers said. Another finding to emerge was a disparity between how professional investors judged stronger and the weaker teams. The researchers found that when investors evaluated the white-led teams, they could easily distinguish between the stronger and weaker firms – they assigned the stronger team higher ratings and the weaker team lower ratings. However, this did not hold up for professional investors’ assessments of either of the black-led teams. When investors were asked to rate the black-led teams, they were unable to distinguish between the stronger and weaker black-led teams. “Over 98 percent of the industry is white and male,” Eberhardt said. “One explanation of this finding could be that investors have rarely seen black-led teams. They simply don’t know how to evaluate them.” Taken together, these findings suggest how black-led teams are likely to encounter more bias when their credentials are stronger, the researchers said. The results also suggest that racial disparities in investing are not only a pipeline problem. “While it is important to work on populating the pipeline, we need to think about how to continue supporting diverse teams who have already established themselves as strong performers,” Lyons-Padilla said. “Our results suggest these teams may not receive as much consideration as their white-male-led counterparts.” Leaving money on the table By undervaluing high-performing funds led by people of color or by overvaluing white-male-led funds, investors may not realize they are missing opportunities for higher financial returns, the researchers said. “In fact, asset allocators might be violating their fiduciary obligations (i.e., to generate the highest possible returns for their investors) by not investing in funds led by people of color that could produce returns as high or higher than white-male-led funds,” the researchers write. “Investors should be knowledgeable about successful firms led by people of color,” Markus said. “They can counter their biases by developing specific investment criteria and establishing a transparent process for making decisions.” Other authors on the paper include Radhika Shah, co-president at Stanford Angels & Entrepreneurs. Sid Radhakrishna and Norris A. “Daryn” Dodson IV of Illumen Capital are also co-authors. SPARQ collaborated with Illumen Capital to examine the sources and consequences of gender and racial disparities in investing. Dodson received his MBA from Stanford Graduate School of Business in 2007 and is a member of the GSB’s Management Board. This research was supported by Prudential Financial; the partnership between SPARQ is also supported by the William and Flora Hewlett Foundation. ",Society,0.0672602437729094
58,Science Daily,Relaxing of Regulations for Regenerative Medicines Has Cascading Effect Internationally,Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815140855.htm,"   Researchers warn that if just one country decides to relax regulations in the field, a heightened sense of competition can spur others to do the same. It's unclear whether this deregulation best serves competition, science or the patients. Regenerative medicine focuses on developing therapies to regenerate or replace injured, diseased or defective cells, tissues or organs, like stem cell 'treatments'. Due to the use of living cells, it can be hard to set regulations in the same way as for other drugs so differences in the rules can occur. But, according to Professor Margaret Sleeboom-Faulkner from the University of Sussex and Douglas Sipp, from the RIKEN Centre for Developmental Biology (CDB), Kobe, Japan, one thing that should always be maintained is efficacy -- the ability to reliably produce a certain therapeutic result. Professor of Social & Medical Anthropology, Margaret Sleeboom-Faulkner said: ""Regenerative medicine contains a lot of economic promise, and there's already been enormous investments into it. ""While this is good in terms of focusing on new and innovative treatments to improve healthcare, it also leaves the field particularly vulnerable to regulatory brokerage. When one country relaxes their regulations, others are tempted to do the same in order to 'keep up'. ""Competition is the last way we want medicine to be progressing."" The paper, published today in the journal Science, uses the example of South Korea as the first country to give preferential regulatory treatment to stem cell medicine. Their decision to issue a flurry of three stem cell-based medical products between 2011-12, and a fourth in 2014, attracted international scepticism for sacrificing clinical data standards in exchange for speed to market. Yet Japan, who had launched a multi-billion dollar initiative to lead the world in regenerative medicine, instead began to see South Korea as a competitor. This resulted in a change of the law in 2013, to allow regenerative medicine products a faster entry to the market. Professor Sleeboom-Faulkner explained: ""The International Society for Stem Cell Research has published general guidelines around regenerative medicines but countries regulate it in different. ""At the moment, Britain has the safety net of the EU stamp on its regulations. While we're not expecting to lose that, with Brexit looming, we could see new interpretations of the existing guidelines and find ourselves starting to compete with other countries too. ""The UK has a reputation for high standards in medicines regulation and should continue to uphold this and to be vigilant in the face of political pressures for eye-catching innovation."" Professor Sleeboom-Faulkner also notes that strict regulations can have a negative impact too, making things hard for developing countries who have to import equipment and resources. As a result, strict regulations can cause some lower income countries to relax the implementation of their regulations and take measures to be able to catch up. Of late, however, it is the wealthy countries that have become more permissive. Professor Sleeboom-Faulkner said: ""What's needed is a greater awareness that the regulations of one country can have cascading effects internationally. ""Regenerative medicines are often trialled on patients with a terminal illness, so it's hard to know the precise effect of all of this. With less stringent clinical trials and the fast-tracking of 'treatment', we'd assume standards are slipping and risks increase, but it's hard to prove that. ""Ideally, regulations should be internationally coordinated and there should be a collaborative global approach in order to maintain basic standards."" Professor Sleeboom-Faulkner is cautioning countries to be vigilant when it comes to developing treatments particularly in the face of deviant ventures such as the recent case of gene-edited foetuses in China, and the current trend of increasing flexibilities in regulations internationally. ",Health,0.06682514830632266
59,Science Daily,Neuronal Mechanism That Is Central to Human Free Recall Identified,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816105906.htm,"   ""The ripple is an amazing event in its intensity and timing. It is an orchestrated burst of synchronous activation by about 15% of hippocampal neurons -- all firing together within about a tenth of a second. It's a nerve-cell fireworks display,"" explains Prof. Rafi Malach of the Institute's Neurobiology Department. It was first revealed that they emerge during mental states of sleep and rest, and that they play an important role in rodents' spatial navigational memory. Only recently it was found that such synchronous electrical activity in large groups of neurons also occurs in the primate hippocampus during the awake state. However, until now, scientists have been kept in the dark as to the roles the ripples play in human cognition and mental activity. Humans can, of course, communicate their thoughts, but most research methods do not give scientists a detailed view of what happens at the same time within the brain. Yitzhak Norman, a PhD student in Malach's lab, who led the current research in collaboration with the group of Prof. Ashesh Mehta from the Feinstein Institute for Medical Research in the US, recruiting patients who undergo invasive recordings in the course of their medical diagnosis. In this clinical procedure, patients suffering from intractable epilepsy get electrodes implanted in multiple brain regions to locate the epileptic focus and surgically remove it. These patients freely volunteered to participate in the memory experiments while they waited in the hospital between seizures. During the experiment, the patients were presented with pictures, rich in color and visual detail, of either faces of famous people (e.g., Barack Obama, Uma Thurman) or famous monuments (e.g., the Statue of Liberty, the Leaning Tower of Pisa). The patients were asked to try to remember these pictures in as much detail as possible. After this picture-viewing stage, and following a short distraction task, they were asked, with their eyes covered, to freely recall the pictures and describe them in detail. Throughout the experiment, the talking of the patients was recorded simultaneously with their corresponding brain activity, which was revealed through the electrodes implanted both in the hippocampus, as well as other regions in the cerebral cortex. Correlating the brain's activity and the patient's verbal reports revealed a number of striking observations. First, it was found that ripple-bursts had a critical role in the free recall process: about a second or two before the patients recalled and began describing a new picture, there was a significant increase in the ripple rate anticipating their recall. Importantly, the hippocampal ripples re-expressed the content of the pictures: pictures that elicited a higher number of ripples during the viewing stage also elicited a higher number of ripples during the subsequent recall. Since brain activity was recorded simultaneously in the hippocampus and the cerebral cortex, the researchers were able to demonstrate that the ripples were synchronized with cortical activation, specifically in the visual centers of the brain where the detailed visual information is likely to be stored. Furthermore, high-level visual centers are known to be specialized in representing specific visual categories -- for example, faces are represented in one cortical region and monuments in another region. Accordingly, when patients recalled a face, for example, Barack Obama, or alternatively, a monument, such as the Eiffel Tower, cortical activity was selectively enhanced in the corresponding visual centers. Norman explains: ""An orchestrated action across a number of centers is revealed during free recall, with the hippocampus playing the role of the conductor."" The findings substantially expand our understanding of the function of the hippocampus. They emphasize the importance of synchronized neuronal group activity. The hippocampal burst, it should be remembered, involves the synchronous activation of hundreds of thousands of nerve cells. ""This constitutes a major advance in our understanding of neuronal mechanisms underlying human memory,"" summarizes Malach. ""Engraving memories, their storage and their recall are naturally dependent on a complex set of processes. However, the 'neuronal drama' of such synchronized hippocampal bursts clearly points to their central role in memory formation and recall."" ",Health,0.06621826768843801
60,Science Daily,Predictability of Parent Interaction Positively Influences Child's Development,Society,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815113732.htm,"   The newly published joint study of the University of Turku, Finland, and the University of California-Irvine, US, used a novel method for analysing interaction between a parent and child. Professor Elysia Poggi Davis from the University of Denver, Professor Tallie Z. Baram from the University of California-Irvine, and their research groups have developed a completely new tool for studying the predictability of parents' interaction signals on a micro level called Estimation of Behavioral Entropy Rate. ""The method is used to objectively calculate how predictable interaction patterns are formed from a parent's single interaction signals. Its development was based on animal studies which showed that the predictability of interaction signals are connected to the development of the offspring's brain,"" says Professor Poggi Davis. Predictable interaction has a positive effect on child's self-regulation The study showed that a higher predictability of the parent's interaction signals in infancy was associated with the child's ability to better control and regulate their own actions and emotions. ""In other words, poorly predictable or intermittent interactions were associated with a poorer self-regulation in the child. The same result was found in both Finnish and Californian data, despite their socio-economic and cultural differences,"" explains Associate Professor Riikka Korja from the University of Turku. The study supports the idea that it is important to have peaceful and uninterrupted moments of interaction with infants every day. ""Parents of young children should be provided with all the support they need to reduce stress. The parent's own self-regulation and ability of settling into their infant's early months is tied to their situation in life,"" adds Korja. At present, it is especially important to study the unpredictability of the environment and its significance as interaction between the parent and infant is threatened by surprising interruptions, such as pressure of being online all the time and stress factors related to a hectic lifestyle. ",Society,0.06492308738163599
61,Science Daily,Altered Behavior and Brain Activity Among People Wearing Bike Helmets,Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815101554.htm,"   Play a risk game and wear a helmet During an experiment, the research team had 40 people play a card game on the computer, in which participants choose between a high-risk and a lower-risk gambling option in each trial. Half of the participants wore a bike helmet under the cover-story that the eye tracker mounted on it measures their eye movements. During the game, the Jena scientists used EEG to observe what was happening in participants' brains, which led them to an exciting discovery: The so-called ""Frontal Midline Theta Power"" -- the brain activity that characterises the weighing up of alternatives in the decision-making process -- was much less pronounced in the helmet wearers. ""Therefore, we conclude that the helmet clearly has an impact on decision-making in the risk game. Obviously, participants associate a feeling of safety with wearing the bike helmet,"" explains Dr Barbara Schmidt, head of the study. Cognitive control, as psychologists call the neuronal mechanism of weighing things up, is less pronounced when wearing a helmet. ""It is possible that this is a priming effect,"" said Schmidt. ""This means that the significance we associate with a helmet automatically has a cognitive effect that is also measurable in the brain."" Influence on risk behaviour The helmet and the no-helmet group were comparable concerning their trait anxiety, which is why the discovery is not attributable to a pre-existing group difference. Barbara Schmidt continues her research on psychological factors influencing risk behaviour. In an earlier study, she had already clearly identified the ""Frontal Midline Theta Power"" as an indicator of weighing up alternatives in the decision-making process and thus laid the foundation for her current work. ""Investigating neuronal parameters allows us to learn more about why we act the way we do -- and how this can be influenced,"" says the expert from Jena. ""In the present study, we used the very subtle manipulation of wearing a bike helmet. But safety can also be suggested more clearly, for example during hypnosis."" This is the connection to another central field of work of the Jena psychologist. Schmidt is investigating the effect of hypnosis. ""It is stunning to observe how suggestions can influence brain activity,"" she says. ""In the hypnotic state, participants are very open to suggestions, for example, the suggestion of a safe place. Wearing a bike helmet can also be interpreted as a suggestion on a subconscious level. The current study shows that even such a subtle intervention significantly affects decision-making processes. Experiments like this help us to understand the mechanisms behind the effect of suggestions on decision-making processes in more depth."" ",Society,0.061833297033051376
62,Science Daily,Cannabis-Related Poison Control Calls for Massachusetts Kids Doubled After Medical Pot Legalized,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816123234.htm,"   The increase in calls to the Regional Center for Poison Control and Prevention at Boston Children's Hospital occurred despite legislative mandates for childproof packaging and warning labels, and before the recreational use of marijuana was legalized for adults. ""As states across the country enact more permissive marijuana policies, we need to do more to promote safe storage in households with children,"" says Whitehill, assistant professor of health promotion and policy and lead author of the research published in JAMA Network Open. Whitehill and former UMass Amherst graduate student Calla Harrington analyzed data from the poison control center in collaboration with staff from the center, including medical director Dr. Michele Burns and clinical fellow Dr. Michael Chary. The research team reviewed the center's data from 2009 through 2016 -- four years before and four years after medical marijuana was legalized in Massachusetts. During the study period, the poison control center received 218 calls from Massachusetts involving cannabis exposure in children and teens, from infancy to age 19, including 98 single-substance calls and 120 polysubstance calls. Those calls represented 0.15 percent of all poison control calls during that time period for that age group. ""While we're pleased to see that the incidence is relatively low, we feel these cases are preventable, and the issue needs to be on the radar of policymakers and parents, particularly now that dispensaries are open for adult-use sales,"" Whitehill says. Some highlights of the findings: The incidence of calls for single-substance cannabis exposure increased 140 percent during the study period -- from 0.4 per 100,000 population before medical marijuana was legalized to 1.1 per 100,000 population after legalization. Nearly 80 percent of the calls to the poison control center came from healthcare facilities, and, in terms of medical outcomes, most of the exposures resulted in moderate and minor effects. Four cases with major effects and no deaths were reported. A little more than a quarter of the cases were reported as unintentional, with 19.4 percent of calls involving children from infancy through age 4. Calls involving edible cannabis products increased for most age groups, including ages 15-19. Because other research has found that the proportion of teens using marijuana is remaining about the same even as marijuana laws are loosening, this finding suggests that teenagers may be caught off guard by the potentially potent effects of edibles and concentrated extracts, Whitehill says.The paper concludes, ""This study suggests that states liberalizing marijuana policies should consider strengthening regulations to prevent unintentional exposure among young children and enhancing efforts to prevent use by teenagers, with particular attention to edible cannabis products and concentrated extracts."" Whitehill says the next step is to study the impact of marijuana's legalization for adult use, which went into effect in late 2016. Two years later, in November 2018, marijuana retail stores began opening. ""Given what we've seen here,"" Whitehill says, ""I would expect the calls to the poison control center to increase even more."" ",Health,0.06179416784860359
63,Science Daily,Researcher Decodes the Brain to Help Patients With Mental Illnesses,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816105902.htm,"   A new study, published in the Journal of Neural Engineering, could improve patients' abilities to manage symptoms of mental illness. Previous research demonstrated that applying electrical stimulation at just the right time helps the brain of a patient with a severe mental illness work through difficult cognitive tasks. However, it was done in a laboratory setting, free from the complexities of real-world activities of daily living. Senior author Alik Widge, MD, Ph.D, Assistant Professor of Psychiatry at the University of Minnesota Medical School, and investigators at Massachusetts General Hospital (MGH), consisting of researchers from Brown University and MGH, including co-senior author David Borton, PhD, Assistant Professor of Engineering at Brown University, were the first to analyze patients' brain activity to detect precisely when a patient is focused and their attention is fully devoted, compared to when he or she is 'at rest'. They studied patients who were undergoing surgery for severe epilepsy, who already had measurement electrodes in the relevant brain areas. The study, which was part of DARPA's SUBNETS program, found that specific signatures and algorithms can be used to tell when someone is focused and really trying to do a task that is hard for them, indicating that they could benefit from an electrical stimulation to get an extra push. The study also demonstrates that there is no single region of the brain that can tell when someone is in this focused, effortful state. In order to detect when the patient started to focus on a cognitive task, the researchers had to analyze the information at the network level. It was essential to look at how the activity of one region coordinated with the activity of another. ""Using the same neural signals that could drive adaptive deep brain stimulation, we have shown that it is possible to detect mental states that might be amenable to closed-loop control,"" said lead author Nicole Provenza, MS, PhD candidate, Brown University. ""While further research is necessary to generalize our findings to real-world applications, we hope that this work will ultimately contribute to the development of more effective brain stimulation therapies for mental illness."" ""We want to take a patient-centered approach to treating mental illness,"" explained Widge. ""The job of a stimulator is not to take away the symptoms; its job is to help the patient manage his or her symptoms. It gives the power back to the individual and just gives them a little extra help when they need it."" There is still more work to be done, but Widge is excited to take the next step and eventually make these ideas into real products that will help people. ",Health,0.06170559311237509
64,Science Daily,"Testosterone Has a Complicated Relationship With Moral Reasoning, Study Finds",Science,2019-08-14,-,https://www.sciencedaily.com/releases/2019/08/190814161824.htm,"   Researchers at The University of Texas at Austin took a deeper look at the hormonal underpinnings of moral reasoning. Previous research has investigated moral judgment on the basis of behavioral responses and brain activity, but the current study goes beyond this to analyze the role of deep-seated biological factors, particularly testosterone. ""There's been an increasing interest in how hormones influence moral judgments in a fundamental way by regulating brain activity,"" said Bertram Gawronski, a psychology professor at UT Austin. ""To the extent that moral reasoning is at least partly rooted in deep-seated biological factors, some moral conflicts might be difficult to resolve with arguments."" The researchers borrowed the paradigm of philosophy's trolley problem to test the influence of the hormone testosterone on moral judgments. In the problem, a runaway trolley will kill five people unless someone chooses to pull a lever, redirecting the trolley to another track, where it will kill one person instead. Instead of the trolley problem itself, the researchers used 24 dilemmas associated with real-life events to simulate a situation that pits utilitarian decisions, which focus on the greater good (saving a large group of people) against deontological decisions, which focus on moral norms (avoiding action that would harm someone). Prior studies on how hormones influence moral judgment suggest that higher levels of testosterone are associated with stronger utilitarian preferences. So, the researchers put the hypothesis to the test in a double-blind study that administered testosterone to a group of 100 participants and a placebo to another 100 participants. ""The study was designed to test whether testosterone directly influences moral judgments and how,"" said Skylar Brannon, a psychology graduate student at UT Austin. ""Our design also allowed us to examine three independent aspects of moral judgment, including sensitivity to consequences, sensitivity to moral norms and general preference for action or inaction."" Unlike previous studies where heightened testosterone was linked to utilitarian judgments, the researchers were surprised to find that those who received testosterone supplements were less likely to act for the greater good, and instead became more sensitive to moral norms. However, participants with high levels of naturally occurring testosterone showed the opposite, making judgments that were less sensitive to moral norms. The study's authors think naturally occurring testosterone may be associated with certain moral judgments because people with particular personality traits tend to have different levels of testosterone. For example, people with high levels of psychopathy tend to have high levels of naturally occurring testosterone and exhibit lower sensitivity to moral norms. But this does not mean that testosterone is the cause of psychopaths' insensitivity to moral norms. If anything, testosterone seems to have the opposite effect, increasing people's sensitivity to moral norms, as found in the current study. ""The current work challenges some dominant hypotheses about the effects of testosterone on moral judgments,"" Gawronski said. ""Our findings echo the importance of distinguishing between causation and correlation in research on neuroendocrine determinants of human behavior, showing that the effects of testosterone supplements on moral judgments can be opposite to association between naturally occurring testosterone and moral judgments."" ",Society,0.06094904629226203
65,Science Daily,Early Education Setback for Summer Premature Births,Society,2019-08-13,-,https://www.sciencedaily.com/releases/2019/08/190813180830.htm,"   Previous research has already shown that children born severely prematurely, more than ten weeks early, are more likely to suffer educational problems. But the new findings highlight the disadvantage children born moderate-to-late premature may face, who were thought to be at a lower risk. New research, from the University of Leeds and Born in Bradford, looked at the complex interplay between the educational disadvantage of being born moderately premature, and when during the year a child was born, to understand whether extra support might be necessary for some children. This was in response to conversations with schools participating in the Bradford Opportunity Area programme, a Department for Education initiative aimed at improving outcomes for children and young people in areas that face significantly higher barriers to social mobility compared to their peers in other parts of the country. They indicated this was a specific area they would value further research into, to help inform their decision making. Co-author Dr Liam Hill, from the University of Leeds' School of Psychology, said: ""Some children born prematurely not only have to contend with having spent less time developing in the womb but also have to start school a year earlier than they would have, had they been born on their due date. This amounts to having less time also developing outside of the womb at the point they start school. ""This can pose additional challenges right from the start of their education, and we found this can have an immediate impact on their performance, after just one year of school."" The researchers looked at more than 10,000 school children from the Born in Bradford birth cohort study and found that the odds of a child not achieving a 'Good Level of Development' at the end of reception, if they were born prematurely, were approximately twice as high as those for children born at full term. The children found to be most at risk were those born prematurely in the summer months (June to August), who consequently started school a year earlier than expected. These children were three times less likely to reach a good level of development compared to other children born prematurely during the summer, whose early arrival didn't change the year they started school in. The researchers also analysed data that suggested that holding premature children back from starting school by a year may not compensate for being born prematurely, although they did not test that directly. Co-author Dr Katherine Pettinger, a neonatal doctor from Born in Bradford and the Bradford Teaching Hospitals NHS Foundation Trust, said: ""Whilst it seems like an obvious solution, delayed entry for premature children is not likely to compensate for being born early, as we found that within a given school year, the risks to development faced by children born premature did not vary depending on when within that school year they were born. ""To try to better support this at risk group we instead suggest that schools should be informed which of their pupils were born prematurely so they can be given extra support, particularly early on in their schooling."" According to national guidelines, once discharged from hospital severely premature children are given follow up medical support, and it is recommended that their schools are informed of their circumstances. But for moderately premature children, born between three to eight weeks early, there is no routine follow up support offered, so schools are unlikely to be informed. To try to tackle the problem the researchers suggest: Tailored advice is provided to families of premature children Learning resources are provided for teachers to support children born prematurely in the classroom Routine sharing of data between health and education servicesAs well as highlighting the risk for premature children who start school earlier than expected, the findings also show evidence for differences in development of children born prematurely at an earlier age than any previous studies. The researchers therefore argue that from an early age there is a complex interplay between health and education, which should help encourage education providers to move away from arbitrary decision making, towards a more targeted, personalised approach. Mark Douglas, Head of Children's Services at Bradford City Council, added: ""As a City of Research, Bradford City Council is committed to supporting the translation of research into policy and practice. ""This research is a great example of how we have used the latest evidence from Born in Bradford to improve the life chances for the children of Bradford, by introducing a smarter system to share information between our health and education services."" ",Society,0.06072326940065599
66,Science Daily,Wiggling It Beats a Path for a Better Performance at School,Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815101544.htm,"   Associate Professor Kate Williams designed a low-cost preschool program focussing exclusively on rhythm and movement activities linked to pathways in the brain to support attentional and emotional development. ""Think heads, shoulders, knees and toes but do the actions backwards while you sing forwards. It tricks the brain into gear,"" Assoc Prof Williams said. The Queensland study, involving 113 children from lower socioeconomic communities, measured the effectiveness of the program to boost self-regulation skills. ""Being able to control your own emotions, cognition and behaviours is an important predictor of school readiness and early school achievement,"" Assoc Prof Williams said. ""The aim is for regular sessions to be introduced into daily activities of young children to help support their attentional and emotional regulation skills, inhibition and working memory. We want all early childhood teachers to feel confident to run these fun and important activities."" The findings have been published in the international peer-reviewed journal Psychology of Music. The study is a unique investigation about preschool children and the application of a rhythm and movement program to address socioeconomic-related school readiness and achievement gaps. Assoc Prof Williams said differences in neurological processes can produce educational inequalities for young children who experience disadvantage. It's been identified by UNICEF as an international priority. The study recognises what Assoc Prof Williams describes as the 'musician advantage' -- enhanced neural plasticity and executive functioning -- particularly among children given formal musical instruction. ""The children who have music lessons from a young age are often from families who can afford them,"" she said. ""The problem is that the children who most need the musician advantage miss out because it isn't affordable for all families to access highly quality music programs."" She said the benefits of early shared book reading between parents and children have long been established. Another recent Australian study, led by Assoc Prof Williams, was the first to show that early shared music activities in the home also contributed to positive development. The preschool program involved group sessions for 30 minutes twice a week across eight weeks, with stages becoming more challenging to stimulate change and development in self-regulation skills. ",Society,0.05880029784142791
67,Science Daily,Adults With Mild Cognitive Impairment Can Learn and Benefit from Mindfulness Meditation,Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815140852.htm,"   But there may be a safe and feasible non-pharmacological treatment that may help patients living with MCI, according to a small pilot study in the current issue of the Journal of Alzheimer's Disease led by a neurologist and researcher with Wake Forest Baptist Health. ""Until treatment options that can prevent the progression to Alzheimer's are found, mindfulness meditation may help patients living with MCI,"" said Rebecca Erwin Wells, M.D., M.P.H., associate professor of neurology at Wake Forest School of Medicine, a practicing neurologist at Wake Forest Baptist Medical Center and associate director of clinical research for its Center for Integrative Medicine. ""Our study showed promising evidence that adults with MCI can learn to practice mindfulness meditation, and by doing so may boost their cognitive reserve."" Mindfulness means maintaining a moment-by-moment, non-judgemental awareness of thoughts, feelings, bodily sensations, and surrounding environment. ""While the concept of mindfulness meditation is simple, the practice itself requires complex cognitive processes, discipline and commitment,"" Wells explained. ""This study suggests that the cognitive impairment in MCI is not prohibitive of what is required to learn this new skill."" Research has demonstrated that high levels of chronic stress negatively impact the hippocampus, a part of the brain involved in memory and learning, and are associated with increased incidence of MCI and Alzheimer's. Other studies have indicated that non-drug interventions such as aerobic exercise can have positive effects on cognition, stress levels and the brain. To test whether a mindfulness-based stress-reduction (MBSR) program could benefit adults with MCI, the study team enlisted 14 men and women between the ages of 55 and 90 with clinically diagnosed MCI and randomized them to either an eight-week course involving mindfulness meditation and yoga or a ""waiting list"" control group. The researchers previously reported that the nine participants who completed the MBSR program showed trends toward improvements on measures of cognition and well-being and indications of positive impacts on the hippocampus as well as other areas of the brain associated with cognitive decline. The newly published study adds context to those quantitative findings with a qualitative analysis of the MBSR participants' responses in interviews conducted at the end of the eight-week course. ""While the MBSR course was not developed or structured to directly address MCI, the qualitative interviews revealed new and important findings specific to MCI,"" Wells said. ""The participants' comments and ratings showed that most of them were able to learn the key tenets of mindfulness, demonstrating that the memory impairment of MCI does not preclude learning such skills."" Those participants who practiced at least 20 minutes a day were most likely to have understood the underlying concepts of mindfulness, Wells noted. The limitations of the study include the small sample size and that the results may not generalize to all patients with MCI, as two-thirds of the participants in this study had a college education or more. Additional research is needed to further test the preliminary hypotheses contained in this study. The research was originally conducted at Beth Israel Deaconess Medical Center in Boston and Harvard Medical School. The study was supported by the Harvard Medical School Osher Research Center, the Division of General Medicine and Primary Care at Beth Israel Deaconess Medical Center and the National Center for Complementary and Integrative Health of the National Institutes of Health (award T32AT000051 and grants K24 AT004095, K24 AT000589, K24 AT004965, K01 AT003459, K23 AT008406, K23 AT009218 and K01 AT008219). ",Health,0.05277749136665577
68,Science Daily,"'Silent' Strokes Common After Surgery, Linked to Cognitive Decline",Health,2019-08-15,-,https://www.sciencedaily.com/releases/2019/08/190815212759.htm,"   While an overt stroke causes obvious symptoms, such as weakness in one arm or speech problems that last more than a day, a covert stroke is not obvious except on brain scans, such as MRI. Each year, approximately 0.5 per cent of the 50 million people age 65 years or greater worldwide who have major, non-cardiac surgery will suffer an overt stroke, but until now little was known about the incidence or impacts of silent stroke after surgery. The results of the NeuroVISION study were published today in The Lancet. ""We've found that 'silent' covert strokes are actually more common than overt strokes in people aged 65 or older who have surgery,"" said Dr. PJ Devereaux, co-principal investigator of the NeuroVISION study. Dr. Devereaux is a cardiologist at Hamilton Health Sciences (HHS), professor in the departments of health research methods, evidence, and impact, and medicine at McMaster University, and a senior scientist at the Population Health Research Institute of McMaster University and HHS. Dr. Devereaux and his team found that one in 14 people over age 65 who had elective, non-cardiac surgery had a silent stroke, suggesting that as many as three million people in this age category globally suffer a covert stroke after surgery each year. NeuroVISION involved 1,114 patients aged 65 years and older from 12 centres in North and South America, Asia, New Zealand, and Europe. All patients received an MRI within nine days of their surgery to look for imaging evidence of silent stroke. The research team followed patients for one year after their surgery to assess their cognitive capabilities. They found that people who had a silent stroke after surgery were more likely to experience cognitive decline, perioperative delirium, overt stroke or transient ischaemic attack within one year, compared to patients who did not have a silent stroke. ""Over the last century, surgery has greatly improved the health and the quality of life of patients around the world,"" said Dr. Marko Mrkobrada, an associate professor of medicine at University of Western Ontario and co-principal investigator for the NeuroVISION study. ""Surgeons are now able to operate on older and sicker patients thanks to improvements in surgical and anesthetic techniques. Despite the benefits of surgery, we also need to understand the risks."" ""Vascular brain injuries, both overt and covert, are more frequently being detected, recognized and prevented through research funded by our Institute and CIHR,"" says Dr. Brian Rowe, scientific director of the Institute of Circulatory and Respiratory Health, Canadian Institutes of Health Research (CIHR). ""The NeuroVISION Study provides important insights into the development of vascular brain injury after surgery, and adds to the mounting evidence of the importance of vascular health on cognitive decline. The results of NeuroVISION are important and represent a meaningful discovery that will facilitate tackling the issue of cognitive decline after surgery."" ",Health,0.05246045477018925
69,Science Daily,"Children With Mild Asthma Can Use Inhalers as Needed, Study Suggests",Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191419.htm,"   The steroid inhaler lowers inflammation, and the bronchodilator -- also known as a rescue inhaler -- relaxes the airway during an asthma attack to quickly make breathing easier. The research focused on African American children, who are disproportionately affected by asthma. The study appears in the Journal of Allergy and Clinical Immunology: In Practice. ""We were pleased to find that as-needed treatment based on symptoms can deliver similar asthma control with less medication,"" said first author Kaharu Sumino, MD, an associate professor of medicine. ""Patients in the group that used both inhalers as needed used about one-fourth the steroid dose of the group that inhaled a prescribed daily amount. We also were pleased to see that the patients and families felt that they had more ownership over their asthma management when practicing as-needed treatment."" In the U.S., about 6.2 million children under age 18 have asthma. Among white children, about 7.4 percent have asthma. Among African American children, that statistic is almost double, at 13.4 percent. According to the Allergy and Asthma Foundation of America, asthma attacks account for 1.8 million emergency room visits per year, and African Americans are three times more likely than average to be hospitalized due to asthma. The study was conducted by primary care doctors at multiple pediatric practices throughout the St. Louis area, suggesting the strategy is widely applicable in a primary care setting and not just effective when implemented by researchers at a single academic medical center. Past clinical trials conducted in the highly controlled settings of academic medical centers had suggested that the medication-as-needed approach worked just as well as a traditional strategy of daily scheduled steroid treatments with a rescue inhaler as needed. But how well that strategy might transfer to individual community medical practices had been an open question. Not only is this alternative, as-needed, symptom-based strategy effective when administered by the children's primary care doctors, the researchers found that approach reduced the amount of steroid medication the children took monthly by almost 75 percent. The study included 206 African American children 6 to 17 years of age with mild asthma that was adequately controlled with asthma controller steroid medication. Participants saw their own pediatricians at 12 primary care providers throughout St. Louis. The patients were randomly assigned to one of two groups. Each participant in one group was advised to take a dose from an inhaler containing the steroid beclomethasone as needed when symptoms arose, along with the rescue bronchodilator albuterol. Symptoms that might prompt the use of medication include shortness of breath, tightness in the chest, coughing, wheezing and difficulty performing physical activities. Each participant in the second group was advised to take a specific inhaled dose of the steroid beclomethasone daily, regardless of symptoms, plus the rescue bronchodilator as needed in response to symptoms, as has been the standard recommendation for almost 30 years by the Global Initiative for Asthma guidelines. At the end of the one-year study, the researchers found no differences between groups in surveys of how well the patients' asthma was controlled, as well as no differences in breathing tests that measure lung function. There also were no differences in the number of participants who sought extra medical care -- such as office or emergency room visits -- for asthma attacks. As might be expected, the group taking daily beclomethasone, an inhaled corticosteroid, used more of the medication per month than those in the symptom-based group. On average, children in the daily-use group used 1,961 micrograms per month, while the symptom-based group used 526 micrograms per month, cutting the amount of this medication by almost three-fourths. The reduced amount is desirable, according to the investigators, because steroids have side effects that include stunted growth. ""Many families are concerned about the cost of this medication as well as the growth-related side effects, and stop taking their steroid medicine altogether. So it's nice to show that less medication -- used as needed -- is just as effective,"" Sumino said. ""This as-needed steroid plus rescue albuterol strategy is now recommended in the Global Initiative for Asthma guidelines as one of the options for the treatment of mild asthma. Given the result of our study and others, primary care doctors may tell their patients with mild asthma that they have an alternative effective strategy other than taking the inhaled steroid every day, if they prefer not to do so."" The researchers also noted that children and caregivers in the group taking medication as needed reported that they felt they were actively managing their asthma care rather than passively relying on doctors' orders. The symptom-based strategy, in other words, gave them a sense of ownership over their asthma management, which is important in the long-term control of the disease, according to the Sumino and her colleagues. This work was supported by a Patient Centered Outcomes Research Institute (PCORI) Award, number AS-1307-05588. ",Health,0.052170792713696956
70,Science Daily,How Stress Can Curb the Desire to Eat in an Animal Model,Health,2019-08-16,-,https://www.sciencedaily.com/releases/2019/08/190816191450.htm,"   The scientists believe their research could aid efforts to develop treatments for a serious eating disorder called anorexia nervosa, which has the highest mortality rate of any mental disorder, according to the National Institute of Mental Health. People with anorexia nervosa avoid food, severely restrict food, or eat very small quantities of only certain foods. Even when they are dangerously underweight, they may see themselves as overweight. ""We have identified a part of the brain in a mouse model that controls the impact of emotions on eating,"" said Qingchun Tong, PhD, the study's senior author and an associate professor in the Center for Metabolic and Degenerative Disease at McGovern Medical School at UTHealth. Because mice and humans have similar nervous systems, Tong, the Cullen Chair in Molecular Medicine at UTHealth, believes their findings could shed light on the part of the human brain that regulates hunger. The investigators believe they are among the first to demonstrate the role of this neurocircuit in the regulation of both stress and hunger. While previous research has established that stress can both reduce and increase a person's desire to eat, the neural mechanisms that act on the regulation of eating by stress-related responses largely remain a mystery. Tong's team focused on a neurocircuit connecting two parts of the mouse brain: the paraventricular hypothalamus, an eating-related zone in the brain, and the ventral lateral septum, an emotional zone in the brain. The neurocircuit acts as an on/off switch. When researchers activated the neurocircuit, there was an increase in anxiety levels and a decrease in appetite. Conversely, when the investigators inhibited the neurocircuit, anxiety levels dropped and hunger increased. The scientists used a research technique called optogenetics to turn the neurons in question on and off. Yuanzhong Xu, PhD, the study's lead author and an instructor at McGovern Medical School, said additional preclinical tests are needed to confirm their findings. Coauthors from UTHealth include Yungang Lu, PhD; Ryan Cassidy; Leandra Mangieri, PhD; Canjun Zhu, PhD; Zhiying Jiang, PhD; Xugen Huang, PhD; and Nicholas Justice, PhD. Also contributing to the paper were Yong Xu, MD, PhD, and Benjamin Arenkiel, PhD, of Baylor College of Medicine. Tong and Justice are on the faculty of The University of Texas MD Anderson Cancer Center UTHealth Graduate School of Biomedical Sciences. ",Health,0.05103188953268209
