,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,ACM,Research Alliance Cements Split on AI Ethics,ACM,2019-08-23,-,https://www.insidehighered.com/news/2019/08/23/new-research-alliance-cements-split-ai-ethics,"    Research Alliance Cements Split on AI Ethics     Inside Higher EdDavid MatthewsAugust 23, 2019   Germany, France, and Japan have partnered to underwrite research into human-centered artificial intelligence (AI), designed to respect privacy and transparency. The three countries issued a joint call for proposals, supported by an initial 7.4-million-euro ($8.2-million) in funding. The German Research Foundation's Susanne Sangenstedt said the partnering nations have common beliefs and standards for ethical AI development, with the call for research proposals seeking projects on AI democratization, data integrity for fairness, AI ethics to avoid gender/age discrimination, and technologies like machine learning, computer vision, and data mining. Holger Hoos at Leiden University in the Netherlands said China has put AI development under the control of the government-state, while the U.S. has permitted the private sector to oversee development. In contrast, Europe's approach has been to try to strike a balance among ""government, industry, and individual.""                 ",Computer Science,0.17696154796222385
1,ACM,Tech Tool Aims to Stop 'Bottlenecks' in Reaching U.K. Homeless,ACM,2019-08-28,-,https://www.reuters.com/article/us-britain-homelessness-technology/tech-tool-aims-to-stop-bottlenecks-in-reaching-uk-homeless-idUSKCN1VI2B4,"    Tech Tool Aims to Stop 'Bottlenecks' in Reaching U.K. Homeless     ReutersSonia ElksAugust 28, 2019   The Homeless Link charity in the U.K. hopes to use a new algorithm to extend its aid to the homeless. The organization’s StreetLink app allows members of the public to flag the locations of “rough sleepers” (the homeless), and to request help for them from relevant local authorities or outreach organizations. The algorithm, developed by a team of students and technology workers, is designed to identify the most useful alerts in order to boost the number of homeless people offered assistance. Matt Harrison of the Streetlink organization, which works to connect the homeless with local services that can help them, said, ""If we can prioritize our scarce resources on the alerts which are mostly likely to result in a homeless person being found and helped, then we should do that.""                 ",Computer Science,0.17686633635870452
2,MIT News,MIT’s fleet of autonomous boats can now shapeshift,Research,2019-08-29,-,http://news.mit.edu/2019/roboats-autonomous-connect-assemble-0829,"  MIT’s fleet of robotic boats has been updated with new capabilities to “shapeshift,” by autonomously disconnecting and reassembling into a variety of configurations, to form floating structures in Amsterdam’s many canals. The autonomous boats — rectangular hulls equipped with sensors, thrusters, microcontrollers, GPS modules, cameras, and other hardware — are being developed as part of the ongoing “Roboat” project between MIT and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute). The project is led by MIT professors Carlo Ratti, Daniela Rus, Dennis Frenchman, and Andrew Whittle. In the future, Amsterdam wants the roboats to cruise its 165 winding canals, transporting goods and people, collecting trash, or self-assembling into “pop-up” platforms — such as bridges and stages — to help relieve congestion on the city’s busy streets. In 2016, MIT researchers tested a roboat prototype that could move forward, backward, and laterally along a preprogrammed path in the canals. Last year, researchers designed low-cost, 3-D-printed, one-quarter scale versions of the boats, which were more efficient and agile, and came equipped with advanced trajectory-tracking algorithms. In June, they created an autonomous latching mechanism that let the boats target and clasp onto each other, and keep trying if they fail. In a new paper presented at the last week’s IEEE International Symposium on Multi-Robot and Multi-Agent Systems, the researchers describe an algorithm that enables the roboats to smoothly reshape themselves as efficiently as possible. The algorithm handles all the planning and tracking that enables groups of roboat units to unlatch from one another in one set configuration, travel a collision-free path, and reattach to their appropriate spot on the new set configuration. In demonstrations in an MIT pool and in computer simulations, groups of linked roboat units rearranged themselves from straight lines or squares into other configurations, such as rectangles and “L” shapes. The experimental transformations only took a few minutes. More complex shapeshifts may take longer, depending on the number of moving units — which could be dozens — and differences between the two shapes.  “We’ve enabled the roboats to now make and break connections with other roboats, with hopes of moving activities on the streets of Amsterdam to the water,” says Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “A set of boats can come together to form linear shapes as pop-up bridges, if we need to send materials or people from one side of a canal to the other. Or, we can create pop-up wider platforms for flower or food markets.” Joining Rus on the paper are: Ratti, director of MIT’s Senseable City Lab, and, also from the lab, first author Banti Gheneti, Ryan Kelly, and Drew Meyers, all researchers; postdoc Shinkyu Park; and research fellow Pietro Leoni. Collision-free trajectories For their work, the researchers had to tackle challenges with autonomous planning, tracking, and connecting groups of roboat units. Giving each unit unique capabilities to, for instance, locate each other, agree on how to break apart and reform, and then move around freely, would require complex communication and control techniques that could make movement inefficient and slow. To enable smoother operations, the researchers developed two types of units: coordinators and workers. One or more workers connect to one coordinator to form a single entity, called a “connected-vessel platform” (CVP). All coordinator and worker units have four propellers, a wireless-enabled microcontroller, and several automated latching mechanisms and sensing systems that enable them to link together. Coordinators, however, also come equipped with GPS for navigation, and an inertial measurement unit (IMU), which computes localization, pose, and velocity. Workers only have actuators that help the CVP steer along a path. Each coordinator is aware of and can wirelessly communicate with all connected workers. Structures comprise multiple CVPs, and individual CVPs can latch onto one another to form a larger entity. During shapeshifting, all connected CVPs in a structure compare the geometric differences between its initial shape and new shape. Then, each CVP determines if it stays in the same spot and if it needs to move. Each moving CVP is then assigned a time to disassemble and a new position in the new shape. Each CVP uses a custom trajectory-planning technique to compute a way to reach its target position without interruption, while optimizing the route for speed. To do so, each CVP precomputes all collision-free regions around the moving CVP as it rotates and moves away from a stationary one. After precomputing those collision-free regions, the CVP then finds the shortest trajectory to its final destination, which still keeps it from hitting the stationary unit. Notably, optimization techniques are used to make the whole trajectory-planning process very efficient, with the precomputation taking little more than 100 milliseconds to find and refine safe paths. Using data from the GPS and IMU, the coordinator then estimates its pose and velocity at its center of mass, and wirelessly controls all the propellers of each unit and moves into the target location. In their experiments, the researchers tested three-unit CVPs, consisting of one coordinator and two workers, in several different shapeshifting scenarios. Each scenario involved one CVP unlatching from the initial shape and moving and relatching to a target spot around a second CVP. Three CVPs, for instance, rearranged themselves from a connected straight line — where they were latched together at their sides — into a straight line connected at front and back, as well as an “L.” In computer simulations, up to 12 roboat units rearranged themselves from, say, a rectangle into a square or from a solid square into a Z-like shape.  Scaling up Experiments were conducted on quarter-sized roboat units, which measure about 1 meter long and half a meter wide. But the researchers believe their trajectory-planning algorithm will scale well in controlling full-sized units, which will measure about 4 meters long and 2 meters wide. The researchers hope to use the roboats to form into a dynamic “bridge” across a 60-meter canal between the NEMO Science Museum in Amsterdam’s city center and an area that’s under development. Called RoundAround, the idea is to employ roboats to sail in a continuous circle across the canal, picking up and dropping off passengers at docks and stopping or rerouting when they detect anything in the way. Currently, walking around that waterway takes about 10 minutes, but the bridge can cut that time to around two minutes. This is still an explorative concept. “This will be the world’s first bridge comprised of a fleet of autonomous boats,” Ratti says. “A regular bridge would be super expensive, because you have boats going through, so you’d need to have a mechanical bridge that opens up or a very high bridge. But we can connect two sides of canal [by using] autonomous boats that become dynamic, responsive architecture that float on the water.” To reach that goal, the researchers are further developing the roboats to ensure they can safely hold people, and are robust to all weather conditions, such as heavy rain. They’re also making sure the roboats can effectively connect to the sides of the canals, which can vary greatly in structure and design. ",Computer Science,0.17426779937633452
3,MIT News,MIT engineers build advanced microprocessor out of carbon nanotubes,Electronics and Technology,2019-08-28,-,http://news.mit.edu/2019/carbon-nanotubes-microprocessor-0828,"  After years of tackling numerous design and manufacturing challenges, MIT researchers have built a modern microprocessor from carbon nanotube transistors, which are widely seen as a faster, greener alternative to their traditional silicon counterparts. The microprocessor, described today in the journal Nature, can be built using traditional silicon-chip fabrication processes, representing a major step toward making carbon nanotube microprocessors more practical. Silicon transistors — critical microprocessor components that switch between 1 and 0 bits to carry out computations — have carried the computer industry for decades. As predicted by Moore’s Law, industry has been able to shrink down and cram more transistors onto chips every couple of years to help carry out increasingly complex computations. But experts now foresee a time when silicon transistors will stop shrinking, and become increasingly inefficient. Making carbon nanotube field-effect transistors (CNFET) has become a major goal for building next-generation computers. Research indicates CNFETs have properties that promise around 10 times the energy efficiency and far greater speeds compared to silicon. But when fabricated at scale, the transistors often come with many defects that affect performance, so they remain impractical. The MIT researchers have invented new techniques to dramatically limit defects and enable full functional control in fabricating CNFETs, using processes in traditional silicon chip foundries. They demonstrated a 16-bit microprocessor with more than 14,000 CNFETs that performs the same tasks as commercial microprocessors. The Nature paper describes the microprocessor design and includes more than 70 pages detailing the manufacturing methodology. The microprocessor is based on the RISC-V open-source chip architecture that has a set of instructions that a microprocessor can execute. The researchers’ microprocessor was able to execute the full set of instructions accurately. It also executed a modified version of the classic “Hello, World!” program, printing out, “Hello, World! I am RV16XNano, made from CNTs.” “This is by far the most advanced chip made from any emerging nanotechnology that is promising for high-performance and energy-efficient computing,” says co-author Max M. Shulaker, the Emanuel E Landsman Career Development Assistant Professor of Electrical Engineering and Computer Science (EECS) and a member of the Microsystems Technology Laboratories. “There are limits to silicon. If we want to continue to have gains in computing, carbon nanotubes represent one of the most promising ways to overcome those limits. [The paper] completely re-invents how we build chips with carbon nanotubes.” Joining Shulaker on the paper are: first author and postdoc Gage Hills, graduate students Christian Lau, Andrew Wright, Mindy D. Bishop, Tathagata Srimani, Pritpal Kanhaiya, Rebecca Ho, and Aya Amer, all of EECS; Arvind, the Johnson Professor of Computer Science and Engineering and a researcher in the Computer Science and Artificial Intelligence Laboratory; Anantha Chandrakasan, the dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science; and Samuel Fuller, Yosi Stein, and Denis Murphy, all of Analog Devices. Fighting the “bane” of CNFETs The microprocessor builds on a previous iteration designed by Shulaker and other researchers six years ago that had only 178 CNFETs and ran on a single bit of data. Since then, Shulaker and his MIT colleagues have tackled three specific challenges in producing the devices: material defects, manufacturing defects, and functional issues. Hills did the bulk of the microprocessor design, while Lau handled most of the manufacturing. For years, the defects intrinsic to carbon nanotubes have been a “bane of the field,” Shulaker says. Ideally, CNFETs need semiconducting properties to switch their conductivity on an off, corresponding to the bits 1 and 0. But unavoidably, a small portion of carbon nanotubes will be metallic, and will slow or stop the transistor from switching. To be robust to those failures, advanced circuits will need carbon nanotubes at around 99.999999 percent purity, which is virtually impossible to produce today.   The researchers came up with a technique called DREAM (an acronym for “designing resiliency against metallic CNTs”), which positions metallic CNFETs in a way that they won’t disrupt computing. In doing so, they relaxed that stringent purity requirement by around four orders of magnitude — or 10,000 times — meaning they only need carbon nanotubes at about 99.99 percent purity, which is currently possible. Designing circuits basically requires a library of different logic gates attached to transistors that can be combined to, say, create adders and multipliers — like combining letters in the alphabet to create words. The researchers realized that the metallic carbon nanotubes impacted different pairings of these gates differently. A single metallic carbon nanotube in gate A, for instance, may break the connection between A and B. But several metallic carbon nanotubes in gates B may not impact any of its connections. In chip design, there are many ways to implement code onto a circuit. The researchers ran simulations to find all the different gate combinations that would be robust and wouldn’t be robust to any metallic carbon nanotubes. They then customized a chip-design program to automatically learn the combinations least likely to be affected by metallic carbon nanotubes. When designing a new chip, the program will only utilize the robust combinations and ignore the vulnerable combinations. “The ‘DREAM’ pun is very much intended, because it’s the dream solution,” Shulaker says. “This allows us to buy carbon nanotubes off the shelf, drop them onto a wafer, and just build our circuit like normal, without doing anything else special.” Exfoliating and tuning CNFET fabrication starts with depositing carbon nanotubes in a solution onto a wafer with predesigned transistor architectures. However, some carbon nanotubes inevitably stick randomly together to form big bundles — like strands of spaghetti formed into little balls — that form big particle contamination on the chip.   To cleanse that contamination, the researchers created RINSE (for “removal of incubated nanotubes through selective exfoliation”). The wafer gets pretreated with an agent that promotes carbon nanotube adhesion. Then, the wafer is coated with a certain polymer and dipped in a special solvent. That washes away the polymer, which only carries away the big bundles, while the single carbon nanotubes remain stuck to the wafer. The technique leads to about a 250-times reduction in particle density on the chip compared to similar methods. Lastly, the researchers tackled common functional issues with CNFETs. Binary computing requires two types of transistors: “N” types, which turn on with a 1 bit and off with a 0 bit, and “P” types, which do the opposite. Traditionally, making the two types out of carbon nanotubes has been challenging, often yielding transistors that vary in performance. For this solution, the researchers developed a technique called MIXED (for “metal interface engineering crossed with electrostatic doping”), which precisely tunes transistors for function and optimization. In this technique, they attach certain metals to each transistor — platinum or titanium — which allows them to fix that transistor as P or N. Then, they coat the CNFETs in an oxide compound through atomic-layer deposition, which allows them to tune the transistors’ characteristics for specific applications. Servers, for instance, often require transistors that act very fast but use up energy and power. Wearables and medical implants, on the other hand, may use slower, low-power transistors.   The main goal is to get the chips out into the real world. To that end, the researchers have now started implementing their manufacturing techniques into a silicon chip foundry through a program by Defense Advanced Research Projects Agency, which supported the research. Although no one can say when chips made entirely from carbon nanotubes will hit the shelves, Shulaker says it could be fewer than five years. “We think it’s no longer a question of if, but when,” he says. The work was also supported by Analog Devices, the National Science Foundation, and the Air Force Research Laboratory. ",Electronics and Technology,0.16397905147888495
4,MIT News,Ultrathin 3-D-printed films convert energy of one form into another,Electronics and Technology,2019-08-28,-,http://news.mit.edu/2019/3-d-printed-piezoelectric-films-0828,"  MIT researchers have developed a simple, low-cost method to 3-D print ultrathin films with high-performing “piezoelectric” properties, which could be used for components in flexible electronics or highly sensitive biosensors. Piezoelectric materials produce a voltage in response to physical strain, and they respond to a voltage by physically deforming. They’re commonly used for transducers, which convert energy of one form into another. Robotic actuators, for instance, use piezoelectric materials to move joints and parts in response to an electrical signal. And various sensors use the materials to convert changes in pressure, temperature, force, and other physical stimuli, into a measurable electrical signal. Researchers have been trying for years to develop piezoelectric ultrathin films that can be used as energy harvesters, sensitive pressure sensors for touch screens, and other components in flexible electronics. The films could also be used as tiny biosensors that are sensitive enough to detect the presence of molecules that are biomarkers for certain diseases and conditions. The material of choice for those applications is often a type of ceramic with a crystal structure that resonates at high frequencies due to its extreme thinness. (Higher frequencies basically translate to faster speeds and higher sensitivity.) But, with traditional fabrication techniques, creating ceramic ultrathin films is a complex and expensive process. In a paper recently published in the journal Applied Materials and Interfaces, the MIT researchers describe a way to 3-D print ceramic transducers about 100 nanometers thin by adapting an additive manufacturing technique for the process that builds objects layer by layer, at room temperature. The films can be printed in flexible substrates with no loss in performance, and can resonate at around 5 gigahertz, which is high enough for high-performance biosensors. “Making transducing components is at the heart of the technological revolution,” says Luis Fernando Velásquez-García, a researcher in the Microsystems Technology Laboratories (MTL) in the Department of Electrical Engineering and Computer Science. “Until now, it’s been thought 3-D-printed transducing materials will have poor performances. But we’ve developed an additive fabrication method for piezoelectric transducers at room temperature, and the materials oscillate at gigahertz-level frequencies, which is orders of magnitude higher than anything previously fabricated through 3-D printing.” Joining Velásquez-García on the paper is first author Brenda García-Farrera of MTL and the Monterrey Institute of Technology and Higher Education in Mexico. Electrospraying nanoparticles Ceramic piezoelectric thin films, made of aluminum nitride or zinc oxide, can be fabricated through physical vapor deposition and chemical vapor deposition. But those processes must be completed in sterile clean rooms, under high temperature and high vacuum conditions. That can be a time-consuming, expensive process. There are lower-cost 3-D-printed piezoelectric thin films available. But those are fabricated with polymers, which must be “poled”— meaning they must be given piezoelectric properties after they’re printed. Moreover, those materials usually end up tens of microns thick and thus can’t be made into ultrathin films capable of high-frequency actuation. The researchers’ system adapts an additive fabrication technique, called near-field electrohydrodynamic deposition (NFEHD), which uses high electric fields to eject a liquid jet through a nozzle to print an ultrathin film. Until now, the technique has not been used to print films with piezoelectric properties. The researchers’ liquid feedstock — raw material used in 3-D printing — contains zinc oxide nanoparticles mixed with some inert solvents, which forms into a piezoelectric material when printed onto a substrate and dried. The feedstock is fed through a hollow needle in a 3-D printer. As it prints, the researchers apply a specific bias voltage to the tip of the needle and control the flow rate, causing the meniscus — the curve seen at the top of a liquid — to form into a cone shape that ejects a fine jet from its tip. The jet is naturally inclined to break into droplets. But when the researchers bring the tip of the needle close to the substrate — about a millimeter — the jet doesn’t break apart. That process prints long, narrow lines on a substrate. They then overlap the lines and dry them at about 76 degrees Fahrenheit, hanging upside down. Printing the film precisely that way creates an ultrathin film of crystal structure with piezoelectric properties that resonates at about 5 gigahertz. “If anything of that process is missing, it doesn’t work,” Velásquez-García says. Using microscopy techniques, the team was able to prove that the films have a much stronger piezoelectric response — meaning the measurable signal it emits — than films made through traditional bulk fabrication methods. Those methods don’t really control the film’s piezoelectric axis direction, which determines the material’s response. “That was a little surprising,” Velásquez-García says. “In those bulk materials, they may have inefficiencies in the structure that affect performance. But when you can manipulate materials at the nanoscale, you get a stronger piezoelectric response.” Low-cost sensors Because the piezoelectric ultrathin films are 3-D printed and resonate at very high frequencies, they can be leveraged to fabricate low-cost, highly sensitive sensors. The researchers are currently working with colleagues in Monterrey Tec as part of a collaborative program in nanoscience and nanotechnology, to make piezoelectric biosensors to detect biomarkers for certain diseases and conditions. A resonating circuit is integrated into these biosensors, which makes the piezoelectric ultrathin film oscillate at a specific frequency, and the piezoelectric material can be functionalized to attract certain molecule biomarkers to its surface. When the molecules stick to the surface, it causes the piezoelectric material to slightly shift the frequency oscillations of the circuit. That small frequency shift can be measured and correlated to a certain amount of the molecule that piles up on its surface. The researchers are also developing a sensor to measure the decay of electrodes in fuel cells. That would function similarly to the biosensor, but the shifts in frequency would correlate to the degradation of a certain alloy in the electrodes. “We’re making sensors that can diagnose the health of fuel cells, to see if they need to be replaced,” Velásquez-García says. “If you assess the health of these systems in real time, you can make decisions about when to replace them, before something serious happens.” ",Electronics and Technology,0.16246279803318173
5,ACM,Giving Smart Vehicles Their Sense of Direction,ACM,2019-08-27,-,https://www.udel.edu/udaily/2019/august/smart-vehicles-sense-direction-algorithms-guoquan-huang/,"       Giving Smart Vehicles Their Sense of Direction     University of DelawareJulie StewartAugust 27, 2019   The University of Delaware's Guoquan Huang has developed algorithms to advance simultaneous localization and mapping (SLAM) as a technique for giving autonomous vehicles a sense of direction. Huang employs visual-inertial navigation systems, integrating cameras and inertial sensors to measure orientation and acceleration. The researcher then calculates motion and localization from the data collected by these components. Huang and colleagues have more accurately combined inertial measurements than previously possible via discrete-integration calculus, and open-sourced it on GitHub. The team also reconfigured SLAM as a formula for computing small increments of movement by sensor-equipped robots.                 ",Computer Science,0.15412933865848555
6,IEEE,AI at the Speed of Light,Electronics and Technology,2019-08-29,-,https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light,"      Neural networks shine for solving tough problems such as facial and voice recognition, but conventional electronic versions are limited in speed and hungry for power. In theory, optics could beat digital electronic computers in the matrix calculations used in neural networks. However, optics had been limited by their inability to do some complex calculations that had required electronics. Now new experiments show that all-optical neural networks can tackle those problems.  The key attraction of neural networks is their massive interconnections among processors, comparable to the complex interconnections among neurons in the brain. This lets them perform many operations simultaneously, like the human brain does when looking at faces or listening to speech, making them more efficient for facial and voice recognition than traditional electronic computers that execute one instruction at a time. Today  electronic neural networks have reached eight million neurons, but their future use in artificial intelligence may be limited by their high power usage and limited parallelism in connections. Optical connections through lenses are inherently parallel. The lens in your eye simultaneously focuses light from across your field of view onto the retina in the back of your eye, where an array of light-detecting nerve cells detects the light. Each cell then relays the signal it receives to neurons in the brain that process the visual signals to show us an image. Glass lenses process optical signals by focusing light, which performs a complex mathematical operation called a Fourier transform that preserves the information in the original scene but rearranges is completely. One use of Fourier transforms is converting time variations in signal intensity into a plot of the frequencies present in the signal. The military used this trick in the 1950s to convert raw radar return signals recorded by an aircraft in flight into a three-dimensional image of the landscape viewed by the plane. Today that conversion is done electronically, but the vacuum-tube computers of the 1950s were not up to the task. Development of neural networks for artificial intelligence started with electronics, but their AI applications have been limited by their slow processing and need for extensive computing resources. Some researchers have developed hybrid neural networks, in which optics perform simple linear operations, but electronics perform more complex nonlinear calculations. Now two groups have demonstrated simple all-optical neural networks that do all processing with light. In May, Wolfram Pernice of the Institute of Physics at the University of Münster in Germany and colleagues reported testing an all-optical ""neuron"" in which signals change target materials between liquid and solid states, an effect that has been used for optical data storage. They demonstrated nonlinear processing, and produced output pulses like those from organic neurons. They then produced an integrated photonic circuit that incorporated four optical neurons operating at different wavelengths, each of which connected to 15 optical synapses. The photonic circuit contained more than 140 components and could recognize simple optical patterns. The group wrote that their device is scalable, and that the technology promises ""access to the high speed and high bandwidth inherent to optical systems, thus enabling the direct processing of optical telecommunication and visual data.”  Now a group at the Hong Kong University of Science and Technology reports in Optica that they have made an all-optical neural network based on a different process, electromagnetically induced transparency, in which incident light affects how atoms shift between quantum-mechanical energy levels. The process is nonlinear and can be triggered by very weak light signals, says Shengwang Du, a physics professor and coauthor of the paper. In their demonstration, they illuminated rubidium-85 atoms cooled by lasers to about 10 microKelvin (10 microdegrees above absolute zero). Although the technique may seem unusually complex, Du said the system was the most accessible one in the lab that could produce the desired effects. ""As a pure quantum atomic system [it] is ideal for this proof-of-principle experiment,"" he says. Next, they plan to scale up the demonstration using a hot atomic vapor center, which is less expensive, does not require time-consuming preparation of cold atoms, and can be integrated with photonic chips. Du says the major challenges are reducing cost of the nonlinear processing medium and increasing the scale of the all-optical neural network for more complex tasks. ""Their demonstration seems valid,"" says Volker Sorger, an electrical engineer at George Washington University in Washington who was not involved in either demonstration. He says the all-optical approach is attractive because it offers very high parallelism, but the update rate is limited to about 100 hertz because of the liquid crystals used in their test, and he is not completely convinced their approach can be scaled error-free.    Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Computer Science,0.1531173856591807
7,ACM,Computational Approach Speeds Advanced Microscopy Imaging,ACM,2019-08-27,-,https://www.osa.org/en-us/about_osa/newsroom/news_releases/2019/computational_approach_speeds_up_advanced_microsco/,"       Computational Approach Speeds Advanced Microscopy Imaging     Optical Society of AmericaAugust 27, 2019   Researchers at the Chinese University of Hong Kong (CUHK) combined multifocus compressive imaging with faster compressive scanning to accelerate the imaging speed of two-photon microscopy without sacrificing resolution. CUHK's Chenyang Wen said, ""This ... will be useful for visualizing a neural network, or monitoring activity from hundreds of neurons simultaneously."" The researchers said the technique is faster because it requires fewer exposures; it performs both sampling and image compression at once, followed by the implementation of an algorithm that fills in any missing information The methodology enables high-quality image production with imaging speeds up to five times faster from any field of view.                 ",Computer Science,0.15018931681086484
8,ACM,AI Analysis Gives Guidance to Crisis Counselors,ACM,2019-08-21,-,https://news.cornell.edu/stories/2019/08/ai-analysis-gives-guidance-crisis-counselors,"    AI Analysis Gives Guidance to Crisis Counselors     Cornell Chronicle (NY)Louis DiPietroAugust 21, 2019   A study by Cornell University researchers and the Crisis Text Line crisis-counselor platform described how volunteer crisis counselors' use of language evolves. The team used state-of-the-art natural language processing to learn that the language employed by counselors systematically changes, based on their training and empathy for callers in distress, giving rise to unique voices for calming those distressed individuals. The researchers analyzed more than 1 million anonymized texts from about 3,500 counselors on the Crisis Text Line. Crisis Text Line's Robert Filbin said the study' insights will help the platform train and guide crisis counselors. Cornell's Cristian Danescu-Niculescu-Mizil said, ""This is an example of how natural language processing techniques can assist the development of skills in conversation-heavy professions.""                 ",Computer Science,0.14846270115669905
9,IEEE,A Carbon Nanotube Microprocessor Mature Enough to Say Hello,Electronics and Technology,2019-08-28,-,https://spectrum.ieee.org/nanoclast/semiconductors/processors/modern-microprocessor-built-using-carbon-nanotubes,"      Engineers at MIT and Analog Devices have created the first fully-programmable 16-bit carbon nanotube microprocessor. It’s the most complex integration of carbon nanotube-based CMOS logic so far, with nearly 15,000 transistors, and it was done using technologies that have already been proven to work in a commercial chip-manufacturing facility. The processor, called RV16X-NANO, is a milestone in the development of beyond-silicon technologies, its inventors say.  Unlike silicon transistors, nanotube devices can easily be made in multiple layers with dense 3D interconnections. The Defense Advanced Research Projects Agency is hoping this 3D aspect will lead to commercial carbon nanotube (CNT) chips with the performance of today’s cutting-edge silicon but without the high design and manufacturing cost. Some of the same researchers created a modest one-bit, 178-transistor processor back in 2013. In contrast, the new one, which is based on the open source RISC-V instruction set, is capable of working with 16-bit data and 32-bit instructions. Naturally, the team, led by MIT assistant professor Max Shulaker, tested the chip by running a version of the obligatory “Hello, World!” program. They reported the achievement this week in Nature. “Ten years ago, we hoped this was possible,” says Shulaker. “Now we know it is possible… and we know it can be done in commercial facilities.” Shulaker’s team, along with engineers at Analog Devices and, later, Skywater Technology Foundry, developed three commercially-viable techniques to create the RV16X-NANO. Two dealt with stubborn issues of carbon nanotube purity and uniformity, and the third allowed for the creation of both n-type and p-type transistors to form complementary logic circuits. 1. When making CNT transistors, the nanotubes are first put into a solution and spread across a silicon wafer. Most of the nanotubes lie uniformly on the silicon, but every once in a while, they ball up into bundles of a thousand or more. These bundles can’t form transistors. When building small-scale test circuits, this was no big deal, Shulaker explains, because even if they killed one circuit, another would work. But for a large-scale integration like for the RV16X-NANO these nanotube-pile-ups would be common enough to mess up the whole processor. RINSE, a solution one of Shulaker’s students, Christian Lau, arrived at, relies on the fact that individual nanotubes are stuck to the substrate by Van Der Waals forces more strongly than bundles are. By first coating the nanotube-covered substrate with a photo resist and then carefully washing it away—under just the right conditions—the process selectively removes the bundles but leaves the individual CNTs. 2. While RINSE dealt with one carbon-nanotube impurity, another purity problem nearly crashed the whole project. CNTs have always come in two basic flavors, metallic and semiconducting. Having some metallic nanotubes in a CNT-based logic gate means the circuit will waste power and produce a noisy signal. But how many metallic nanotubes is too many when you’re trying to build a full-scale processor? “It’s a very basic question,” says Shulaker. And to his surprise, it hadn’t been answered. The answer his team came up with was “pretty depressing.” The best today’s commercial processes could produce is 99.99 percent semiconducting nanotubes and 0.01 percent metallic. But what’s needed is 99.999999 percent purity—impossibly far out of reach. “We thought, if we can’t process our way out of this… then somehow we had to design our way around it,” says Shulaker. The team found that, by far, the main driver for the needed purity was not the power issue but the noise. Amongst the many logic circuits they’d made, they found a pattern that suggested some combinations were much more susceptible to the noise problem than others. “So the solution at that point was simple: We’ll just design circuits with the good combinations of logic gates and avoid using the bad combinations.” DREAM, the set of design rules post-doctoral researcher Gage Hill came up with, allows large-scale integration using carbon nanotubes you can purchase off-the-shelf. 3. The third big breakthrough, called MIXED, allowed for the creation of the two types of transistors needed for CMOS logic, the kind in use in all-kinds of processors for decades. For that you need both electron-conducting (NMOS) and hole conducting (PMOS) transistors. Previous attempts at nanotube processors, such as the one-bit system Shulaker built as a graduate student, used only PMOS. In silicon, the distinction is achieved by doping the transistor’s channel region with different atoms to effectively add electrons to the silicon crystal lattice or steal some. But such “substitutional doping” doesn’t work for carbon nanotubes. “It’s difficult to swap out an atom without destroying the properties of the nanotube,” says Shulaker. So instead they turned to “electrostatic doping.” Here, a dielectric oxide is engineered to add or subtract electrons from the nanotube. Using a common semiconductor manufacturing technology called atomic layer deposition, the team was able to deposit dielectrics, such as halfnium dioxide, one atomic layer at a time. By manipulating the exact composition of the layer, say to have slightly fewer oxygens or a bit more, the oxide “wants to either donate electrons to the nanotube or steal from the nanotube,” explains Shulaker. Between careful selection of the metal electrodes involved and the ALD process, the researchers were able to reliably build PMOS and NMOS devices together. Crucially, MIXED is a low-temperature process, so the transistors can be built on top of other layers of circuitry without damaging them. In fact, the transistors in RV16X-NANO were built in between a layer of interconnects that provide power to the transistors and another layer that connects the transistors into logic gates and larger systems. Engineers are interested in such “buried power line” schemes in order to free up space that would allow for better-performing or smaller systems. But they are more difficult to achieve in silicon, in part because of high processing temperatures.  Monthly newsletter about how new materials, designs, and processes drive the chip industry.  IEEE Spectrum’s nanotechnology blog, featuring news and analysis about the development, applications, and future of science and technology at the nanoscale. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Electronics and Technology,0.14832517876684145
10,ACM,DHS Launches Smart City Sensor Pilot in St. Louis,ACM,2019-08-28,-,https://www.nextgov.com/emerging-tech/2019/08/dhs-launches-smart-city-sensor-pilot-st-louis/159517/,"       DHS Launches Smart City Sensor Pilot in St. Louis     NextGov.comBrandi VincentAugust 28, 2019   The U.S Department of Homeland Security's (DHS) Science and Technology Directorate is launching a pilot program in St. Louis, MO, that will study the integration of smart city technologies. Agency officials will work with the city and the Open Geospatial Consortium, a self-described “international voluntary consensus standards organization,” to help develop and assess the state of Smart Cities standards as they arise in the public safety arena. The results of the project also will serve as the basis for developing an open architecture for interoperable Internet of Things (IoT) sensors. Said program manager Norman Speicher, “There’s tremendous pressure on cities, right now. Many municipalities, I do hear that they are being pressured and that there’s this expectation that they know what ‘smart cities’ means—and it really means many things to many people.”                 ",Computer Science,0.1441575335780971
11,IEEE,Eben Upton on the Raspberry Pi’s Industrial Crossover and Why There Will Never Be a Pi 9,Electronics and Technology,2019-08-28,-,https://spectrum.ieee.org/semiconductors/processors/eben-upton-on-the-raspberry-pis-industrial-crossover-and-why-there-will-never-be-a-pi-9,"      Seven years ago, Eben Upton created the first Raspberry Pi. As Upton told IEEE Spectrum in our March 2015 cover story, the Pi was inspired in part by his childhood experiments with a BBC Micro home computer: He wanted modern kids to have a simple machine that allowed for similar experimentation. Since then, the Pi has exploded in popularity, and the fourth major revision of the Pi was released in June. Upton talked with Spectrum senior editor Stephen Cass about the Pi 4’s design, its growing commercial use, and what might be next. Stephen Cass: How has the Pi’s user base evolved? Eben Upton: Our first year, our volume was almost entirely bought by hobbyists. But you have a lot of hobbyists who are also professional design engineers, and when their boss asked them to do something, often they used a Pi. So now you have people who are building industrial products around the Pi to resell. And then you have what we call, for want of a better word, DIY industrial, which is “I own a factory and I need control computers.” And where I might have historically gone and bought an embedded PC, I’ll buy a Pi. Last year we sold 6 million units and [we think as much as] half of those went to some kind of commercial use. S.C.: How did that evolution shape the design of the Pi 4? E.U.: We’re quite lucky in that all our markets have similar requirements. The things we do to make it a better toy make it a better industrial computer. And the things you do to make it a better industrial computer make it a better hobbyist platform. For example, we had a number of people building thin-client solutions, and the feedback was that most of the people doing that wanted the option of being able to deploy two monitors. So we added that feature. Another obvious example would be the serial interfaces. Prior generations are a little underprovisioned for things like UARTs, serial ports, SPIs, I2C interfaces, but because we were redesigning the silicon from scratch, putting UARTs in was pretty straightforward. S.C.: What other changes did you make in redesigning the Pi’s system processor? E.U.: The last three Pis have all been made using the same [40-nanometer] process, so modifications have been limited, mostly putting in a larger ARM [CPU]. If you look at the floor plan of the chip, each new ARM gets stuck on the left-hand side. The chip stretches horizontally and becomes bigger and bigger. But nothing on the right-hand side changes, and the right-hand side is where the memory controller is, where the UARTs are.... But putting in a larger ARM core means your power goes up and eventually you run out of your thermal budget.... We realized we needed to go to a 28-nm process node. And once you’re going to a new process node, you might as well fix all of the wrinkles. And that’s why you see two displays, many UARTs, PCI Express, Gigabit Ethernet. S.C.: How long before the slowdown in Moore’s Law affects the Pi? E.U.: I think it’s relevant to think in terms of how much is left. How much for a given thermal footprint? We’ve come a factor of 40 [in the Pi’s computing power from the first Pi to the Pi 4]. There’s not another factor of 40 left, which means you’ve come through more than half this process.... On some level that’s intimidating. But I’m a software engineer. It’s actually really nice to feel that we’re going into an era where software engineering makes a contribution again! My Ph.D. is in optimizing compilers, and while I was doing it I felt really depressed, because [improving software by a factor of two was small compared to the exponential hardware improvements of Moore’s Law]. But the thing about that factor of two is that it’s there at the end. There will come a time when they’ll be very grateful for it! This article appears in the September 2019 print issue as “How the Raspberry Pi Infiltrated Industry.” Seven years ago, Eben Upton created the first Raspberry Pi. As Upton told IEEE Spectrum in our March 2015 cover story, the Pi was inspired in part by his childhood experiments with a BBC Micro home computer: He wanted modern kids to have a simple machine that allowed for similar experimentation. Since then, the Pi has exploded in popularity, and the fourth major revision of the Pi was released in June. Upton talked with Spectrum senior editor Stephen Cass about the Pi 4’s design, its growing commercial use, and what might be next. Stephen Cass: How has the Pi’s user base evolved? Eben Upton: Our first year, our volume was almost entirely bought by hobbyists. But you have a lot of hobbyists who are also professional design engineers, and when their boss asked them to do something, often they used a Pi. So now you have people who are building industrial products around the Pi to resell. And then you have what we call, for want of a better word, DIY industrial, which is “I own a factory and I need control computers.” And where I might have historically gone and bought an embedded PC, I’ll buy a Pi. Last year we sold 6 million units and [we think as much as] half of those went to some kind of commercial use. S.C.: How did that evolution shape the design of the Pi 4? E.U.: We’re quite lucky in that all our markets have similar requirements. The things we do to make it a better toy make it a better industrial computer. And the things you do to make it a better industrial computer make it a better hobbyist platform. For example, we had a number of people building thin-client solutions, and the feedback was that most of the people doing that wanted the option of being able to deploy two monitors. So we added that feature. Another obvious example would be the serial interfaces. Prior generations are a little underprovisioned for things like UARTs, serial ports, SPIs, I2C interfaces, but because we were redesigning the silicon from scratch, putting UARTs in was pretty straightforward. S.C.: What other changes did you make in redesigning the Pi’s system processor? E.U.: The last three Pis have all been made using the same [40-nanometer] process, so modifications have been limited, mostly putting in a larger ARM [CPU]. If you look at the floor plan of the chip, each new ARM gets stuck on the left-hand side. The chip stretches horizontally and becomes bigger and bigger. But nothing on the right-hand side changes, and the right-hand side is where the memory controller is, where the UARTs are.... But putting in a larger ARM core means your power goes up and eventually you run out of your thermal budget.... We realized we needed to go to a 28-nm process node. And once you’re going to a new process node, you might as well fix all of the wrinkles. And that’s why you see two displays, many UARTs, PCI Express, Gigabit Ethernet. S.C.: How long before the slowdown in Moore’s Law affects the Pi? E.U.: I think it’s relevant to think in terms of how much is left. How much for a given thermal footprint? We’ve come a factor of 40 [in the Pi’s computing power from the first Pi to the Pi 4]. There’s not another factor of 40 left, which means you’ve come through more than half this process.... On some level that’s intimidating. But I’m a software engineer. It’s actually really nice to feel that we’re going into an era where software engineering makes a contribution again! My Ph.D. is in optimizing compilers, and while I was doing it I felt really depressed, because [improving software by a factor of two was small compared to the exponential hardware improvements of Moore’s Law]. But the thing about that factor of two is that it’s there at the end. There will come a time when they’ll be very grateful for it! This article appears in the September 2019 print issue as “How the Raspberry Pi Infiltrated Industry.” Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Computer Science,0.14106616671941055
12,ACM,Facial Recognition: School ID Checks Lead to GDPR Fine,ACM,2019-08-27,-,https://www.bbc.com/news/technology-49489154,"       Facial Recognition: School ID Checks Lead to GDPR Fine     BBC NewsAugust 27, 2019   The Swedish Data Protection Authority (DPA) fined the municipality of Skelleftea 200,000 Swedish Krona ($20,700) for using facial recognition to track students, in violation of the European Union's General Data Protection Regulation (GDPR). Anderstorp High School used the technology to track 22 high school students for three weeks, to detect when each individual entered a classroom. GDPR restricts the use of facial imaging and the collection of other biometric data. The DPA said while the school obtained parents' permission to track the students, that was not legally sufficient to allow the school to capture personal data, as attendance could have been checked without video surveillance. The DPA said Skelleftea's local authority had illegally processed sensitive biometric data and failed to complete a sufficient impact assessment.                 ",Computer Science,0.1366023906279696
13,ACM,Algorithms Aid Search for Source of Spacetime Rumbles,ACM,2019-08-28,-,https://spectrum.ieee.org/tech-talk/aerospace/astrophysics/new-algorithms-lead-search-for-origins-of-spacetime-rumbles,"       Algorithms Aid Search for Source of Spacetime Rumbles     IEEE SpectrumMark AndersonAugust 28, 2019   Researchers working on the SAGUARO (Searches After Gravitational-waves Using ARizona Observatories) project are using machine learning algorithms to help identify gravitational wavefronts that come from the collision of two neutron stars. The algorithms automatically consider each potential candidate source for more trivial explanations, such as a known variable star or an over-saturated image that caused noisy pixel data. This process allows the team to pare the list of possible gravitational wave sources to several dozen objects, a manageable number for follow-up by astronomers. The researchers' main goal is to improve the machine learning algorithms as much as possible in order to help them locate and identify a kilonova, a recently discovered astronomical event.                 ",Space & Time,0.1333301848502497
14,ACM,Self-Folding 'Rollbot' Paves the Way for Fully Untethered Soft Robots,ACM,2019-08-21,-,https://www.seas.harvard.edu/news/2019/08/self-folding-rollbot-paves-way-for-fully-untethered-soft-robots,"    Self-Folding 'Rollbot' Paves the Way for Fully Untethered Soft Robots     Harvard School of Engineering and Applied SciencesLeah BurrowsAugust 21, 2019   Researchers at Harvard University's John A. Paulson School of Engineering and Applied Sciences (SEAS) and the California Institute of Technology have developed soft robotic systems inspired by origami that can move and change shape in response to external stimuli. The researchers used liquid crystal elastomers that change shape when exposed to heat, and utilized three-dimensional (3D) printing to create two types of soft hinges that fold at different temperatures. The team used its designs to build several soft robotic devices. Said Harvard’s Jennifer A. Lewis, “The ability to integrate active materials within 3D-printed objects enables the design and fabrication of entirely new classes of soft robotic matter.”                 ",Robotics,0.12945381172240505
15,MIT News,Robotic thread is designed to slip through the brain’s blood vessels,Research,2019-08-28,-,http://news.mit.edu/2019/robot-brain-blood-vessels-0828,"  MIT engineers have developed a magnetically steerable, thread-like robot that can actively glide through narrow, winding pathways, such as the labrynthine vasculature of the brain. In the future, this robotic thread may be paired with existing endovascular technologies, enabling doctors to remotely guide the robot through a patient’s brain vessels to quickly treat blockages and lesions, such as those that occur in aneurysms and stroke. “Stroke is the number five cause of death and a leading cause of disability in the United States. If acute stroke can be treated within the first 90 minutes or so, patients’ survival rates could increase significantly,” says Xuanhe Zhao, associate professor of mechanical engineering and of civil and environmental engineering at MIT. “If we could design a device to reverse blood vessel blockage within this ‘golden hour,’ we could potentially avoid permanent brain damage. That’s our hope.” Zhao and his team, including lead author Yoonho Kim, a graduate student in MIT’s Department of Mechanical Engineering, describe their soft robotic design today in the journal Science Robotics. The paper’s other co-authors are MIT graduate student German Alberto Parada and visiting student Shengduo Liu. In a tight spot To clear blood clots in the brain, doctors often perform an endovascular procedure, a minimally invasive surgery in which a surgeon inserts a thin wire through a patient’s main artery, usually in the leg or groin. Guided by a fluoroscope that simultaneously images the blood vessels using X-rays, the surgeon then manually rotates the wire up into the damaged brain vessel. A catheter can then be threaded up along the wire to deliver drugs or clot-retrieval devices to the affected region. Kim says the procedure can be physically taxing, requiring surgeons, who must be specifically trained in the task, to endure repeated radiation exposure from fluoroscopy. “It’s a demanding skill, and there are simply not enough surgeons for the patients, especially in suburban or rural areas,” Kim says. The medical guidewires used in such procedures are passive, meaning they must be manipulated manually, and are typically made from a core of metallic alloys, coated in polymer, a material that Kim says could potentially generate friction and damage vessel linings if the wire were to get temporarily stuck in a particularly tight space. The team realized that developments in their lab could help improve such endovascular procedures, both in the design of the guidewire and in reducing doctors’ exposure to any associated radiation.        Threading a needle Over the past few years, the team has built up expertise in both hydrogels — biocompatible materials made mostly of water — and 3-D-printed magnetically-actuated materials that can be designed to crawl, jump, and even catch a ball, simply by following the direction of a magnet. In this new paper, the researchers combined their work in hydrogels and in magnetic actuation, to produce a magnetically steerable, hydrogel-coated robotic thread, or guidewire, which they were able to make thin enough to magnetically guide through a life-size silicone replica of the brain’s blood vessels. The core of the robotic thread is made from nickel-titanium alloy, or “nitinol,” a material that is both bendy and springy. Unlike a clothes hanger, which would retain its shape when bent, a nitinol wire would return to its original shape, giving it more flexibility in winding through tight, tortuous vessels. The team coated the wire’s core in a rubbery paste, or ink, which they embedded throughout with magnetic particles. Finally, they used a chemical process they developed previously, to coat and bond the magnetic covering with hydrogel — a material that does not affect the responsiveness of the underlying magnetic particles and yet provides the wire with a smooth, friction-free, biocompatible surface. They demonstrated the robotic thread’s precision and activation by using a large magnet, much like the strings of a marionette, to steer the thread through an obstacle course of small rings, reminiscent of a thread working its way through the eye of a needle. The researchers also tested the thread in a life-size silicone replica of the brain’s major blood vessels, including clots and aneurysms, modeled after the CT scans of an actual patient’s brain. The team filled the silicone vessels with a liquid simulating the viscosity of blood, then manually manipulated a large magnet around the model to steer the robot through the vessels’ winding, narrow paths. Kim says the robotic thread can be functionalized, meaning that features can be added — for example, to deliver clot-reducing drugs or break up blockages with laser light. To demonstrate the latter, the team replaced the thread’s nitinol core with an optical fiber and found that they could magnetically steer the robot and activate the laser once the robot reached a target region. When the researchers ran comparisons between the robotic thread coated versus uncoated with hydrogel, they found that the hydrogel gave the thread a much-needed, slippery advantage, allowing it to glide through tighter spaces without getting stuck. In an endovascular surgery, this property would be key to preventing friction and injury to vessel linings as the thread works its way through. “One of the challenges in surgery has been to be able to navigate through complicated blood vessels in the brain, which has a very small diameter, where commercial catheters can’t reach,” says Kyujin Cho, professor of mechanical engineering at Seoul National University. “This research has shown potential to overcome this challenge and enable surgical procedures in the brain without open surgery.” And just how can this new robotic thread keep surgeons radiation-free? Kim says that a magnetically steerable guidewire does away with the necessity for surgeons to physically push a wire through a patient’s blood vessels. This means that doctors also wouldn’t have to be in close proximity to a patient, and more importantly, the radiation-generating fluoroscope. In the near future, he envisions endovascular surgeries that incorporate existing magnetic technologies, such as pairs of large magnets, the directions of which doctors can manipulate from just outside the operating room, away from the fluoroscope imaging the patient’s brain, or even in an entirely different location. “Existing platforms could apply magnetic field and do the fluoroscopy procedure at the same time to the patient, and the doctor could be in the other room, or even in a different city, controlling the magnetic field with a joystick,” Kim says. “Our hope is to leverage existing technologies to test our robotic thread in vivo in the next step.” This research was funded, in part, by the Office of Naval Research, the MIT Institute for Soldier Nanotechnologies, and the National Science Foundation (NSF). ",Electronics and Technology,0.12071629296375583
16,ACM,"Developer Jobs: From SQL to Java, These are the Skills Companies are Looking for Now",ACM,2019-08-27,-,https://www.zdnet.com/article/developer-jobs-from-sql-to-java-these-are-the-skills-companies-are-looking-for/,"    Developer Jobs: From SQL to Java, These are the Skills Companies are Looking for Now     ZDNetSteve RangerAugust 27, 2019   More than 140,000 advertisements for IT jobs were listed in the U.K. in the second quarter of this year, amounting to about 9% of all jobs advertised, according to trade association CompTIA. Ads aimed at finding programmers and software developers were the largest single category of those IT job ads, accounting for more than 50,000 ads. CompTIA said the most sought-after skills in those ads included software development principles; SQL; technical support; JavaScript and JQuery; Web development; Microsoft Dev Tools; operating systems; system design and implementation; project management, and Java. The organization said the number of IT jobs posted in the U.K. in the second quarter had declined 13% from the first quarter of this year, and was lower than seen in the second quarter of last year.                 ",Society,0.11395063273482246
17,IEEE,New Double 3 Robot Makes Telepresence Easier than Ever,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever,"      Today, Double Robotics is announcing Double 3, the latest major upgrade to its line of consumer(ish) telepresence robots. We had a (mostly) fantastic time testing out Double 2 back in 2016. One of the things that we found out back then was that it takes a lot of practice to remotely drive the robot around. Double 3 solves this problem by leveraging the substantial advances in 3D sensing and computing that have taken place over the past few years, giving their new robot a level of intelligence that promises to make telepresence more accessible for everyone. Double 2’s iPad has been replaced by “a fully integrated solution”—which is a fancy way of saying a dedicated 9.7-inch touchscreen and a whole bunch of other stuff. That other stuff includes an NVIDIA Jetson TX2 AI computing module, a beamforming six-microphone array, an 8-watt speaker, a pair of 13-megapixel cameras (wide angle and zoom) on a tilting mount, five ultrasonic rangefinders, and most excitingly, a pair of Intel RealSense D430 depth sensors.  It’s those new depth sensors that really make Double 3 special. The D430 modules each uses a pair of stereo cameras with a pattern projector to generate 1280 x 720 depth data with a range of between 0.2 and 10 meters away. The Double 3 robot uses all of this high quality depth data to locate obstacles, but at this point, it still doesn’t drive completely autonomously. Instead, it presents the remote operator with a slick, augmented reality view of drivable areas in the form of a grid of dots. You just click where you want the robot to go, and it will skillfully take itself there while avoiding obstacles (including dynamic obstacles) and related mishaps along the way.  This effectively offloads the most stressful part of telepresence—not running into stuff—from the remote user to the robot itself, which is the way it should be. That makes it that much easier to encourage people to utilize telepresence for the first time. The way the system is implemented through augmented reality is particularly impressive, I think. It looks like it’s intuitive enough for an inexperienced user without being restrictive, and is a clever way of mitigating even significant amounts of lag.  Otherwise, Double 3’s mobility system is exactly the same as the one featured on Double 2. In fact, that you can stick a Double 3 head on a Double 2 body and it instantly becomes a Double 3. Double Robotics is thoughtfully offering this to current Double 2 owners as a significantly more affordable upgrade option than buying a whole new robot. For more details on all of Double 3  new features, we spoke with the co-founders of Double Robotics, Marc DeVidts and David Cann. IEEE Spectrum: Why use this augmented reality system instead of just letting the user click on a regular camera image? Why make things more visually complicated, especially for new users? Marc DeVidts and David Cann: One of the things that we realized about nine months ago when we got this whole thing working was that without the mixed reality for driving, it was really too magical of an experience for the customer. Even us—we had a hard time understanding whether the robot could really see obstacles and understand where the floor is and that kind of thing. So, we said “What would be the best way of communicating this information to the user?” And the right way to do it ended up drawing the graphics directly onto the scene. It’s really awesome—we have a full, real time 3D scene with the depth information drawn on top of it. We’re starting with some relatively simple graphics, and we’ll be adding more graphics in the future to help the user understand what the robot is seeing. How robust is the vision system when it comes to obstacle detection and avoidance? Does it work with featureless surfaces, IR absorbent surfaces, in low light, in direct sunlight, etc? We’ve looked at all of those cases, and one of the reasons that we’re going with the RealSense is the projector that helps us to see blank walls. We also found that having two sensors—one facing the floor and one facing forward—gives us a great coverage area. Having ultrasonic sensors in there as well helps us to detect anything that we can't see with the cameras. They're sort of a last safety measure, especially useful for detecting glass.  It seems like there’s a lot more that you could do with this sensing and mapping capability. What else are you working on? We're starting with this semi-autonomous driving variant, and we're doing a private beta of full mapping. So, we’re going to do full SLAM of your environment that will be mapped by multiple robots at the same time while you're driving, and then you'll be able to zoom out to a map and click anywhere and it will drive there. That  where we're going with it, but we want to take baby steps to get there. It  the obvious next step, I think, and there are a lot more possibilities there. Do you expect developers to be excited for this new mapping capability? We're using a very powerful computer in the robot, a NVIDIA Jetson TX2 running Ubuntu. There  room to grow. It’s actually really exciting to be able to see, in real time, the 3D pose of the robot along with all of the depth data that gets transformed in real time into one view that gives you a full map. Having all of that data and just putting those pieces together and getting everything to work has been a huge feat in of itself.  We have an extensive API for developers to do custom implementations, either for telepresence or other kinds of robotics research. Our system isn't running ROS, but we're going to be adding ROS adapters for all of our hardware components. Telepresence robots depend heavily on wireless connectivity, which is usually not something that telepresence robotics companies like Double have direct control over. Have you found that connectivity has been getting significantly better since you first introduced Double? When we started in 2013, we had a lot of customers that didn’t have WiFi in their hallways, just in the conference rooms. We very rarely hear about customers having WiFi connectivity issues these days. The bigger issue we see is when people are calling into the robot from home, where they don't have proper traffic management on their home network. The robot doesn't need a ton of bandwidth, but it does need consistent, low latency bandwidth. And so, if someone else in the house is watching Netflix or something like that, it’s going to saturate your connection. But for the most part, it’s gotten a lot better over the last few years, and it’s no longer a big problem for us. Do you think 5G will make a significant difference to telepresence robots? We’ll see. We like the low latency possibilities and the better bandwidth, but it  all going to be a matter of what kind of reception you get. LTE can be great, if you have good reception; it’s all about where the tower is. I’m pretty sure that WiFi is going to be the primary thing for at least the next few years. DeVidts also mentioned that an unfortunate side effect of the new depth sensors is that hanging a t-shirt on your Double to give it some personality will likely render it partially blind, so that  just something to keep in mind. To make up for this, you can switch around the colorful trim surrounding the screen, which is nowhere near as fun. When the Double 3 is ready for shipping in late September, US $2,000 will get you the new head with all the sensors and stuff, which seamlessly integrates with your Double 2 base. Buying Double 3 straight up (with the included charging dock) will run you $4,ooo. This is by no means an inexpensive robot, and my impression is that it’s not really designed for individual consumers. But for commercial, corporate, healthcare, or education applications, $4k for a robot as capable as the Double 3 is really quite a good deal—especially considering the kinds of use cases for which it’s ideal. [ Double Robotics ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.1138742701220733
18,IEEE,Blue Ocean Robotics Acquires Beam Telepresence Robot From Suitable Technologies,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot,"      Today, Blue Ocean Robotics, a Danish robotics company, is announcing the acquisition of Suitable Technologies’ Beam telepresence robot business. Blue Ocean has been a Beam partner for five years, but now they’re taking things over completely. The Beam robot began its life as an internal project within Willow Garage. It was spun out in 2012 as Suitable Technologies, which produced a couple different versions of the Beam. As telepresence platforms go, Beam is on the powerful and expensive side, designed primarily for commercial and enterprise customers.  The most recent news from Suitable was the introduction of the BeamPro 2, which was announced over a year ago at CES 2018. The Suitable Tech website still lists it as “coming soon,” and our guess is that it’s now up to Blue Ocean to decide whether to go forward with this new version. Blue Ocean calls itself a “robot venture factory.” I’m not entirely sure what a “robot venture factory” is but Blue Ocean describes itself thusly: The company is known for developing professional service robots from the problem, idea and design phase to the development, commercialization and scaling phase. Every robot is placed in its own subsidiary which is responsible for scaling sales, customer service, support and everything else oriented towards global markets and customers. The parent company handles all development and production of robots across the organization.  Ah, that explains it! Blue Ocean does already have a couple portfolio companies making very specific robots, including a UV disinfection robot for hospitals and a sort of mobile patient lift also for hospitals. They’re working on some kind of agriculture robot, too. I’d love to be able to tell you more, but the press release doesn’t offer much: With the acquisition, Blue Ocean Robotics sees an opportunity to generate additional synergy: “Our development of robots is based on our own in-house created toolbox with reusable technology components. This means that we can build all of our robots fast, inexpensively, and better than others,” says Blue Ocean Robotics’ CTO John Erland Østergaard. “Some of our robots, for example the UVD disinfection robot, are already equipped with remote controls. With the Beam technology being a big seller in the healthcare sector, we can continue to grow our business within this industry by having our distributors present both UVD and Beam when they visit customers.” The press release is very specific that Blue Ocean isn’t acquiring Suitable Technologies itself—they’re acquiring the “assets and rights associated with the robot Beam” from Suitable, which I guess means that Suitable is still around somehow. But it’s really not clear what Suitable is without Beam, which (as far as we can make out) is the entirety of what the company does. Anyway, we’re glad that there’s enough interest in high-end telepresence robots to support this acquisition, and we hope that Blue Ocean will be investing in BeamPro 2 and further generations of the robot. It’s come a long way from the original Texai robot from Willow Garage, and still has a lot of potential. For more information, visit the new Beam website that Blue Ocean has just launched. [ Beam ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11152792061413662
19,IEEE,All of the Winners in the DARPA Subterranean Challenge Tunnel Circuit,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit,"      The first competitive event in the DARPA Subterranean Challenge concluded last week—hopefully you were able to follow along on the livestream, on Twitter, or with some of the articles that we’ve posted about the event. We’ll have plenty more to say about how things went for the SubT teams, but while they take a bit of a (well earned) rest, we can take a look at the winning teams as well as who won DARPA’s special superlative awards for the competition.  With their rugged, reliable robots featuring giant wheels and the ability to drop communications nodes, Team Explorer was in the lead from day 1, scoring in double digits on every single run.  Team CoSTAR had one of the more diverse lineups of robots, and they switched up which robots they decided to send into the mine as they learned more about the course.   While many teams came to SubT with DARPA funding, Team CTU-CRAS was self-funded, making them eligible for a special $200,000 Tunnel Circuit prize.   DARPA also awarded a bunch of “superlative awards” after SubT: To score a point, teams had to submit the location of an artifact that was correct to within 5 meters of the artifact itself. However, DARPA was tracking the artifact locations with much higher precision—for example, the “zero” point on the backpack artifact was the center of the label on the front, which DARPA tracked to the millimeter. Team Explorer managed to return the location of a backpack with an error of just 0.18 meter, which is kind of amazing. With just an hour to find as many artifacts as possible, teams had to find the right balance between sending robots off to explore and bringing them back into communication range to download artifact locations. Team CSIRO Data61 cut their last point pretty close, sliding their final point in with a mere 22 seconds to spare.  Team Robotika had some of the quirkiest and most recognizable robots, which DARPA recognized with the “Most Distinctive” award. Robotika told us that part of the reason for that distinctiveness was practical—having a robot that was effectively in two parts meant that they could disassemble it so that it would fit in the baggage compartment of an airplane, very important for a team based in the Czech Republic. Kevin Knoedler, who won NASA’s Space Robotics Challenge entirely by himself, brought his own personal swarm of drones to SubT. With a ratio of seven robots to one human, Kevin was almost certainly the hardest working single human at the challenge. The Fan Favorite award went to the team that was most popular on Twitter (with the #SubTChallenge hashtag), and it may or may not be the case that I personally tweeted enough about Team NCTU’s blimp to win them this award. It’s also true that whenever we asked anyone on other teams what their favorite robot was (besides their own, of course), the blimp was overwhelmingly popular. So either way, the award is well deserved.  DARPA shared this little behind-the-scenes clip of the blimp in action (sort of), showing what happened to the poor thing when the mine ventilation system was turned on between runs and DARPA staff had to chase it down and rescue it: The thing to keep in mind about the results of the Tunnel Circuit is that unlike past DARPA robotics challenges (like the DRC), they don’t necessarily indicate how things are going to go for the Urban or Cave circuits because of how different things are going to be. Explorer did a great job with a team of rugged wheeled vehicles, which turned out to be ideal for navigating through mines, but they’re likely going to need to change things up substantially for the rest of the challenges, where the terrain will be much more complex. DARPA hasn’t provided any details on the location of the Urban Circuit yet; all we know is that it’ll be sometime in February 2020. This gives teams just six months to take all the lessons that they learned from the Tunnel Circuit and update their hardware, software, and strategies. What were those lessons, and what do teams plan to do differently next year? Check back next week, and we’ll tell you. [ DARPA SubT ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11048195193931849
20,IEEE,ETH Zurich Demonstrates PuppetMaster Robot,Robotics,2019-08-30,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot,"      As far as I know, the universe does not have a desperate need for robot puppeteers, and considering the difficulty of making even a halfway decent robot puppeteer, you’d think that any sensible roboticist would keep well clear of the problem. But some folks over at ETH Zurich decided that they’d have a crack at it anyway, and they started by describing why they’d likely be better off if they hadn’t: Marionettes are underactuated, high-dimensional, highly non-linear coupled pendulum systems. They are driven by gravity, the tension forces generated by a small number of cables, and the internal forces arising from mechanical articulation constraints. As such, the map between the actions of a puppeteer and the motions performed by the marionette is notoriously unintuitive, and mastering this unique art form takes unfaltering dedication and a great deal of practice. Our goal is to enable autonomous robots to animate marionettes with a level of skill that approaches that of human puppeteers.   I’m not much of a puppeteer myself, but this looks not bad at all, considering that the ABB YuMi robot is missing quite a few degrees of freedom in its hands. For context, here’s someone who has mastered this unique artform through unfaltering dedication and a great deal of practice, master puppeteer Scott Land:   The ETH Zurich project can’t yet animate a complex marionette, but it’s a respectable showing with the dragon, I think. As input, all the robot needs to know is the design of the puppet at the target motion you want the puppet to make. While moving the puppet in real life, the robot is continuously simulating its motions over the next second while iteratively optimizing to try to get the puppet to move the way it’s supposed to. The usefulness of this research, thankfully, is not constrained to puppets: Our long term goal is to enable robots to manipulate various types of complex physical systems – clothing, soft parcels in warehouses or stores, flexible sheets and cables in hospitals or on construction sites, plush toys or bedding in our homes, etc – as skillfully as humans do. We believe the technical framework we have set up for robotic puppeteering will also prove useful in beginning to address this very important grand-challenge. [ Paper ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11005753237125886
21,IEEE,Video Friday: This Robotic Thread Could One Day Travel Inside Your Brain,Robotics,2019-08-31,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain,"      Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (send us your events!): Let us know if you have suggestions for next week, and enjoy today’s videos. Eight engineering students from ETH Zurich are working on a year-long focus project to develop a multimodal robot called Dipper, which can fly, swim, dive underwater, and manage that difficult air-water transition:    The robot uses one motor to selectively drive either a propeller or a marine screw depending on whether it’s in flight or not. We’re told that getting the robot to autonomously do the water to air transition is still a work in progress, but that within a few weeks things should be much smoother. [ Dipper ] Thanks Simon!   Giving a jellyfish a hug without stressing them out is exactly as hard as you think, but Harvard’s robot will make sure that all jellyfish get the emotional (and physical) support that they need.    The gripper’s six “fingers” are composed of thin, flat strips of silicone with a hollow channel inside bonded to a layer of flexible but stiffer polymer nanofibers. The fingers are attached to a rectangular, 3D-printed plastic “palm” and, when their channels are filled with water, curl in the direction of the nanofiber-coated side. Each finger exerts an extremely low amount of pressure — about 0.0455 kPA, or less than one-tenth of the pressure of a human’s eyelid on their eye. By contrast, current state-of-the-art soft marine grippers, which are used to capture delicate but more robust animals than jellyfish, exert about 1 kPA. The gripper was successfully able to trap each jellyfish against the palm of the device, and the jellyfish were unable to break free from the fingers’ grasp until the gripper was depressurized. The jellyfish showed no signs of stress or other adverse effects after being released, and the fingers were able to open and close roughly 100 times before showing signs of wear and tear. [ Harvard ]   MIT engineers have developed a magnetically steerable, thread-like robot that can actively glide through narrow, winding pathways, such as the labyrinthine vasculature of the brain. In the future, this robotic thread may be paired with existing endovascular technologies, enabling doctors to remotely guide the robot through a patient’s brain vessels to quickly treat blockages and lesions, such as those that occur in aneurysms and stroke.    [ MIT ]   See NASA’s next Mars rover quite literally coming together inside a clean room at the Jet Propulsion Laboratory. This behind-the-scenes look at what goes into building and preparing a rover for Mars, including extensive tests in simulated space environments, was captured from March to July 2019. The rover is expected to launch to the Red Planet in summer 2020 and touch down in February 2021.    The Mars 2020 rover doesn’t have a name yet, but you can give it one! As long as you’re not too old! Which you probably are!  [ Mars 2020 ]   I desperately wish that we could watch this next video at normal speed, not just slowed down, but it’s quite impressive anyway.    Here’s one more video from the Namiki Lab showing some high speed tracking with a pair of very enthusiastic robotic cameras:  [ Namiki Lab ]   Normally, tedious modeling of mechanics, electronics, and information science is required to understand how insects’ or robots’ moving parts coordinate smoothly to take them places. But in a new study, biomechanics researchers at the Georgia Institute of Technology boiled down the sprints of cockroaches to handy principles and equations they then used to make a test robot amble about better.    [ Georgia Tech ]   More magical obstacle-dodging footage from Skydio’s still secret new drone.    We’ve been hard at work extending the capabilities of our upcoming drone, giving you ways to get the control you want without the stress of crashing. The result is you can fly in ways, and get shots, that would simply be impossible any other way. How about flying through obstacles at full speed, backwards? [ Skydio ]   This is a cute demo with Misty:    [ Misty Robotics ]   We’ve seen pieces of hardware like this before, but always made out of hard materials—a soft version is certainly something new.    Utilizing vacuum power and soft material actuators, we have developed a soft reconfigurable surface (SRS) with multi-modal control and performance capabilities. The SRS is comprised of a square grid array of linear vacuum-powered soft pneumatic actuators (linear V-SPAs), built into plug-and-play modules which enable the arrangement, consolidation, and control of many DoF. [ RRL ]   The EksoVest is not really a robot, but it’ll make you a cyborg! With super strength!    ""This is NOT intended to give you super strength but instead give you super endurance and reduce fatigue so that you have more energy and less soreness at the end of your shift."" Drat! [ EksoVest ]   We have created a solution for parents, grandparents, and their children who are living separated. This is an amazing tool to stay connected from a distance through the intimacy that comes through interactive play with a child. For parents who travel for work, deployed military, and families spread across the country, the Cushybot One is much more than a toy; it is the opportunity for maintaining a deep connection with your young child from a distance.    Hmm. I think the concept here is great, but it’s going to be a serious challenge to successfully commercialize. [ Indiegogo ]   What happens when you equip RVR with a parachute and send it off a cliff? Watch this episode of RVR Launchpad to find out – then go Behind the Build to see how we (eventually) accomplished this high-flying feat.    [ Sphero ]   These omnidirectional crawler robots aren’t new, but that doesn’t keep them from being fun to watch.    [ NEDO ] via [ Impress ]     We’ll finish up the week with a couple of past ICRA and IROS keynote talks—one by Gill Pratt on The Reliability Challenges of Autonomous Driving, and the other from Peter Hart, on Making Shakey.     [ IEEE RAS ]    Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11001508254830898
22,MIT News,"For first time, astronomers catch asteroid in the act of changing color",Space & Time,2019-08-29,-,http://news.mit.edu/2019/asteroid-changing-color-first-0830,"  Last December, scientists discovered an “active” asteroid within the asteroid belt, sandwiched between the orbits of Mars and Jupiter. The space rock, designated by astronomers as 6478 Gault, appeared to be leaving two trails of dust in its wake — active behavior that is associated with comets but rarely seen in asteroids. While astronomers are still puzzling over the cause of Gault’s comet-like activity, an MIT-led team now reports that it has caught the asteroid in the act of changing color, in the near-infrared spectrum, from red to blue. It is the first time scientists have observed a color-shifting asteroid, in real-time. “That was a very big surprise,” says Michael Marsset, a postdoc in MIT’s Department of Earth, Atmospheric and Planetary Sciences (EAPS). “We think we have witnessed the asteroid losing its reddish dust to space, and we are seeing the asteroid’s underlying, fresh blue layers.” Marsset and his colleagues have also confirmed that the asteroid is rocky — proof that the asteroid’s tail, though seemingly comet-like, is caused by an entirely different mechanism, as comets are not rocky but more like loose snowballs of ice and dust. “It’s the first time to my knowledge that we see a rocky body emitting dust, a little bit like a comet,” Marsset says. “It means that probably some mechanism responsible for dust emission is different from comets, and different from most other active main-belt asteroids.” Marsset and his colleagues, including EAPS Research Scientist Francesca DeMeo and Professor Richard Binzel, have published their results today in the journal Astrophysical Journal Letters. A rock with tails Astronomers first discovered 6478 Gault in 1988 and named the asteroid after planetary geologist Donald Gault. Until recently, the space rock was seen as relatively average, measuring about 2.5 miles wide and orbiting along with millions of other bits of rock and dust within the inner region of the asteroid belt, 214 million miles from the sun. In January, images from various observatories, including NASA’s Hubble Space Telescope, captured two narrow, comet-like tails trailing the asteroid. Astronomers estimate that the longer tail stretches half a million miles out, while the shorter tail is about a quarter as long. The tails, they concluded, must consist of tens of millions of kilograms of dust, actively ejected by the asteroid, into space. But how? The question reignited interest in Gault, and studies since then have unearthed past instances of similar activity by the asteroid. “We know of about a million bodies between Mars and Jupiter, and maybe about 20 that are active in the asteroid belt,” Marsset says. “So this is very rare.” He and his colleagues joined the search for answers to Gault’s activity in March, when they secured observation time at NASA’s Infrared Telescope Facility (IRTF) on Mauna Kea, Hawaii. Over two nights, they observed the asteroid and used a high-precision spectrograph to divide the asteroid’s incoming light into various frequencies, or colors, the relative intensities of which can give scientists an idea of an object’s composition. From their analysis, the team determined that the asteroid’s surface is composed mainly of silicate, a dry, rocky material, similar to most other asteroids, and, more importantly, not at all like most comets. Comets typically come from the far colder edges of the solar system. When they approach the sun, any surface ice instantly sublimates, or vaporizes into gas, creating the comet’s characteristic tail. Since Marsset’s team has found 6478 Gault is a dry, rocky body, this means it likely is generating dust tails by some other active mechanism. A fresh change As the team observed the asteroid, they discovered, to their surprise, that the rock was changing color in the near-infrared, from red to blue. “We've never seen such a dramatic change like this over such a short period of time,” says co-author DeMeo. The scientists say they are likely seeing the asteroid’s surface dust, turned red over millions of years of exposure to the sun, being ejected into space, revealing a fresh, less irradiated surface beneath, that appears blue at near-infrared wavelengths. “Interestingly, you only need a very thin layer to be removed to see a change in the spectrum,” DeMeo says. “It could be as thin as a single layer of grains just microns deep.” So what could be causing the asteroid to turn color? The team and other groups studying 6478 Gault believe the reason for the color shift, and the asteroid’s comet-like activity, is likely due to the same mechanism: a fast spin. The asteroid may be spinning fast enough to whip off layers of dust from its surface, through sheer centrifugal force. The researchers estimate it would need to have about a two-hour rotation period, spinning around every couple of hours, versus Earth’s 24-hour period. “About 10 percent of asteroids spin very fast, meaning with a two- to three-hour rotation period, and it’s most likely due to the sun spinning them up,” says Marsset. This spinning phenomenon is known as the YORP effect (or, the Yarkovsky-O’Keefe-Radzievskii-Paddack effect, named after the scientists who discovered it), which refers to the effect of solar radiation, or photons, on small, nearby bodies such as asteroids. While asteroids reflect most of this radiation back into space, a fraction of these photons is absorbed, then reemitted as heat, and also momentum. This creates a small force that, over millions of years, can cause the asteroid to spin faster. Astronomers have observed the YORP effect on a handful of asteroids in the past. To confirm a similar effect is acting on 6478 Gault, researchers will have to detect its spin through light curves — measurements of the asteroid’s brightness over time. The challenge will be to see through the asteroid’s considerable dust tail, which can obscure key portions of the asteroid’s light. Marsset’s team, along with other groups, plan to study the asteroid for further clues to activity, when it next becomes visible in the sky. “I think [the group’s study] reinforces the fact that the asteroid belt is a really dynamic place,” DeMeo says. “While the asteroid fields you see in the movies, all crashing into each other, is an exaggeration, there is definitely a lot happening out there every moment.” This research was funded, in part, by the NASA Planetary Astronomy Program. ",Space & Time,0.09986691148111519
23,IEEE,"SpaceX, OneWeb, or Kepler Communications: Who Really Launched the First Ku-band Satellite?",Space & Time,2019-08-30,-,https://spectrum.ieee.org/tech-talk/aerospace/satellites/spacex-oneweb-or-kepler-communications-who-really-launched-the-first-kuband-satellite,"      No one denies that the Soviet Union put the first man-made object into orbit nor, a few wackos aside, that American astronauts were first to reach the moon. But deciding which of three companies’ new broadband internet satellites was first to launch is proving somewhat more contentious. Elon Musk’s SpaceX, SoftBank’s OneWeb and Canadian start-up Kepler Communications are all claiming that they launched the first satellites capable of delivering high-speed internet using Ku-band (12-18 GHz) frequencies. At stake is far more than just bragging rights or national pride. According to US law, the first operator to launch gets first choice of back-up spectrum should there be any interference between the rival systems—a near certainty given the more than 10,000 satellites they intend to deploy. The Federal Communications Commission (FCC) now finds itself in the bizarre position of having to rule on something that might seem utterly obvious but that will affect the future of all three companies. It all seemed much simpler around the turn of the millennium, when two companies, Teledesic and Skybridge, announced plans for a few hundred low earth orbit (LEO) internet satellites. The FCC quickly settled on a plan for them to share the available spectrum. All the satellites could use the entire frequency range in the US, only switching to non-interfering frequencies during in-line events—when a ground station happened to line up with satellites from both systems. This would happen just six percent of the time, the FCC concluded. In those rare cases, the first operator to have launched would get to choose which half of the spectrum it would prefer to use. “But as there is no difference in quantity or kind between the halves of the spectrum,” the FCC wrote, “This first choice, a kind of coordination priority, has little significance.” In the end, it didn’t matter at all, as Teledesic and Skybridge both ran out of money before launching a single commercial satellite. But fast forward to 2019, and the significance of the FCC’s decision is looming much larger. “The rules were made for two systems and for just a few hundred satellites,” says Tim Farrar, president of satellite and telecommunications consulting firm TMF Associates. “If you’ve got thousands going around, you’ll have conjunctions happening almost all the time, and then you’re effectively splitting the band in two almost everywhere.” In a worst case scenario, operators would be operating with half the spectrum, and effectively half the capacity, their systems were designed for. Suddenly, the quality of an operator’s “home spectrum”—the dedicated frequencies it can retreat to—looks far more important. “Some portions of the Ku-band have more terrestrial incumbent users than other portions,” wrote OneWeb in a letter to the FCC. “Home spectrum matters... because it allows an operator to maximize network capacity and, in turn, service to customers by choosing the portion of the frequency band in which it prefers to operate.” Another issue is that only the first two systems can choose home spectrum bands anchored to either end of the available spectrum. “If there’s going to be more than two operators, you really, really want to be one of the first two,” says Farrar. “If you’re the third, you could be all hopping all over the place.” The home spectrum rules only come into play if the operators cannot agree beforehand how to coordinate with one another’s systems. But while everyone agrees that would be the best solution for all concerned, the challenges of such complex negotiations between multiple operators are considerable. “With operators all coming online at different times, we anticipate conversations will always be ongoing,” OneWeb told IEEE Spectrum. In order to hedge their bets, SpaceX, OneWeb and Kepler would all very much like to be first in line to choose their home spectrum. After launching its initial six satellites in February, OneWeb sent a letter to the FCC asserting victory. It wrote: “OneWeb hereby notifies the Commission that the first space station in the OneWeb System has met the requirement to be launched and capable of operating.... [Therefore] OneWeb hereby claims first priority in home spectrum selection in the Ku-band.” In May, Kepler put in its own claim, noting that its KIPP spacecraft reached orbit more than a year earlier, in January 2018, and had since been carrying out commercial operations. “As far as Kepler is aware, this launch represented the first deployment of a Ku-band satellite within the Processing Round, and as such it should have first priority in any selection of home spectrum within Ku-band,” the company wrote to the FCC. “Needless to say, it is disappointing to see OneWeb try and undercut Kepler’s position.” A few weeks after that, SpaceX made its own case. Despite launching its first experimental satellite in February 2018 (after Kepler), and its first 60 commercial satellites in May 2019 (after OneWeb), SpaceX believes that under FCC regulations, it still officially launched first. “The scope of this rule makes clear that to be considered ‘capable of operating,’ an operator must not only launch satellites but must also communicate with a U.S.-licensed earth station in the specific frequency band,” it wrote to the Commission. Although OneWeb applied for an FCC license for its earth stations before SpaceX, it has not yet been granted. SpaceX, however, applied for and was granted a special temporary license to communicate with its first batch of satellites shortly before their launch. OneWeb and Kepler dispute this interpretation, calling it “flawed” and “extraordinary,” with both sides insisting that their readings embody common sense. SpaceX notes that a foreign operator that launched first but had no intention of offering service in the US could hold domestic systems hostage. Kepler, on the other hand, points out that “to preserve fairness... home spectrum selection order cannot be based...on arbitrary barriers such as approval delays.” SpaceX has another Starlink mission scheduled for September, and OneWeb hopes to launch 30 satellites at a time on future rockets. With Tim Farrar estimating that the FCC could take 6 to 12 months to rule on home spectrum priority, the skies could be full of satellites by the time we find out, officially, who actually launched first.  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Space & Time,0.09795415462100274
24,ACM,"Waterloo Researchers Develop 200X Faster, Low-Cost Network for 5G Connectivity",ACM,2019-08-29,-,https://www.ibtimes.sg/waterloo-researchers-develop-200-times-faster-low-cost-network-5g-connectivity-32273,"       Waterloo Researchers Develop 200X Faster, Low-Cost Network for 5G Connectivity     International Business TimesSoorya KiranAugust 29, 2019   Researchers at the University of Waterloo in Canada have developed a more affordable and efficient technique to enable 5G wireless connectivity for Internet of Things (IoT) devices. The mmX millimeter wave network delivers multi-gigahertz of unlicensed bandwidth, 200-fold more bandwidth than that allocated to current Wi-Fi and cellular networks. MmX supports a significantly higher bitrate than Wi-Fi and Bluetooth. Said Waterloo's Ali Abedi. ""Any sensor you have in your home, which traditionally used Wi-Fi and lower frequency, can now communicate using high-speed millimeter wave networks. Autonomous cars are also going to use a huge number of sensors in them which will be connected through wire; now you can make all of them wireless and more reliable.""                 ",Telecom,0.09613335708891615
25,MIT News,"New science blooms after star researchers die, study finds",Research,2019-08-29,-,http://news.mit.edu/2019/life-science-funding-researchers-die-0829,"  The famed quantum physicist Max Planck had an idiosyncratic view about what spurred scientific progress: death. That is, Planck thought, new concepts generally take hold after older scientists with entrenched ideas vanish from the discipline. “A great scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it,” Planck once wrote. Now a new study co-authored by MIT economist Pierre Azoulay, an expert on the dynamics of scientific research, concludes that Planck was right. In many areas of the life sciences, at least, the deaths of prominent researchers are often followed by a surge in highly cited research by newcomers to those fields. Indeed, when star scientists die, their subfields see a subsequent 8.6 percent increase, on average, of articles by researchers who have not previously collaborated with those star scientists. Moreover, those papers published by the newcomers to these fields are much more likely to be influential and highly cited than other pieces of research. “The conclusion of this paper is not that stars are bad,” says Azoulay, who has co-authored a new paper detailing the study’s findings. “It’s just that, once safely ensconsed at the top of their fields, maybe they tend to overstay their welcome.” The paper, “Does Science Advance one Funeral at a Time?” is co-authored by Azoulay, the International Programs Professor of Management at the MIT Sloan School of Management; Christian Fons-Rosen, an assistant professor of economics at the University of California at Merced; and Joshua Graff Zivin, a professor of economics at the University of California at San Diego and faculty member in the university’s School of Global Policy and Strategy. It is forthcoming in the American Economic Review. To conduct the study, the researchers used a database of life scientists that Azoulay and Graff Zivin have been building for well over a decade. In it, the researchers chart the careers of life scientists, looking at accomplishments that include funding awards, published papers and the citations of those papers, and patent statistics. In this case, Azoulay, Graff Zivin, and Fons-Rosen studied what occurred after the unexpected deaths of 452 life scientists, who were still active in their disciplines. In addition to the 8.6 percent increase in papers by new entrants to those subfields, there was a 20.7 percent decrease in papers by the rather smaller number of scientists who had previously co-authored papers with the star scientists. Overall, Azoulay notes, the study provides a window into the power structures of scientific disciplines. Even if well-established scientists are not intentionally blocking the work of researchers with alternate ideas, a group of tightly connected colleagues may wield considerable influence over journals and grant awards. In those cases, “it’s going to be harder for those outsiders to make a mark on the domain,” Azoulay notes. “The fact that if you’re successful, you get to set the intellectual agenda of your field, that is part of the incentive system of science, and people do extraordinary positive things in the hope of getting to that position,” Azoulay notes. “It’s just that, once they get there, over time, maybe they tend to discount ‘foreign’ ideas too quickly and for too long.” Thus what the researchers call “Planck’s Principle” serves as an unexpected — and tragic — mechanism for diversifying bioscience research. The researchers note that in referencing Planck, they are extending his ideas to a slightly different setting than the one he himself was describing. In his writing, Planck was discussing the birth of quantum physics — the kind of epochal, paradigm-setting shift that rarely occurs in science. The current study, Azoulay notes, examines what happens in everyday “normal science,” in the phrase of philosopher Thomas Kuhn. The process of bringing new ideas into science, and then hanging on to them, is only to be expected in many areas of research, according to Azoulay. Today’s seemingly stodgy research veterans were once themselves innovators facing an old guard. “They had to hoist themselves atop the field in the first place, when presumably they were [fighting] the same thing,” Azoulay says. “It’s the circle of life.” Or, in this case, the circle of life science. The research received support from the National Science Foundation, the Spanish Ministry of Economy and Competitiveness, and the Severo Ochoa Programme for Centres of Excellence in R&D. ",Society,0.09054789349791724
26,Stanford,Ancient die-off greater than dinosaur extinction,Science,2019-08-31,-,https://news.stanford.edu/2019/08/28/ancient-die-off-greater-dinosaur-extinction/,"   Clues from Canadian rocks formed billions of year ago reveal a previously unknown loss of life even greater than that of the mass extinction of the dinosaurs 65 million years ago, when Earth lost nearly three-quarters of its plant and animal species.  This photograph shows rocks from the Belcher Islands in Hudson Bay, Canada, from which doctoral candidate Malcolm Hodgskiss collected barite samples dating 2.02 to 1.87 billion years old. (Image credit: Malcolm Hodgskiss)  Rather than prowling animals, this die-off involved miniscule microorganisms that shaped the Earth’s atmosphere and ultimately paved the way for those larger animals to thrive. “This shows that even when biology on Earth is comprised entirely of microbes, you can still have what could be considered an enormous die-off event that otherwise is not recorded in the fossil record,” said Malcolm Hodgskiss, co-lead author of a new study published in Proceedings of the National Academy of Sciences. Invisible clues Because this time period preceded complex life, researchers cannot simply dig up fossils to learn what was living 2 billion years ago. Even clues left behind in mud and rocks can be difficult to uncover and analyze. Instead, the group turned to barite, a mineral collected from the Belcher Islands in Hudson Bay, Canada, that encapsulates a record of oxygen in the atmosphere. Those samples revealed that Earth experienced huge changes to its biosphere – the part of the planet occupied by living organisms – ending with an enormous drop in life approximately 2.05 billion years ago that may also be linked to declining oxygen levels. “The fact that this geochemical signature was preserved was very surprising,” Hodgskiss said. “What was especially unusual about these barites is that they clearly had a complex history.” Looking at the Earth’s productivity through ancient history provides a glimpse into how life is likely to behave over its entire existence – in addition to informing observations of atmospheres on planets outside our solar system. “The size of the biosphere through geologic time has always been one of our biggest questions in studying the history of the Earth,” said Erik Sperling, an assistant professor of geological sciences at Stanford who was not involved with the study. “This new proxy demonstrates how interlinked the biosphere and levels of oxygen and carbon dioxide in the atmosphere are.” Biological angle This relationship between the proliferation of life and atmospheric oxygen has given researchers new evidence of the hypothesized “oxygen overshoot.” According to this theory, photosynthesis from ancient microorganisms and the weathering of rocks created a huge amount of oxygen in the atmosphere that later waned as oxygen-emitting organisms exhausted their nutrient supply in the ocean and became less abundant. This situation is in contrast to the stable atmosphere we know on Earth today, where the oxygen created and consumed balances out. The researchers’ measurements of oxygen, sulfur and barium isotopes in barite support this oxygen overshoot hypothesis. The research helps scientists hone their estimates of the size of the oxygen overshoot by revealing the significant biological consequences of oxygen levels above or below the capacity of the planet. “Some of these oxygen estimates likely require too many microorganisms living in the ocean in Earth’s past,” said co-lead author Peter Crockford, a postdoctoral researcher at the Weizmann Institute of Science and Princeton University. “So we can now start to narrow in on what the composition of the atmosphere could have been through this biological angle.” Co-authors include researchers from Nanjing University, the University of Colorado Boulder and Woods Hole Oceanographic Institution The research was supported by Stanford University McGee and Compton Grants, the Northern Scientific Training Program, NSERC, National Geographic, the American Philosophical Society, Geological Society of America and the Agouron Institute. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Matter & Energy,0.08264827320998318
27,Stanford,How immigration in Seattle is driving urban change,Science,2019-08-31,-,https://news.stanford.edu/2019/08/28/immigration-seattle-driving-urban-change/,"   A competitive housing market combined with the rapid rise of immigration is driving gentrification in Seattle’s low-cost black neighborhoods, according to a new study by Stanford sociologist Jackelyn Hwang.  Jackelyn Hwang examined gentrification in two very different cities: Chicago and Seattle. (Image credit: Steve Gladfelter)  While gentrification – which Hwang defines as an influx of investment and middle/upper-income residents into previously low-income neighborhoods – is more likely to occur in areas with higher populations of African Americans, areas with higher populations of Asians have not seen that same level of redevelopment in Seattle – a divergence Hwang’s data suggests is attributed to immigration. Her findings have been published in City & Community. This research follows similar work Hwang has carried out to better understand the relationship between neighborhood change and inequality in U.S. cities. Hwang hopes this research could help policymakers to consider long-term implications of economic redevelopment and investment, especially its effect on housing for disadvantaged residents. Here, Hwang found that arriving immigrants, who are predominantly Asian in Seattle, have concentrated in neighborhoods with more Asians, which has deterred gentrification in those areas. Combined with tight housing constraints, pressure has shifted to low-cost African American neighborhoods where an influx of investment and of middle- and upper-middle-class residents has led to demographic changes, Hwang said. “The results suggest that increased immigration to a city with a tight housing market may have unintended consequences on black urban neighborhoods,” said Hwang, an assistant professor of sociology in the Stanford School of Humanities & Sciences. “Because black urban residents may disproportionately face displacement and subsequent disadvantages on the housing market, the findings have implications for the future prospects of housing for blacks.” Seattle as a case study The study grew out of an earlier study Hwang conducted on gentrification in Chicago, a historically highly segregated city. Hwang found that the higher the percentage of African Americans in a neighborhood, the less likely it was to gentrify. She found that a threshold of 40 percent black residents in a neighborhood limited gentrification. Hwang wondered whether the same pattern she found in Chicago would occur in a city with lower levels of segregation that was also undergoing gentrification. Seattle was a compelling city to examine, Hwang said. “Because it has low segregation levels, Seattle doesn’t really have neighborhoods with these high concentrations of minorities. It actually has diverse neighborhoods that should satisfy these preferences,” Hwang said.  Seattle has low levels of segregation compared with many other cities, but is still experiencing some of the problems that come with gentrification. (Image credit: Getty Images)  Drawing on U.S. Census data, Hwang found that in the 1970s and 1980s, the same tendency to avoid neighborhoods that are primarily made up of minorities that she found in Chicago also occurred in Seattle: Gentrification was more likely to occur in neighborhoods with a higher percentage of whites. But in the 1990s and 2000s, a different trend emerged in Seattle. Unlike Chicago, Hwang found that gentrification began happening in neighborhoods with greater numbers of African Americans. However, she did not see the same pattern in Seattle’s areas with more Asians. Hwang wanted to know what led one minority area to gentrify over the other. So, she examined a wide set of possible explanations, including racial preferences of neighborhoods, race-based biases related to neighborhood quality, socioeconomic differences among African Americans and Asians in Seattle, and state policies such as transit options and public housing. Hwang found that none of those factors had an effect – except one: Neighborhoods where immigrants were settling did not gentrify. Seattle’s Asian population has rapidly increased, said Hwang. In 2013, 14 percent of the total population was comprised of Asians, double its size in 1980. Hwang’s analysis showed that a 1 percentage point increase in a neighborhood’s share of Asians in 1990 was associated with a 3.9 percent decrease in the odds of a neighborhood gentrifying by 2013. However, when there was a 1 percentage point increase in a neighborhood’s share of African Americans it was associated with a 3.4 percent increase in the odds of a neighborhood gentrifying. Unintended consequences Hwang found that as neighborhoods with higher shares of African Americans gentrified, these same areas also experienced large declines in their black populations. As cities transform, Hwang calls for policymakers to consider long-term implications and the unintended consequences of economic redevelopment and investment, especially the impact on housing for disadvantaged residents. “Given the long-standing inequities that blacks face when it comes to residential inequality and the housing market, the findings of this study call for targeted interventions to stem processes of neighborhood change exacerbating disadvantage for black urban residents,” said Hwang, noting that policies and programs should address racial wealth disparities. “Although black neighborhoods were once neglected and in decline, the recent investment and socioeconomic upgrading in these areas should not benefit only its landlords and newcomers.” While Seattle’s ethno-racial groups may be distinct from groups in highly segregated cities often studied in gentrification, her findings may extend to other diversifying cities in the United States with low levels of segregation and experiencing gentrification, such as Portland, Oregon, and Portland, Maine, Hwang said in the paper. Hwang is currently working on a national and multilevel analysis to check whether these patterns are happening elsewhere, as well as a nationwide study on gentrification’s consequences on segregation. The work was funded by the Joint Center for Housing Studies at Harvard University and Eunice Kennedy Shriver National Institute of Child Health and Human Development.  ",Society,0.06070528312779187
