,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,MIT News,Artificial intelligence could help data centers run far more efficiently,Research,2019-08-21,-,http://news.mit.edu/2019/decima-data-processing-0821,"  A novel system developed by MIT researchers automatically “learns” how to schedule data-processing operations across thousands of servers — a task traditionally reserved for imprecise, human-designed algorithms. Doing so could help today’s power-hungry data centers run far more efficiently. Data centers can contain tens of thousands of servers, which constantly run data-processing tasks from developers and users. Cluster scheduling algorithms allocate the incoming tasks across the servers, in real-time, to efficiently utilize all available computing resources and get jobs done fast. Traditionally, however, humans fine-tune those scheduling algorithms, based on some basic guidelines (“policies”) and various tradeoffs. They may, for instance, code the algorithm to get certain jobs done quickly or split resource equally between jobs. But workloads — meaning groups of combined tasks — come in all sizes. Therefore, it’s virtually impossible for humans to optimize their scheduling algorithms for specific workloads and, as a result, they often fall short of their true efficiency potential. The MIT researchers instead offloaded all of the manual coding to machines. In a paper being presented at SIGCOMM, they describe a system that leverages “reinforcement learning” (RL), a trial-and-error machine-learning technique, to tailor scheduling decisions to specific workloads in specific server clusters. To do so, they built novel RL techniques that could train on complex workloads. In training, the system tries many possible ways to allocate incoming workloads across the servers, eventually finding an optimal tradeoff in utilizing computation resources and quick processing speeds. No human intervention is required beyond a simple instruction, such as, “minimize job-completion times.” Compared to the best handwritten scheduling algorithms, the researchers’ system completes jobs about 20 to 30 percent faster, and twice as fast during high-traffic times. Mostly, however, the system learns how to compact workloads efficiently to leave little waste. Results indicate the system could enable data centers to handle the same workload at higher speeds, using fewer resources. “If you have a way of doing trial and error using machines, they can try different ways of scheduling jobs and automatically figure out which strategy is better than others,” says Hongzi Mao, a PhD student in the Department of Electrical Engineering and Computer Science (EECS). “That can improve the system performance automatically. And any slight improvement in utilization, even 1 percent, can save millions of dollars and a lot of energy in data centers.” “There’s no one-size-fits-all to making scheduling decisions,” adds co-author Mohammad Alizadeh, an EECS professor and researcher in the Computer Science and Artificial Intelligence Laboratory (CSAIL). “In existing systems, these are hard-coded parameters that you have to decide up front. Our system instead learns to tune its schedule policy characteristics, depending on the data center and workload.” Joining Mao and Alizadeh on the paper are: postdocs Malte Schwarzkopf and Shaileshh Bojja Venkatakrishnan, and graduate research assistant Zili Meng, all of CSAIL.  RL for scheduling  Typically, data processing jobs come into data centers represented as graphs of “nodes” and “edges.” Each node represents some computation task that needs to be done, where the larger the node, the more computation power needed. The edges connecting the nodes link connected tasks together. Scheduling algorithms assign nodes to servers, based on various policies. But traditional RL systems are not accustomed to processing such dynamic graphs. These systems use a software “agent” that makes decisions and receives a feedback signal as a reward. Essentially, it tries to maximize its rewards for any given action to learn an ideal behavior in a certain context. They can, for instance, help robots learn to perform a task like picking up an object by interacting with the environment, but that involves processing video or images through an easier set grid of pixels. To build their RL-based scheduler, called Decima, the researchers had to develop a model that could process graph-structured jobs, and scale to a large number of jobs and servers. Their system’s “agent” is a scheduling algorithm that leverages a graph neural network, commonly used to process graph-structured data. To come up with a graph neural network suitable for scheduling, they implemented a custom component that aggregates information across paths in the graph — such as quickly estimating how much computation is needed to complete a given part of the graph. That’s important for job scheduling, because “child” (lower) nodes cannot begin executing until their “parent” (upper) nodes finish, so anticipating future work along different paths in the graph is central to making good scheduling decisions. To train their RL system, the researchers simulated many different graph sequences that mimic workloads coming into data centers. The agent then makes decisions about how to allocate each node along the graph to each server. For each decision, a component computes a reward based on how well it did at a specific task — such as minimizing the average time it took to process a single job. The agent keeps going, improving its decisions, until it gets the highest reward possible. Baselining workloads One concern, however, is that some workload sequences are more difficult than others to process, because they have larger tasks or more complicated structures. Those will always take longer to process — and, therefore, the reward signal will always be lower — than simpler ones. But that doesn’t necessarily mean the system performed poorly: It could make good time on a challenging workload but still be slower than an easier workload. That variability in difficulty makes it challenging for the model to decide what actions are good or not. To address that, the researchers adapted a technique called “baselining” in this context. This technique takes averages of scenarios with a large number of variables and uses those averages as a baseline to compare future results. During training, they computed a baseline for every input sequence. Then, they let the scheduler train on each workload sequence multiple times. Next, the system took the average performance across all of the decisions made for the same input workload. That average is the baseline against which the model could then compare its future decisions to determine if its decisions are good or bad. They refer to this new technique as “input-dependent baselining.” That innovation, the researchers say, is applicable to many different computer systems. “This is general way to do reinforcement learning in environments where there’s this input process that effects environment, and you want every training event to consider one sample of that input process,” he says. “Almost all computer systems deal with environments where things are constantly changing.” Aditya Akella, a professor of computer science at the University of Wisconsin at Madison, whose group has designed several high-performance schedulers, found the MIT system could help further improve their own policies. “Decima can go a step further and find opportunities for [scheduling] optimization that are simply too onerous to realize via manual design/tuning processes,” Akella says. “The schedulers we designed achieved significant improvements over techniques used in production in terms of application performance and cluster efficiency, but there was still a gap with the ideal improvements we could possibly achieve. Decima shows that an RL-based approach can discover [policies] that help bridge the gap further. Decima improved on our techniques by a [roughly] 30 percent, which came as a huge surprise.” Right now, their model is trained on simulations that try to recreate incoming online traffic in real-time. Next, the researchers hope to train the model on real-time traffic, which could potentially crash the servers. So, they’re currently developing a “safety net” that will stop their system when it’s about to cause a crash. “We think of it as training wheels,” Alizadeh says. “We want this system to continuously train, but it has certain training wheels that if it goes too far we can ensure it doesn’t fall over.” ",Computer Science,0.20807926125272694
1,Stanford,Atomically thin heat shield protects electronics,Science,2019-08-22,-,https://news.stanford.edu/2019/08/16/atomically-thin-heat-shield-protects-electronics/,"   Excess heat given off by smartphones, laptops and other electronic devices can be annoying, but beyond that it contributes to malfunctions and, in extreme cases, can even cause lithium batteries to explode.  This greatly magnified image shows four layers of atomically thin materials that form a heat-shield just two to three nanometers thick, or roughly 50,000 times thinner than a sheet of paper. (Image credit: National Institute of Standards and Technology)  To guard against such ills, engineers often insert glass, plastic or even layers of air as insulation to prevent heat-generating components like microprocessors from causing damage or discomforting users. Now, Stanford researchers have shown that a few layers of atomically thin materials, stacked like sheets of paper atop hot spots, can provide the same insulation as a sheet of glass 100 times thicker. In the near term, thinner heat shields will enable engineers to make electronic devices even more compact than those we have today, said Eric Pop, professor of electrical engineering and senior author of a paper published Aug. 16 in Science Advances. “We’re looking at the heat in electronic devices in an entirely new way,” Pop said. Detecting sound as heat The heat we feel from smartphones or laptops is actually an inaudible form of high-frequency sound. If that seems crazy, consider the underlying physics. Electricity flows through wires as a stream of electrons. As these electrons move, they collide with the atoms of the materials through which they pass. With each such collision an electron causes an atom to vibrate, and the more current flows, the more collisions occur, until electrons are beating on atoms like so many hammers on so many bells – except that this cacophony of vibrations moves through the solid material at frequencies far above the threshold of hearing, generating energy that we feel as heat. Thinking about heat as a form of sound inspired the Stanford researchers to borrow some principles from the physical world. From his days as a radio DJ at Stanford’s KZSU 90.1 FM, Pop knew that music recording studios are quiet thanks to thick glass windows that block the exterior sound. A similar principle applies to the heat shields in today’s electronics. If better insulation were their only concern, the researchers could simply borrow the music studio principle and thicken their heat barriers. But that would frustrate efforts to make electronics thinner. Their solution was to borrow a trick from homeowners, who install multi-paned windows – usually, layers of air between sheets of glass with varying thickness – to make interiors warmer and quieter. “We adapted that idea by creating an insulator that used several layers of atomically thin materials instead of a thick mass of glass,” said postdoctoral scholar Sam Vaziri, the lead author on the paper. Atomically thin materials are a relatively recent discovery. It was only 15 years ago that scientists were able to isolate some materials into such thin layers. The first example discovered was graphene, which is a single layer of carbon atoms and, ever since it was found, scientists have been looking for, and experimenting with, other sheet-like materials. The Stanford team used a layer of graphene and three other sheet-like materials – each three atoms thick – to create a four-layered insulator just 10 atoms deep. Despite its thinness, the insulator is effective because the atomic heat vibrations are dampened and lose much of their energy as they pass through each layer. To make nanoscale heat shields practical, the researchers will have to find some mass production technique to spray or otherwise deposit atom-thin layers of materials onto electronic components during manufacturing. But behind the immediate goal of developing thinner insulators looms a larger ambition: Scientists hope to one day control the vibrational energy inside materials the way they now control electricity and light. As they come to understand the heat in solid objects as a form of sound, a new field of phononics is emerging, a name taken from the Greek root word behind telephone, phonograph and phonetics. “As engineers, we know quite a lot about how to control electricity, and we’re getting better with light, but we’re just starting to understand how to manipulate the high-frequency sound that manifests itself as heat at the atomic scale,” Pop said. Eric Pop is an affiliate of the Precourt Institute for Energy. Stanford authors include former postdoctoral scholars Eilam Yalon and Miguel Muñoz Rojo, and graduate students Connor McClellan, Connor Bailey, Kirby Smithe, Alexander Gabourie, Victoria Chen, Sanchit Deshmukh and Saurabh Suryavanshi. Other authors are from Theiss Research and the National Institute of Standards and Technology. This research was supported by the Stanford Nanofabrication Facility, the Stanford Nano Shared Facilities, the National Science Foundation, the Semiconductor Research Corporation, the Defense Advanced Research Projects Agency, the Air Force Office of Scientific Research, the Stanford SystemX Alliance, the Knut and Alice Wallenberg Foundation, the Stanford Graduate Fellowship program and the National Institute of Standards and Technology. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Electronics and Technology,0.15088255153223304
2,Science Daily,Understanding the Animal Brain Could Help Robots Wash Your Dishes,Environment,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821173704.htm,"   Anthony Zador, M.D., Ph.D., has spent his career working to describe, down to the individual neuron, the complex neural networks that make up a living brain. But he started his career studying artificial neural networks (ANNs). ANNs, which are the computing systems behind the recent AI revolution, are inspired by the branching networks of neurons in animal and human brains. However, this broad concept is usually where the inspiration ends. In a perspective piece recently published in Nature Communications, Zador describes how improved learning algorithms are allowing AI systems to achieve superhuman performance on an increasing number of more complex problems like chess and poker. Yet, machines are still stumped by what we consider to be the simplest problems. Solving this paradox may finally enable robots to learn how to do something as organic as stalking prey or building a nest, or even something as human and mundane as doing the dishes-a task that Google CEO Eric Schmidt once called ""literally the number one request... but an extraordinarily difficult problem"" for a robot. ""The things that we find hard, like abstract thought or chess playing, are actually not the hard thing for machines. The things that we find easy, like interacting with the physical world, that's what's hard,"" Zador explained. ""The reason that we think it's easy is that we had half a billion years of evolution that has wired up our circuits so that we do it effortlessly."" That's why Zador writes that the secret to quick learning might not be a perfected general learning algorithm. Instead, he suggests that biological neural networks sculpted by evolution provide a kind of scaffolding to facilitate the quick and easy learning for specific kinds of tasks-usually those crucial for survival. For an example, Zador points to your backyard. ""You have squirrels that can jump from tree to tree within a few weeks after birth, but we don't have mice learning the same thing. Why not?"" Zador said. ""It's because one is genetically predetermined to become a tree-dwelling creature."" Zador suggests that one result of this genetic predisposition is the innate circuitry that helps guide an animal's early learning. However, these scaffolding networks are far less generalized than the perceived panacea of machine learning that most AI experts are pursuing. If ANNs identified and adapted similar sets of circuitry, Zador argues, the future's household robots might just one day surprise us with clean dishes. ",Computer Science,0.14473198901113424
3,ACM,New Tool Makes Web Browsing Easier for the Visually Impaired,ACM,2019-08-09,-,https://news.mit.edu/2019/single-photon-source-fluorescent-quantum-defects-0809,"       A Single-Photon Source You Can Make at Home     MIT NewsDaniel DarlingAugust 9, 2019   Massachusetts Institute of Technology (MIT) researchers have discovered a simple technique for generating carbon nanotube-based single-photon emitters, or fluorescent quantum defects, using household bleach and ultraviolet light. The team submerged carbon nanotubes in bleach, then irradiated them with ultraviolet light for less than 60 seconds, producing fluorescent quantum defects. Operations at longer wavelengths with brighter defect emitters allow clearer and deeper tissue penetration for optical imaging, which could augment cancer detection and treatment. Said MIT’s Angela Belcher, “We have demonstrated a clear visualization of vasculature structure and lymphatic systems using 150 times less amount of probes compared to previous generation of imaging systems,” which constitutes a step towards cancer early detection.                 ",Electronics and Technology,0.13977888746741585
4,IEEE,6 Things to Know About the Biggest Chip Ever Built,Electronics and Technology,2019-08-22,-,https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built,"        On Monday at the IEEE Hot Chips symposium at Stanford University, startup Cerebras unveiled the largest chip ever built. It is roughly a silicon wafer-size system meant to reduce AI training time from months to minutes. It is the first commercial attempt at a wafer-scale processor since Trilogy Systems failed at the task in the 1980s.  1 | The stats As the largest chip ever built, Cerebras’s Wafer Scale Engine (WSE) naturally comes with a bunch of superlatives. Here they are with a bit of context where possible:  Size: 46,225 square millimeters. That’s about 75 percent of a sheet of letter-size paper, but 56 times as large as the biggest GPU. Transistors: 1.2 trillion. Nvidia’s GV100 Volta packs in 2.1 billion. Processor cores: 400,000. Not to pick on the GV100 too much, but it has 5,660. Memory: 18 gigabytes of on-chip SRAM, about 3,000 times as much as our pal the GV100.  Memory bandwidth: 9 petabytes per second. That’s 10,000 times our favorite GPU, according to Cerebras.   2 | Why do you need this monster? Cerebras makes a pretty good case in its white paper [PDF] for why such a ridiculously large chip makes sense. Basically, the company argues that the demand for training deep learning systems and other AI systems is getting out of hand. The company says that training a new model—creating a system that, once trained, can recognize people or win a game of Go—is taking weeks or months and costing hundreds of thousands of dollars of compute time. That cost means there’s little room for experimentation, and that’s stifling new ideas and innovation. The startup’s answer is that the world needs more, and cheaper, training compute resources. Training needs to take minutes not months, and to do that you need more cores, more memory close to those cores, and a low-latency, high-bandwidth connection between the cores. Those are goals that are clearly in effect for everyone in the AI space. But, by its own admission, Cerebras took the idea to its logical extreme. A big chip offers more silicon area for processor cores and the memory that needs to snuggle up next to it. And a high-bandwidth, low-latency connection is only achievable if data never has to leave the short, dense interconnects on a chip. Thus one big chip.   3 | What’s in those 400,000 cores? According to the company, the WSE’s cores are specialized to do AI, but still programmable enough that they’re not locked into only one flavor of it. They call them Sparse Linear Algebra (SLA) cores. These processing units are specialized to “tensor” operations key to AI work, but they also include a feature that reduces the work, particularly for deep-learning networks. According to the company, 50 to 98 percent of all the data in a deep learning training set are zeros. The nonzero data is therefore “sparse.” The SLA cores cut down on the work by simply not multiplying anything by zero. The cores have built in data-flow elements that trigger computing actions based on the data, so when it encounters a zero in the data, it doesn’t waste its time.   4 | How did they do this? The fundamental idea behind Cerebras’s massive single chip has been obvious for decades, but it has also been impractical. To quote myself: Back in the 1980s, parallel computing pioneer Gene Amdahl hatched a plan to speed mainframe computing: a silicon-wafer-sized processor. By keeping most of the data on the processor itself instead of pushing it through a circuit board to memory and other chips, computing would be faster and more energy efficient. With US $230 million from venture capitalists, the most ever at the time, Amdahl founded Trilogy Systems to make his vision a reality. This first commercial attempt at “wafer-scale integration” was such a disaster that it reportedly introduced the verb “to crater” into the financial press lexicon.   The most basic problem is that the bigger the chip, the worse the yield; that’s the fraction of working chips you get from each wafer. Logically, this should mean a wafer-scale chip would be unprofitable, because there would always be flaws in your product. Cerebras’s solution is to add a certain amount of redundancy. According to EE Times, the Swarm communications networks have redundant links to route around damaged cores, and about 1 percent of the cores are spares. Cerebras also had to work around some key manufacturing limits. For one, chip tools are designed to cast their feature-defining patterns onto relatively small rectangles and do that over and over, perfectly across the wafer. That alone would keep a lot of systems from being built on a single wafer, because of the cost and difficulty of casting different patterns in different places on the wafer. But the WSE resembles a typical wafer full of the exact same chips, just as you’d ordinarily manufacture. The big difference was a method they worked out with TSMC to make connections across the space between the chips, an area called the scribe lines. This space is typically left blank because the chips are diced up along those lines. According to Tech Crunch, Cerebras also had to invent a way to provide the chips 15 kilowatts of power and cool the system as well as create new kinds of connectors that could deal with the way it expands when it heats up.    5 | Is this the only way to make a wafer-scale computer? Of course not. For example, a team at University of California, Los Angeles, and University of Illinois Urbana-Champaign is working on a similar system that would eventually use bare processor dies that have already been built and tested and mounts them on a silicon wafer that’s already patterned with the needed dense network of interconnects. This concept, called a silicon interconnect fabric, allows these dielets to sit as close as 100 micrometers from each other, allowing for interchip communication that nears the characteristics of a single chip. “This is a huge validation of the research we’ve been doing,” says the University of Illinois’s Rakesh Kumar. “We like the fact that there is commercial interest in something like this.” Kumar believes that the silicon interconnect fabric approach has some advantages over Cerebras’s monolithic wafer-scale scheme. For one, it allows a designer to mix and match technologies, and use the best manufacturing process for each. A monolithic approach means picking the best process for the most crucial subsystem—logic, for example—and using it for memory and other components even if it’s not ideal for them. In that approach, Cerebras could be limited in the amount of memory it can put on the processor, Kumar suggests. “They have 18 gigabits of SRAM on the wafer. Maybe that’s enough for some models today, but what about models tomorrow and the day after?”  6 | When does it come out? According to Fortune, the first systems ship to customers in September, and some have already received prototypes. According to EE Times, the company plans to reveal results from complete systems at the Supercomputing Conference in November.   Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Telecom,0.13664992958295674
5,Stanford,Smart faucet could help save water,Science,2019-08-22,-,https://news.stanford.edu/2019/08/20/smart-faucet-could-help-save-water/,"   Barely hidden from his study participants, William Jou, a former graduate student in mechanical engineering at Stanford University, pulled off a ruse straight out of The Wizard of Oz. Except, instead of impersonating a great and powerful wizard, Jou pretended to be an autonomous sink. He did this to test whether a sink that adapts to personal washing styles could reduce water use.  Go to the web site to view the video.  Kurt Hickman    Stanford mechanical engineering researchers tackled the humble faucet to explore how to reduce home water usage.  A faucet with anything close to the brains of a mechanical engineering student doesn’t yet exist. So, Jou and his colleagues in the lab of Erin MacDonald, assistant professor of mechanical engineering, made the next best thing: a faucet that seemed to automatically adjust to a user’s preferences, but was actually controlled by Jou. The results of their sly experiment, detailed in a paper presented Aug. 20 at the ASME 2019 International Design Engineering Technical Conferences & Computers and Information in Engineering Conference (IDETC/CIE), support the idea that thoughtfully designed smart sinks could help conserve water by regulating water use and nudging users to develop more water-conscious habits. “We looked at the faucet because that’s where a lot of water usage in the home occurs, but when you compare your sink to other products in the house – a thermostat or refrigerator – you see that there haven’t been updates to how the sink works in a very long time,” said MacDonald, who is senior author of the paper. “There have been small updates but nothing that really harnesses the power of technology.” Participants in this experiment had to wash dishes three times, with Jou secretly controlling the temperature and flow of the sink during the second washing only. With Jou involved, participants used about 26 percent less water compared to their first washing. In the third round, they still used 10 percent less water compared to the first round, even though the sink was back to being brainless. This shift in water use happened without participants knowing the experiment was about water conservation. “Water conservation is particularly relevant given our location in California,” said Samantha Beaulieu, a graduate student and co-author of the paper. “We also wanted to see if people’s habits were adjustable; if interacting with this faucet could then change how people interact with a manual faucet. The results we found seem to indicate that’s possible.” Pay no attention to the student behind the wall In order to create a situation where people would trust – and hopefully enjoy – a sink that makes water decisions for them, Jou closely monitored the participants’ washing styles during their first round of cleaning so he could emulate them in the second round.   Many drops in the bucket Both the researchers and their participants underestimated the amount of water people use when washing dishes. The experimental sink drained into a hidden bucket – for measurement purposes – and the researchers had to increase the size of that bucket twice while designing the experiment because it kept overflowing. Even with a 14-gallon bucket in place, they had to dump the water out between the second and third washings. When asked, the participants usually estimated they used about a gallon per wash. “It was a weird wakeup call that I’m probably using that much water too,” said study co-author Samantha Beaulieu.  “As the algorithm, I’m trying to use that information to leverage their cognitive style or user behavior style to see if I can help them use less water while still keeping them happy,” described Jou, who is lead author of the paper. “Whereas a lot of products today are made for general use, this is a product that’s learning about you and adapting to what your style is.” In surveys after the experiment, 96 percent of participants who interacted with the smart sink (there was a control group that washed dishes three times without Jou) said they thought there was potential for smart faucets to save water. Many of them even expressed interest in buying such a product. “Most people were pretty amazed by the sink,” said Beaulieu. “A lot of people left the experiment asking what the algorithm was or asking how it worked or how to see more. We basically told them we’d have to wait until the end of the experiment to answer those questions.” While the results from and reaction to washing with Jou’s assistance were impressive, the researchers were particularly heartened by how such a brief interaction with the “autonomous” function changed participants’ water use. “We didn’t even plan on having that third step until very late in the research, when we were pilot testing,” said MacDonald. “I never would have thought that having just one experience with ‘William the Algorithm’ people would retain the training and wash their dishes differently.” The sink of the future The researchers imagine a future where hospital sinks encourage employees to wash their hands properly and our personal sink and shower preferences can be transferred to hotels and friends’ houses. Schools and neighborhoods could organize competitions to save water and raise water conservation awareness. Through additional features, the sink could even detect leaks. That being said, creating this one sink required years of work and didn’t even include an algorithm. In addition to implanting artificial intelligence, making a version for mass production that’s actually autonomous would require sensors that could differentiate between users and between scenarios – such as washing a pot versus a fork versus hands. Still, the researchers are optimistic that studies like these could lay the groundwork to support those developments. “We’re all human beings – we have good days and bad days. A product like this could have a large impact because it’s growing and learning with you as you change,” said Jou. “This faucet is working toward saving water but it’s also keeping its user happy. In the long term, products like this might be our future.” Adrienne Lim, an undergraduate at Stanford, is also co-author of the paper. This research was funded by the National Science Foundation. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Computer Science,0.12817031775142662
6,Science Daily,Self-Folding 'Rollbot' Paves the Way for Fully Untethered Soft Robots,Computers & Math,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142714.htm,"   The research is published in Science Robotics. ""The ability to integrate active materials within 3D-printed objects enables the design and fabrication of entirely new classes of soft robotic matter,"" said Jennifer A. Lewis, the Hansjorg Wyss Professor of Biologically Inspired Engineering at SEAS and co-lead author of the study. The researchers turned to origami to create multifunctional soft robots. Through sequential folds, origami can encode multiple shapes and functionalities in a single structure. Using materials known as liquid crystal elastomers that change shape when exposed to heat, the research team 3D-printed two types of soft hinges that fold at different temperatures and thus can be programmed to fold in a specific order. ""With our method of 3D printing active hinges, we have full programmability over temperature response, the amount of torque the hinges can exert, their bending angle, and fold orientation. Our fabrication method facilitates integrating these active components with other materials,"" said Arda Kotikian, a graduate student at SEAS and the Graduate School of Arts and Sciences and co-first author of the paper. ""Using hinges makes it easier to program robotic functions and control how a robot will change shape. Instead of having the entire body of a soft robot deform in ways that can be difficult to predict, you only need to program how a few small regions of your structure will respond to changes in temperature,"" said Connor McMahan, a graduate student at Caltech and co-first author of the paper. To demonstrate this method, Kotikian, McMahan, and the team built several soft devices, including an untethered soft robot nicknamed the ""Rollbot."" The Rollbot begins as a flat sheet, about 8 centimeters long and 4 centimeters wide. When placed on a hot surface, about 200°C, one set of hinges folds and the robot curls into a pentagonal wheel. Another set of hinges is embedded on each of the five sides of the wheel. A hinge folds when in contact with the hot surface, propelling the wheel to turn to the next side, where the next hinge folds. As they roll off the hot surface, the hinges unfold and are ready for the next cycle. ""Many existing soft robots require a tether to external power and control systems or are limited by the amount of force they can exert. These active hinges are useful because they allow soft robots to operate in environments where tethers are impractical and to lift objects many times heavier than the hinges,"" said McMahan. Another device, when placed in a hot environment, can fold into a compact folded shape resembling a paper clip and unfold itself when cooled. ""These untethered structures can be passively controlled,"" said Kotikian. ""In other words, all we need to do is expose the structures to specific temperature environments and they will respond according to how we programmed the hinges."" While this research only focused on temperature responses, liquid crystal elastomers can also be programmed to respond to light, pH, humidity and other external stimuli. ""This works demonstrates how the combination of responsive polymers in an architected composite can lead to materials with self-actuation in response to different stimuli. In the future, such materials can be programmed to perform ever more complex tasks, blurring the boundaries between materials and robots,"" said Chiara Daraio, Professor of Mechanical Engineering and Applied Physics at Caltech and co-lead author of the study. This research was co-authored by Emily C. Davidson, Jalilah M. Muhammad, and Robert D. Weeks. It was supported by Army Research Office, and Harvard Materials Research Science and Engineering Center through the National Science Foundation, and the NASA Space Technology Research Fellowship. ",Robotics,0.12797273997977163
7,Stanford,Wireless sensors stick to skin and track health,Science,2019-08-22,-,https://news.stanford.edu/2019/08/16/wireless-sensors-stick-skin-track-health/,"   We tend to take our skin’s protective function for granted, ignoring its other roles in signaling subtleties like a fluttering heart or a flush of embarrassment.  Using metallic ink, researchers screen-print an antenna and sensor onto a stretchable sticker designed to adhere to skin and track pulse and other health indicators, and beam these readings to a receiver on a person’s clothing. (Image credit: Bao Lab)  Now, Stanford engineers have developed a way to detect physiological signals emanating from the skin with sensors that stick like band-aids and beam wireless readings to a receiver clipped onto clothing. To demonstrate this wearable technology, the researchers stuck sensors to the wrist and abdomen of one test subject to monitor the person’s pulse and respiration by detecting how their skin stretched and contracted with each heartbeat or breath. Likewise, stickers on the person’s elbows and knees tracked arm and leg motions by gauging the minute tightening or relaxation of the skin each time the corresponding muscle flexed. Zhenan Bao, the chemical engineering professor whose lab described the system in an Aug. 15 article in Nature Electronics, thinks this wearable technology, which they call BodyNet, will first be used in medical settings such as monitoring patients with sleep disorders or heart conditions. Her lab is already trying to develop new stickers to sense sweat and other secretions to track variables such as body temperature and stress. Her ultimate goal is to create an array of wireless sensors that stick to the skin and work in conjunction with smart clothing to more accurately track a wider variety of health indicators than the smart phones or watches consumers use today. “We think one day it will be possible to create a full-body skin-sensor array to collect physiological data without interfering with a person’s normal behavior,” said Bao, who is also the K.K. Lee Professor in the School of Engineering. Stretchable, comfortable, functional Postdoctoral scholars Simiao Niu and Naoji Matsuhisa led the 14-person team that spent three years designing the sensors. Their goal was to develop a technology that would be comfortable to wear and have no batteries or rigid circuits to prevent the stickers from stretching and contracting with the skin. Their eventual design met these parameters with a variation of the RFID – radiofrequency identification – technology used to control keyless entry to locked rooms. When a person holds an ID card up to an RFID receiver, an antenna in the ID card harvests a tiny bit of RFID energy from the receiver and uses this to generate a code that it then beams back to the receiver.  The rubber sticker attached to the wrist can bend and stretch as the person’s skin moves, beaming pulse readings to a receiver clipped to the person’s clothing. (Image credit: Bao Lab)  The BodyNet sticker is similar to the ID card: It has an antenna that harvests a bit of the incoming RFID energy from a receiver on the clothing to power its sensors. It then takes readings from the skin and beams them back to the nearby receiver. But to make the wireless sticker work, the researchers had to create an antenna that could stretch and bend like skin. They did this by screen-printing metallic ink on a rubber sticker. However, whenever the antenna bent or stretched, those movements made its signal too weak and unstable to be useful. To get around this problem, the Stanford researchers developed a new type of RFID system that could beam strong and accurate signals to the receiver despite constant fluctuations. The battery-powered receiver then uses Bluetooth to periodically upload data from the stickers to a smartphone, computer or other permanent storage system. The initial version of the stickers relied on tiny motion sensors to take respiration and pulse readings. The researchers are now studying how to integrate sweat, temperature and other sensors into their antenna systems. To move their technology beyond clinical applications and into consumer-friendly devices, the researchers need to overcome another challenge – keeping the sensor and receiver close to each other. In their experiments, the researchers clipped a receiver on clothing just above each sensor. One-to-one pairings of sensors and receivers would be fine in medical monitoring, but to create a BodyNet that someone could wear while exercising, antennas would have to be woven into clothing to receive and transmit signals no matter where a person sticks a sensor. Bao is also a senior fellow of the Precourt Institute for Energy, a member of Stanford Bio-X, a faculty fellow of Stanford ChEM-H, an affiliate of the Stanford Woods Institute for the Environment and a member of the Wu Tsai Neurosciences Institute. Other Stanford co-authors are Jeffrey B.-H. Tok, research scientist; Ada Poon, associate professor of electrical engineering; William Burnett, adjunct professor of mechanical engineering; postdoctoral scholars Yuanwen Jiang and Jinxing Li; graduate student Jiechen Wang; and former visiting scholar Youngjun Yun and former postdoctoral scholars Sihong Wang, Xuzhou Yan and Levent Beker. Researchers from Singapore’s Nanyang Technological University also co-authored the study. This research was supported by Samsung Electronics; the Singapore Agency for Science, Technology and Research; the Japan Society for the Promotion of Science; and the Stanford Precision Health and Integrated Diagnosis Center. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Computer Science,0.12540005730553458
8,Stanford,Making physics more inclusive,Science,2019-08-22,-,https://news.stanford.edu/2019/08/14/making-physics-inclusive/,"   It was no surprise when, in 2016, a Stanford University survey of undergraduates revealed physics as among the least diverse departments at the university – also the case for physics as an academic field nationally. But, deeper analysis of the survey responses revealed a telling and crucial difference between the answers from incoming students and those on their way to graduation.  Students in the spring break program “Physics Outreach through Inclusive Science Education” (POISE) during a trip to Monterey, where they visited the Monterey Bay Aquarium. (Image credit: Courtesy of Josie Meyer)  “Many students from all backgrounds and identities come to Stanford excited about physics, and this interest does not strongly depend on race or gender. But we lose a larger number of Black, Latinx and Native students, as well as women of all races, in the first two years of undergraduate study,” said Risa Wechsler, a professor of physics and of particle physics and astrophysics at Stanford University. “A lot of that is due to the lack of community and overall climate. People from underrepresented groups often do not feel welcome in physics classes.” For Wechsler, who is also director of the Kavli Institute for Particle Astrophysics and Cosmology, this issue is personal. Even she experienced doubts about her place in physics early in her studies and career. To address this problem head on, Wechsler and six other physics faculty members formed the Equity and Inclusion Committee, which also includes students, research staff and postdoctoral scholars. While the committee was formulating a strategic plan, students formed the group Physics Undergraduate Women and Gender Minorities at Stanford (PUWMAS) and Wechsler joined Heising-Simons Foundation’s Physics and Astronomy Leadership Council. That position, which aims to help the foundation diversify physics in the United States, provided her with grant money that she could use to fund local projects. The fact that these efforts are helmed and helped along by so many people, representing various areas of the Physics Department, reflects a larger trend in the field. “In the last five to eight years, there’s been a growing awareness about the importance of identity within the physics community,” said Lauren Tompkins, assistant professor of physics and member of the Equity and Inclusion Committee. “Your identity affects your experience as a physicist and even the physics that you do. If we can acknowledge and understand that, it makes us better physicists.” Groups, classes and a book club PUWMAS was one of the earliest results of this current push for more inclusive physics at Stanford. Undergraduates Deepti Kannan, Nicel Mohamed-Hinds and Manisha Patel founded the group with the mission to “promote diversity in physics by uniting and uplifting minority voices” and “provide opportunities for personal, academic and career development.” In service of these missions, the group organizes social events, a lunchtime speaker series, a mentorship program and professional development workshops on topics such as CV/resume writing, negotiation and interviewing.   Other inclusive efforts Physics 41E: The same as Physics 41: Mechanics, which is a required course for physics majors, but with added support. Students from underrepresented groups often don’t have the same level of preparation from high school as their majority peers. The difference in preparation is large enough that it may lead students to drop out of the major but small enough that the kind of support offered by this course can be enough to keep them in. Physics 94SI: Diverse Perspectives in Physics: A seminar course, also initiated by Meyer and Patel, where physics faculty members from diverse backgrounds share the story of their lives and careers. This course meets over lunch and includes Q&A sessions after each presentation. Rising Stars in Physics: Stanford and MIT co-sponsor this workshop that brings together top early career women in physics and astronomy who are interested in careers in academia. The two-day workshop includes research presentations, panels on issues relevant to academic careers and informal networking opportunities.  “Developing this kind of group involves risk and took incredible effort but it has paid off,” said Wechsler. “It’s only two years old and I feel like they have transformed the atmosphere for undergrads in the department.” In 2018, PUWMAS won an Office of Student Engagement Campus Impact Award. Upon their graduation this year, the founders received the department’s first-ever Departmental Service Award for their efforts. Amidst the success of PUWMAS, co-founder Patel and inaugural director of diversity and education Josie Meyer, BS ’19, brought Wechsler and Tompkins another idea for further development, focused on the fact that students who leave the department often do so between winter and spring quarter. Led by Meyer and Patel, and advised by Tompkins and Wechsler, the team created a pair of initiatives. Physics 93SI: Beyond the Laboratory: Physics, Identity and Society is a new student-taught course that explores issues of diversity and culture in physics directly, drawing directly from disciplines as varied as history, anthropology and critical race theory. Physics Outreach through Inclusive Science Education (POISE) is an optional extension of the course where students spend spring break developing an hour-long workshop for high school students while learning and applying lessons about inclusion in science. “We want the students to see that they can pursue their love of physics and still make a social impact in the world,” said Meyer. “One reason for leaving the department frequently cited by underrepresented minorities is that they feel physics is not relevant to their communities, and I hope POISE changes that.” POISE was offered for the first time in 2019, supported by Wechsler’s Heising-Simons funding. It included field trips to Bay Area museums, labs, companies and Pinnacles National Park. At the end, students taught workshops about the physics of sound and music, spacetime and the universe, and the physics of earthquakes to students at San Lorenzo High School. One unintended advantage was that all of the POISE students were in their first or last years of undergraduate study, which created natural opportunities for mentorship and access to perspectives from students who were in high school last year. “Having a science class where students were asked to reflect about their experience allowed them to bring their whole selves to their work in a way that was empowering,” said Tompkins, who is the faculty advisor for 93SI, POISE, PUWMAS and the Stanford University Physics Society. In a broader effort, the Equity and Inclusion Committee also created the Equity and Inclusion reading group, which has met twice a quarter since early 2018 and is open to anyone interested in equity in STEM. Each meeting features a speaker from the department talking about themes related to the latest reading. Just the beginning Now, Meyer, Tompkins and Wechsler are assessing how the first year of POISE went and what it might look like in 2021. They have also been interested in efforts centered on incoming potential physics students. Ideally, students would know right away about the community the department is building and improving – and reach out to the department about what more they might do.  POISE students at San Lorenzo High School, where they presented their teaching projects. (Image credit: Courtesy of Josie Meyer)  “We want to have the best people in the world involved in this research, but also I really believe that it is a human right to understand how the universe came to be,” said Wechsler. “So, we need to think very deliberately about having a scientific community that includes the whole world’s population. And we’re not there.” Meyer graduated in June but stayed over the summer as the department’s Equity and Inclusion intern. Some of her work so far has included developing training for future POISE leaders, constructing a “Tools for Practical Allyship” workshop series and developing a midterm assessment for progress on the Equity and Inclusion Strategic Plan. “Students should feel empowered to create positive change within the Stanford physics department and for the rest of their careers,” she said. “Over the past few years, undergraduate students have led the way toward a more inclusive climate in the physics community and I hope we can inspire the next generation of leaders to do the same.” To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Computer Science,0.12376872877191546
9,Science Daily,Scientists Probe How Distinct Liquid Organelles in Cells Are Created,Environment,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821163818.htm,"   One way these biological compounds stay organized is through membrane-less organelles (MLOs) -- wall-less liquid droplets made from proteins and RNA that clump together and stay separate from the rest of the cellular stew. You can think of these fluid compartments as being akin to oil droplets in water. MLOs facilitate storage of molecules within cells and can serve as a center of biochemical activity, recruiting molecules needed to carry out essential cellular reactions. Though these droplets are plentiful within cells, they represent an emerging field of study in cell biology. Little is known about how they are created and maintained with unique functionalities. To address this knowledge gap, one University at Buffalo laboratory is using cutting-edge scientific techniques to probe the fundamental properties of how MLOs work. The research is led by Priya R. Banerjee, PhD, an assistant professor of physics in the UB College of Arts and Sciences. In a paper published on Aug. 21 in Scientific Reports, Banerjee and colleagues report that MLOs may be highly sensitive to the level of divalent cations inside cells. This is important because divalent calcium and magnesium ions aid in cellular signaling and are vital to life. In experiments, MLOs containing both proteins and RNA form when divalent cations were found in low concentrations. But when concentrations of these cations were high, liquid organelles holding only RNA molecules were favored. The tests were systematically performed using controlled model systems comprising protein and RNA molecules floating in a buffer solution. ""It's interesting because you haven't changed the basic ingredients,"" Banerjee says. ""But when you alter the ionic environment, you find that these organelles are highly tunable. They 'switch' from one type to the other, with each type having a distinct internal design."" The study was led by Banerjee and Ashok Deniz, PhD, associate professor of integrative structural and computational biology at Scripps Research, a nonprofit medical research institution. The team demonstrated that fluctuations in divalent cations can profoundly tune the liquid properties of MLOs, altering the internal environment of the droplet. This is important since cells are believed to control some MLO functionality by changing their interior design. The concept of tunable intracellular droplet organelles is currently being actively investigated in Banerjee's lab at UB. In a separate paper published earlier in 2019, Banerjee and colleagues explored another fundamental property of MLOs: conditions that drive such droplets to switch from a fluid, liquidy state to a harder, gel-like state. ""The concept that protein and nucleic acid droplets can function as organelles in a cell has started shifting the paradigm of cell biology that is written in a textbook,"" Banerjee says. ""Reports started emerging from several different laboratories across the world that MLOs are relevant in gene regulation, protection of cells during stress, immune response and many other biological functions, as well as diseases such as neurological disorders and cancer. Therefore, understanding how MLOs are formed, tuned and altered in diseases are of key importance in the field now."" ",Electronics and Technology,0.1196673827877911
10,IEEE,How Robotics Teams Are Solving the Biggest Problem at DARPA’s Subterranean Challenge,Robotics,2019-08-21,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt,"      When DARPA announced its Subterranean Challenge, the agency framed things with a comprehensive list of “technical challenge elements” that it expected to be particularly, you know, challenging. One of those elements was communication constraints, which DARPA said teams should expect to be “severe.” That may have been an understatement, based on what we're seeing at the SubT Tunnel Circuit—teams have had a lot of trouble consistently talking to their robots on the course. Even though most teams are emphasizing autonomy as much as possible, they do still have to deal with communication challenges, because finding artifacts won’t earn you any points unless the robot reports its location back to base before time runs out. And if your robot has to come all the way back to base to make its report, it’s going to run out of time after finding just a few artifacts. There’s no magical solution for this, which is awesome, because teams have come up with all kinds of fantastically creative and unique strategies. We went around and asked them about it when we visited the SubT Tunnel Circuit in Pittsburgh yesterday.  Some quick context to help you understand the challenge here—robots usually do fine through the first several tens of meters just past the mine entrance. Once they turn the first corner, however, they lose their wireless connection back to the base station almost immediately, because radio waves don’t pass through solid rock. And even if the robots manage to hang on to a signal for a little bit, the twists and turns and sheer distances that the robots must travel (hundreds and hundreds of meters, ideally) means it simply isn’t possible to maintain a direct connection back to base.  Most teams are adopting a robot-to-robot mesh networking approach to help with this, meaning that the robots themselves can serve as network nodes, and if one robot can communicate back to base, any robot that it can communicate with can also communicate back to base—and so on down as long a chain of robots as you can manage. Robots need to move around to do their jobs, though, and teams only have a limited number of robots, so here are some other strategies that teams are using to keep in contact. This is about as straightforward as it gets—Team PLUTO mounted massive dipole antennas on the butts of their Ghost Robotics Vision 60 quadrupeds, which in their experience improves communication performance by an order of magnitude. The team is emphasizing autonomy, and like many teams, their robots are designed to operate for extended periods without any communications at all, but they do still need to report back on what they found from time to time. Team Explorer, currently leading (by a lot) in artifacts found, uses deployable network nodes (pulled halfway out of a protective casing in the picture above) that its robots can drop when the strength of the network starts getting low. Each robot can carry about 10 nodes in the two racks that you can see, and while the nodes are dropped at the command of the remote operator at the moment, the robots will eventually be able to decide autonomously when they need to plop one down. The team also plans to shrink the nodes down, and to improve the dropping mechanism, which can get jammed by mud. The beefy deployable network node idea was a popular one, and Team CoSTAR (tied for second place in artifacts found at the end of day three) sent in multiple robots carrying nodes in several different configurations.  DARPA not-so-subtly suggested that teams may not want to rely on a physical tether between the base station and robots (“teams should seriously consider the limitations [on tethers] imposed by large-scale, potentially dynamic, complex environments,” says the agency), but Team CERBERUS didn’t let that scare them. They’re using a fiber-optic tether and a dedicated communications robot with a massive antenna on it to essentially extend their base station deep into the mine. The tether isn’t long enough to explore the whole mine, of course, and it has to be carefully managed around corners, but even if the robot just makes it down to the end of the first passage and around the first corner, it’s a massive improvement. These are “Anchorballs” that Team NCTU’s robots can deploy as mesh network nodes, but they’re also used as active landmarks to help with SLAM localization. They’re weighted at the bottom so that, after being dropped, they stop rolling with a camera pointing at the ceiling, and the robot can then correlate the image from the Anchorball camera with its own internal map.  After a bit of communication trouble, Team NCTU decided that they needed to boost their system a bit, so they deployed their own tether of sorts yesterday. If that looks like a router wrapped in a plastic bag connected to a really long Ethernet cord, well, you nailed it. The team placed the router on a robot, let the cable out from the base station as the robot drove into the mine, and then just gave the cable a tug to pull the router off the robot and onto the ground once the robot got far enough to deploy the router where the team wanted it. These are some of the most cleverly designed droppable network nodes that we’ve seen—once dropped, the node waits for a few seconds for the robot to drive itself off, and then uses an actuator and springs to flip itself up and deploy its antennas. The reflective surfaces are a nice touch as well; presumably, they help DARPA find the dropped nodes at the end of a run, while also encouraging any following robots not to run over them quite as much. Why drop a network node that just sits there, when you could instead drop a network node that can drive itself around? Team CRETISE is dropping FirstLook robots from a beastly mothership robot to serve as cute little mobile nodes that can zip around to optimize your network. The FirstLooks are themselves descendants of LANdroids, which were developed by iRobot with DARPA funding so long ago that I wrote about them back before I was even writing about robots, in early 2007. The original idea with LANdroids was that they would self-deploy to create dynamic and resilient mesh networks in challenging environments, but eventually iRobot turned them into the FirstLook tossable surveillance robots. And now, more than a decade later, Team CRETISE is turning them back into LANdroids again, which is pretty cool.  In addition to little puck-shaped deployable mesh-network nodes, Team MARBLE has been experimenting with these robots built on top of remote-controlled off-road racing cars. They have onboard sensing and computing and communications, of course, but they’re not intended to search for artifacts. Instead, their job is to ferry information—quickly navigating between areas with base station connectivity and other (slower) robots that are exploring elsewhere. Rather than acting as mobile nodes, these little cars are more like delivery robots, picking up data and carrying it back home. Without the constraints of having to maintain a network, the idea is that the exploration robots will be able to travel much farther while spending more time exploring autonomously, relying on the information-ferrying cars for all of their communication needs. The team doesn’t have it all working yet, but it’s a super interesting idea, and we’re looking forward to seeing it in action.  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11359150324861672
11,Stanford,What it’s like to be a chemist,Science,2019-08-22,-,https://news.stanford.edu/2019/08/19/what-its-like-to-be-a-chemist/,"   Michael Fayer surveys the world around him and sees something that most of us do not: chemistry. It’s not that Fayer, a professor of chemistry, sees no value in anything but academic chemistry research. It’s just that when it comes down to it, chemistry has more to do with ordinary human experience than any other field. To Fayer, everything from color to nutrition to the greenhouse effect is, first and foremost, chemistry.   What it’s like... Physics is a social endeavor. There’s beauty in neuroscience. Chemistry is all about building things – and it’s all hard work. Find out more about what it’s like to be a researcher at Stanford.  What it’s like to be a chemist What it’s like to be a neuroscientist What it’s like to be a theoretical physicist    “Chemistry just permeates our lives,” he said. He is not alone in this view. For Noah Burns, it is the basis of life. For Laura Dassama, it is the path to advances in health. And for Hemamala Karunadasa, chemistry is the foundation of civilizations. Chemists call their discipline “the central science” for a reason, because it touches everything in our daily lives. Here, Fayer, Burns, Dassama and Karunadasa share how they got into chemistry, the joys and frustrations of their academic lives and what chemistry, the central science, means to them. Fayer is the David Mulvane Ehrsam and Edward Curtis Franklin Professor in Chemistry. Burns is an assistant professor of chemistry and a member of Stanford ChEM-H. Dassama is an assistant professor of chemistry and an Institute Scholar of ChEM-H. Karunadasa is an assistant professor of chemistry and a Center Fellow of the Precourt Institute for Energy. Photographs by L.A. Cicero ",Matter & Energy,0.10653989056255433
12,Science Daily,Separate Polarization and Brightness Channels Give Crabs the Edge Over Predators,Environment,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142717.htm,"   The key advantage of this method is that the separate visual channels provide a greater range of information for the crab. The research also suggests that when it comes to detecting predators', polarisation can provide a more reliable source of information than brightness. The researchers from the Ecology of Vision Group in the School of Biological Sciences tested how crabs responded to visual stimuli presented on a special computer monitor developed by the lab. By changing the polarisation and brightness of the stimuli the researchers were able to test whether certain combinations of polarisation and brightness appeared to cancel each other out. Sam Smithers, PhD student in the School of Biological Sciences and one of the authors, said: ""If you look through Polaroid sunglasses at the sky, you will notice that the brightness changes when you tilt your head. This is because the light from the sky is polarised and your sunglasses allow you to see differences in polarisation as differences in brightness."" ""For our experiments we tested whether fiddler crabs see the same effect. However, we discovered that the crabs detect polarisation in a completely different way to this and polarisation has no effect on how the crabs detect the brightness of a scene."" The next step for the research team is to find out what happens to the brightness and polarisation information deeper in the brain. This will help them to understand how and when polarisation and colour information provide a visual advantage to the viewer. ",Space & Time,0.09963034833391603
13,Stanford,Online dating is the most popular way couples meet,Science,2019-08-22,-,https://news.stanford.edu/2019/08/21/online-dating-popular-way-u-s-couples-meet/,"   Algorithms, and not friends and family, are now the go-to matchmaker for people looking for love, Stanford sociologist Michael Rosenfeld has found.  Online dating has become the most common way for Americans to find romantic partners. (Image credit: altmodern / Getty Images)  In a new study published in the Proceedings of the National Academy of Sciences, Rosenfeld found that heterosexual couples are more likely to meet a romantic partner online than through personal contacts and connections. Since 1940, traditional ways of meeting partners – through family, in church and in the neighborhood – have all been in decline, Rosenfeld said. Rosenfeld, a lead author on the research and a professor of sociology in the School of Humanities and Sciences, drew on a nationally representative 2017 survey of American adults and found that about 39 percent of heterosexual couples reported meeting their partner online, compared to 22 percent in 2009. Sonia Hausen, a graduate student in sociology, was a co-author of the paper and contributed to the research. Rosenfeld has studied mating and dating as well as the internet’s effect on society for two decades. Stanford News Service interviewed Rosenfeld about his research.   What’s the main takeaway from your research on online dating? Meeting a significant other online has replaced meeting through friends. People trust the new dating technology more and more, and the stigma of meeting online seems to have worn off. In 2009, when I last researched how people find their significant others, most people were still using a friend as an intermediary to meet their partners. Back then, if people used online websites, they still turned to friends for help setting up their profile page. Friends also helped screen potential romantic interests.   What were you surprised to find? I was surprised at how much online dating has displaced the help of friends in meeting a romantic partner. Our previous thinking was that the role of friends in dating would never be displaced. But it seems like online dating is displacing it. That’s an important development in people’s relationship with technology.   What do you believe led to the shift in how people meet their significant other? There are two core technological innovations that have each elevated online dating. The first innovation was the birth of the graphical World Wide Web around 1995. There had been a trickle of online dating in the old text-based bulletin board systems prior to 1995, but the graphical web put pictures and search at the forefront of the internet. Pictures and search appear to have added a lot to the internet dating experience. The second core innovation is the spectacular rise of the smart phone in the 2010s. The rise of the smart phone took internet dating off the desktop and put it in everyone’s pocket, all the time. Also, the online dating systems have much larger pools of potential partners compared to the number of people your mother knows, or the number of people your best friend knows. Dating websites have enormous advantages of scale. Even if most of the people in the pool are not to your taste, a larger choice set makes it more likely you can find someone who suits you.   Does your finding indicate that people are increasingly less social? No. If we spend more time online, it does not mean we are less social. When it comes to single people looking for romantic partners, the online dating technology is only a good thing, in my view. It seems to me that it’s a basic human need to find someone else to partner with and if technology is helping that, then it’s doing something useful. The decline of meeting partners through family isn’t a sign that people don’t need their family anymore. It’s just a sign that romantic partnership is taking place later in life. In addition, in our study we found that the success of a relationship did not depend on whether the people met online or not. Ultimately, it doesn’t matter how you met your significant other, the relationship takes a life of its own after the initial meeting.   What does your research reveal about the online world? I think that internet dating is a modest positive addition to our world. It is generating interaction between people that we otherwise wouldn’t have. People who have in the past had trouble finding a potential partner benefit the most from the broader choice set provided by the dating apps. Internet dating has the potential to serve people who were ill-served by family, friends and work. One group of people who was ill-served was the LGBTQ+ community. So the rate of gay couples meeting online is much higher than for heterosexual couples.   You’ve studied dating for over two decades. Why did you decide to research online dating? The landscape of dating is just one aspect of our lives that is being affected by technology. And I always had a natural interest in how new technology was overturning the way we build our relationships. I was curious how couples meet and how has it changed over time. But no one has looked too deeply into that question, so I decided to research it myself. ",Society,0.09718154611525147
14,Science Daily,New Cyclization Reactions for Synthesizing Macrocyclic Drug Leads,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142721.htm,"   In fact, macrocycles show a great ability to bind difficult targets that have flat, featureless surfaces. This has raised tremendous interest in the pharmaceutical industry, which is particularly interested in macrocyclic compounds with a molecular weight below 1 KDa, which would be small enough to cross the cell membrane and reach intracellular disease targets, e.g. proteins or even genes in the cell. Still, there is a hurdle: there aren't enough suitable macrocycle libraries or methods to generate such small macrocycles. The compound libraries that pharmaceutical companies use today in high-throughput screens do contain 1-2 million different molecules, but those are mostly classical small molecules and only a handful are actual macrocyclics -- at most, only a few hundred. This is too small a number for the screens to yield good hits when searching for possible drug candidates against challenging disease targets. Now, scientists at EPFL have found a way to generate libraries of more than 9,000 of macrocyclic molecules below 1 KDa, all with high structural diversity. ""Initially, what we wanted to do is generate orally available or cell-permeable macrocyclic drugs,"" says Professor Christian Heinis, whose lab led the study. The libraries were generated by ""cyclizing"" short linear peptides in combination with diverse linker reagents, which promote chemical bonding. The yields of the macrocyclization reactions turned out to be so efficient that there was no need for purification. And in a key breakthrough, the new method also led to the discovery of surprisingly efficient macrocyclization reactions based on the ligation of thiol and amino groups of short peptides. The work was supported by EPFL's Biomolecular Screening Facility (BSF), headed by Gerardo Turcatti. ""EPFL has already developed the liquid handling processes to perform the combinatorial synthesis and to screen the macrocyclic compound libraries,"" he says. Screening identified binders of different disease targets, including inhibitors of thrombin, an important target of coagulation disorders. X-ray structure analysis of a thrombin inhibitor by partners in Italy showed a snug fit of the macrocycle to its target. Heinis's lab is now further developing the macrocycle synthesis approach in order to screen even larger combinatorial libraries. Working closely with the BSF with the support of NCCR Chemical Biology, the next step is to generate macrocyclic inhibitors of intracellular protein-protein interactions, for which we currently have no good inhibitors. ",Health,0.09284997456982536
15,Science Daily,Scientists Discover the Basics of How Pressure-Sensing Piezo Proteins Work,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821163637.htm,"   In the study, published Aug. 21 in Nature, the scientists used advanced microscopy techniques to image the Piezo1 protein at rest and during the application of mechanical forces. They confirmed this complex protein's structure and showed essentially how it can convert mechanical stimuli into an electrical signal. ""Our analysis shows that tension on the cell membrane in which Piezo1 is embedded can flatten and widen the protein's structure,"" said co-senior author Dr. Simon Scheuring, a professor of physiology and biophysics in anesthesiology at Weill Cornell Medicine. Dr. Scheuring and his laboratory collaborated on the study with the laboratory of Dr. Roderick MacKinnon, a professor of molecular neurobiology and biophysics at The Rockefeller University. Dr. MacKinnon was co-recipient of the Nobel Prize in Chemistry in 2003 for his work determining the structures and mechanisms of ion channel proteins. Piezo1 and Piezo2 are very large and complex proteins with unique structures. They are embedded within the membranes of certain cell types, and their function is to transduce mechanical force on cells into electrical signals that alter cell activity. Piezo1 proteins work for example in bladder cells to detect when the bladder is full, and in blood vessel-lining cells to detect and help regulate changes in blood pressure. Piezo2 proteins work in sensory nerve endings in the skin and joints, helping to mediate the senses of touch, pain, and proprioception -- the sense of how one's limbs are arranged. Advances in imaging techniques have enabled scientists in recent years to determine the basic structure of Piezo1 -- a structure that Piezo2 is thought to mostly share. From above this structure has a three-armed, propeller or ""triskelion"" appearance. From the side it looks like a shallow bowl embedded in the cell membrane, with an ion channel at its center. The latter, when opened, allows a flow of calcium and other positively charged ions into the cell. The basic mechanism by which mechanical force opens the ion channel has remained mysterious. But in the new study Dr. Scheuring and Dr. MacKinnon and their colleagues, including lead author Dr. Yi-Chih Lin, a postdoctoral associate in anesthesiology, were able to get a clearer picture of how it works. They combined cryo-electron microscopy with a less well-known technique called high-speed atomic force microscopy, which produces an image of an object essentially by feeling its surface with a super-sensitive mechanical probe. They showed with these methods that Piezo1 is a springy structure that normally bends the cell membrane where it sits, but will flatten out when, for example, a mechanical force is applied to the cell membrane. ""As the membrane tension increases, the structure of Piezo1 flattens and stretches out to occupy a larger area, which in turn opens the ion channel,"" Dr. Scheuring said. He noted the possibility that other stimuli that stretch and flatten the Piezo1 structure, such as a pulling force on its arms from the inside or on an external domain called the CED from the outside the cell, in principle could open the ion channel -- making it a suitably versatile mechanism for the wide range of cell types and physiological functions in which it works. Moreover, given this wide range of cell types -- in organs including the lungs, bladder, intestines, and pancreas, as well as in blood vessels and the sensory nervous system -- the discovery of the basic Piezo-protein mechanism could lead to new ways of understanding and treating many human diseases. To take one example, Dr. Scheuring said, if the membranes of cells lining blood vessels contain excess cholesterol they would become stiffer, increasing the background tension on embedded Piezo 1 proteins and potentially disrupting these proteins' normal ability to detect and help regulate blood pressure. ""Our finding leads to a great many predictions about Piezo proteins' roles in disease that we and others can now go and investigate,"" he said. ",Health,0.09244442670157406
16,Stanford,New theory explains earthquakes we can’t feel,Science,2019-08-22,-,https://news.stanford.edu/2019/08/21/new-theory-explains-earthquakes-cant-feel/,"   The Earth’s subsurface is an extremely active place, where the movements and friction of plates deep underground shape our landscape and govern the intensity of hazards above. While the Earth’s movements during earthquakes and volcanic eruptions have been recorded by delicate instruments, analyzed by researchers and constrained by mathematical equations, they don’t tell the whole story of the shifting plates beneath our feet.  Slow slip events often occur in subduction zones, such as the one below Olympic National Park that stretches from Northern California to the Pacific Northwest. (Image credit: Alisha Bube/iStock)  Over the past two decades, the advent of the global positioning system – including receivers with extremely sensitive sensors that capture millimeters of movement – has made scientists aware of earthquake-like phenomena that have been challenging to untangle. Among them are so-called slow slip events, or slow-moving earthquakes – sliding that occurs over weeks at a time unbeknownst to humans on the surface. These slow slip events occur all over the world and possibly help trigger larger earthquakes. The largest slow slip events occur in subduction zones, where one tectonic plate dives beneath another, eventually forming mountains and volcanoes over millions of years. New computer simulations produced by researchers at Stanford University and published online June 15 in the Journal of the Mechanics and Physics of Solids may explain these hidden movements. “Slow slip is such an intriguing phenomenon. Slow slip events are both so widespread and really so unexplained that they’re a puzzle that dangles before us as scientists that we all want to solve,” said study co-author Eric Dunham, an associate professor of geophysics in Stanford’s School of Earth, Energy & Environmental Sciences (Stanford Earth). “We’ve known about slow slip for almost 20 years and there’s still not a great understanding of why it happens.” Stealthy but strong These events are especially challenging to explain because of their unstable but sluggish nature. The fault does not slide steadily but instead, sliding periodically, accelerates, yet never reaches the point where it sends out seismic waves large enough for humans to detect. Despite their stealthy nature, slow slip events can add up. In an ice stream in Antarctica, the slow slip events occur twice daily, last 30 minutes and are equivalent to magnitude 7.0 earthquakes, Dunham said. Researchers think changes in friction explain how quickly rock on either side of the fault slips. With that in mind, they assumed slow slip events started as earthquakes, with a type of friction known as rate-weakening that makes sliding fundamentally unstable. But many laboratory friction experiments contradicted that idea. Instead, they had found that rocks from slow slip regions display a more stable kind of friction known as rate-strengthening, widely thought to produce stable sliding. The new computer simulations resolved this inconsistency by showing how slow slip can arise with contrary-seeming rate-strengthening friction. “A handful of studies had shown that there are ways to destabilize rate-strengthening friction. However, until our paper, no one had realized that if you simulated these instabilities, they actually turn into slow slip, they don’t turn into earthquakes,” according to lead author Elias Heimisson, a doctoral candidate at Stanford Earth. “We also identified a new mechanism for generating slow slip instabilities.” Laws of physics Dunham’s research group approaches unanswered questions about the Earth by considering all the possible physical processes that might be at play. In this case, faults occur in rocks that are saturated in fluid, giving them what’s known as a poroelastic nature in which the pores allow the rock to expand and contract, which changes the fluid pressure. The group was curious about how those changes in pressure can change the frictional resistance on faults. “In this case, we did not start on this project to explain slow slip events – we started on it because we knew that rocks have this poroelastic nature and we wanted to see what consequences it had,” Dunham said. “We never thought it would give rise to slow slip events and we never thought it would destabilize faults with this type of friction.” With these new simulations that account for the rock’s porous nature, the group found that as rocks get squeezed and fluids cannot escape, the pressure increases. That pressure increase reduces friction, leading to a slow slip event. “The theory is high-level,” Heimisson said. “We see these interesting things when you account for poroelasticity and people might want to use it more broadly in models of seismic cycles or specific earthquakes.” Heimisson will be creating a 3D simulation based on this theory as a postdoctoral researcher at the California Institute of Technology. Martin Almquist, a postdoctoral research fellow in the Department of Geophysics, is a co-author on the study. The research was supported by the Stanford Consortium for Induced and Triggered Seismicity, the Southern California Earthquake Center, NASA Headquarters under the NASA Earth and Space Science Fellowship Program and the Knut and Alice Wallenberg Foundation. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Environment,0.08841235727979013
17,Science Daily,Shift to More Intense Rains Threatens Historic Italian Winery,Environment,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821163820.htm,"   Researchers found a shift from steady, gentle rains to more intense storms over the past several decades has led to earlier grape harvests, even when seasonal rainfall totals are similar. Early harvests can prevent grapes from fully developing the complex flavors found in wines. Intense precipitation events represent the second most important factor, behind temperature, in predicting when grapes were ready at one vineyard that's been producing wine using traditional methods since the 1650s and recording harvest dates for 200 years, the scientists said. ""Our results are consistent with the hypothesis that the increasing tendency of precipitation intensity could exacerbate the effect of global warming on some premium wines that have been produced for almost 400 years,"" said Piero Di Carlo, associate professor of atmospheric sciences at D'Annunzio University of Chieti-Pescara in Italy. Because the winery doesn't use irrigation or other modern techniques, its harvest records more accurately reflect what was happening with the climate each year. Scientists gathered local meteorological data and used models to simulate what factors likely most influenced grape readiness. They recently reported their findings in the journal Science of the Total Environment. ""Because they haven't changed their techniques, a lot of other variables that may have changed harvest date are taken out of the picture,"" said William Brune, distinguished professor of meteorology at Penn State. ""It makes it cleaner statistically to look at things like precipitation intensity and temperature, and I think that's one reason why we were able to tease these findings out."" Previous studies have established a link between higher temperatures and earlier grape harvest dates at other European wineries. Higher rainfall totals can help offset advances in harvest dates caused by rising temperatures, but the impact of rain intensity was not well understood, the scientists said. Steady rains are better for agriculture because heavy storms cause water runoff, and plants and soils can absorb less moisture. The findings indicate a feedback in the climate system -- the increase in temperature precipitates more intense rainfall, which further advances the grape harvest date, according to the researchers. Three wine barrels in a row ""We really need to think more broadly about how increases in temperature may have an influence on other variables and how that amplification can affect not only this winery, but all wineries, and in fact all agriculture,"" Brune said. ""I think it's a cautionary tale in that regard."" The winery in the study faces particularly difficult challenges because mitigation strategies like irrigation would change the way the vineyard has cultivated its grapes for hundreds of years. Moving to higher elevations may be an option, but land is limited, and such a change could have other, unforeseen impacts on the wine, the scientists said. ""If we would like to keep our excellence in terms of production of wine, but also in other agricultural sectors, we have to be careful about climate change,"" said Di Carlo, lead author on the study. ""Even if we can use some strategies to adapt, we don't know if we can compensate for all of the effects. We could lose a valuable part of our local economy and tradition."" ",Environment,0.08075303945621888
18,Science Daily,Ocean Temperatures Turbocharge April Tornadoes Over Great Plains Region,Environment,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142729.htm,"   2019 has seen the second highest number of January to May tornadoes in the United States since 2000, with several deadly outbreaks claiming more than 38 fatalities. Why some years are very active, whereas others are relatively calm, has remained an unresolved mystery for scientists and weather forecasters. Climate researchers from the IBS Center for Climate Physics (ICCP), South Korea have found new evidence implicating a role for ocean temperatures in US tornado activity, particularly in April. Analyzing a large number of atmospheric data and climate computer model experiments, the scientists discovered that a cold tropical Pacific and/or a warm Gulf of Mexico are likely to generate large-scale atmospheric conditions that enhance thunderstorms and a tornado-favorable environment over the Southern Great Plains. This particular atmospheric situation, with alternating high- and low-pressure centers located in the central Pacific, Eastern United States and over the Gulf of Mexico, is known as the negative Pacific North America (PNA) pattern. According to the new research, ocean temperatures can boost this weather pattern in April. The corresponding high pressure over the Gulf of Mexico then funnels quickly-rotating moist air into the Great Plains region, which in turn fuels thunderstorms and tornadoes. ""Previous studies have overlooked the temporal evolution of ocean-tornado linkages. We found a clear relationship in April, but not in May ,"" says Dr. Jung-Eun Chu, lead author of the study and research fellow at the ICCP. ""Extreme tornado occurrences in the past, such as those in April 2011, were consistent with this blueprint. Cooler than normal conditions in the tropical Pacific and a warm Gulf of Mexico intensify the negative PNA, which then turbocharges the atmosphere with humid air and more storm systems,"" explains Axel Timmermann, Director of the ICCP and Professor at Pusan National University. ""Seasonal ocean temperature forecasts for April, which many climate modeling centers issue regularly, may further help in predicting the severity of extreme weather conditions over the United States,"" says June-Yi Lee, Professor at Pusan National University and Coordinating Lead Author of the 6th Assessment report of the Intergovernmental Panel on Climate Change. ""How Global Warming will influence extreme weather over North America, including tornadoes still remains unknown ,"" says. Dr. Chu. To address this question, the researchers are currently conducting ultra-high-resolution supercomputer simulations on the institute's new supercomputer Aleph. ",Environment,0.08071560158125761
19,Science Daily,"20-Million-Year-Old Skull Suggests Complex Brain Evolution in Monkeys, Apes",Matter & Energy,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142715.htm,"   ""Human beings have exceptionally enlarged brains, but we know very little about how far back this key trait started to develop,"" said lead author Xijun Ni, a research associate at the Museum and a researcher at the Chinese Academy of Sciences. ""This is in part because of the scarcity of well-preserved fossil skulls of much more ancient relatives."" As part of a long-term collaboration with John Flynn, the Museum's Frick Curator of Fossil Mammals, Ni spearheaded a detailed study of an exceptional 20-million-year-old anthropoid fossil discovered high in the Andes mountains of Chile, the skull and only known specimen of Chilecebus carrascoensis. ""Through more than three decades of partnership and close collaboration with the National Museum of Chile, we have recovered many remarkable new fossils from unexpected places in the rugged volcanic terrain of the Andes,"" Flynn said. ""Chilecebus is one of those rare and truly spectacular fossils, revealing new insights and surprising conclusions every time new analytical methods are applied to studying it."" Previous research by Flynn, Ni, and their colleagues on Chilecebus provided a rough idea of the animal's encephalization, or the brain size relative to body size. A high encephalization quotient (EQ) signifies a large brain for an animal of a given body size. Most primates have high EQs relative to other mammals, although some primates -- especially humans and their closest relatives -- have even higher EQs than others. The latest study takes this understanding one step further, illustrating the patterns across the broader anthropoid family tree. The resulting ""PEQ"" -- or phylogenetic encephalization quotient, to correct for the effects of close evolutionary relationships -- for Chilecebus is relatively small, at 0.79. Most living monkeys, by comparison, have PEQs ranging from 0.86 to 3.39, with humans coming in at an extraordinary 13.46 and having expanded brain sizes dramatically even compared to nearest relatives. With this new framework, the researchers confirmed that cerebral enlargement occurred repeatedly and independently in anthropoid evolution, in both New and Old World lineages, with occasional decreases in size. High-resolution x-ray computed tomography (CT) scanning and 3D digital reconstruction of the inside of Chilecebus' skull gave the research team new insights into the anatomy of its brain. In modern primates, the size of the visual and olfactory centers in the brain are negatively correlated, reflecting a potential evolutionary ""trade-off,"" meaning that visually acute primates typically have weaker senses of smell. Surprisingly, the researchers discovered that a small olfactory bulb in Chilecebus was not counterbalanced by an amplified visual system. This finding indicates that in primate evolution the visual and olfactory systems were far less tightly coupled than was widely assumed. Other findings: The size of the opening for the optic nerve suggests that Chilecebus was diurnal. Also, the infolding (sulcus) pattern of the brain of Chilecebus, although far simpler than in most modern anthropoids, possesses at least seven pairs of sulcal grooves and is surprisingly complex for such an ancient primate. ""During his epic voyage on the Beagle, Charles Darwin explored the mouth of the canyon where Chilecebus was discovered 160 years later. Shut out of the higher cordillera by winter snow, Darwin was inspired by 'scenes of the highest interest' his vista presented. This exquisite fossil, found just a few kilometers east of where Darwin stood, would have thrilled him,"" said co-author André Wyss from the University of California Santa Barbara. ",Matter & Energy,0.07995875968347373
20,Stanford,Food security from oceans,Science,2019-08-22,-,https://news.stanford.edu/2019/08/20/food-security-oceans/,"   The world will have an additional 2 billion people to feed over the next 30 years – and doing that without decimating the planet’s resources will require exploring as many options as possible. Yet, a significant option – seafood – is often overlooked in global food security planning and discussions about future diets.  Stanford scientists say food from the oceans should play an important role in feeding a growing global human population, along with much more protein from plants. (Image credit: Pexels.com)  Stanford Report spoke with Jim Leape, co-director of the Stanford Center for Ocean Solutions, and Rosamond Naylor, the William Wrigley Professor in Earth System Science, about integrating oceans into a sustainable and equitable food future. Leape is an expert in seafood sustainability issues, with over three decades of experience in conservation; he serves on the board of the Marine Stewardship Council and on the Global Future Council for the Environment of the World Economic Forum. Naylor’s research focuses on economic and biophysical dimensions of food security and environmental impacts of food production. Both will take part in a panel discussion about oceans and food security during the Planetary Health Annual Meeting at Stanford Sept. 4-6. The event aims to deepen understanding of how people’s impacts on the environment affect human health.   Why focus on oceans as a solution to global food security? Leape: Food production takes up around 40 percent of the planet’s land surface, and livestock represents almost 80 percent of that agricultural land – livestock is the world’s largest user of natural resources. Thinking about it another way, of all the mammals on Earth, 96 percent are livestock and humans – only 4 percent are wild. As the global population grows from 7 to 10 billion people, and growing prosperity increases appetites for protein, it will be vitally important that we become less reliant on cattle, pigs, sheep and chickens to produce the protein we consume. Food from the oceans, along with much more protein from plants, will be a key part of the solution. Naylor: Oceans provide an alternative source of protein to beef or chicken, and can create more resilience in the overall food system by meeting global food demands and averting price shocks in a single sector. The broader the portfolio for food production, the more resilient the system to shocks, including climate shocks. Aquaculture, or aquafarming, is actually the fastest growing segment of the world food economy. It offers promise of meeting food demands, but also carries the risk of ecological damage to marine ecosystems if not managed in an environmentally sound way. Oceans also provide important input for animal feeds. In these ways, the oceans play a major, yet not widely recognized, role in the global food system.   What are some examples of win-win opportunities in terms of protecting oceans/seafood? Naylor: When fish is farmed in a sustainable way – not feeding aquafarms with wild-caught fish – it can reduce pressure on wild fisheries while also adding to fish supplies. Sustainable aquaculture practices can also prevent diseases and parasites that might spread to wild fish populations and can harm local habitats if fish are treated with medicines or pesticides. Sustainable fisheries management by small-scale fishing communities also provides a win-win outcome by raising incomes over the long run and protecting fish populations from over-fishing. Leape: Harvest of wild fish rose sharply in the last several decades, driving many fisheries into decline and even collapse. We’ve since learned that if we exclude some areas for fishing – creating parks, or marine protected areas – we can restore the health of reefs, seagrass beds and other vital marine ecosystems and at the same time restore the productivity of fish stocks. Protected reserves allow fish to grow big and to produce many more offspring, and thus offer an actual win-win for the ocean and for fishers.   What are some surprising ways the oceans can help mitigate food insecurity? Naylor: Most people focus on the role of fish in meeting protein requirements for a growing global population in the future. But many types of fish also contribute significantly to micronutrient demands. The most important problem related to food insecurity today is nutrition insecurity. Most of the world’s population – with the exception of communities in protracted conflict – has access to adequate calories. Yet 2 to 3 billion people around the world suffer from micronutrient deficiencies that compromise physical and cognitive development. The kinds of fish that are typically used in animal feeds, like anchovies and sardines, also provide omega-3 fatty acids and essential vitamins (D, A and B) and minerals (calcium, iodine, zinc, iron and selenium) to consumers.   What is an example of an exciting/promising development in the seafood sector? Naylor: Aquaculture, when practiced sustainably, is the most important development in the seafood sector, and now contributes over half the fish consumed directly by humans. Within the aquaculture sector, innovations in fish feed technologies are key to reducing the dependence on wild fish and adding to global net fish supplies. Feed innovations include, for example, the development of algal-based and insect feeds, genetic engineering to promote long-chain omega-3 fatty acids in plant-based feeds, and the production of high-quality fish proteins from methane capture in wastewater and natural gas processing. The opportunities in feed innovation are massive. Leape: Over the last couple of decades we have also seen the rise of the sustainable seafood movement – scores of consumers, fishers, processors, retailers and chefs who have committed to producing and selling seafood from well-managed fisheries. Large companies like Walmart and McDonald’s have helped take this movement mainstream. Today, in some of the world’s most important seafood sectors, such as whitefish and wild salmon, most fisheries are now certified as sustainable. As new technological capabilities come online, it will increasingly be possible to trace each fish back to the boat that caught it, and the day it was caught.   What can individual people do to improve ocean health? Leape: One of the most important things you can do every day is be careful about the seafood you buy. Look for seafood that carries the blue logo of the Marine Stewardship Council or that is rated highly by Seafood Watch or another guide. If seafood isn’t labeled, ask the waiter or the clerk at the fish counter where it came from. You’ll be able to make better choices, and you’ll be sending a strong signal to the restaurant or the store that they should pay attention too.   What are the major obstacles to and opportunities for improving ocean health and expanding the role of seafood in food security? Leape: For millennia, our ability to protect the health of the oceans has been hampered by the fact that it has been impossible to know very much about what is happening in the water or even on the surface. That is now rapidly changing, as new sensors in the water, on satellites, on boats and even on fishing nets provide a new era of transparency in the use of ocean resources. This allows us to manage those resources much more successfully and to create real accountability for those who abuse them. Leape is also the William and Eva Price Senior Fellow at the Stanford Woods Institute for the Environment. Naylor is a senior fellow and founding director of Stanford’s Center for Food Security and the Environment, a senior fellow at the Stanford Woods Institute and the Freeman Spogli Institute for International Studies, and a professor of economics, by courtesy, at Stanford University. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Environment,0.07993606971751896
21,Science Daily,Nordic Bronze Age Attracted Wide Variety of Migrants to Denmark,Matter & Energy,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142723.htm,"   The 2nd and 3rd millennia BC are known to have been a period of significant migrations in western Europe, including the movement of steppe populations into more temperate regions. Starting around 1600 BC, southern Scandinavia became closely linked to long-distance metal trade elsewhere in Europe, which gave rise to a Nordic Bronze Age and a period of significant wealth in the region of present-day Denmark. In this study, Frei and colleagues investigated whether patterns of migration changed during this Nordic Bronze Age. They examined skeletal remains of 88 individuals from 37 localities across present-day Denmark. Since strontium isotopes in tooth enamel can record geographic signatures from an early age, analysis of such isotopes was used to determine individuals' regions of provenance. Radiocarbon dating was used to determine the age of each skeleton and physical anthropological analyses were also conducted to add information on sex, age and potential injuries or illness. From c. 1600 BC onwards, around the beginning of the Nordic Bronze Age, the geographic signal of migrants became more varied, an indication that this period of economic growth attracted migrants from a wide variety of foreign locales, possibly including more distant regions. The authors suggest this might reflect the establishment of new cultural alliances as southern Scandinavia flourished economically. They propose that further study using ancient DNA may further elucidate such social dynamics at large scales. Co-author Kristian Kristiansen notes: ""Around 1600 BC, the amount of metal coming into southern Scandinavia increased dramatically, arriving mostly from the Italian Alps, whereas tin came from Cornwall in south England. Our results support the development of highly international trade, a forerunner for the Viking Age period."" Karin Frei adds: ""Our data indicates a clear shift in human mobility at the breakthrough point of the Nordic Bronze Age, when an unprecedented rich period in southern Scandinavia emerged. This suggests to us that these aspects might have been closely related."" ",Matter & Energy,0.07698087854569328
22,Science Daily,Earliest Evidence of Artificial Cranial Deformation in Croatia During 5th-6th Century,Matter & Energy,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142725.htm,"   The Hermanov vinograd archaeological site in Osijek Croatia has been known since the 1800s. A new pit excavated in 2013 contained three human skeletons dating to 415-560 CE, during the Great Migration Period, a time of significant movement and interaction of various European cultures. Two of the skeletons showed dramatically modified head shapes, one whose skull had been lengthened obliquely and another whose skull had been compressed and heightened. This is the oldest known incidence of Artificial Cranial Deformation (ACD) in Croatia. ACD is the practice of modifying the skull from infancy to create a permanently altered shape, often to signify social status. In this study, genetic, isotopic and skeletal analysis of the bodies revealed that all were males between 12 and 16 years of age at death and that they all suffered from malnutrition. They are not obviously of different social status, but genetic analysis found that the two with cranial modifications exhibited very distinct ancestries, one from the Near East and the other from East Asia. The latter is the first individual from the Migration Period with a majority East Asian ancestry to be found in Europe. The authors suggest the ACD observed here may have functioned to distinguish members of different cultural groups as these groups interacted closely during the Migration Period. From the evidence at hand, it is unclear if these individuals were associated with Huns, Ostrogoths, or another population. It is also unclear whether the use of ACD to signify cultural identity was a widespread practice or something peculiar to these individuals. Dr Novak adds: ""The most striking observation, based on nuclear ancient DNA, is that these individuals vary greatly in their genetic ancestries: the individual without artificial cranial deformation shows broadly West Eurasian associated-ancestry, the individual with the so-called circular-erect type cranial deformation has Near Eastern associated-ancestry, while the individual with the elongated skull has East Asian ancestry."" ",Matter & Energy,0.07194558656533204
23,Science Daily,Most Patients Willing to Share Medical Records for Research Purposes,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821163816.htm,"   But the looming, lingering question is to what degree are individual patients willing to share medical records and biospecimens with researchers and institutions beyond their personal physician or health care system? And more specifically, how should patients be asked and what information are they most likely to share? In a novel attempt to answer these questions, researchers at University of California San Diego School of Medicine, with collaborators in California, North Carolina and Texas, asked patients at two academic hospitals to respond to a variety of different approaches seeking to share their medical data with other researchers. The findings are published in the August 21, 2019 online issue of JAMA Network Open. The survey was conducted at two academic hospitals -- UC San Diego Health and UC Irvine Health between May 1, 2017 and September 31, 2018. Participants were randomly selected to one of four options with different layouts and formats for indicating sharing preferences: opt-in simple, opt-in detailed, opt-out simple and opt-out detailed. In the simple forms, there were 18 categories where participants could choose to share information; in the detailed forms, there were 59 items. The items ranged from demographics like age, sex and race and socioeconomic status to lab results (genetic tests, drug screening, etc.), imaging (x-rays, MRI) and biospecimens (blood, urine, tissue). Participants were also asked to what degree they would be willing to share their medical data: with researchers only in the same health care organization or with those working at other nonprofit or for-profit institutions. Among 1,800 eligible participants, 1,246 completed the data sharing survey and were included in the analysis and 850 responded to a satisfaction survey. Slightly less than 60 percent were female and slightly less than 80 percent were white. The mean age was 51 years old. More than 67 percent of survey participants indicated they would share all items with researchers from the home institution (which patients presumably already trust with their health care), with progressively smaller percentages for sharing with other nonprofit institutions or with other for-profit institutions. Many of the respondents indicated that they were only unwilling to share a few items. ""These results are important because data from a single institution is often insufficient to achieve statistical significance in research findings,"" said the study's senior author, Lucila Ohno-Machado, MD, PhD, professor of medicine, associate dean for informatics and technology in the UC San Diego School of Medicine and chair of the Department of Biomedical Informatics at UC San Diego Health. ""When sample sizes are small, it is unclear whether the research findings generalize to a larger population. Additionally, in alignment with the concept of personalized medicine, it is important to see whether it is possible to personalize privacy settings for sharing clinical data."" Generally speaking, the current state of affairs concerning the sharing of ""anonymized"" patient health data for secondary research is uneven and unsettled. It has been shown that anonymization methods -- in which data sets are either encrypted or stripped of personally identifiable information -- are not 100 percent effective. Since 2013, newly enrolled patients are required to proactively consent to sharing their personal health information for research studies or future secondary use. In California, a patient's specific permission is required to share mental health, substance abuse, HIV status and genetic information, but other items or conditions are not specified. In many states, there is no requirement for a patient's specific permission on these types of items before they can be shared. Today, for practical purposes, patients have the option to decline any part of their medical record be used for research. They cannot indicate what types of research or researcher should be able to obtain their records. Almost three-quarters of respondents -- 67.1 percent -- said they would be willing to share all items with researchers from their health care institutions; almost one-quarter said they would be willing to share all items with all interested researchers, a finding the authors said was reassuring and could help in the planning of studies based on EHRs and biospecimens that would be expected to be broadly shared. Equally encouraging: Less than 4 percent of participants said they were not willing to share any information with anyone. Ohno-Machado said the way in which preferences are elicited also has an influence. There was greater sharing per item when respondents were asked to opt-out than when they were asked to opt-in. Whether the form had details about the items or used broad categories did not have an influence on sharing. ""This is important because a simple form could be used in the future to elicit choices from all patients, saving their time without significantly affecting their privacy preferences,"" said Ohno-Machado. ""However, different rates of sharing are expected for opt-in and opt-out of sharing clinical records for research."" A key finding was that a majority of survey participants identified at least one item that they did not want to share with a particular type of researcher (for example, a scientist at another for-profit institution), though they were willing to share other items. ""This finding is important,"" wrote the authors, ""because the item to withhold may not be of relevance to a certain study, but the current all-or-nothing option, if chosen, would remove that patient's data from all research studies."" The authors said the survey's tiered-permission system that allows specific removal of data items or categories proves both doable and appealing to patients, in part because there are differences among individuals in where and with whom they share what. The findings, said the authors, trigger further questions about the ideal balance between giving patients the ability to choose what portions of their data they want to share for research and with whom and the ""greater good,"" i.e., how fast research can be accelerated for the benefit of all. ""Institutions currently make decisions on sharing on behalf of all patients who do not explicitly decline sharing. It is possible that asking patients directly would increase the amount of data shared for research. On the other hand, it is also possible that some types of research would suffer from small sample sizes if patients consistently decline certain categories of items,"" Ohno-Machado said. ",Health,0.06320959948430592
24,Stanford,Potential treatments for citrus greening,Science,2019-08-22,-,https://news.stanford.edu/2019/08/19/potential-treatments-citrus-greening/,"   Over the course of 40 years, biologist Sharon Long has become an expert in symbiotic bacteria that help alfalfa grow. She has published over 150 papers on this one topic but when she realized her lab’s decades of highly focused research could contribute to a solution for citrus greening – a disease that devastates citrus crops – she was inspired to go in a new direction.  An orange that shows signs of citrus greening, an incurable citrus disease that causes trees to produce misshapen, bitter fruit. (Image credit: Getty Images)  “I’m only two generations off a farm, and I read about citrus farmers losing their livelihood and land, and thus also losing generations of family tradition,” said Long, who is the William C. Steere, Jr. – Pfizer Inc. Professor in Biological Sciences in the School of Humanities and Sciences. “We decided to redirect our efforts to work on this problem because we wanted to make a difference.” That risk paid off with a new way of finding potential treatments for the disease, and a short list of 130 compounds to explore further. Details of the system and their screenings were published Aug. 19 in the Proceedings of the National Academy of Sciences. “What we’ve completed is just a small part of what needs to be done,” said Melanie Barnett, a senior researcher in the Long lab and lead author of the paper. “It’s beyond our expertise to pursue these findings to the level needed for real-world application, but it’s a foot in the door for researchers who can take those next steps.” Decades of knowledge Citrus greening has devastated the citrus industry in Florida and is found in many of the country’s citrus growing regions. Even with high surveillance, the disease is spreading, and by the time symptoms of the lethal bacterial infection appear, it’s too late – the plants, bearing mottled leaves and ugly fruit with unpalatably bitter juice, must be uprooted and destroyed. An increasingly common treatment for the infection is spraying whole orchards with antibiotics, which is a risky procedure that could allow drug-resistant bacteria to emerge and spread.   More information on citrus greening disease Save Our Citrus: an educational public outreach campaign from the USDA Citrus Pest & Disease Prevention Program: information about citrus greening disease in California  Despite its devastation, citrus greening has been difficult for researchers to study. The bacteria that cause the disease – Liberibacter asiaticus – won’t grow in a lab, and studying infected plants is possible only in a few highly protected and sealed locations in the U.S. Some researchers have turned to a close, but less harmful, bacterial relative to find answers. But the Long lab realized they could tackle the problem by focusing on a more distant relative – Sinorhizobium meliloti, which partners with certain plants, allowing them to grow without added nitrogen fertilizer. “We’ve been working on this bacterium for 40 years and have developed tools that allow finely detailed genetic studies to be done,” Long said. “That provides an experimental platform not possible by working directly on this pathogen or even its close relatives.” The researchers started by introducing genes from the citrus greening bacterium into their familiar S. meliloti cell. Those genes each code for a protein that the scientists think regulates aspects of infection. Then, they engineered the bacteria so that when those infection-critical proteins were active, the bacteria glowed green in certain light. With this setup, if they exposed the bacteria to a chemical that inhibits the proteins – and perhaps also decreases the bacteria’s ability to infect citrus – the cell would become visibly less green. This visual signal made it possible to screen over 120,000 different compounds with help from the Stanford High-Throughput Bioscience Center. That screen identified 130 compounds that dimmed the cells’ green glow without affecting its growth. “Our system allowed us to find very specific inhibitors that do not harm beneficial bacteria,” explained Long. “Such inhibitors would be a big improvement compared to environmental spraying of general antibiotics.” Beyond studying the 130 compounds, the group said other researchers could now test additional chemicals with the system they devised, or examine different genes. “With this system, any gene from this pathogen or closely related pathogens can be tested in a very controlled way, very efficiently,” said Barnett. “The years of research that have gone into studying and working with Sinorhizobium can now save years of time that others would have spent developing such a system from scratch.” ",Health,0.06038909785805384
25,Science Daily,Parasite Needs Chemical (Lipid/nutrient) in Cat Intestines for Sex,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821173713.htm,"   Toxoplasma can live asexually in any mammal, including humans. But it forms gametes (sexual cells) only in cats, a restriction that has long been recognized, but whose reason was not understood. The authors suspected something was missing when reproducing the infections in vitro and not observing sexual development. It is known that fungi require linoleic acid for sexual development. To test Toxoplasma's requirement for the same lipid, they generated cat intestinal organoids -- three-dimensional ""test tube"" models that share several essential properties with actual intestines -- and showed that linoleic acid was required for sexual reproduction of the parasite. Cats lack an enzyme called delta-6-desaturase, which catalyzes the conversion of linoleic acid to arachidonic acid, accounting for the peculiarly high levels of linoleic acid in the cat intestine, but not in other mammals. When the authors supplemented the diet of mice with linoleic acid, and added a specific inhibitor of the enzyme, Toxoplasma could complete the sexual phase of its life cycle in the mouse intestine. Toxoplasma is the leading cause of foodborne illness in the US, according to CDC. The most common route of infection for human is by consumption of contaminated raw or undercooked meat. Cat litter, after 24-48 of being cleaned, can also a source of Toxoplasma infection. However, cats can only shed oocysts once in their life time. Pregnant women are urged to avoid eating raw or undercooked meat, as also avoiding cleaning the litterbox after 48 hours, to prevent Toxoplasma infection. Congenital toxoplasmosis can have potentially serious consequences for the unborn child. An improved understanding of the parasite life cycle stemming from this study may lead to production of vaccines that could inhibit Toxoplasma's sexual reproduction or the transmission of Toxoplasma to livestock. ",Health,0.056370939270313974
26,Science Daily,Scientists Unlock Secrets of Maternal/fetal Cellular Communication During Pregnancy,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142731.htm,"   ""During pregnancy, there is constant communication between maternal and fetal cells using sacs filled with chemicals called exosomes,"" said senior author Ramkumar Menon, UTMB associate professor in the department of obstetrics and gynecology. ""Our prior studies have shown that the fetal exosomes signal to the mother's body that her/his organs have fully matured, which triggers the labor and delivery process. Given this, we sought out to learn more about the extent and capabilities of this communication system in order to develop new ways to monitor and support the fetus during pregnancy."" To test exosome trafficking and function, the research team used mice that were genetically engineered to have certain exosome proteins glow florescent red and green when blood and tissue samples are stained and viewed under a microscope in order to distinguish between the fetal and maternal exosomes. The researchers learned that isolating and tracking fetal exosomes travelling to the maternal side is a useful indicator of the fetus's health and development that can be measured in minimally invasive maternal blood samples. Likewise, they now know that trafficking of exosomes from the maternal side to the fetus produces functional changes. ""We've just received a $ 1.5 million three-year contract to test a novel approach in treating preterm birth,"" said Menon. ""We will test the usefulness of drugs enclosed in exosomes that can potentially cross the placenta barrier, reach the fetus and prevent fetal inflammation, a major cause of preterm birth for which there is currently no drug treatment. Fetal inflammatory response is primarily responsible for preterm delivery, which impacts 15 million pregnancies yearly and responsible for 1 million neonatal deaths."" ",Health,0.05226607254939034
27,Science Daily,New Pharmaceutical Target Reverses Osteoporosis in Mice,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821173709.htm,"   ""The most widely used drugs currently approved by the FDA to treat osteoporosis can prevent further bone loss, but they don't help rebuild the bone,"" said Shyni Varghese, professor of biomedical engineering, mechanical engineering and materials science, and orthopedics at Duke. ""We've discovered a biochemical receptor that, when activated, can do both,"" added Yu-Ru ""Vernon"" Shih, a research scientist in Varghese's lab and the study's first author. The findings appear online August 21 in the journal Science Advances. Despite its apparent rigidity, bone is living tissue constantly being broken down and replaced by the body. Osteoporosis occurs when old tissue is lost faster than new tissue can be created, causing the bone to become weak and brittle. The disease afflicts more than 40 million men and women in the United States alone and is most common in older women past menopause. In 2014, Varghese was studying the role of popular biomedical devices made of calcium phosphate in promoting bone repair and regeneration. She discovered that the biochemical adenosine acting on the A2B receptor plays a particularly large role in promoting bone growth. It stood to reason that a lack of the chemical might play a role in the development of osteoporosis, so Varghese decided to find out. In the study, Varghese and her research team studied mice that had had their ovaries removed to mimic post-menopause osteoporosis. They looked at the expression levels of two enzymes that help produce adenosine as well as the levels of adenosine traveling between cells. As predicted, they discovered that the mice's lack of estrogen was causing all three to plummet. The researchers then tested to see if increasing the levels of adenosine in the mice would help reverse the damaging effects of the disease. But rather than pumping in adenosine itself, they injected a non-hormonal small molecule produced by Bayer that activates the A2B receptor. ""The mice that received the drug were completely cured,"" said Varghese. ""Their bones were just as healthy as the control group without osteoporosis."" While the discovery of a pharmaceutical target capable of reversing osteoporosis is exciting, creating a small molecule drug that can activate it without side effects is a difficult task. Adenosine is created naturally throughout the entire body and has many roles such as modulating neurons and regulating blood flow to various organs. Researchers can't simply dump a bunch of it into the bloodstream to stop bone degradation without side effects. But with the A2B receptor identified, Varghese and others can start looking for ways to deliver activators to bones without flooding other areas of the body. For example, one of Varghese's students is beginning to study ways of tethering adenosine-like molecules to carriers that target bone tissue. Her lab is also pursuing a sort of bandage that can deliver growth-supporting drugs directly to damaged or broken bones. This research was supported by the National Institutes of Health (R01-AR063184, R01-AR071552). ",Health,0.052161693751576456
28,Science Daily,New Pathway for Potential Glioblastoma Treatment,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821163812.htm,"   The Texas A&M team's research focuses on the AH receptor, which controls expression of a diverse set of genes, essentially contradicts what was previously understood in glioblastoma research. The Texas A&M study was published in July in the Journal of Biological Chemistry. ""We found the AH receptor -- which was previously reported in the literature to be a pro-invasion gene -- actually blocked invasion of glioblastoma cells,"" said Safe, who is a Distinguished Professor of Veterinary Physiology & Pharmacology (VTPP) in the CVM. ""When we add certain AH receptor ligands (molecules that bonds to another molecule), we observed a potent inhibition of glioblastoma cell invasion. Basically, we've shown that it's a good gene that can be targeted by drugs to make it even more effective."" Glioblastomas, the most common and aggressive malignant brain tumor in adults, are comprised of tumor cells that rapidly reproduce and divide, which allow the tumor to grow into nearby normal brain tissue. Currently, these brain tumors are incurable -- a patient's median life expectancy after diagnosis is 11-15 months with standard treatments. According to the American Brain Tumor Association, glioblastomas also form new blood vessels so they can maintain their rapid growth and may use connection fibers to spread to the opposite side of the brain. Safe said that the tumors are difficult to treat. Because glioblastomas often have finger-like tentacles that spread through the brain, they may not be completely removed through surgery. The tumor's individual cells also respond differently to various therapies. The Texas A&M study used patient glioblastoma cells in collaboration with colleagues at the Detroit Medical Center, as well as cells that were used in previously published glioblastoma studies. The researchers analyzed the AH receptor and several receptor ligands, including Kynurenine. Previous published studies in the journal Nature found that the AH receptor and Kynurenine were involved in glioblastoma cells' invasion of the brain. However, the Texas A&M researchers refuted these findings by showing that AH receptors actually serve a protective function and do not promote the invasion of glioblastoma cells. In addition, when researchers added AH receptor ligands but Kynurenine was not active, the level of protection to the brain was enhanced. These findings suggest that the AH receptor could be a target for the development of drugs to inhibit glioblastoma. The Texas A&M team is now studying the use of the AH receptor as a target for inhibiting glioblastoma and identifying compounds that bind to the AH receptor to provide additional protection to the brain. ""This study opens up a new way for developing potential clinical applications,"" Safe said. ""Whether this line of inquiry will be successful remains to be seen, but our work may offer hope for a disease which has such a poor prognosis."" ",Health,0.04441591639130944
29,Science Daily,Health Records Pin Broad Set of Health Risks on Genetic Premutation,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821163824.htm,"   In recent years, however, at least two clinical conditions have been well documented in the carriers themselves: an age-dependent neurodegenerative disorder and, in female carriers, early menopause. Now, a team of researchers from the University of Wisconsin-Madison and Marshfield Clinic has found that there may be a much broader health risk to carriers, with potentially dozens of clinical conditions that can be ascribed directly to carrying the premutation. The researchers employed machine learning, a form of artificial intelligence, to mine decades of electronic health records of nearly 20,000 individuals. In a study published Aug. 21 in Science Advances, a team led by Marsha Mailick, a researcher and professor at UW-Madison's Waisman Center, and UW-Madison graduate student Arezoo Movaghar provide a better understanding of the previously disputed relationship between this well-known genetic premutation and a wide range of clinical conditions. At the same time, the interdisciplinary study richly illustrates the power of data-driven discovery. In the study, carriers were found to experience a much higher prevalence and severity of such conditions as depression, anxiety, mood disorders, sleep apnea, respiratory and stomach problems, bone fractures, incontinence, and a host of other conditions. ""Our extensive phenotyping shows that premutation carriers experience a clinical profile that is significantly different from controls, and that is evident throughout adulthood,"" the study's authors write. ""The research was especially challenging as not all carriers are affected and the specific conditions each may experience varies,"" explains Mailick. ""Yet the overall pattern revealed in the electronic health record was striking."" The findings have important implications for the estimated 1 million people in the United States and many more in the rest of the world who carry premutations of the FMR1 gene. People are generally unaware they carry the altered gene unless a family member is found to have fragile X syndrome, which is rare. The new study depended on the electronic health records of 19,996 adults collected over 40 years from Marshfield Clinic, a health system in central Wisconsin that was among the first in the nation to adopt electronic record keeping. In addition to their extensive, searchable medical histories, study subjects also contributed their DNA through the clinic's Personalized Medicine Research Project, enabling the study's authors to identify 98 patients who carry the FMR1 premutation. A primary goal of the new study, according to Mailick, was to use electronic health records to set up a double-blind methodology -- where both clinicians and patients were blind to genotype -- to assess whether premutation carriers differed in their patterns of clinical diagnoses from those lacking the premutation. The question is an important one as some clinical reports have suggested that premutation carriers were at greater risk for a wide range of health conditions, including autoimmune diseases, migraines, neuropathy, depression, infertility, fibromyalgia, anxiety and cognitive deficits, all with varying frequencies and emerging at different stages of life. Such correlations, notes Mailick, are the stuff of biomedical controversy as nearly all of the studies associating those conditions with the premutation were the result of a family member being diagnosed with fragile X. Following diagnosis of fragile X, family members are offered genetic testing leading to the identification of relatives who carry the premutation. Awareness of that genetic status by patients and clinicians can confound prevalence estimates of symptoms. Frequently, carriers' medical conditions are blamed on the stress of care giving and having a family member with fragile X syndrome. ""This is a controversy,"" says Mailick, who has spent a career studying the life course trajectories of people with developmental disabilities as well as their families through UW-Madison's Waisman Center. ""There wasn't unbiased data available before this study to inform the controversy. This study does so in a powerful way. The data validate the clinical experience."" By scouring the medical histories of patients with the premutation, the Wisconsin researchers identified an astonishing range of health conditions and symptoms affecting FMR1 carriers later in life. For women, these include agoraphobia, social phobia, panic disorders, infertility, menstrual disorders, and falls and fractures, among others. For men, clinical diagnoses commonly found in health records of FMR1 carriers include depression, diseases of the respiratory system, and urinary incontinence, as well as half a dozen other afflictions. Movaghar and Mailick say some of the conditions associated with FMR1 carriers have long been suspected, but the number and range of health effects they found was a surprise: ""We see conditions nobody ever associated with the premutation,"" Movaghar says. To be included in the study data, subjects had to experience the diagnosis in at least two separate visits, helping weed out misdiagnoses or recording errors. The approach used in this study, the Wisconsin researchers say, could also be applied to other genetic variants similarly believed to be innocuous but that potentially may have health effects. ",Health,0.04411322050371755
30,Science Daily,Lower Back Pain? Self-Administered Acupressure Could Help,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821163805.htm,"   ""Acupressure is similar to acupuncture, but instead of needles, pressure is applied with a finger, thumb or device to specific points on the body,"" says Susan Murphy, ScD, OTR, an associate professor of physical medicine and rehabilitation at Michigan Medicine and lead author of the study. Murphy says that while acupressure has been previously studied -- and found to be beneficial -- in people with cancer-related or osteoarthritis pain, there are few studies that have examined acupressure in people with back pain. In the study, published in Pain Medicine, the research team randomly assigned 67 participants with chronic low back pain into three groups: relaxing acupressure, stimulating acupressure or usual care. ""Relaxing acupressure is thought to be effective in reducing insomnia, while stimulating acupressure is thought to be effective in fatigue reduction,"" Murphy says. Participants in the acupressure groups were trained to administer acupressure on certain points of the body, and spent between 27 and 30 minutes daily, over the course of six weeks, performing the technique. Participants in the usual care group were asked to continue whatever treatments they were currently receiving from their care providers to manage their back pain and fatigue. ""Compared to the usual care group, we found that people who performed stimulating acupressure experienced pain and fatigue improvement and those that performed relaxing acupressure felt their pain had improved after six weeks,"" Murphy says. ""We found no differences among the groups in terms of sleep quality or disability after the six weeks."" Potential treatment option Murphy notes that chronic pain is difficult to manage and people with the condition tend to have additional symptoms such as fatigue, sleep disturbance and depression. ""Better treatments are needed for chronic pain,"" Murphy says. ""Most treatments offered are medications, which have side effects, and in some cases, may increase the risk of abuse and addiction."" She says this study highlights the benefits of a non-pharmacological treatment option that patients could perform easily on their own and see positive results. ""Although larger studies are needed, acupressure may be a useful pain management strategy given that it is low risk, low cost and easy to administer,"" Murphy says. ""We also recommend additional studies into the different types of acupressure and how they could more specifically be targeted to patients based on their symptoms."" ",Health,0.04214961948605809
31,Science Daily,Can Pomegranate Juice Protect the Infant Brain?,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142719.htm,"   ""Our study provides preliminary evidence suggesting potential protective effects for newborns exposed to pomegranate juice while in utero,"" said senior author Terrie Inder, MBCHB, chair of the Department of Pediatric Newborn Medicine at the Brigham. ""These findings warrant continued investigation into the potential neuroprotective effects of polyphenols in at-risk newborns, such as those with hypoxic-ischemic injury."" In cases of IUGR, a baby in the womb is measuring small for its gestational age, often because of issues with the placenta, which brings oxygen and nutrients to the growing fetus. One out of every 10 babies is considered to have IUGR. The process of birth itself can further decrease blood flow or oxygen to the baby, including to the baby's brain. If this is very severe, it can result in a condition known as hypoxic-ischemic injury, which contributes to almost one-quarter of newborn deaths worldwide. Polyphenols, which include tannic acid and ellagitannins, are part of a class of antioxidants found in many foods and beverages, including nuts, berries, red wine and teas. Pomegranate juice is a particularly rich source of these molecules. Polyphenols are known to cross the blood-brain barrier, and studies in animal models have demonstrated protective effects against neurodegenerative diseases. To date, no clinical studies had evaluated the potential effects of giving pregnant women pomegranate juice to protect the brains of at-risk newborns. The current randomized, controlled, double-blinded study enrolled 78 mothers from Barnes-Jewish Hospital obstetric clinic in St. Louis with IUGR diagnosed at 24-43 weeks' gestation. Women were randomized to receive 8 ounces of pomegranate juice daily or a taste/calorie matched placebo that was polyphenol free. Women drank the juice daily from enrollment until delivery. The team measured several aspects of brain development and injury, including infant brain macrostructure, microstructural organization and functional connectivity. While the team did not observe differences in brain macrostructure, they did find regional differences in white matter microstructure and functional connectivity. ""These measures tell us about how the brain is developing functionally,"" said Inder. ""We saw no difference in brain growth and baby growth, but we did see improvement in cabling network and brain development measured by synchronous blood flow and visual development of the brain."" The authors note that the findings warrant the need for a larger, rigorously designed clinical trial to allow continued investigation into the potential neuroprotective effects of polyphenols. Such a study is now underway at the Brigham. ""We plan to continue investigating these exciting findings,"" said Inder. ""While the preliminary evidence shows promise, additional study and replication is needed."" ",Health,0.041808922153554445
32,Science Daily,Meaningful PTSD Symptom Decrease May Lower Type 2 Diabetes Risk,Health,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821142734.htm,"   The study, ""Clinically Meaningful PTSD Improvement and Risk for Type 2 Diabetes,"" by Jeffrey Scherrer, Ph.D., professor in Family and Community Medicine at SLU, was published online Aug. 21 in JAMA Psychiatry. ""Some long-term chronic health conditions associated with PTSD may be less likely to occur among patients who experience clinically meaningful symptom reduction either through treatment or spontaneous improvement,"" Scherrer said. PTSD affects up to 12 percent of civilians and nearly 30 percent of the veteran population. Those with PTSD are at risk for other health issues and improvement in PTSD symptoms is associated with parallel improvements in depression, emotional well-being, sleep, blood pressure and general physical health. PTSD is associated with an increased risk of type 2 diabetes, which may be explained by the high prevalence of obesity, glucose dysregulation, inflammation, metabolic syndrome and depression among those diagnosed with PTSD versus those without PTSD. This retrospective cohort study reviewed Veterans Health Affairs medical record data from 2008 to 2015. The researchers randomly selected 5,916 cases from among a veteran patient population aged 18 to 70 who had more than two visits to PTSD specialty care between 2008 and 2012. The patients were followed through until 2015. After applying eligibility criteria, 1,598 patients with PTSD and free of diabetes risk were available for analysis. Clinically meaningful symptom reduction is a decrease of 20 points on the PTSD Checklist score. The research found the results were independent of numerous demographic, psychiatric and physical comorbidities. The sample was 84.3 percent male, 66 percent Caucasian and 22 percent African-American. The mean age of the patients was 42. The association was also independent of the number of PTSD psychotherapy sessions used. ""In patients with only PTSD, clinically meaningful PCL decrease is associated with lower risk for diabetes and in patients with PTSD and depression, we found improvement in PTSD was coupled with a decrease in depression,"" Scherrer said. ""Thus decreased risk for type 2 diabetes appears to follow large PTSD symptom decrease and in patients with both PTSD and depression, improvement in both conditions may be necessary to reduce risk for type 2 diabetes."" ""Surprisingly, clinically meaningful PTSD improvement was not associated with a change in BMI and A1C values."" A prospective study is needed to advance research, Scherrer says, due in part to the limitations of medical record data. Such a study could determine if large decreases in PTSD checklist scores are associated with improved insulin resistance and reduced inflammation. Take-aways The observational study examined whether veterans who experienced a greater reduction in symptoms of posttraumatic stress disorder (PTSD) had an associated lower risk of developing type 2 diabetes. The analysis included medical records from almost 1,600 veterans who received PTSD specialty care and had repeated completion of the PTSD Checklist as part of their treatment at the VA. Patients with versus patients without clinically meaningful improvement in PTSD symptoms had a 49 percent lower risk for type 2 diabetes over a 3-6-year follow-up period. In patients with PTSD and depression, improvement in both conditions was associated with lower risk for diabetes. ",Health,0.04105673351468986
