,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,MIT News,What a little more computing power can do,Computer Science,2019-09-16,-,http://news.mit.edu/2019/what-extra-computing-power-can-do-0916,"  Neural networks have given researchers a powerful tool for looking into the future and making predictions. But one drawback is their insatiable need for data and computing power (""compute"") to process all that information. At MIT, demand for compute is estimated to be five times greater than what the Institute can offer. To help ease the crunch, industry has stepped in. An $11.6 million supercomputer recently donated by IBM comes online this fall, and in the past year, both IBM and Google have provided cloud credits to MIT Quest for Intelligence for distribution across campus. Four projects made possible by IBM and Google cloud donations are highlighted below. Smaller, faster, smarter neural networks To recognize a cat in a picture, a deep learning model may need to see millions of photos before its artificial neurons “learn” to identify a cat. The process is computationally intensive and carries a steep environmental cost, as new research attempting to measure artificial intelligence's (AI’s) carbon footprint has highlighted.  But there may be a more efficient way. New MIT research shows that models only a fraction of the size are needed. “When you train a big network there’s a small one that could have done everything,” says Jonathan Frankle, a graduate student in MIT’s Department of Electrical Engineering and Computer Science (EECS). With study co-author and EECS Professor Michael Carbin, Frankle estimates that a neural network could get by with on-tenth the number of connections if the right subnetwork is found at the outset. Normally, neural networks are trimmed after the training process, with irrelevant connections removed then. Why not train the small model to begin with, Frankle wondered? Experimenting with a two-neuron network on his laptop, Frankle got encouraging results and moved to larger image-datasets like MNIST and CIFAR-10, borrowing GPUs where he could. Finally, through IBM Cloud, he secured enough compute power to train a real ResNet model. “Everything I’d done previously was toy experiments,” he says. “I was finally able to run dozens of different settings to make sure I could make the claims in our paper.” Frankle spoke from Facebook’s offices, where he worked for the summer to explore ideas raised by his Lottery Ticket Hypothesis paper, one of two picked for a best paper award at this year’s International Conference on Learning Representations. Potential applications for the work go beyond image classification, Frankle says, and include reinforcement learning and natural language processing models. Already, researchers at Facebook AI Research, Princeton University, and Uber have published follow-on studies.  “What I love about neural networks is we haven’t even laid the foundation yet,” says Frankle, who recently shifted from studying cryptography and tech policy to AI. “We really don’t understand how it learns, where it’s good and where it fails. This is physics 1,000 years before Newton.” Distinguishing fact from fake news Networking platforms like Facebook and Twitter have made it easier than ever to find quality news. But too often, real news is drowned out by misleading or outright false information posted online. Confusion over a recent video of U.S. House Speaker Nancy Pelosi doctored to make her sound drunk is just the latest example of the threat misinformation and fake news pose to democracy.  “You can put just about anything up on the internet now, and some people will believe it,” says Moin Nadeem, a senior and EECS major at MIT. If technology helped create the problem, it can also help fix it. That was Nadeem’s reason for picking a superUROP project focused on building an automated system to fight fake and misleading news. Working in the lab of James Glass, a researcher at MIT’s Computer Science and Artificial Intelligence Laboratory, and supervised by Mitra Mohtarami, Nadeem helped train a language model to fact-check claims by searching through Wikipedia and three types of news sources rated by journalists as high-quality, mixed-quality or low-quality. To verify a claim, the model measures how closely the sources agree, with higher agreement scores indicating the claim is likely true. A high disagreement score for a claim like, “ISIS infiltrates the United States,” is a strong indicator of fake news. One drawback of this method, he says, is that the model doesn’t identify the independent truth so much as describe what most people think is true. With the help of Google Cloud Platform, Nadeem ran experiments and built an interactive website that lets users instantly assess the accuracy of a claim. He and his co-authors presented their results at the North American Association of Computational Linguistics (NAACL) conference in June and are continuing to expand on the work. “The saying used to be that seeing is believing,” says Nadeem, in this video about his work. “But we’re entering a world where that isn’t true. If people can’t trust their eyes and ears it becomes a question of what can we trust?” Visualizing a warming climate From rising seas to increased droughts, the effects of climate change are already being felt. A few decades from now, the world will be a warmer, drier, and more unpredictable place. Brandon Leshchinskiy, a graduate student in MIT’s Department of Aeronautics and Astronautics (AeroAstro), is experimenting with generative adversarial networks, or GANs, to imagine what Earth will look like then.  GANs produce hyper-realistic imagery by pitting one neural network against another. The first network learns the underlying structure of a set of images and tries to reproduce them, while the second decides which images look implausible and tells the first network to try again. Inspired by researchers who used GANs to visualize sea-level rise projections from street-view images, Leshchinskiy wanted to see if satellite imagery could similarly personalize climate projections. With his advisor, AeroAstro Professor Dava Newman, Leshchinskiy is currently using free IBM Cloud credits to train a pair of GANs on images of the eastern U.S. coastline with their corresponding elevation points. The goal is to visualize how sea-level rise projections for 2050 will redraw the coastline. If the project works, Leshinskiy hopes to use other NASA datasets to imagine future ocean acidification and changes in phytoplankton abundance.  “We’re past the point of mitigation,” he says. “Visualizing what the world will look like three decades from now can help us adapt to climate change.” Identifying athletes from a few gestures A few moves on the field or court are enough for a computer vision model to identify individual athletes. That’s according to preliminary research by a team led by Katherine Gallagher, a researcher at MIT Quest for Intelligence. The team trained computer vision models on video recordings of tennis matches and soccer and basketball games and found that the models could recognize individual players in just a few frames from key points on their body providing a rough outline of their skeleton.  The team used a Google Cloud API to process the video data, and compared their models' performance against models trained on Google Cloud's AI platform. “This pose information is so distinctive that our models can identify players with accuracy almost as good as models provided with much more information, like hair color and clothing,” she says.  Their results are relevant for automated player identification in sports analytics systems, and they could provide a basis for further research on inferring player fatigue to anticipate when players should be swapped out. Automated pose detection could also help athletes refine their technique by allowing them to isolate the precise moves associated with a golfer’s expert drive or a tennis player’s winning swing. ",Computer Science,0.2063986181392404
1,MIT News,Faster video recognition for the smartphone era,Computer Science,2019-10-11,-,http://news.mit.edu/2019/faster-video-recognition-smartphone-era-1011,"  A branch of machine learning called deep learning has helped computers surpass humans at well-defined visual tasks like reading medical scans, but as the technology expands into interpreting videos and real-world events, the models are getting larger and more computationally intensive.  By one estimate, training a video-recognition model can take up to 50 times more data and eight times more processing power than training an image-classification model. That’s a problem as demand for processing power to train deep learning models continues to rise exponentially and concerns about AI’s massive carbon footprint grow. Running large video-recognition models on low-power mobile devices, where many AI applications are heading, also remains a challenge.  Song Han, an assistant professor at MIT’s Department of Electrical Engineering and Computer Science (EECS), is tackling the problem by designing more efficient deep learning models. In a paper at the International Conference on Computer Vision, Han, MIT graduate student Ji Lin and MIT-IBM Watson AI Lab researcher Chuang Gan, outline a method for shrinking video-recognition models to speed up training and improve runtime performance on smartphones and other mobile devices. Their method makes it possible to shrink the model to one-sixth the size by reducing the 150 million parameters in a state-of-the-art model to 25 million parameters.         “Our goal is to make AI accessible to anyone with a low-power device,” says Han. “To do that, we need to design efficient AI models that use less energy and can run smoothly on edge devices, where so much of AI is moving.”  The falling cost of cameras and video-editing software and the rise of new video-streaming platforms has flooded the internet with new content. Each hour, 30,000 hours of new video are uploaded to YouTube alone. Tools to catalog that content more efficiently would help viewers and advertisers locate videos faster, the researchers say. Such tools would also help institutions like hospitals and nursing homes to run AI applications locally, rather than in the cloud, to keep sensitive data private and secure.  Underlying image and video-recognition models are neural networks, which are loosely modeled on how the brain processes information. Whether it’s a digital photo or sequence of video images, neural nets look for patterns in the pixels and build an increasingly abstract representation of what they see. With enough examples, neural nets “learn” to recognize people, objects, and how they relate.  Top video-recognition models currently use three-dimensional convolutions to encode the passage of time in a sequence of images, which creates bigger, more computationally-intensive models. To reduce the calculations involved, Han and his colleagues designed an operation they call a temporal shift module which shifts the feature maps of a selected video frame to its neighboring frames. By mingling spatial representations of the past, present, and future, the model gets a sense of time passing without explicitly representing it. The result: a model that outperformed its peers at recognizing actions in the Something-Something video dataset, earning first place in version 1 and version 2, in recent public rankings. An online version of the shift module is also nimble enough to read movements in real-time. In a recent demo, Lin, a PhD student in EECS, showed how a single-board computer rigged to a video camera could instantly classify hand gestures with the amount of energy to power a bike light.  Normally it would take about two days to train such a powerful model on a machine with just one graphics processor. But the researchers managed to borrow time on the U.S. Department of Energy’s Summit supercomputer, currently ranked the fastest on Earth. With Summit’s extra firepower, the researchers showed that with 1,536 graphics processors the model could be trained in just 14 minutes, near its theoretical limit. That’s up to three times faster than 3-D state-of-the-art models, they say. Dario Gil, director of IBM Research, highlighted the work in his recent opening remarks at AI Research Week hosted by the MIT-IBM Watson AI Lab. “Compute requirements for large AI training jobs is doubling every 3.5 months,” he said later. “Our ability to continue pushing the limits of the technology will depend on strategies like this that match hyper-efficient algorithms with powerful machines.”  ",Computer Science,0.206104150404258
2,MIT News,Lincoln Laboratory's new artificial intelligence supercomputer is the most powerful at a university,Computer Science,2019-09-27,-,http://news.mit.edu/2019/lincoln-laboratory-ai-supercomputer-tx-gaia-0927,"  The new TX-GAIA (Green AI Accelerator) computing system at the Lincoln Laboratory Supercomputing Center (LLSC) has been ranked as the most powerful artificial intelligence supercomputer at any university in the world. The ranking comes from TOP500, which publishes a list of the top supercomputers in various categories biannually. The system, which was built by Hewlett Packard Enterprise, combines traditional high-performance computing hardware — nearly 900 Intel processors — with hardware optimized for AI applications — 900 Nvidia graphics processing unit (GPU) accelerators. ""We are thrilled by the opportunity to enable researchers across Lincoln and MIT to achieve incredible scientific and engineering breakthroughs,"" says Jeremy Kepner, a Lincoln Laboratory fellow who heads the LLSC. ""TX-GAIA will play a large role in supporting AI, physical simulation, and data analysis across all laboratory missions."" TOP500 rankings are based on a LINPACK Benchmark, which is a measure of a system's floating-point computing power, or how fast a computer solves a dense system of linear equations. TX-GAIA's TOP500 benchmark performance is 3.9 quadrillion floating-point operations per second, or petaflops (though since the ranking was announced in June 2019, Hewlett Packard Enterprise has updated the system's benchmark to 4.725 petaflops). The June TOP500 benchmark performance places the system No. 1 in the Northeast, No. 20 in the United States, and No. 51 in the world for supercomputing power. The system's peak performance is more than 6 petaflops. But more notably, TX-GAIA has a peak performance of 100 AI petaflops, which makes it No. 1 for AI flops at any university in the world. An AI flop is a measure of how fast a computer can perform deep neural network (DNN) operations. DNNs are a class of AI algorithms that learn to recognize patterns in huge amounts of data. This ability has given rise to ""AI miracles,"" as Kepner puts it, in speech recognition and computer vision; the technology is what allows Amazon's Alexa to understand questions and self-driving cars to recognize objects in their surroundings. The more complex these DNNs grow, the longer it takes for them to process the massive datasets they learn from. TX-GAIA's Nvidia GPU accelerators are specially designed for performing these DNN operations quickly. TX-GAIA is housed in a new modular data center, called an EcoPOD, at the LLSC’s green, hydroelectrically powered site in Holyoke, Massachusetts. It joins the ranks of other powerful systems at the LLSC, such as the TX-E1, which supports collaborations with the MIT campus and other institutions, and TX-Green, which is currently ranked 490th on the TOP500 list. Kepner says that the system's integration into the LLSC will be completely transparent to users when it comes online this fall. ""The only thing users should see is that many of their computations will be dramatically faster,"" he says. Among its AI applications, TX-GAIA will be tapped for training machine learning algorithms, including those that use DNNs. It will more quickly crunch through terabytes of data — for example, hundreds of thousands of images or years' worth of speech samples — to teach these algorithms to figure out solutions on their own. The system's compute power will also expedite simulations and data analysis. These capabilities will support projects across the laboratory's R&D areas, such as improving weather forecasting, accelerating medical data analysis, building autonomous systems, designing synthetic DNA, and developing new materials and devices. TX-GAIA, which is also ranked the No. 1 system in the U.S. Department of Defense, will also support the recently announced MIT-Air Force AI Accelerator. The partnership will combine the expertise and resources of MIT, including those at the LLSC, and the U.S. Air Force to conduct fundamental research directed at enabling rapid prototyping, scaling, and application of AI algorithms and systems. ",Computer Science,0.2039007564928293
3,MIT News,Letter regarding update on the MIT Schwarzman College of Computing,Computer Science,2019-09-11,-,http://news.mit.edu/2019/update-schwarzman-college-computing-0911,"  The following letter was sent to MIT faculty members on Sept. 11 by Provost Martin A. Schmidt. Dear Colleagues, I am writing to share with you a status update from Dean Dan Huttenlocher on the formation of the MIT Stephen A. Schwarzman College of Computing. This is the first of what I expect to be regular updates over the next few months as the initial set of academic units and structure of the college are being determined. As I have mentioned in the past, the college stands on three legs: advancing computer science, building strong connections between computer science and all other disciplines at MIT, and being intentional in contemplating the societal implications of computing. Dean Huttenlocher is working to advance each of these legs as we build the college, and his note below provides an update on where we stand. Out of necessity, the plans for each leg are moving at different paces, but it is important for me to stress that each one of these legs is vital to the college’s overall success.   Sincerely, Martin A. Schmidt   MIT Schwarzman College of Computing  September 11, 2019 Update Dan Huttenlocher, Dean I would first like to thank the MIT community—particularly those who have given so generously of their time and ideas—for their continued support in the creation and establishment of the MIT Schwarzman College of Computing (SCC). While much remains to be done, I am excited to share this update on progress made over the summer, including: Establishment of an advisory group, building on the College of Computing Task Force. Initial administrative leadership appointments and planned additional appointments over the coming year. Process for creating an organizational plan for the college. Organizational plan for the Department of Electrical Engineering and Computer Science (EECS). Summary of the college’s mission and scope. 1. Advisory Group. I am delighted that members of last year’s College of Computing Task Force Steering Committee have agreed to serve this year as an advisory group for the formation of the SCC, as they have a unique understanding of the opportunities and challenges from their deep involvement in the working groups. The advisory group members are Eran Ben-Joseph, Anantha Chandrakasan, Rick Danheiser, Srinivas Devadas, Benoit Forget, William Freeman, Melissa Nobles, Asu Ozdaglar, Nelson Repenning, Nicholas Roy, Julie Shah, and Troy Van Voorhis. I have also recently met with undergraduate and graduate student groups and will be working with them on avenues for student input, which are likely to largely be through academic units, as those are the main loci of educational programs. 2. Leadership Appointments. I am also pleased to announce initial leadership appointments for the SCC. These take effect immediately and will report directly to me. Daniela Rus will serve as deputy dean of research and continue as director of CSAIL; Asu Ozdaglar will serve as deputy dean of academics and continue as head of EECS; David Kaiser and Julie Shah will serve as co-heads of social and ethical responsibilities of computing; Eileen Ng will serve as assistant dean of administration; moving from the School of Engineering; and Terri Park will serve as director of communications, moving from the Innovation Initiative. Please join me in congratulating them and thanking them for taking on these important new roles. Over the coming year, we expect to fill other SCC leadership positions, including an assistant or associate dean of equity and inclusion, assistant dean for development, director of a new Center for Advanced Study of Computing, and leadership of a new teaching collaborative for both disciplinary and interdisciplinary computing classes, once the structure of the college and these additional roles becomes more fully defined. We further expect that Institute for Data, Systems, and Society (IDSS), Center for Computational Engineering (CCE), Computer Science and Artificial Intelligence Lab (CSAIL), Laboratory for Information and Decision Systems (LIDS), the Quest for Intelligence, and perhaps other units, will likely become part of the SCC as planning proceeds, and that their directors will be asked to remain in their roles. 3. College Planning Process. Based on last spring’s College of Computing Task Force Working Group reports, a strawman organizational plan for the college is being developed through an iterative process. The school deans and the aforementioned advisory group are reviewing an early draft. A revised strawman will then be circulated to the school councils, and then to the full MIT faculty for feedback and final revision. We expect to begin sharing the strawman more broadly in the coming months through forums to solicit feedback from the MIT community. We do not foresee much in the way of curriculum or subject changes this school year, as these are defined by the faculty of academic units through deliberative processes, although there will likely be some pilots. 4. EECS Plan. An organizational plan for EECS, developed over the summer based on the task force working group reports, is now being implemented. This plan also was developed iteratively, first involving the co-chairs of the Organizational Structure and Faculty Appointments working groups, followed by the Engineering Council and the EECS faculty. The plan calls for the department to form three overlapping academic units, as suggested by the Organizational Structure Working Group. These units are termed faculties: a faculty of electrical engineering (EE), a faculty of computer science (CS), and a faculty of artificial intelligence and decision making (AI+D). Each faculty will have a head, and will be overseen by the head of EECS. The faculties will be the main locus of faculty recruiting, mentoring, and promotion for the department. The department will report jointly to the School of Engineering and the Schwarzman College of Computing, and will remain responsible for Course 6. A search committee for the heads of the three faculties has just been formed. No specific curricular changes are foreseen at this time, as those will take time and will be led by the faculties and the department. 5. Mission and Scope. The planning efforts for the Schwarzman College of Computing are driven by the following mission and scope of activities. Widespread advances in computing—from hardware to software to algorithms to artificial intelligence—have improved people’s lives in myriad ways, with numerous promising opportunities on the horizon. Yet at the same time, we face critical and growing challenges regarding the social and ethical implications and responsibilities of computing, particularly with the increasing applicability of artificial intelligence. Moreover, despite the unprecedented growth of computer science, artificial intelligence, and related academic program areas, substantial unmet demand for expertise in computing remains, as does the constant need to keep up with rapidly changing academic content. The mission of the MIT Stephen A. Schwarzman College of Computing (SCC) is to address the challenges and opportunities of the computing age by transforming the capabilities of academia in three key areas: Computing fields: Support the rapid growth and evolution of computer science and computational areas of allied fields such as electrical engineering, as reflected notably in the rise of artificial intelligence; Computing across disciplines: Facilitate productive research and teaching collaborations between computing and other fields, rather than place one field in service of another; Social and ethical aspects of computing: Lead the development of and changes in academic research and education, and effectively inform practice and policy in industry and government. These three areas are integral to the SCC mission. Moreover, they are not independent but rather should inform and amplify one another. Based on the College of Computing Task Force Working Group reports and follow-on planning over the summer, the SCC is expected to: Have academic units that are more flexible and interconnected than conventional departments and schools while simultaneously reinforcing MIT’s strength in computing fields such as CS and large parts of EE, which generally have traditional departments at other top institutions. The SCC is expected to have multiple types of academic structures to meet this variety of needs. Involve faculty from a broad range of departments and schools who are engaged in computing education and research through a variety of programs and affiliations including: (i) the Common Ground, a teaching collaborative for both disciplinary and interdisciplinary computing classes, (ii) centers that offer graduate programs in computing, (iii) computing research labs and centers, and (iv) additional scholarly activities in computing, including workshops and other convenings. Lead in the social and ethical aspects of computing with: (i) education that helps develop “habits of mind and action” for those who create and deploy computing technologies, (ii) research that brings technological, social science, and humanistic approaches together, and (iii) impact on government and corporate policy as well as the development of a better understanding among the general public. Lead in the rapid evolution of computing fields, both in research and education, currently exemplified by the rise of areas such as AI and machine learning (ML). Deliver outstanding undergraduate and graduate education in: (i) CS and EE, (ii) evolving areas of computing such as AI/ML and others, (iii) computing across the disciplines, and (iv) social and ethical aspects of computing. Improve equity and inclusion in computing at MIT with the aim of helping address diversity in computing with regard to gender, race, and range of backgrounds and experience. Focus on increasing the diversity of top faculty candidates, with new programs for faculty, postdocs, and PhD students, as well as improvements to existing programs. Broaden participation in computing classes and academic programs at all levels, including improvements to the climate and the development of more effective connections with other, more diverse, disciplines. The SCC will include both existing and new academic units and programs. Bringing existing activities together will help facilitate coordination and alignment of computing education and research as well as provide new opportunities for improvement, such as bringing more coordination to the academic programs that mix computing and other disciplines and to the teaching of social and ethical issues in computing. Creating new programs and units will help address areas that are not well covered by existing ones and also help in fostering new connections. ",Computer Science,0.2016036784020181
4,MIT News,Computing in Earth science: a non-linear path,Computer Science,2019-09-11,-,http://news.mit.edu/2019/computing-earth-science-non-linear-path-sonia-reilly-0911,"  Machine learning is undeniably a tool that most disciplines like to have in their toolbox. However, scientists are still investigating the limits and barriers to incorporating machine learning into their research. Junior Sonia Reilly spent her summer opening up the machine learning black box to better understand how information flows through neural networks as part of the Undergraduate Research Opportunities Program (UROP). Her project, which investigates how machine learning works with the intention of improving its application to the observation of natural phenomena, was overseen by Sai Ravela in the Department of Earth, Atmospheric and Planetary Sciences (EAPS). As a major in Course 18C (Mathematics with Computer Science), Reilly is uniquely equipped to help investigate these connections. “In recent years, deep learning has become an immensely popular tool in all kinds of research fields, but the mathematics of how and why it is so effective is still very poorly understood,” says Reilly. “Having that knowledge will enable the design of better-performing learning machines.” To do that, she looks more closely at how the algorithms evolve to produce their final most-probable conclusions, with the end goal of providing insights on information flow, bottlenecks, and maximizing gain from neural networks. “We don’t want to be drowning in big data. On the contrary, we want to transform big data into perhaps what we might call smart data,” Ravela says of how machine learning must proceed. “The end goal is always a sensing agent that gathers data from our environment, but one that is knowledge-driven and does just enough work to gather just enough information for meaningful inferences.” For Ravela, who leads the Earth Signals and Systems Group (ESSG), better-performing learning machines means more robust early predictions of potential disasters. His group’s research lies largely in how the Earth works as a system, primarily focusing on climate and natural hazards. They observe natural phenomena to produce effective predictive models for dynamic natural processes, such as hurricanes, clouds, volcanoes, earthquakes, glaciers, and wildlife conservation strategies, as well as making advances in engineering and learning itself. “In all these projects, it’s impossible to gather dense data in space and time. We show that actively mining the environment through a systems analytic approach is promising,” he says. Ravela recently delivered his group’s latest work — including Reilly’s contributions — to the Association of Computing Machinery’s special interest group on knowledge discovery and data mining (SIGKDD 2019) in early August. He teaches an “infinite course” with a duology of classes taught in spring and fall semesters that provides an overview of machine learning foundations for natural systems science, which anyone can follow along with online. According to Ravela, if Reilly is to succeed at advancing the mathematical basis for computational learning models, she will be one of the “early pioneers of learning that can be explained,” an achievement that can provide a promising career path. That is ideal for Reilly’s goals of obtaining a PhD in mathematics after graduating from MIT and remaining a contributor to research that can positively impact the world. She’s starting with cramming as much research as she can manage into her schedule over her final two undergraduate years at MIT, including her experience this summer. Although this was Reilly’s first UROP experience, it is her second time undertaking a research project that blends mathematics, computer science, and Earth science. Previously, at the Johns Hopkins University Applied Physics Laboratory, Reilly helped develop signal processing techniques and software that would improve the retrieval of useful climate change information from low-quality satellite data. “I’ve always wanted to be part of an interdisciplinary research environment where I could use my knowledge of math to contribute to the work of scientists and engineers,” Reilly says of working within EAPS. “It’s encouraging to see that type of environment and get a taste of what it would be like to work in one.” Ravela explains that the ESSG is fond of the mutually beneficial inclusion of UROP students. “For me, UROPs are better than grad student and postdocs if, and only if, one can create the right-sized questions for them to run with. But then they run the fastest and are the most clever of all.” He says he feels the UROP program is invaluable and could be beneficial for all students to incorporate, as it offers a chance to learn about other fields and interdisciplinary research, as well as how to incorporate what they learn into tangible results. For Reilly, research builds on her foundation obtained from taking classes at MIT, which are a controlled and predictable environment, she says, “but research is nowhere near so linear.” She has relied on her foundation of mathematics and computer science from her courses during her UROP experience while having to learn how to connect and apply them to new fields and to consider topics often outside an undergraduate education. “It often feels like every step I take requires me to learn about an entirely new field of mathematics, and it’s difficult to know where to start. I definitely feel lost sometimes, but I’m also learning an incredible amount.” ",Computer Science,0.2007209532192004
5,MIT News,Using machine learning to hunt down cybercriminals ,Research,2019-10-08,-,http://news.mit.edu/2019/using-machine-learning-hunt-down-cybercriminals-1009,"   Hijacking IP addresses is an increasingly popular form of cyber-attack. This is done for a range of reasons, from sending spam and malware to stealing Bitcoin. It’s estimated that in 2017 alone, routing incidents such as IP hijacks affected more than 10 percent of all the world’s routing domains. There have been major incidents at Amazon and Google and even in nation-states — a study last year suggested that a Chinese telecom company used the approach to gather intelligence on western countries by rerouting their internet traffic through China. Existing efforts to detect IP hijacks tend to look at specific cases when they’re already in process. But what if we could predict these incidents in advance by tracing things back to the hijackers themselves?   That’s the idea behind a new machine-learning system developed by researchers at MIT and the University of California at San Diego (UCSD). By illuminating some of the common qualities of what they call “serial hijackers,” the team trained their system to be able to identify roughly 800 suspicious networks — and found that some of them had been hijacking IP addresses for years.  “Network operators normally have to handle such incidents reactively and on a case-by-case basis, making it easy for cybercriminals to continue to thrive,” says lead author Cecilia Testart, a graduate student at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) who will present the paper at the ACM Internet Measurement Conference in Amsterdam on Oct. 23. “This is a key first step in being able to shed light on serial hijackers’ behavior and proactively defend against their attacks.” The paper is a collaboration between CSAIL and the Center for Applied Internet Data Analysis at UCSD’s Supercomputer Center. The paper was written by Testart and David Clark, an MIT senior research scientist, alongside MIT postdoc Philipp Richter and data scientist Alistair King as well as research scientist Alberto Dainotti of UCSD. The nature of nearby networks IP hijackers exploit a key shortcoming in the Border Gateway Protocol (BGP), a routing mechanism that essentially allows different parts of the internet to talk to each other. Through BGP, networks exchange routing information so that data packets find their way to the correct destination.  In a BGP hijack, a malicious actor convinces nearby networks that the best path to reach a specific IP address is through their network. That’s unfortunately not very hard to do, since BGP itself doesn’t have any security procedures for validating that a message is actually coming from the place it says it’s coming from. “It’s like a game of Telephone, where you know who your nearest neighbor is, but you don’t know the neighbors five or 10 nodes away,” says Testart. In 1998 the U.S. Senate's first-ever cybersecurity hearing featured a team of hackers who claimed that they could use IP hijacking to take down the Internet in under 30 minutes. Dainotti says that, more than 20 years later, the lack of deployment of security mechanisms in BGP is still a serious concern. To better pinpoint serial attacks, the group first pulled data from several years’ worth of network operator mailing lists, as well as historical BGP data taken every five minutes from the global routing table. From that, they observed particular qualities of malicious actors and then trained a machine-learning model to automatically identify such behaviors. The system flagged networks that had several key characteristics, particularly with respect to the nature of the specific blocks of IP addresses they use: Volatile changes in activity: Hijackers’ address blocks seem to disappear much faster than those of legitimate networks. The average duration of a flagged network’s prefix was under 50 days, compared to almost two years for legitimate networks. Multiple address blocks: Serial hijackers tend to advertise many more blocks of IP addresses, also known as “network prefixes.” IP addresses in multiple countries: Most networks don’t have foreign IP addresses. In contrast, for the networks that serial hijackers advertised that they had, they were much more likely to be registered in different countries and continents. Identifying false positives Testart said that one challenge in developing the system was that events that look like IP hijacks can often be the result of human error, or otherwise legitimate. For example, a network operator might use BGP to defend against distributed denial-of-service attacks in which there’s huge amounts of traffic going to their network. Modifying the route is a legitimate way to shut down the attack, but it looks virtually identical to an actual hijack. Because of this issue, the team often had to manually jump in to identify false positives, which accounted for roughly 20 percent of the cases identified by their classifier. Moving forward, the researchers are hopeful that future iterations will require minimal human supervision and could eventually be deployed in production environments. “The authors' results show that past behaviors are clearly not being used to limit bad behaviors and prevent subsequent attacks,” says David Plonka, a senior research scientist at Akamai Technologies who was not involved in the work. “One implication of this work is that network operators can take a step back and examine global Internet routing across years, rather than just myopically focusing on individual incidents.” As people increasingly rely on the Internet for critical transactions, Testart says that she expects IP hijacking’s potential for damage to only get worse. But she is also hopeful that it could be made more difficult by new security measures. In particular, large backbone networks such as AT&T have recently announced the adoption of resource public key infrastructure (RPKI), a mechanism that uses cryptographic certificates to ensure that a network announces only its legitimate IP addresses.  “This project could nicely complement the existing best solutions to prevent such abuse that include filtering, antispoofing, coordination via contact databases, and sharing routing policies so that other networks can validate it,” says Plonka. “It remains to be seen whether misbehaving networks will continue to be able to game their way to a good reputation. But this work is a great way to either validate or redirect the network operator community's efforts to put an end to these present dangers.” The project was supported, in part, by the MIT Internet Policy Research Initiative, the William and Flora Hewlett Foundation, the National Science Foundation, the Department of Homeland Security, and the Air Force Research Laboratory. ",Computer Science,0.19647909578452996
6,ACM,Cybersecurity Giants to Combat Cyberthreats Under OASIS Umbrella,ACM,2019-10-08,-,https://www.zdnet.com/article/cybersecurity-firms-join-forces-to-combat-open-source-security-woes-under-oasis-umbrella/,"    Cybersecurity Giants to Combat Cyberthreats Under OASIS Umbrella     ZDNetCharlie OsborneOctober 8, 2019   IBM, McAfee, and 16 more firms have launched an effort to address fragmentation and interoperability problems in cybersecurity, by linking market products via the Organization for the Advancement of Structured Information Standards (OASIS) consortium. Each member of the Open Cybersecurity Alliance (OCA) has agreed to contribute cybersecurity resources to ""develop open source security technologies which can freely exchange information, insights, analytics, and orchestrated responses."" OCA goals include the development of open source content, code, tools, and practices aimed at enhancing the interoperability of cybersecurity solutions. Two OCA projects are currently underway: IBM Security's STIX-Shifter, aimed at developing a search function for cybersecurity solutions through an open source, standardized cybersecurity data model/library, and a McAfee initiative to create an interoperable messaging format backed by the OpenDXL messaging bus.                 ",Computer Science,0.19161224939005359
7,MIT News,System helps smart devices find their position,Research,2019-10-02,-,http://news.mit.edu/2019/iot-smart-device-position-1003,"  A new system developed by researchers at MIT and elsewhere helps networks of smart devices cooperate to find their positions in environments where GPS usually fails. Today, the “internet of things” concept is fairly well-known: Billions of interconnected sensors around the world — embedded in everyday objects, equipment, and vehicles, or worn by humans or animals — collect and share data for a range of applications. An emerging concept, the “localization of things,” enables those devices to sense and communicate their position. This capability could be helpful in supply chain monitoring, autonomous navigation, highly connected smart cities, and even forming a real-time “living map” of the world. Experts project that the localization-of-things market will grow to $128 billion by 2027. The concept hinges on precise localization techniques. Traditional methods leverage GPS satellites or wireless signals shared between devices to establish their relative distances and positions from each other. But there’s a snag: Accuracy suffers greatly in places with reflective surfaces, obstructions, or other interfering signals, such as inside buildings, in underground tunnels, or in “urban canyons” where tall buildings flank both sides of a street. Researchers from MIT, the University of Ferrara, the Basque Center of Applied Mathematics (BCAM), and the University of Southern California have developed a system that captures location information even in these noisy, GPS-denied areas. A paper describing the system appears in the Proceedings of the IEEE. When devices in a network, called “nodes,” communicate wirelessly in a signal-obstructing, or “harsh,” environment, the system fuses various types of positional information from dodgy wireless signals exchanged between the nodes, as well as digital maps and inertial data. In doing so, each node considers information associated with all possible locations — called “soft information” — in relation to those of all other nodes. The system leverages machine-learning techniques and techniques that reduce the dimensions of processed data to determine possible positions from measurements and contextual data. Using that information, it then pinpoints the node’s position. In simulations of harsh scenarios, the system operates significantly better than traditional methods. Notably, it consistently performed near the theoretical limit for localization accuracy. Moreover, as the wireless environment got increasingly worse, traditional systems’ accuracy dipped dramatically while the new soft information-based system held steady. “When the tough gets tougher, our system keeps localization accurate,” says Moe Win, a professor in the Department of Aeronautics and Astronautics and the Laboratory for Information and Decision Systems (LIDS), and head of the Wireless Information and Network Sciences Laboratory. “In harsh wireless environments, you have reflections and echoes that make it far more difficult to get accurate location information. Places like the Stata Center [on the MIT campus] are particularly challenging, because there are surfaces reflecting signals everywhere. Our soft information method is particularly robust in such harsh wireless environments.” Joining Win on the paper are: Andrea Conti of the University of Ferrara; Santiago Mazuelas of BCAM; Stefania Bartoletti of the University of Ferrara; and William C. Lindsey of the University of Southern California. Capturing “soft information” In network localization, nodes are generally referred to as anchors or agents. Anchors are nodes with known positions, such as GPS satellites or wireless base stations. Agents are nodes that have unknown positions — such as autonomous cars, smartphones, or wearables. To localize, agents can use anchors as reference points, or they can share information with other agents to orient themselves. That involves transmitting wireless signals, which arrive at the receiver carrying positional information. The power, angle, and time-of-arrival of the received waveform, for instance, correlate to the distance and orientation between nodes. Traditional localization methods extract one feature of the signal to estimate a single value for, say, the distance or angle between two nodes. Localization accuracy relies entirely on the accuracy of those inflexible (or “hard”) values, and accuracy has been shown to decrease drastically as environments get harsher. Say a node transmits a signal to another node that’s 10 meters away in a building with many reflective surfaces. The signal may bounce around and reach the receiving node at a time corresponding to 13 meters away. Traditional methods would likely assign that incorrect distance as a value. For the new work, the researchers decided to try using soft information for localization. The method leverages many signal features and contextual information to create a probability distribution of all possible distances, angles, and other metrics. “It’s called ‘soft information’ because we don’t make any hard choices about the values,” Conti says. The system takes many sample measurements of signal features, including its power, angle, and time of flight. Contextual data come from external sources, such as digital maps and models that capture and predict how the node moves. Back to the previous example: Based on the initial measurement of the signal’s time of arrival, the system still assigns a high probability that the nodes are 13 meters apart. But it assigns a small possibility that they’re 10 meters apart, based on some delay or power loss of the signal. As the system fuses all other information from surrounding nodes, it updates the likelihood for each possible value. For instance, it could ping a map and see that the room’s layout shows it’s highly unlikely both nodes are 13 meters apart. Combining all the updated information, it decides the node is far more likely to be in the position that is 10 meters away. “In the end, keeping that low-probability value matters,” Win says. “Instead of giving a definite value, I’m telling you I’m really confident that you’re 13 meters away, but there’s a smaller possibility you’re also closer. This gives additional information that benefits significantly in determining the positions of the nodes.” Reducing complexity Extracting many features from signals, however, leads to data with large dimensions that can be too complex and inefficient for the system. To improve efficiency, the researchers reduced all signal data into a reduced-dimension and easily computable space. To do so, they identified aspects of the received waveforms that are the most and least useful for pinpointing location based on “principal component analysis,” a technique that keeps the most useful aspects in multidimensional datasets and discards the rest, creating a dataset with reduced dimensions. If received waveforms contain 100 sample measurements each, the technique might reduce that number to, say, eight. A final innovation was using machine-learning techniques to learn a statistical model describing possible positions from measurements and contextual data. That model runs in the background to measure how that signal-bouncing may affect measurements, helping to further refine the system’s accuracy. The researchers are now designing ways to use less computation power to work with resource-strapped nodes that can’t transmit or compute all necessary information. They’re also working on bringing the system to “device-free” localization, where some of the nodes can’t or won’t share information. This will use information about how the signals are backscattered off these nodes, so other nodes know they exist and where they are located. ",Computer Science,0.19141720516204175
8,Science Daily,Combination of Techniques Could Improve Security for IoT Devices,Computers & Math,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010164838.htm,"   ""By 2020, more than 20 billion IoT devices will be in operation, and these devices can leave people vulnerable to security breaches that can put their personal data at risk or worse, affect their safety,"" said Beulah Samuel, a student in the Penn State World Campus information sciences and technology program. ""Yet no strategy exists to identify when and where a network security attack on these devices is taking place and what such an attack even looks like."" The team applied a combination of approaches often used in traditional network security management to an IoT network simulated by the University of New South Wales Canberra. Specifically, they showed how statistical data, machine learning and other data analysis methods could be applied to assure the security of IoT systems across their lifecycle. They then used intrusion detection and a visualization tool, to determine whether or not an attack had already occurred or was in progress within that network. The researchers describe their approach and findings in a paper to be presented today (Oct. 10) at the 2019 IEEE Ubiquitous Computing, Electronics and Mobile Communication Conference. The team received the ""Best Paper"" award for their work. One of the data analysis techniques the team applied was the open-source freely available R statistical suite, which they used to characterize the IoT systems in use on the Canberra network. In addition, they used machine learning solutions to search for patterns in the data that were not apparent using R. ""One of the challenges in maintaining security for IoT networks is simply identifying all the devices that are operating on the network,"" said John Haller, a student in the Penn State World Campus information sciences and technology program. ""Statistical programs, like R, can characterize and identify the user agents."" The researchers used the widely available Splunk intrusion detection tool, which comprises software for searching, monitoring and analyzing network traffic, via a Web-style interface.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Splunk is an analytical tool that is often used in traditional network traffic monitoring, but had only seen limited application to IoT traffic, until now,"" said Melanie Seekins. Using these tools, and others, the team identified three IP addresses that were actively trying to break into the Canberra network's devices. ""We observed three IP addresses attempting to attach to the IoT devices multiple times over a period of time using different protocols,"" said Andrew Brandon. ""This clearly indicates a Distributed Denial of Service attack, which aims to disrupt and/or render devices unavailable to the owners."" As the basis for their approach, the researchers compared it to a common framework used to help manage risk, the National Institute of Standards and Technology (NIST) Risk Management Framework (RMF). ""The NIST RMF was not created for IoT systems, but it provides a framework that organizations can use to tailor, test, and monitor implemented security controls. This lends credibility to our approach,"" said Brandon.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Ultimately, Seekins said, the ability to analyze IoT data using the team's approach may enable security professionals to identify and manage controls to mitigate risk and analyze incidents as they occur. ""Knowing what has taken place in an actual attack helps us write scripts and monitors to look for those patterns,"" she said. ""These predictive patterns and the use of machine learning and artificial intelligence can help us anticipate and prepare for major attacks using IoT devices."" The team hopes their approach will contribute to the creation of a standard protocol for IoT network security. ""There is no standardization for IoT security,"" said Seekins. ""Each manufacturer or vendor creates their own idea of what security looks like, and this can become proprietary and may or may not work with other devices. Our strategy is a good first step toward alleviating this problem."" ",Computer Science,0.19015555353107025
9,ACM,Flaw in iTunes for Windows Abused for Ransomware Attacks,ACM,2019-10-10,-,https://www.pcmag.com/news/371261/flaw-in-itunes-for-windows-abused-for-ransomware-attacks,"       Flaw in iTunes for Windows Abused for Ransomware Attacks     PC MagazineMichael KanOctober 10, 2019   Hackers behind the BitPaymer ransomware are exploiting a vulnerability in iTunes for Windows' Bonjour updater to bypass antivirus software detection, say researchers at security firm Morphisec. The researchers also found an ""unquoted path vulnerability"" within the app that causes the updater to indiscriminately run files, whether harmless or malicious. Apple has corrected the unquoted path vulnerability via iCloud updates for Windows 7 and 10. However, Morphisec said many users may still be running unpatched versions of the Bonjour updater on their PCs, even with the removal of iTunes.                 ",Computer Science,0.18817199775116525
10,Science Daily,Diversity May Be Key to Reducing Errors in Quantum Computing,Computers & Math,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103411.htm,"   Unlike conventional computers, the processing in quantum-based machines is noisy, which produces error rates dramatically higher than those of silicon-based computers. So quantum operations are repeated thousands of times to make the correct answer stands out statistically from all the wrong ones. But running the same operation over and over again on the same qubit set may just generate the same incorrect answers that can appear statistically to be the correct answer. The solution, according to researchers at the Georgia institute of Technology, is to repeat the operation on different qubit sets that have different error signatures - and therefore won't produce the same correlated errors. ""The idea here is to generate a diversity of errors so you are not seeing the same error again and again,"" said Moinuddin Qureshi, a professor in Georgia Tech's School of Electrical and Computer Engineering, who worked out the technique with his senior Ph.D. student, Swamit Tannu. ""Different qubits tend to have different error signatures. When you combine the results from diverse sets, the right answer appears even though each of them individually did not get the right answer,"" said Tannu. Tannu compares the technique, known as Ensemble of Diverse Mappings (EDM), to the game show Who Wants to be a Millionaire. Contestants who aren't sure of the answer to a multiple choice question can ask the studio audience for help. ""It's not necessary that the majority of the people in the audience know the right answer,"" Qureshi said. ""If even 20% know it, you can identify it. If the answers go equally in the four buckets from the people who don't know, the right answer will get 40% and you can select it even if only a relatively small number of people get it right."" Experiments with an existing Noisy Intermediate Scale Quantum (NISQ) computer showed that EDM improves the inference quality by 2.3 times compared to state-of-the-art mapping algorithms. By combining the output probability distributions of the diverse ensemble, EDM amplifies the correct answer by suppressing the incorrect ones.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The EDM technique, Tannu admits, is counterintuitive. Qubits can be ranked according to their error rate on specific types of problems, and the most logical course of action might be to use the set that's most accurate. But even the best qubits produce errors, and those errors are likely to be the same when the operation is done thousands of times. Choosing qubits with different error rates - and therefore different types of error - guards against that by ensuring that the one correct answer will rise above the diversity of errors. ""The goal of the research is to create several different versions of the program, each of which can make a mistake, but they will not make identical mistakes,"" Tannu explained. ""As long as they make diverse mistakes, when you average things out, the mistakes get canceled out and the right answer emerges."" Qureshi compares the EDM technique to team-building techniques promoted by human resource consultants. ""If you form a team of experts with identical backgrounds, all of them may have the same blind spot,"" he said, adding a human dimension. ""If you want to make a team resilient to blind spots, collect a group of people who have different blind spots. As a whole, the team will be guarded against specific blind spots."" Error rates in conventional silicon-based computers are practically negligible, about one in a thousand-trillion operations, but today's NISQ quantum computers produce an error in a mere 100 operations.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""These are really early-stage machines in which the devices have a lot of error,"" Qureshi said. ""That will likely improve over time, but because we are dependent on matter that has extremely low energy and lacks stability, we will never get the reliability we have come to expect with silicon. Quantum states are inherently about a single particle, but with silicon you are packing a lot of molecules together and averaging their activity. ""If the hardware is inherently unreliable, we have to write software to make the most of it,"" he said. ""We have to take the hardware characteristics into account to make these unique machines useful."" The notion of running a quantum operation thousands of times to get what's likely to be the right answer at first seems counterproductive. But quantum computing is so much faster than conventional computing that nobody would object to doing a few thousand duplicate runs. ""The objective with quantum computers is not to take a current program and run it faster,"" Qureshi said. ""Using quantum, we can solve problems that are virtually impossible to solve with even the fastest supercomputers. With several hundred qubits, which is beyond the current state of the art, we could solve problems that would take a thousand years with the fastest supercomputer."" Added Qureshi: ""You don't mind doing the computation a few thousand times to get an answer like that."" The quantum error mitigation scheme is scheduled to be presented on Oct. 14 at the 52nd Annual IEEE/ACM International Symposium on Microarchitecture. The work was supported by a gift from Microsoft. ",Computer Science,0.18746820245556575
11,MIT News,MIT.nano awards inaugural NCSOFT seed grants for gaming technologies,Electronics and Technology,2019-09-30,-,http://news.mit.edu/2019/mitnano-awards-inaugural-ncsoft-seed-grants-gaming-technologies-0930,"  MIT.nano has announced the first recipients of NCSOFT seed grants to foster hardware and software innovations in gaming technology. The grants are part of the new MIT.nano Immersion Lab Gaming program, with inaugural funding provided by video game developer NCSOFT, a founding member of the MIT.nano Consortium. The newly awarded projects address topics such as 3-D/4-D data interaction and analysis, behavioral learning, fabrication of sensors, light field manipulation, and micro-display optics.  “New technologies and new paradigms of gaming will change the way researchers conduct their work by enabling immersive visualization and multi-dimensional interaction,” says MIT.nano Associate Director Brian W. Anthony. “This year’s funded projects highlight the wide range of topics that will be enhanced and influenced by augmented and virtual reality.” In addition to the sponsored research funds, each awardee will be given funds specifically to foster a community of collaborative users of MIT.nano’s Immersion Lab. The MIT.nano Immersion Lab is a new, two-story immersive space dedicated to visualization, augmented and virtual reality (AR/VR), and the depiction and analysis of spatially related data. Currently being outfitted with equipment and software tools, the facility will be available starting this semester for use by researchers and educators interested in using and creating new experiences, including the seed grant projects.  The five projects to receive NCSOFT seed grants are: Stefanie Mueller: connecting the virtual and physical world Virtual game play is often accompanied by a prop — a steering wheel, a tennis racket, or some other object the gamer uses in the physical world to create a reaction in the virtual game. Build-it-yourself cardboard kits have expanded access to these props by lowering costs; however, these kits are pre-cut, and thus limited in form and function. What if users could build their own dynamic props that evolve as they progress through the game? Department of Electrical Engineering and Computer Science (EECS) Professor Stefanie Mueller aims to enhance the user’s experience by developing a new type of gameplay with tighter virtual-physical connection. In Mueller’s game, the player unlocks a physical template after completing a virtual challenge, builds a prop from this template, and then, as the game progresses, can unlock new functionalities to that same item. The prop can be expanded upon and take on new meaning, and the user learns new technical skills by building physical prototypes. Luca Daniel and Micha Feigin-Almon: replicating human movements in virtual characters Athletes, martial artists, and ballerinas share the ability to move their body in an elegant manner that efficiently converts energy and minimizes injury risk. Professor Luca Daniel, EECS and Research Laboratory of Electronics, and Micha Feigin-Almon, research scientist in mechanical engineering, seek to compare the movements of trained and untrained individuals to learn the limits of the human body with the goal of generating elegant, realistic movement trajectories for virtual reality characters. In addition to use in gaming software, their research on different movement patterns will predict stresses on joints, which could lead to nervous system models for use by artists and athletes. Wojciech Matusik: using phase-only holograms Holographic displays are optimal for use in augmented and virtual reality. However, critical issues show a need for improvement. Out-of-focus objects look unnatural, and complex holograms have to be converted to phase-only or amplitude-only in order to be physically realized. To combat these issues, EECS Professor Wojciech Matusik proposes to adopt machine learning techniques for synthesis of phase-only holograms in an end-to-end fashion. Using a learning-based approach, the holograms could display visually appealing three-dimensional objects. “While this system is specifically designed for varifocal, multifocal, and light field displays, we firmly believe that extending it to work with holographic displays has the greatest potential to revolutionize the future of near-eye displays and provide the best experiences for gaming,” says Matusik. Fox Harrell: teaching socially impactful behavior Project VISIBLE — Virtuality for Immersive Socially Impactful Behavioral Learning Enhancement — utilizes virtual reality in an educational setting to teach users how to recognize, cope with, and avoid committing microaggressions. In a virtual environment designed by Comparative Media Studies Professor Fox Harrell, users will encounter micro-insults, followed by major micro-aggression themes. The user’s physical response drives the narrative of the scenario, so one person can play the game multiple times and reach different conclusions, thus learning the various implications of social behavior. Juejun Hu: displaying a wider field of view in high resolution Professor Juejun Hu from the Department of Materials Science and Engineering seeks to develop high-performance, ultra-thin immersive micro-displays for AR/VR applications. These displays, based on metasurface optics, will allow for a large, continuous field of view, on-demand control of optical wavefronts, high-resolution projection, and a compact, flat, lightweight engine. While current commercial waveguide AR/VR systems offer less than 45 degrees of visibility, Hu and his team aim to design a high-quality display with a field of view close to 180 degrees. ",Computer Science,0.18707783662894323
12,MIT News,Machine learning you can dance to,Computer Science,2019-09-18,-,http://news.mit.edu/2019/machine-learning-you-can-dance-to-samply-0918,"  Rhythmic flashes from a computer screen illuminate a dark room as sounds fill the air. The snare drum sample comes out crisp and clean by itself, but turns muddy in the mix, no matter how the levels are set. Welcome to the world of modern music-making — and its discontents. Today’s digital music producers face a common dilemma: how to mesh samples that may sound great on their own but do not necessarily fit into a song like they originally imagined. One solution is to find and audit dozens of different samples, a tedious process that can take time to finesse. “There’s a lot of manual searching to get the right musical result, which can be distracting and time-consuming,” says Justin Swaney, a PhD student in the MIT Department of Chemical Engineering, a music producer, and co-creator of a new tool that uses machine learning to help producers find just the perfect sound. Called Samply, Swaney’s visual sample-library explorer combines music and machine learning into a new technology for producers. The top winner at the MIT Stephen A. Schwarzman College of Computing Machine Learning Across Disciplines Challenge at the Hello World celebration last winter, the tool uses a convolutional neural network to analyze audio waveforms. “Samply organizes samples based on their sonic characteristics,” explains Swaney. “The result is an interactive plot where similar sounds are closer together and different sounds are farther apart. Samply allows multiple sample libraries to be visualized simultaneously, shortening the lag between imagining a sound in your head and finding it.” For Swaney, the development of Samply drew on both his research expertise and personal life. Before coming to MIT, he had produced albums with indie musicians including Eric Schirtzinger, a drummer and co-creator of the tool. The two recorded drums in a basement and tried to improvise with cheap hardware and hacks — like hanging rugs from the ceiling to dampen reverberation. “The constraints made us get creative,” says Schirtzinger, who is now a computer science major at the University of Wisconsin at Madison. That creativity was further honed after Swaney completed 6.862 (Applied Machine Learning). He saw an opportunity to rekindle his music production hobby by applying what he had learned from the project-based course, devising a way to automate the search for the right samples when producing a new song. “I figured the computer could listen to samples much faster than I could,” he says. Beyond the clever use of machine learning, the real magic of Samply is that conceptually, it is founded on a deep understanding of what it takes to make music. “We aren’t just AI enthusiasts applying machine learning to music,” says Schirtzinger. “We are musicians who want better tools for making music.” It turns out that at MIT, they aren’t the only ones with a song in their hearts. While presenting Samply at the Schwarzman College of Computing exposition last winter, dozens of faculty, staff, and students gathered around Swaney’s poster and live demonstration to exchange ideas. Some had years of experience producing music with professional software, while others simply appreciated the visualizations and sounds in the demo. Spurred by the interest in Samply at the exposition, Swaney and Shirtzinger are in the process of turning their project into a startup company. As a first step, the two reached out to the Technology Licensing Office (TLO) for advice, which referred them to the Venture Mentoring Service (VMS). Samply joined VMS in April and was paired with two MIT-affiliated mentors and entrepreneurs, Stephen Bayle and John Stempeck. After pitching Samply to his mentors, Swaney received sage advice on a crafting a business plan and sales strategy, and then began making connections with others interested in music technology as a business. Samply has since been accepted into the ELEVATE accelerator, sponsored by the local digital marketing firm HubSpot, and Swaney is applying for seed funding through the MIT Sandbox Innovation Fund. “Starting a company as a student can be daunting, but the MIT community gives us confidence,” he says. “If we can’t do it at MIT, then where can we?” In fact, the time and attention he has spent on Samply has had an “almost paradoxical” benefit to his academic life as a graduate student. “I was spending all of my time in the lab,” he says. “When I took a step back to make Samply, I could see the forest from the trees in my research.” Swaney found that focusing on his love of music served as an “emotional outlet,” helping to mitigate intellectual burnout. Although Samply may have taken him away from the lab bench, it has also ended up informing his research. The original idea of visualizing samples, he says, stemmed from “my work on single-cell analysis.” Applying the method to the tool clarified his thinking in the biological realm, leading to a new method to produce better clustering, or a way to better sort, recognize, and visualize groups of cells. “It was a bit like a musical theme and variation, but with my research,” Swaney says. As for Samply, there will be a free beta version of the app launching in September, and a Kickstarter campaign is due in the coming year to fuel future developments. “We want to get Samply into the hands of more producers and content creators so that we can establish a feedback loop that guides our priorities,” he says. “Our technology may also have applications in live music performance, instrumentation, and in film and videography. We are excited to explore those possibilities.” ",Computer Science,0.1840840390244918
13,MIT News,Using math to blend musical notes seamlessly,Computer Science,2019-09-27,-,http://news.mit.edu/2019/math-portamento-music-0927,"  In music, “portamento” is a term that’s been used for hundreds of years, referring to the effect of gliding a note at one pitch into a note of a lower or higher pitch. But only instruments that can continuously vary in pitch — such as the human voice, string instruments, and trombones — can pull off the effect. Now an MIT student has invented a novel algorithm that produces a portamento effect between any two audio signals in real-time. In experiments, the algorithm seamlessly merged various audio clips, such as a piano note gliding into a human voice, and one song blending into another. His paper describing the algorithm won the “best student paper” award at the recent International Conference on Digital Audio Effects. The algorithm relies on “optimal transport,” a geometry-based framework that determines the most efficient ways to move objects — or data points — between multiple origin and destination configurations. Formulated in the 1700s, the framework has been applied to supply chains, fluid dynamics, image alignment, 3-D modeling, computer graphics, and more.        In work that originated in a class project, Trevor Henderson, now a graduate student in computer science, applied optimal transport to interpolating audio signals — or blending one signal into another. The algorithm first breaks the audio signals into brief segments. Then, it finds the optimal way to move the pitches in  each segment to pitches in the other signal, to produce the smooth glide of the portamento effect. The algorithm also includes specialized techniques to maintain the fidelity of the audio signal as it transitions. “Optimal transport is used here to determine how to map pitches in one sound to the pitches in the other,” says Henderson, a classically trained organist who performs electronic music and has been a DJ on WMBR 88.1, MIT’s radio station. “If it’s transforming one chord into a chord with a different harmony, or with more notes, for instance, the notes will split from the first chord and find a position to seamlessly glide to in the other chord.” According to Henderson, this is one of the first techniques to apply optimal transport to transforming audio signals. He has already used the algorithm to build equipment that seamlessly transitions between songs on his radio show. DJs could also use the equipment to transition between tracks during live performances. Other musicians might use it to blend instruments and voice on stage or in the studio. Henderson’s co-author on the paper is Justin Solomon, an X-Consortium Career Development Assistant Professor in the Department of Electrical Engineering and Computer Science. Solomon — who also plays cello and piano — leads the Geometric Data Processing Group in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and is a member of the Center for Computational Engineering. Henderson took Solomon’s class, 6.838 (Shape Analysis), which tasks students with applying geometric tools like optimal transport to real-world applications. Student projects usually focus on 3-D shapes from virtual reality or computer graphics. So Henderson’s project came as a surprise to Solomon. “Trevor saw an abstract connection between geometry and moving frequencies around in audio signals to create a portamento effect,” Solomon says. “He was in and out of my office all semester with DJ equipment. It wasn’t what I expected to see, but it was pretty entertaining.” For Henderson, it wasn’t too much of a stretch. “When I see a new idea, I ask, ‘Is this applicable to music?’” he says. “So, when we talked about optimal transport, I wondered what would happen if I connected it to audio spectra.” A good way to think of optimal transport, Henderson says, is finding “a lazy way to build a sand castle.” In that analogy, the framework is used to calculate the way to move each grain of sand from its position in a shapeless pile into a corresponding position in a sand castle, using as little work as possible. In computer graphics, for instance, optimal transport can be used to transform or morph shapes by finding the optimal movement from each point on one shape into the other. Applying this theory to audio clips involves some additional ideas from signal processing. Musical instruments produce sound through vibrations of components, depending on the instrument. Violins use strings, brass instruments use air inside hollow bodies, and humans use vocal cords. These vibrations can be captured as audio signals, where the frequency and amplitude (peak height) represent different pitches.  Conventionally, the transition between two audio signals is done with a fade, where one signal is reduced in volume while the other rises. Henderson’s algorithm, on the other hand, smoothly slides frequency segments from one clip into another, with no fading of volume. To do so, the algorithm splits any two audio clips into windows of about 50 milliseconds. Then, it runs a Fourier transform, which turns each window into its frequency components. The frequency components within a window are lumped together into individual synthesized “notes.” Optimal transport then maps how the notes in one signal’s window will move to the notes in the other. Then, an “interpolation parameter” takes over. That’s basically a value that determines where each note will be on the path from its starting pitch in one signal to its ending pitch in the other. Manually changing the parameter value will sweep the pitches between the two positions, producing the portamento effect. That single parameter can also be programmed into and controlled by, say, a crossfader, a slider component on a DJ’s mixing board that smoothly fades between songs. As the crossfader slides, the interpolation parameter changes to produce the effect. Behind the scenes are two innovations that ensure a distortion-free signal. First, Henderson used a novel application of a signal-processing technique, called “frequency reassignment,” that lumps the frequency bins together to form single notes that can easily transition between signals. Second, he invented a way to synthesize new phases for each audio signal while stitching together the 50-millisecond windows, so neighboring windows don’t interfere with each other. Next, Henderson wants to experiment with feeding the output of the effect back into its input. This, he thinks, could automatically create another classic music effect, “legato,” which is a smooth transition between distinct notes. Unlike a portamento — which plays all notes between a start and end note — a legato seamlessly transitions between two distinct notes, without capturing any notes in between. ",Computer Science,0.18316440214134494
14,Stanford,An artificial retina that could help restore sight to the blind,Science,2019-10-15,-,https://engineering.stanford.edu/magazine/article/artificial-retina-could-help-restore-sight-blind,"  For more than a decade, researchers have been working to create artificial digital retinas that can be implanted in the eye to allow the blind to see again. Many challenges stand in the way, but researchers at Stanford University may have found the key to solving one of the most vexing: heat. The artificial retina requires a very small computer chip with many metal electrodes poking out. The electrodes first record the activity of the neurons around them to create a map of cell types. This information is then used to transmit visual data from a camera to the brain. Unfortunately, the eye produces so much data during recording that the electronics get too darn hot. “The chips required to build a high-quality artificial retina would essentially fry the human tissue they are trying to interface with,” says E.J. Chichilnisky, a professor in the Neurosurgery and Ophthalmology departments, who is on Stanford’s artificial retina team. Members of the team, including Chichilnisky and his collaborators in Stanford’s Electrical Engineering and Computer Science departments, recently announced they have devised a way to solve that problem by significantly compressing the massive amounts of visual data that all those neurons in the eye create. They discuss their advance in a study published in the IEEE Transactions on Biomedical Circuits and Systems. To convey visual information, neurons in the retina send electrical impulses, known as spikes, to the brain. The problem is that the digital retina needs to record and decode those spikes to understand the properties of the neurons, but that generates a lot of heat in the digitization process, even with only a few hundred electrodes used in today’s prototypes. The first true digital retina will need to have tens of thousands of such electrodes, complicating the issue further. Boris Murmann, a professor of electrical engineering on the retina project, says the team found a way to extract the same level of visual understanding using less data. By better understanding which signal samples matter and which can be ignored, the team was able to reduce the amount of data that has to be processed. It’s a bit like being at a party trying to extract a single coherent conversation amid the din of a crowded room — a few voices matter a lot, but most are noise and can be ignored. Boris Murmann, professor of electrical engineering, member of Stanford Bio-X and of Wu Tsai Neurosciences Institute. “We compress the data by being more selective, ignoring the noise and baseline samples and digitizing only the unique spikes,” Murmann says. Previously, digitization and compression were done separately, leading to a lot of extra data storage and data transfer. “Our innovation inserts compression techniques into the digitization process,” says team member Subhasish Mitra, a professor of electrical engineering and of computer science. This approach retains the most useful information and is easier to implement in hardware. Dante Muratore, a postdoctoral researcher on the team, says the process is surprisingly straightforward conceptually. Each spike has its own wave-like shape that helps researchers determine what sort of cell produced it — a key bit of knowledge in the retina, where different cells have different functions. Whenever two or more electrodes in the artificial retina record identical signal samples it is treated as a “collision,” effectively wiping out the data. The collisions can be safely ignored. On the other hand, whenever a unique signal sample is recorded by a single electrode, it is considered to have high value and gets stored for further processing. In testing their approach, the researchers say their efficient data-gathering method misses just 5% of cells, yet reduces the acquired data by 40 times. The researchers believe it is a first step to a day of efficient, cool-running implantable chips that would work not just in the eye but in other so-called “neuroprosthetic” brain-machine interfaces that turn nerve impulses into computer signals. Such applications might include brain-controlled machines that restore motion to the paralyzed and hearing to the deaf, or that open new approaches that aid memory, alleviate mental illness or even improve self-driving vehicles. “This is an important step that might someday allow us to build a digital retina with over 10,000 channels,” Muratore says. Subhasish Mitra, professor of electrical engineering and of computer science, member of Stanford Bio-X and of Wu Tsai Neurosciences Institute. Additional authors include Mary Wooters, assistant professor of computer science and of electrical engineering, and doctoral candidate Pulkit Tandon. ",Computer Science,0.18160347719318215
15,ACM,Engineers Solve 50-Year-Old Puzzle in Signal Processing,ACM,2019-10-10,-,https://www.news.iastate.edu/news/2019/10/10/signalprocessing,"       Engineers Solve 50-Year-Old Puzzle in Signal Processing     Iowa State University News ServiceOctober 10, 2019   Iowa State University researchers have solved the mystery of the inverse fast Fourier transform (IFFT) algorithm, which along with the FFT algorithm comprise the core of signal processing. Iowa State's Alexander Stoytchev and Vladimir Sukhoy developed the inverse chirp z-transform (ICZT) algorithm to generalize the IFFT algorithm, as the FFT was generalized into the CZT. The ICZT plots the output of the CZT back to its input, matching the computational complexity or speed of its counterpart so it can be employed with exponentially decaying or growing frequency elements. Sukhoy said the inverse algorithm was a harder challenge than the original forward algorithm, and ""we needed better precision and more powerful computers to attack it."" Sukhoy added that visualizing the algorithm within the mathematical framework of structured matrices was critical.                 ",Computer Science,0.18063684504713476
16,MIT News,How cities can leverage citizen data while protecting privacy,Computer Science,2019-09-25,-,http://news.mit.edu/2019/how-cities-citizen-data-privacy-0925,"  India is on a path with dual — and potentially conflicting — goals related to the use of citizen data. To improve the efficiency their municipal services, many Indian cities have started enabling government-service requests, which involves collecting and sharing citizen data with government officials and, potentially, the public. But there’s also a national push to protect citizen privacy, potentially restricting data usage. Cities are now beginning to question how much citizen data, if any, they can use to track government operations. In a new study, MIT researchers find that there is, in fact, a way for Indian cities to preserve citizen privacy while using their data to improve efficiency. The researchers obtained and analyzed data from more than 380,000 government service requests by citizens across 112 cities in one Indian state for an entire year. They used the dataset to measure each city government’s efficiency based on how quickly they completed each service request. Based on field research in three of these cities, they also identified the citizen data that’s necessary, useful (but not critical), or unnecessary for improving efficiency when delivering the requested service. In doing so, they identified “model” cities that performed very well in both categories, meaning they maximized privacy and efficiency. Cities worldwide could use similar methodologies to evaluate their own government services, the researchers say. The study was presented at this past weekend’s Technology Policy Research Conference. “How do municipal governments collect citizen data to try to be transparent and efficient, and, at the same time, protect privacy? How do you find a balance?” says co-author Karen Sollins, a researcher in the Computer Science and Artificial Intelligence Laboratory (CSAIL), a principal investigator for the Internet Policy Research Initiative (IPRI), and a member of the Privacy, Innovation and e-Governance using Quantitative Systems (PIEQS) group. “We show there are opportunities to improve privacy and efficiency simultaneously, instead of saying you get one or the other, but not both.” Joining Sollins on the paper are: first author Nikita Kodali, a graduate student in the Department of Electrical Engineering and Computer Science; and Chintan Vaishnav, a senior lecturer in the MIT Sloan School of Management, a principal investigator for IPRI, and a member PIEQS. Intersections of privacy and efficiency In recent years, India’s eGovernment Foundation has aimed to significantly improve the transparency, accountability, and efficiency of operations in its many municipal governments. The foundation aims to move all of these governments from paper-based systems to fully digitized systems with citizen interfaces to request and interact with government service departments. In 2017, however, India’s Supreme Court ruled that its citizens have a constitutional right to data privacy and have a say in whether or not their personal data could be used by governments and the private sector. That could potentially limit the information that towns and cities could use to track the performance of their services. Around that time, the researchers had started studying privacy and efficiency issues surrounding the eGovernment Foundation’s digitization efforts. That led to a report that determined which types of citizen data could be used to track government service operations. Building on that work, the researchers were provided 383,959 anonymized citizen-government transactions from digitized modules from 112 local governments in an Indian state for all of 2018. The modules focused on three areas: new water tap tax assessment; new property tax assessment; and public grievances about sanitation, stray animals, infrastructure, schools, and other issues. Citizens send requests to those modules via mobile or web apps by entering various types of personal and property information, and then monitor the progress of the requests. The request and related data pass through various officials that each complete an individual subtask, known as a service level agreement, within a designated time limit. Then, the request passes on to another official, and so on. But much of that citizen information is also visible to the public. The software captured each step of each request, moving from initiation to completion, with time stamps, for each municipal government. The researchers then could rank each task within a town or city, or in aggregation across each town or city on two metrics: a government efficiency index and an information privacy index. The government efficiency index primarily measures a service’s timeliness, compared to the predetermined service level agreement. If a service is completed before its timeframe, it’s more efficient; if it’s completed after, it’s less efficient. The information privacy index measures how responsible is a government in collecting, using, and disclosing citizen data that may be privacy sensitive, such as personally identifiable information. The more the city collects and shares inessential data, the lower its privacy rating. Phone numbers and home addresses, for instance, aren’t needed for many of the services or grievances, yet are collected — and publicly disclosed — by many of the modules. In fact, the researchers found that some modules historically collected detailed personal and property information across dozens of data fields, yet the governments only needed about half of those fields to get the job done. Model behavior By analyzing the two indices, they found eight “model” municipal governments that performed in the top 25 percent for all services in both the efficiency and privacy indices. In short, they used only the essential data — and passed that essential data through fewer officials — to complete a service in a timely manner. The researchers now plan to study how the model cities are able to get services done so quickly. They also hope to study why some cities performed so poorly, in the bottom 25 percent, for any given service. “First, we’re showing India that this is what your best cities look like and what other cities should become,” Vaishnav says. “Then we want to look at why a city becomes a model city.” Similar studies can be conducted in places where similar citizen and government data are available and which have equivalents to India’s service level agreements — which serve as a baseline for measuring efficiency. That information isn’t common worldwide yet, but could be in the near future, especially in cities like Boston and Cambridge, Vaishnav says. “We gather a large amount of data and there’s an urge to do something with the data to improve governments and engage citizens better,” he says. “That may soon be a requirement in democracies around the globe.” Next, the researchers want to create an innovation-based matrix, which will determine which citizen data can and cannot be made public to private parties to help develop new technologies. They’re also working on a model that provides information on a city’s government efficiency and information privacy scores in real time, as citizen requests are being processed. ",Computer Science,0.1783777624907708
17,MIT News,Computing and artificial intelligence: Humanistic perspectives from MIT ,Computer Science,2019-09-24,-,http://news.mit.edu/2019/computing-and-ai-humanistic-perspectives-0924,"  The MIT Stephen A. Schwarzman College of Computing (SCC) will reorient the Institute to bring the power of computing and artificial intelligence to all fields at MIT, and to allow the future of computing and AI to be shaped by all MIT disciplines. To support ongoing planning for the new college, Dean Melissa Nobles invited faculty from all 14 of MIT’s humanistic disciplines in the School of Humanities, Arts, and Social Sciences to respond to two questions:    1) What domain knowledge, perspectives, and methods from your field should be integrated into the new MIT Schwarzman College of Computing, and why?  2) What are some of the meaningful opportunities that advanced computing makes possible in your field?   As Nobles says in her foreword to the series, “Together, the following responses to these two questions offer something of a guidebook to the myriad, productive ways that technical, humanistic, and scientific fields can join forces at MIT, and elsewhere, to further human and planetary well-being.”  The following excerpts highlight faculty responses, with links to full commentaries. The excerpts are sequenced by fields in the following order: the humanities, arts, and social sciences.  Foreword by Melissa Nobles, professor of political science and the Kenan Sahin Dean of the MIT School of Humanities, Arts, and Social Sciences  “The advent of artificial intelligence presents our species with an historic opportunity — disguised as an existential challenge: Can we stay human in the age of AI?  In fact, can we grow in humanity, can we shape a more humane, more just, and sustainable world? With a sense of promise and urgency, we are embarked at MIT on an accelerated effort to more fully integrate the technical and humanistic forms of discovery in our curriculum and research, and in our habits of mind and action.” Read more >> Comparative Media Studies: William Uricchio, professor of comparative media studies “Given our research and practice focus, the CMS perspective can be key for understanding the implications of computation for knowledge and representation, as well as computation’s relationship to the critical process of how knowledge works in culture — the way it is formed, shared, and validated.” Recommended action: “Bring media and computer scholars together to explore issues that require both areas of expertise: text-generating algorithms (that force us to ask what it means to be human); the nature of computational gatekeepers (that compels us to reflect on implicit cultural priorities); and personalized filters and texts (that require us to consider the shape of our own biases).” Read more >> Global Languages: Emma J. Teng, the T.T. and Wei Fong Chao Professor of Asian Civilizations “Language and culture learning are gateways to international experiences and an important means to develop cross-cultural understanding and sensitivity. Such understanding is essential to addressing the social and ethical implications of the expanding array of technology affecting everyday life across the globe.” Recommended action: “We aim to create a 21st-century language center to provide a convening space for cross-cultural communication, collaboration, action research, and global classrooms. We also plan to keep the intimate size and human experience of MIT’s language classes, which only increase in value as technology saturates the world.” Read more >> History: Jeffrey Ravel, professor of history and head of MIT History  “Emerging innovations in computational methods will continue to improve our access to the past and the tools through which we interpret evidence. But the field of history will continue to be served by older methods of scholarship as well; critical thinking by human beings is fundamental to our endeavors in the humanities.” Recommended action: “Call on the nuanced debates in which historians engage about causality to provide a useful frame of reference for considering the issues that will inevitably emerge from new computing technologies. This methodology of the history field is a powerful way to help imagine our way out of today’s existential threats.” Read more >> Linguistics: Faculty of MIT Linguistics “Perhaps the most obvious opportunities for computational and linguistics research concern the interrelation between specific hypotheses about the formal properties of language and their computational implementation in the form of systems that learn, parse, and produce human language.” Recommended action: “Critically, transformative new tools have come from researchers at institutions where linguists work side-by-side with computational researchers who are able to translate back and forth between computational properties of linguistic grammars and of other systems.” Read more >> Literature: Shankar Raman, with Mary C. Fuller, professors of literature “In the age of AI, we could invent new tools for reading. Making the expert reading skills we teach MIT students even partially available to readers outside the academy would widen access to our materials in profound ways.” Recommended action: At least three priorities of current literary engagement with the digital should be integrated into the SCC’s research and curriculum: democratization of knowledge; new modes of and possibilities for knowledge production; and critical analysis of the social conditions governing what can be known and who can know it.” Read more >> Philosophy: Alex Byrne, professor of philosophy and head of MIT Philosophy; and Tamar Schapiro, associate professor of philosophy “Computing and AI pose many ethical problems related to: privacy (e.g., data systems design), discrimination (e.g., bias in machine learning), policing (e.g., surveillance), democracy (e.g., the Facebook-Cambridge Analytica data scandal), remote warfare, intellectual property, political regulation, and corporate responsibility.” Recommended action: “The SCC presents an opportunity for MIT to be an intellectual leader in the ethics of technology. The ethics lab we propose could turn this opportunity into reality.” Read more >> Science, Technology, and Society: Eden Medina and Dwaipayan Banerjee, associate professors of science, technology, and society “A more global view of computing would demonstrate a broader range of possibilities than one centered on the American experience, while also illuminating how computer systems can reflect and respond to different needs and systems. Such experiences can prove generative for thinking about the future of computing writ large.” Recommended action: “Adopt a global approach to the research and teaching in the SCC, an approach that views the U.S. experience as one among many.” Read more >> Women's and Gender Studies: Ruth Perry, the Ann Friedlaender Professor of Literature; with Sally Haslanger, the Ford Professor of Philosophy, and Elizabeth Wood, professor of history “The SCC presents MIT with a unique opportunity to take a leadership role in addressing some of most pressing challenges that have emerged from the role computing technologies play in our society — including how these technologies are reinforcing social inequalities.” Recommended action: “Ensure that women’s voices are heard and that coursework and research is designed with a keen awareness of the difference that gender makes. This is the single-most powerful way that MIT can address the inequities in the computing fields.” Read more >> Writing: Tom Levenson, professor of science writing  “Computation and its applications in fields that directly affect society cannot be an unexamined good. Professional science and technology writers are a crucial resource for the mission of new college of computing, and they need to be embedded within its research apparatus.” Recommended action: “Intertwine writing and the ideas in coursework to provide conceptual depth that purely technical mastery cannot offer.” Read more >> Music: Eran Egozy, professor of the practice in music technology “Creating tomorrow’s music systems responsibly will require a truly multidisciplinary education, one that covers everything from scientific models and engineering challenges to artistic practice and societal implications. The new music technology will be accompanied by difficult questions. Who owns the output of generative music algorithms that are trained on human compositions? How do we ensure that music, an art form intrinsic to all humans, does not become controlled by only a few?” Recommended action: Through the SCC, our responsibility will be not only to develop the new technologies of music creation, distribution, and interaction, but also to study their cultural implications and define the parameters of a harmonious outcome for all.” Read more >> Theater Arts: Sara Brown, assistant professor of theater arts and MIT Theater Arts director of design “As a subject, AI problematizes what is means to be human. There are an unending series of questions posed by the presence of an intelligent machine. The theater, as a synthetic art form that values and exploits liveness, is an ideal place to explore the complex and layered problems posed by AI and advanced computing.” Recommended action: “There are myriad opportunities for advanced computing to be integrated into theater, both as a tool and as a subject of exploration. As a tool, advanced computing can be used to develop performance systems that respond directly to a live performer in real time, or to integrate virtual reality as a previsualization tool for designers.” Read more >> Anthropology: Heather Paxson, the William R. Kenan, Jr. Professor of Anthropology “The methods used in anthropology — a field that systematically studies human cultural beliefs and practices — are uniquely suited to studying the effects of automation and digital technologies in social life. For anthropologists, ‘Can artificial intelligence be ethical?’ is an empirical, not a hypothetical, question. Ethical for what? To whom? Under what circumstances?” Recommended action: “Incorporate anthropological thinking into the new college to prepare students to live and work effectively and responsibly in a world of technological, demographic, and cultural exchanges. We envision an ethnography lab that will provide digital and computing tools tailored to anthropological research and projects.” Read more >> Economics: Nancy L. Rose, the Charles P. Kindleberger Professor of Applied Economics and head of the Department of Economics; and David Autor, the Ford Professor of Economics and co-director of the MIT Task Force on the Work of the Future “The intellectual affinity between economics and computer science traces back almost a century, to the founding of game theory in 1928. Today, the practical synergies between economics and computer science are flourishing. We outline some of the many opportunities for the two disciplines to engage more deeply through the new SCC.” Recommended action: “Research that engages the tools and expertise of economics on matters of fairness, expertise, and cognitive biases in machine-supported and machine-delegated decision-making; and on market design, industrial organization, and the future of work. Scholarship at the intersection of data science, econometrics, and causal inference. Cultivate depth in network science, algorithmic game theory and mechanism design, and online learning. Develop tools for rapid, cost-effective, and ongoing education and retraining for workers.” Read more >> Political Science: Faculty of the Department of Political Science “The advance of computation gives rise to a number of conceptual and normative questions that are political, rather than ethical in character. Political science and theory have a significant role in addressing such questions as: How do major players in the technology sector seek to legitimate their authority to make decisions that affect us all? And where should that authority actually reside in a democratic polity?” Recommended action: “Incorporate the research and perspectives of political science in SCC research and education to help ensure that computational research is socially aware, especially with issues involving governing institutions, the relations between nations, and human rights.” Read more >> Series prepared by SHASS Communications Series Editor and Designer: Emily Hiestand Series Co-Editor: Kathryn O’Neill ",Computer Science,0.1767329635279554
18,MIT News,An interdisciplinary approach to accelerating human-machine collaboration,Robotics,2019-10-02,-,http://news.mit.edu/2019/humatics-david-mindell-machine-1002,"  David Mindell has spent his career defying traditional distinctions between disciplines. His work has explored the ways humans interact with machines, drive innovation, and maintain societal well-being as technology transforms our economy. And, Mindell says, he couldn’t have done it anywhere but MIT. He joined MIT’s faculty 23 years ago after completing his PhD in the Program in Science, Technology, and Society, and he currently holds a dual appointment in engineering and humanities as the Frances and David Dibner Professor of the History of Engineering and Manufacturing in the School of Humanities, Arts, and Social Sciences and professor of aeronautics and astronautics. Mindell’s experience combining fields of study has shaped his ideas about the relationship between humans and machines. Those ideas are what led him to found Humatics — a startup named from the merger of “human” and “robotics.” Humatics is trying to change the way humans work alongside machines, by enabling location tracking and navigation indoors, underground, and in other areas where technologies like GPS are limited. It accomplishes this by using radio frequencies to track things at the millimeter scale — unlocking what Mindell calls microlocation technology. The company’s solution is already being used in places like shipping ports and factories, where humans work alongside cranes, industrial tools, automated guided vehicles (AGVs), and other machines. These businesses often lack consistent location data for their machines and are forced to adopt inflexible routes for their mobile robots. “One of the holy grails is to have humans and robots share the same space and collaborate, and we’re enabling mobile robots to work in human environments safely and on a large scale,” Mindell says. “Safety is a critical first form of collaboration, but beyond that, we’re just beginning to learn how to work [in settings] where robots and people are exquisitely aware of where they are.” A company decades in the making MIT has a long history of transcending research fields to improve our understanding of the world. Take, for example, Norbert Wiener, who served on MIT’s faculty in the Department of Mathematics between 1919 and his death in 1964. Wiener is credited with formalizing the field of cybernetics, which is an approach to understanding feedback systems he defined as “the scientific study of control and communication in the animal and the machine."" Cybernetics can be applied to mechanical, biological, cognitive, and social systems, among others, and it sparked a frenzy of interdisciplinary study and scientific collaboration. In 2002, Mindell wrote a book exploring the history of cybernetics before Wiener and its emergence at the intersection of a range of disciplines during World War II. It is one of several books Mindell has written that deal with interdisciplinary responses to complex problems, particularly in extreme environments like lunar landings and the deep sea. The interdisciplinary perspective Mindell forged at MIT has helped him identify the limitations of technology that prevent machines and humans from working together seamlessly. One particular shortcoming that Mindell has thought about for years is the lack of precise location data in places like warehouses, subway systems, and shipping ports. “In five years, we’ll look back at 2019 and say, ‘I can’t believe we didn’t know where anything was,’” Mindell says. “We’ve got so much data floating around, but the link between the actual physical world we all inhabit and move around in and the digital world that’s exploding is really still very poor.” In 2014, Mindell partnered with Humatics co-founder Gary Cohen, who has worked as an intellectual property strategist for biotech companies in the Kendall Square area, to solve the problem. In the beginning of 2015, Mindell collaborated with Lincoln Laboratory alumnus and radar expert Greg Charvat; the two built a prototype navigation system and started the company two weeks later. Charvat became Humatics’ CTO and first employee. “It was clear there was about to be this huge flowering of robotics and autonomous systems and AI, and I thought the things we learned in extreme environments, notably under sea and in aviation, had an enormous amount of application to industrial environments,” Mindell says. “The company is about bringing insights from years of experience with remote and autonomous systems in extreme environments into transit, logistics, e-commerce, and manufacturing.” Bringing microlocation to industry Factories, ports, and other locations where GPS data is unworkable or insufficient adopt a variety of solutions to meet their tracking and navigation needs. But each workaround has its drawbacks. RFID and Bluetooth technologies, for instance, can track assets but have short ranges and are expensive to deploy across large areas. Cameras and sensing methods like LIDAR can be used to help machines see their environment, but they struggle with things like rain and different lighting conditions. Floor tape embedded with wires or magnets is also often used to guide machines through fixed routes, but it isn’t well-suited for today’s increasingly dynamic warehouses and production lines. Humatics has focused on making the capabilities of its microlocation location system as easy to leverage as possible. The location and tracking data it collects can be integrated into whatever warehouse management system or “internet of things” (IoT) platforms customers are already using. Its radio frequency beacons have a range of up to 500 meters and, when installed as part of a constellation, can pinpoint three dimensional locations to within 2 centimeters, creating a virtual grid of the surrounding environment. The beacons can be combined with an onboard navigation hub that helps mobile robots move around dynamic environments. Humatics’ system also gathers location data from multiple points at once, monitoring the speed of a forklift, helping a crane operator place a shipping crate, and guiding a robot around obstacles simultaneously. The data Humatics collects don’t just help customers improve their processes; they can also transform the way workers and machines share space and work together. Indeed, with a new chip just emerging from its labs, Mindell says Humatics is moving industries such as manufacturing and logistics into “the world of ubiquitous, millimeter-accurate positioning.” It’s all possible because of the company’s holistic approach to the age-old problem of human-machine interaction. “Humatics is an example of what can happen when we think about technology in a unique, broader context,” Mindell says. “It’s an example of what MIT can accomplish when it pays serious attention to these two ways [from humanities and engineering] of looking at the world.” ",Computer Science,0.17611898961172717
19,MIT News,An immersive experience in industry ,Robotics,2019-09-19,-,http://news.mit.edu/2019/immersive-engineering-experience-industry-0919,"  This summer, four mechanical engineering graduate students had the opportunity to gain hands-on experience working in industry. Through the recently launched Industry Immersion Project Program (I2P), students were paired with a company and tasked with tackling a short-term project. Projects in this inaugural year for the program came from a diverse range of industries, including manufacturing, robotics, and aerospace engineering. A flagship program of the MechE Alliance, the I2P Program matches students with a company and project that best fits within their own academic experience at MIT. Projects are designed to be short term, lasting three to six months. Building upon programs such as the Master of Engineering in Advanced Manufacturing and Design and Leaders for Global Operations, which foster collaborations between students and the manufacturing industry, the I2P Program offers graduate students real-world experiences across industries. “For some students, this could be their first experience working in industry before graduating,” says Brian W. Anthony, program faculty director of the I2P Program. “Having that industry experience arms them with knowledge to help make career choices, may inform their further research, and provides skills they will utilize throughout their careers — whether they end up working in academia or industry.” Throughout the course of the projects, students are supported by both a supervisor at the company they’re working for and an academic supervisor from MIT’s mechanical engineering faculty. They also produce a report of their experience and receive academic credit for their industry projects and are enrolled in the class 2.992 (Professional Industry Immersion Project). “It’s been great hearing just how rich the experience has been from the students who participated this summer,” adds Theresa Werth, program manager for the MechE Alliance. “Not only have they spent the summer working on a project that’s relevant to their own research or thesis, they have honed some of the softer skills of professional development.” The four students participating in this year’s I2P Program have shared highlights and takeaways from their experiences: Sara Nagelberg — 3M A PhD candidate working with Associate Professor Mathias Kolle in the Bio-Inspired Photonic Engineering research group, Sara Nagelberg studies optical engineering. Through the I2P Program, this summer she worked at 3M on a project that seeks to automate surface finish analysis in manufacturing by understanding visual perception. While much of manufacturing involves automation, automating quality inspection for the surface finish on appliances or cars offers some technical challenges. The project Nagelberg worked on at 3M hopes to define what makes a surface ""good,"" then develop algorithms so that a computer can determine whether a surface finish is good quality or flawed. “The long-term goal of the project is to automate surface-quality inspection,” Nagelberg explains. She and her team identified parameters that could be used to judge the visual appearance of surfaces — things like color, glossiness, shape, and texture. “By working on this project, I learned about a variety of instruments and metrics that can be used to quantify visual surface finish parameters,” she adds. In addition to gaining experience on an interdisciplinary team at 3M, Nagelberg learned about computer vision, machine learning, and how to relate human perception to measurable parameters. Katie Hahm — Amazon Robotics This summer was one of transition for Katie Hahm. Having graduated with her master’s degree in June, Hahm is now a PhD candidate working in the Device Realization Lab with program director Anthony. As a master’s student, Hahm previously worked with Professor Harry Asada on designing robotic limbs to help manufacturing workers maintain positions for extended periods of time. Through the I2P Program, Hahm worked on a project at Amazon Robotics to improve efficiencies in the robotic process. “Working on this project was a great academic experience,” says Hahm. “I gained insights into the many facets and complexities of robotics.” Hahm also received a ground truth in what it’s like to work at a company like Amazon. She visited a local fulfillment center to gain a deeper understanding of their operations and visited Seattle to attend a company conference. At the conference, she and her fellow interns met with company leadership and teams from other Amazon sectors. One of the biggest takeaways from her experience at Amazon, according to Hahm, was how to approach research projects moving forward. “I learned not only valuable information from working with other professionals, but also the skills and approaches to asking more effective questions for research-oriented work,” she adds. Sai Nithin Reddy Kantareddy — Amazon Robotics A junior PhD candidate, much of Sai Nithin Reddy Kantareddy’s work involves using radio frequency identification (RFID) tags to sense activity and gather data about the surrounding environment. These RFID tags can then be used to connect objects to the internet of things. “Going into this summer, I knew I wanted to work on something related to sensors because of my research interest in environmental sensing,” explains Kantareddy. Through the I2P Program, Kantareddy was assigned to a project about material identification and sensing in robotics at Amazon Robotics. “Material identification for robotic applications really aligns with my own research interests,” he adds. While at Amazon Robotics, he gained hands-on experience working with sensors, cameras, and robots. He also built machine learning models on experimental data. While his background isn’t in robotics research, Kantareddy quickly learned about how robots are designed and what some of the challenges are in field implementation and warehouse automation. In addition to this in-depth technical knowledge, he also gained firsthand experience working in a team setting. “I enjoyed being part of a very resourceful and talented R&D team,” he recalls.  “I hope to take back these real-world insights and technical learnings and put them to practice in my PhD work.” Abhishek Patkar — Systems Technology Inc. A sophomore master’s student, Abhishek Patkar works in the flight controls group the Active Adaptive Control Laboratory, led by in Senior Research Scientist Anuradha Annaswamy. Working at Systems Technology Inc. (STI) was a natural fit. Much of STI’s work focuses on aerospace engineering. For his internship, Patkar was matched with Aditya Kotikalpudi, a senior research engineer at STI and the principal investigator for NASA’s project entitled Performance Adaptive Aeroelastic Wing. “I primarily worked on system identification and model parameter update for an aeroelastic vehicle,” says Patkar. While his internship was based in Los Angeles, California, Patkar had the opportunity to visit the University of Minnesota and witness the actual process of flight testing. He worked with the real data taken from these flight tests. Patkar also used STI software to identify aeroelastic mode shapes and obtain transfer function estimates from control surfaces to measured quantities like center body pitch rate.  “Through this internship, I was able to learn a lot about aircraft dynamics, aeroelasticity, and the process of performing system identification on an aircraft,” Patkar adds. He expects to use this knowledge back in the flight controls group in the Active Adaptive Control Laboratory. ",Computer Science,0.1715768985008331
20,ACM,Gymnastics' Latest Twist? AI Judges That See Everything,ACM,2019-10-10,-,https://www.nytimes.com/2019/10/10/sports/olympics/gymnastics-robot-judges.html,"       Gymnastics' Latest Twist? AI Judges That See Everything     The New York TimesAndrew KehOctober 10, 2019   The gymnastics world championships in Germany, the biggest gymnastics meet outside the Olympics, for the first time used an artificial intelligence (AI) system to evaluate athletes' performance by measuring and analyzing skeletal positions, speed, and angles via three-dimensional laser sensors. International Gymnastics Federation president Morinari Watanabe envisions such robot judges eliminating human error and subjectivity from gymnastics contests; “this is a step toward the challenge of justice through technology,” Watanabe said. At the world championships, the AI system was a means for human judges to confirm scores when gymnasts either formally contested their score, or the score widely deviated between judges. International Gymnastics Federation sports director Steve Butcher said all athlete information collected at the competition would be discarded at a predetermined expiration date, to address privacy concerns.               *May Require Paid Registration  ",Computer Science,0.16959373434002814
21,MIT News,Quantum sensing on a chip,Electronics and Technology,2019-09-25,-,http://news.mit.edu/2019/quantum-sensing-chip-0925,"  MIT researchers have, for the first time, fabricated a diamond-based quantum sensor on a silicon chip. The advance could pave the way toward low-cost, scalable hardware for quantum computing, sensing, and communication. “Nitrogen-vacancy (NV) centers” in diamonds are defects with electrons that can be manipulated by light and microwaves. In response, they emit colored photons that carry quantum information about surrounding magnetic and electric fields, which can be used for biosensing, neuroimaging, object detection, and other sensing applications. But traditional NV-based quantum sensors are about the size of a kitchen table, with expensive, discrete components that limit practicality and scalability. In a paper published in Nature Electronics, the researchers found a way to integrate all those bulky components — including a microwave generator, optical filter, and photodetector — onto a millimeter-scale package, using traditional semiconductor fabrication techniques. Notably, the sensor operates at room temperature with capabilities for sensing the direction and magnitude of magnetic fields. The researchers demonstrated the sensor’s use for magnetometry, meaning they were able to measure atomic-scale shifts in the frequency due to surrounding magnetic fields, which could contain information about the environment. With further refining, the sensor could have a range of applications, from mapping electrical impulses in the brain to detecting objects, even without a line of sight. “It’s very difficult to block magnetic fields, so that’s a huge advantage for quantum sensors,” says co-author Christopher Foy, a graduate student in the Department of Electrical Engineering and Computer Science (EECS). “If there’s a vehicle traveling in, say, an underground tunnel below you, you’d be able to detect it even if you don’t see it there.” Joining Foy on the paper are: Mohamed Ibrahim, a graduate student in EECS; Donggyu Kim PhD ’19; Matthew E. Trusheim, a postdoc in EECS; Ruonan Han, an associate professor in EECS and head of the Terahertz Integrated Electronics Group, which is part of MIT's Microsystems Technology Laboratories (MTL); and Dirk Englund, an MIT associate professor of electrical engineering and computer science, a researcher in Research Laboratory of Electronics (RLE), and head of the Quantum Photonics Laboratory. Shrinking and stacking NV centers in diamonds occur where carbon atoms in two adjacent places in the lattice structure are missing — one atom is replaced by a nitrogen atom, and the other space is an empty “vacancy.” That leaves missing bonds in the structure, where the electrons are extremely sensitive to tiny variations in electrical, magnetic, and optical characteristics in the surrounding environment. The NV center essentially functions as an atom, with a nucleus and surrounding electrons. It also has photoluminescent properties, meaning it absorbs and emits colored photons. Sweeping microwaves across the center can make it change states — positive, neutral, and negative — which in turn changes the spin of its electrons. Then, it emits different amounts of red photons, depending on the spin. A technique, called optically detected magnetic resonance (ODMR), measures how many photons are emitted by interacting with the surrounding magnetic field. That interaction produces further, quantifiable information about the field. For all of that to work, traditional sensors require bulky components, including a mounted laser, power supply, microwave generator, conductors to route the light and microwaves, an optical filter and sensor, and a readout component. The researchers instead developed a novel chip architecture that positions and stacks tiny, inexpensive components in a certain way using standard complementary metal-oxide-semiconductor (CMOS) technology, so they function like those components. “CMOS technologies enable very complex 3-D structures on a chip,” Ibrahim says. “We can have a complete system on the chip, and we only need  a piece of diamond and green light source on top. But that can be a regular chip-scale LED.” NV centers within a diamond slab are positioned in a “sensing area” of the chip. A small green pump laser excites the NV centers, while a nanowire placed close to the NV centers generates sweeping microwaves in response to current. Basically, the light and microwave work together to make the NV centers emit a different amount of red photons — with the difference being the target signal for readout in the researchers’ experiments. Below the NV centers is a photodiode, designed to eliminate noise and measure the photons. In between the diamond and photodiode is a metal grating that acts as a filter that absorbs the green laser photons while allowing the red photons to reach the photodiode. In short, this enables an on-chip ODMR device, which measures resonance frequency shifts with the red photons that carry information about the surrounding magnetic field. But how can one chip do the work of a large machine? A key trick is simply moving the conducting wire, which produces the microwaves, at an optimal distance from the NV centers. Even if the chip is very small, this precise distance enables the wire current to generate enough magnetic field to manipulate the electrons. The tight integration and codesign of the microwave conducting wires and generation circuitry also help. In their paper, the researchers were able to generate enough magnetic field to enable practical applications in object detection. Only the beginning In another paper presented earlier this year at the International Solid-State Circuits Conference, the researchers describe a second-generation sensor that makes various improvements on this design to achieve 100-fold greater sensitivity. Next, the researchers say they have a “roadmap” for how to increase sensitivity by 1,000 times. That basically involves scaling up the chip to increase the density of the NV centers, which determines sensitivity. If they do, the sensor could be used even in neuroimaging applications. That means putting the sensor near neurons, where it can detect the intensity and direction of firing neurons. That could help researchers map connections between neurons and see which neurons trigger each other. Other future applications including a GPS replacement for vehicles and airplanes. Because the magnetic field on Earth has been mapped so well, quantum sensors can serve as extremely precise compasses, even in GPS-denied environments. “We’re only at the beginning of what we can accomplish,” Han says. “It’s a long journey, but we already have two milestones on the track, with the first-and second-generation sensors. We plan to go from sensing to communication to computing. We know the way forward and we know how to get there.” “I am enthusiastic about this quantum sensor technology and foresee major impact in several fields,” says Ron Walsworth, a senior lecturer at Harvard University whose group develops high-resolution magnetometry tools using NV centers. “They have taken a key step in the integration of quantum-diamond sensors with CMOS technology, including on-chip microwave generation and delivery, as well as on-chip filtering and detection of the information-carrying fluorescent light from the quantum defects in diamond. The resulting unit is compact and relatively low-power. Next steps will be to further enhance the sensitivity and bandwidth of the quantum diamond sensor [and] integrate the CMOS-diamond sensor with wide-ranging applications, including chemical analysis, NMR spectroscopy, and materials characterization.” ",Electronics and Technology,0.16465342071751343
22,MIT News,Uncovering the hidden “noise” that can kill qubits,Electronics and Technology,2019-09-16,-,http://news.mit.edu/2019/non-gaussian-noise-detect-qubits-0916,"  MIT and Dartmouth College researchers have demonstrated, for the first time, a tool that detects new characteristics of environmental “noise” that can destroy the fragile quantum state of qubits, the fundamental components of quantum computers. The advance may provide insights into microscopic noise mechanisms to help engineer new ways of protecting qubits.   Qubits can represent the two states corresponding to the classic binary bits, a 0 or 1. But, they can also maintain a “quantum superposition” of both states simultaneously, enabling quantum computers to solve complex problems that are practically impossible for classical computers. But a qubit’s quantum “coherence” — meaning  its ability to maintain the superposition state — can fall apart due to noise coming from environment around the qubit. Noise can arise from control electronics, heat, or impurities in the qubit material itself, and can also cause serious computing errors that may be difficult to correct. Researchers have developed statistics-based models to estimate the impact of unwanted noise sources surrounding qubits to create new ways to protect them, and to gain insights into the noise mechanisms themselves. But, those tools generally capture simplistic “Gaussian noise,” essentially the collection of random disruptions from a large number of sources. In short, it’s like white noise coming from the murmuring of a large crowd, where there’s no specific disruptive pattern that stands out, so the qubit isn’t particularly affected by any one particular source. In this type of model, the probability distribution of the noise would form a standard symmetrical bell curve, regardless of the statistical significance of individual contributors. In a paper published today in the journal Nature Communications, the researchers describe a new tool that, for the first time, measures “non-Gaussian noise” affecting a qubit. This noise features distinctive patterns that generally stem from a few particularly strong noise sources. The researchers designed techniques to separate that noise from the background Gaussian noise, and then used signal-processing techniques to reconstruct highly detailed information about those noise signals. Those reconstructions can help researchers build more realistic noise models, which may enable more robust methods to protect qubits from specific noise types. There is now a need for such tools, the researchers say: Qubits are being fabricated with fewer and fewer defects, which could increase the presence of non-Gaussian noise. “It’s like being in a crowded room. If everyone speaks with the same volume, there is a lot of background noise, but I can still maintain my own conversation. However, if a few people are talking particularly loudly, I can’t help but lock on to their conversation. It can be very distracting,” says William Oliver, an associate professor of electrical engineering and computer science, professor of the practice of physics, MIT Lincoln Laboratory Fellow, and associate director of the Research Laboratory for Electronics (RLE). “For qubits with many defects, there is noise that decoheres, but we generally know how to handle that type of aggregate, usually Gaussian noise. However, as qubits improve and there are fewer defects, the individuals start to stand out, and the noise may no longer be simply of a Gaussian nature. We can find ways to handle that, too, but we first need to know the specific type of non-Gaussian noise and its statistics.” “It is not common for theoretical physicists to be able to conceive of an idea and also find an experimental platform and experimental colleagues willing to invest in seeing it through,” says co-author Lorenza Viola, a professor of physics at Dartmouth. “It was great to be able to come to such an important result with the MIT team.” Joining Oliver and Viola on the paper are: first author Youngkyu Sung, Fei Yan, Jack Y. Qiu, Uwe von Lüpke, Terry P. Orlando, and Simon Gustavsson, all of RLE; David K. Kim and Jonilyn L. Yoder of the Lincoln Laboratory; and Félix Beaudoin and Leigh M. Norris of Dartmouth. Pulse filters For their work, the researchers leveraged the fact that superconducting qubits are good sensors for detecting their own noise. Specifically, they use a “flux” qubit, which consists of a superconducting loop that is capable of detecting a particular type of disruptive noise, called magnetic flux, from its surrounding environment. In the experiments, they induced non-Gaussian “dephasing” noise by injecting engineered flux noise that disturbs the qubit and makes it lose coherence, which in turn is then used as a measuring tool. “Usually, we want to avoid decoherence, but in this case, how the qubit decoheres tells us something about the noise in its environment,” Oliver says. Specifically, they shot 110 “pi-pulses” — which are used to flip the states of qubits — in specific sequences over tens of microseconds. Each pulse sequence effectively created a narrow frequency “filter” which masks out much of the noise, except in a particular band of frequency. By measuring the response of a qubit sensor to the bandpass-filtered noise, they extracted the noise power in that frequency band. By modifying the pulse sequences, they could move filters up and down to sample the noise at different frequencies. Notably, in doing so, they tracked how the non-Gaussian noise distinctly causes the qubit to decohere, which provided a high-dimensional spectrum of the non-Gaussian noise. Error suppression and correction The key innovation behind the work is carefully engineering the pulses to act as specific filters that extract properties of the “bispectrum,” a two-dimension representation that gives information about distinctive time correlations of non-Gaussian noise. Essentially, by reconstructing the bispectrum, they could find properties of non-Gaussian noise signals impinging on the qubit over time — ones that don’t exist in Gaussian noise signals. The general idea is that, for Gaussian noise, there will be only correlation between two points in time, which is referred to as a “second-order time correlation.” But, for non-Gaussian noise, the properties at one point in time will directly correlate to properties at multiple future points. Such “higher-order” correlations are the hallmark of non-Gaussian noise. In this work, the authors were able to extract noise with correlations between three points in time. This information can help programmers validate and tailor dynamical error suppression and error-correcting codes for qubits, which fixes noise-induced errors and ensures accurate computation. Such protocols use information from the noise model to make implementations that are more efficient for practical quantum computers. But, because the details of noise aren’t yet well-understood, today’s error-correcting codes are designed with that standard bell curve in mind. With the researchers’ tool, programmers can either gauge how their code will work effectively in realistic scenarios or start to zero in on non-Gaussian noise. Keeping with the crowded-room analogy, Oliver says: “If you know there’s only one loud person in the room, then you’ll design a code that effectively muffles that one person, rather than trying to address every possible scenario.” ",Computer Science,0.16317090156017205
23,MIT News,Engineers develop multimaterial fiber “ink” for 3-D-printed devices,Electronics,2019-09-11,-,http://news.mit.edu/2019/fiber-3-d-print-electronics-0912,"  A new method developed by MIT researchers uses standard 3-D printers to produce functioning devices with the electronics already embedded inside. The devices are made of fibers containing multiple interconnected materials, which can light up, sense their surroundings, store energy, or perform other actions. The new 3-D printing method is described in the journal Nature Communication, in a paper by MIT doctoral student Gabriel Loke, professors John Joannopoulos and Yoel Fink, and four others at MIT and elsewhere. The system makes use of conventional 3-D printers outfitted with a special nozzle and a new kind of filament to replace the usual single-material polymer filament, which typically gets fully melted before it’s extruded from the printer’s nozzle. The researchers’ new filament has a complex internal structure made up of different materials arranged in a precise configuration, and is surrounded by polymer cladding on the outside. In the new printer, the nozzle operates at a lower temperature and pulls the filament through faster conventional printers do, so that only its outer layer gets partially molten. The interior stays cool and solid, with its embedded electronic functions unaffected. In this way, the surface is melted just enough to make it adhere solidly to adjacent filaments during the printing process, to produce a sturdy 3-D structure. The internal components in the filament include metal wires that serve as conductors, semiconductors that can be used to control active functions, and polymer insulators to prevent wires from contacting each other. As a demonstration, the team printed a wing for a model airplane, using filaments that contained both light-emitting and light-detecting electronics. These components could potentially reveal the formation of any microscopic cracks that might develop. While the filaments used in the model wing contained eight different materials, Loke says that in principle they could contain even more. Until this work, he says, “a printer capable of depositing metals, semiconductors, and polymers in a single platform still did not exist, because printing each of these materials requires different hardware and techniques.” This method is up to three times faster than any other current approach to fabricating 3-D devices, Loke says, and as with all 3-D printers, offers much more flexibility regarding the kinds of forms that can be produced than typical manufacturing methods do. “Unique to 3-D printing, this approach is able to construct devices of any freeform shapes, which are not achievable by any other methods thus far,” he says. The method makes use of thermally drawn fibers that contain a variety of different materials embedded within them, a process that Fink and his collaborators have been perfecting for two decades. They have created an array of fibers that have electronic components within them, making the fibers able to carry out a variety of functions. For example, for communications applications, flashing lights can transmit data that is then picked up by other fibers containing light sensors. This approach has for the first time produced fibers, and fabrics woven from them, that have these functions built in. Now, this new process makes this whole family of fibers available as the raw material for producing functional 3-D devices that can sense, communicate, or store energy, among other actions. To make the fibers themselves, the different materials are initially assembled into a larger-scale version called a preform, which is then heated and drawn in a furnace to produce a very narrow fiber that contains all those materials, in their same exact relative positions but greatly reduced in size. The method could potentially be developed further to produce a variety of different kinds of devices, especially for applications where the ability to precisely customize each device is essential. One such area is for biomedical devices, where matching the device to the patient’s own body can be important, says Fink, who is a professor of materials science as well as of electrical engineering and computer science and the CEO of the nonprofit Advanced Functional Fabrics of America, and associate director of the Research Laboratory of Electronics. For example, prosthetic limbs might someday be printed using this method, not only matching the precise dimensions and contours of the patient’s limb, but with all the electronics to monitor and control the limb embedded in place. Over the years, the group has developed a wide array of fibers containing different materials and functionalities. Loke says virtually all of these can be adapted for the new 3-D-printing technique, making it possible to print objects with a wide variety of different combinations of materials and functions. The device makes use of a standard type of 3-D printer known as a fused deposition modeling (FDM) printer, which is already found in many labs, offices, and even homes. One application that may be possible in the future would be to print materials for biomedical implants that would provide a scaffolding for the growth of new cells to replace a damaged organ, and include within it sensors to monitor the progress of that growth. The new method could also be useful for prototyping of devices — already a major application for 3-D printing, but in this case the prototypes would have actual functionality, rather than being static models. The research team included MIT graduate student Rodger Yuan; former MIT graduate student Michael Rein, who now works at AFFOA; postdoc Tural Khudiyev, and undergraduate student Yash Jain at Stony Brook University in New York. The work was partly supported by the National Science Foundation, the U.S. Army Research Laboratory and the U.S. Army Research Office through the Institute for Soldier Nanotechnologies. ",Electronics and Technology,0.16090718758989042
24,MIT News,A new way to corrosion-proof thin atomic sheets,Electronics and Technology,2019-10-04,-,http://news.mit.edu/2019/corrosion-proof-atomic-sheets-1004,"  A variety of two-dimensional materials that have promising properties for optical, electronic, or optoelectronic applications have been held back by the fact that they quickly degrade when exposed to oxygen and water vapor. The protective coatings developed thus far have proven to be expensive and toxic, and cannot be taken off. Now, a team of researchers at MIT and elsewhere has developed an ultrathin coating that is inexpensive, simple to apply, and can be removed by applying certain acids. The new coating could open up a wide variety of potential applications for these “fascinating” 2D materials, the researchers say. Their findings are reported this week in the journal PNAS, in a paper by MIT graduate student Cong Su; professors Ju Li, Jing Kong, Mircea Dinca, and Juejun Hu; and 13 others at MIT and in Australia, China, Denmark, Japan, and the U.K. Research on 2D materials, which form thin sheets just one or a few atoms thick, is “a very active field,” Li says. Because of their unusual electronic and optical properties, these materials have promising applications, such as highly sensitive light detectors. But many of them, including black phosphorus and a whole category of materials known as transition metal dichalcogenides (TMDs), corrode when exposed to humid air or to various chemicals. Many of them degrade significantly in just hours, precluding their usefulness for real-world applications. “It’s a key issue” for the development of such materials, Li says. “If you cannot stabilize them in air, their processability and usefulness is limited.” One reason silicon has become such a ubiquitous material for electronic devices, he says, is because it naturally forms a protective layer of silicon dioxide on its surface when exposed to air, preventing further degradation of the surface. But that’s more difficult with these atomically thin materials, whose total thickness could be even less than the silicon dioxide protective layer. There have been attempts to coat various 2D materials with a protective barrier, but so far they have had serious limitations. Most coatings are much thicker than the 2D materials themselves. Most are also very brittle, easily forming cracks that let through the corroding liquid or vapor, and many are also quite toxic, creating problems with handling and disposal. The new coating, based on a family of compounds known as linear alkylamines, improves on these drawbacks, the researchers say. The material can be applied in ultrathin layers, as little as 1 nanometer (a billionth of a meter) thick, and further heating of the material after application heals tiny cracks to form a contiguous barrier. The coating is not only impervious to a variety of liquids and solvents but also significantly blocks the penetration of oxygen. And, it can be removed later if needed by certain organic acids. “This is a unique approach” to protecting thin atomic sheets, Li says, that produces an extra layer just a single molecule thick, known as a monolayer, that provides remarkably durable protection. “This gives the material a factor of 100 longer lifetime,” he says, extending the processability and usability of some of these materials from a few hours up to months. And the coating compound is “very cheap and easy to apply,” he adds. In addition to theoretical modeling of the molecular behavior of these coatings, the team made a working photodetector from flakes of TMD material protected with the new coating, as a proof of concept. The coating material is hydrophobic, meaning that it strongly repels water, which otherwise would diffuse into the coating and dissolve away a naturally formed protective oxide layer within the coating, leading to rapid corrosion. The application of the coating is a very simple process, Su explains. The 2D material is simply placed into bath of liquid hexylamine, a form of the linear alkylamine, which builds up the protective coating after about 20 minutes, at a temperature of 130 degrees Celsius at normal pressure. Then, to produce a smooth, crack-free surface, the material is immersed for another 20 minutes in vapor of the same hexylamine. “You just put the wafer into this liquid chemical and let it be heated,” Su says. “Basically, that’s it.” The coating “is pretty stable, but it can be removed by certain very specific organic acids.” The use of such coatings could open up new areas of research on promising 2D materials, including the TMDs and black phosphorous, but potentially also silicene, stanine, and other related materials. Since black phosphorous is the most vulnerable and easily degraded of all these materials, that’s what the team used for their initial proof of concept. The new coating could provide a way of overcoming “the first hurdle to using these fascinating 2D materials,” Su says. “Practically speaking, you need to deal with the degradation during processing before you can use these for any applications,” and that step has now been accomplished, he says. The team included researchers in MIT’s departments of Nuclear Science and Engineering, Chemistry, Materials Science and Engineering, Electrical Engineering and Computer Science, and the Research Laboratory of Electronics, as well as others at the Australian National University, the University of Chinese Academy of Sciences, Aarhus University in Denmark, Oxford University, and Shinshu University in Japan. The work was supported by the Center for Excitonics and the Energy Frontier Research Center funded by the U.S. Department of Energy, and by the National Science Foundation, the Chinese Academy of Sciences, the Royal Society, the U.S. Army Research Office through the MIT Institute for Soldier Nanotechnologies, and Tohoku University. ",Electronics and Technology,0.1599856106601784
25,Science Daily,Controlling the Charge State of Organic Molecule Quantum Dots in a 2D Nanoarray,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103353.htm,"   Molecular self-assembly on a metal results in a high-density, 2D, organic quantum-dot array with electric-field-controllable charge state, with the organic molecules used as 'nano-sized building blocks' in fabrication of functional nanomaterials. Achieved densities are an order of magnitude larger than conventional inorganic systems. The atomically-thin nanofilm consists of an ordered two-dimensional (2D) array of molecules which behave as 'zero dimensional' entities called quantum dots (QDs). This system has exciting implications for fields such as computer memory, light-emitting devices and quantum computing. The School of Physics and Astronomy study shows that a single-component, self-assembled 2D array of the organic (carbon-based) molecule dicyanoanthracene can be synthesised on a metal, such that the charge state of each molecule can be controlled individually via an applied electric field.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""This discovery would enable the fabrication of 2D arrays of individually addressable (switchable) quantum dots from the bottom-up, via self-assembly, says lead author Dhaneesh Kumar. ""We would be able to achieve densities tens of times larger than state-of-the-art, top-down synthesised inorganic systems."" QUANTUM DOTS: TINY, 'ZERO-DIMENSIONAL' POWERHOUSES Quantum dots are extremely small -- about one nanometre across (ie, a millionth of a millimetre). Because their size is similar to the wavelength of electrons, their electronic properties are radically different to conventional materials.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     In quantum dots, the motion of electrons is constrained by this extremely small scale, resulting in discrete electronic quantum energy levels. Effectively, they behave as 'zero-dimensional' (0D) objects, where the degree of occupancy (filled or empty) of their quantised electronic states determines the charge (in this study, neutral or negative) of the quantum dot. Ordered arrays of charge-controllable quantum dots can find application in computing memory as well as light-emitting devices (eg, low-energy TV or smartphone screens). Arrays of quantum dots are conventionally synthesised from inorganic materials via top-down fabrication approaches. However, using such 'top-down' approaches, it can be challenging to achieve arrays with large densities and high homogeneity (in terms of quantum-dot size and spacing). Because of their tunability and self-assembling capability, using organic (carbon-based) molecules as nano-sized building blocks can be particularly useful for the fabrication of functional nanomaterials, in particular well-defined scalable ensembles of quantum dots. THE STUDY The researchers synthesised a homogeneous, single-component, self-assembled 2D array of the organic molecule dicyanoanthracene (DCA) on a metal surface. The study was led by Monash University's Faculty of Science, with support by theory from the Monash Faculty of Engineering. This atomic-scale structural and electronic properties of this nanoscale array were studied experimentally via low-temperature scanning tunnelling microscopy (STM) and atomic force microscopy (AFM) (School of Physics and Astronomy, under Dr Agustin Schiffrin). Theoretical studies using density functional theory supported the experimental findings (Department of Material Science and Engineering, under A/Prof Nikhil Medhekar). The researchers found that the charge of individual DCA molecules in the self-assembled 2D array can be controlled (switched from neutral to negative and vice versa) by an applied electric field. This charge state electric-field-control is enabled by an effective tunneling barrier between molecule and surface (resulting from limited metal-adsorbate interactions) and a significant DCA electron affinity. Subtle, site-dependent variations of the molecular adsorption geometry were found to give rise to significant variations in the susceptibility for electric-field-induced charging. ",Electronics and Technology,0.15985411257600043
26,MIT News,“Electroadhesive” stamp picks up and puts down microscopic structures,Electronics and Technology,2019-10-11,-,http://news.mit.edu/2019/electroadhesive-stamp-microscopic-manufacturing-1011,"  If you were to pry open your smartphone, you would see an array of electronic chips and components laid out across a circuit board, like a miniature city. Each component might contain even smaller “chiplets,” some no wider than a human hair. These elements are often assembled with robotic grippers designed to pick up the components and place them down in precise configurations. As circuit boards are packed with ever smaller components, however, robotic grippers’ ability to manipulate these objects is approaching a limit.   “Electronics manufacturing requires handling and assembling small components in a size similar to or smaller than grains of flour,” says Sanha Kim, a former MIT postdoc and research scientist who worked in the lab of mechanical engineering associate professor John Hart. “So a special pick-and-place solution is needed, rather than simply miniaturizing [existing] robotic grippers and vacuum systems.” Now Kim, Hart, and others have developed a miniature “electroadhesive” stamp that can pick up and place down objects as small as 20 nanometers wide — about 1,000 times finer than a human hair. The stamp is made from a sparse forest of ceramic-coated carbon nanotubes arranged like bristles on a tiny brush. When a small voltage is applied to the stamp, the carbon nanotubes become temporarily charged, forming prickles of electrical attraction that can attract a minute particle. By turning the voltage off, the stamp’s “stickiness” goes away, enabling it to release the object onto a desired location. Hart says the stamping technique can be scaled up to a manufacturing setting to print micro- and nanoscale features, for instance to pack more elements onto ever smaller computer chips. The technique may also be used to pattern other small, intricate features, such as cells for artificial tissues. And, the team envisions macroscale, bioinspired electroadhesive surfaces, such as voltage-activated pads for grasping everyday objects and for gecko-like climbing robots. “Simply by controlling voltage, you can switch the surface from basically having zero adhesion to pulling on something so strongly, on a per unit area basis, that it can act somewhat like a gecko’s foot,” Hart says. The team has published its results today in the journal Science Advances. Like dry Scotch tape Existing mechanical grippers are unable to pick up objects smaller than about 50 to 100 microns, mainly because at smaller scales surface forces tend to win over gravity. You may see this when pouring flour from a spoon — inevitably, some tiny particles stick to the spoon’s surface, rather than letting gravity drag them off. “The dominance of surface forces over gravity forces becomes a problem when trying to precisely place smaller things — which is the foundational process by which electronics are assembled into integrated systems,” Hart says. He and his colleagues noted that electroadhesion, the process of adhering materials via an applied voltage, has been used in some industrial settings to pick and place large objects, such as fabrics, textiles, and whole silicon wafers. But this same electroadhesion had never been applied to objects at the microscopic level, because a new material design for controlling electroadhesion at smaller scales was needed. Hart’s group has previously worked with carbon nanotubes (CNTs) — atoms of carbon linked in a lattice pattern and rolled into microscopic tubes. CNTs are known for their exceptional mechanical, electrical, and chemical properties, and they have been widely studied as dry adhesives. “Previous work on CNT-based dry adhesives focused on maximizing the contact area of the nanotubes to essentially create a dry Scotch tape,” Hart says. “We took the opposite approach, and said, ‘let’s design a nanotube surface to minimize the contact area, but use electrostatics to turn on adhesion when we need it.’”  New electroadhesive stamp picks and places a 170-micrometer sized LED chiplet, using an external voltage of 30V to temporarily “stick” to the LED. Courtesy of the researchers A sticky on/off switch The team found that if they coated CNTs with a thin dielectric material such as aluminum oxide, when they applied a voltage to the nanotubes, the ceramic layer became polarized, meaning its positive and negative charges became temporarily separated. For instance, the positive charges of the tips of the nanotubes induced an opposite polarization in any nearby conducting material, such as a microscopic electronic element. As a result, the nanotube-based stamp adhered to the element, picking it up like tiny, electrostatic fingers. When the researchers turned the voltage off, the nanotubes and the element depolarized, and the “stickiness” went away, allowing the stamp to detach and place the object onto a given surface. The team explored various formulations of stamp designs, altering the density of carbon nanotubes grown on the stamp, as well as the thickness of the ceramic layer that they used to coat each nanotube. They found that the thinner the ceramic layer and the more sparsely spaced the carbon nanotubes were, the greater the stamp’s on/off ratio, meaning the greater the stamp’s stickiness was when the voltage was on, versus when it was off. In their experiments, the team used the stamp to pick up and place down films of nanowires, each about 1,000 times thinner than a human hair. They also used the technique to pick and place intricate patterns of polymer and metal microparticles, as well as micro-LEDs. Hart says the electroadhesive printing technology could be scaled up to manufacture circuit boards and systems of miniature electronic chips, as well as displays with microscale LED pixels. “With ever-advancing capabilities of semiconductor devices, an important need and opportunity is to integrate smaller and more diverse components, such as microprocessors, sensors, and optical devices,” Hart says. “Often, these are necessarily made separately but must be integrated together to create next-generation electronic systems. Our technology possibly bridges the gap necessary for scalable, cost-effective assembly of these systems.” This research was supported in part by the Toyota Research Insititute, the National Science Foundation, and the MIT-Skoltech Next Generation Program. ",Electronics and Technology,0.15900702161163177
27,MIT News,"Professor Emeritus Woodie Flowers, innovator in design and engineering education, dies at 75",Robotics,2019-10-14,-,http://news.mit.edu/2019/Professor-Emeritus-Woodie-Flowers-dies-75-1014,"  Woodie Flowers SM ’68, MEng ’71, PhD ’73, the Pappalardo Professor Emeritus of Mechanical Engineering, passed away on Oct. 11 at the age of 75. Flowers’ passion for design and his infectious kindness have impacted countless engineering students across the world. Flowers was instrumental in shaping MIT’s hands-on approach to engineering design education, first developing teaching methods and learning opportunities that culminated in a design competition for class 2.70, now called 2.007 (Design and Manufacturing I). This annual MIT event, which has now been held for nearly five decades, has impacted generations of students and has been emulated at universities around the world. Flowers expanded this concept to high school and elementary school students, working to help found the world-wide FIRST Robotics Competition, which has introduced millions of children to science and engineering. Born in 1943, Flowers was reared in Jena, Louisiana. He became interested in mechanical engineering and design at a young age thanks in large part to his mother and his father, who was a welder with a penchant for tinkering and building. Growing up, Flowers also expressed a love of nature and traveling. When he wasn’t working on cars or building rockets as a teenager, he was camping with his family in Louisiana or collecting butterflies. This interest in nature led to an award-winning science fair project on the impact the environment has on Lepidoptera. Flowers’ passion for both building and nature also helped him earn the rank of Eagle Scout. Flowers received his bachelor’s degree in engineering from Louisiana Tech University in 1966. After graduating, he spent a summer as an engineering trainee for the Humble Oil Company before enrolling in MIT for graduate school. He received his master’s of science in mechanical engineering in 1968 and an engineer’s degree in 1971. Two years later he earned his doctoral degree under the supervision of the late Professor Robert Mann. For his thesis, Flowers designed a “man-interactive simulator system” for the development of prosthetics for above-knee amputees. He would continue to design above-knee prosthetics throughout his career. As Flower’s academic career progressed, his wife Margaret acted as a partner in everything he did. Early in their marriage, when Flowers was just starting out at MIT, she worked to support their family financially. Later in life, she left her own career to partner with Flowers on his work in FIRST. After earning his PhD, Flowers joined MIT’s faculty as assistant professor of mechanical engineering. Within his first year he was teaching what was then known as 2.70, now called 2.007. Under Flowers’ leadership, the class evolved into a hands-on experience for undergraduate students, culminating in a final robot competition. “From the beginning 2.007/2.70 was about building a device to accomplish a task,” Flowers explained in a 2015 video. Students were given an assortment of materials to design and build their devices. In the 1970s these materials included tongue depressors and rubber bands, but over the years the competition has gone on to include 3D-printed parts and computer chips. Despite the increased sophistication, according to Flowers the core of the course remained unchanged. “Some of the stuff that stayed the same is the wonderful way you compete like crazy but help each other out,” he said. Flowers would coin the phrase “gracious professionalism” to describe this idea of being kind and respecting and valuing others, even in the heat of competition. PBS highlighted Flowers’ innovative educational approach to class 2.007 in a 1981 documentary “Discover: The World of Science.” The network continued to cover the 2.007 robotics competition throughout the 1980s, and nearly a decade later, Flowers hosted the popular PBS series “Scientific American Frontiers,” from 1990-1993. One of the program’s objectives was to get people interested in science and engineering. He was awarded a regional Emmy Award for his work on the series. At the same time, Flowers helped develop a new program to inspire young people that built upon the competition he developed for 2.007. He collaborated with Dean Kamen, founder of FIRST (For Inspiration and Recognition of Science and Technology), to develop a robotics competition for high school students. In 1992, the inaugural FIRST Robotics Competition was held, giving high school students from around the world an opportunity to design and build their own robots. Over the past three decades, FIRST robotics has grown into a global movement serving 660,000 students from over 100 countries each year. It provides scholarship opportunities totaling over $80 million available to FIRST high school students. Flowers’ mantra of “gracious professionalism” remains at FIRST’s core. In 1996, William P. Murphy, Jr. founded the annual Woodie Flowers Award within FIRST to celebrate communication in engineering and design. The award “recognizes an individual who has done an outstanding job of motivation through communication while also challenging the students to be clear and succinct in recognizing the value of communication.” While working on FIRST, Flowers continued to have impact on mechanical engineering education, its future directions, and engineers’ professional role in society, in addition to envisioning how the use of digital resources could enhance residential learning. At MIT, he served as head for the systems and design division in the Department of Mechanical Engineering in the early 1990s and was named Pappalardo Professor of Mechanical Engineering in 1994.  While Flowers retired in 2007, he remained an active member of the MIT community as professor emeritus up until his death. Flowers mentored countless engineering students during his 35 years on the MIT faculty. He served as undergraduate and master’s thesis advisor for Megan Smith ’86, SM ’88, former chief technology officer of the United States, as well as doctoral advisor to David Wallace SM ’91, PhD ’95, Ely Sachs ’76, SM ’76, PhD ’83, and Alexander Slocum ’82, SM ’83, PhD ’85, all of whom are professors of mechanical engineering at MIT. Flowers has had a lasting impact on the generations of mechanical engineering students he taught. From encouraging students to embrace ambiguity to pulling out a massive dictionary in the middle of class to help students find a precise word to articulate their point, his role in shaping students’ lives went far beyond the tenants of design and engineering. Many of his students who have gone on to be educators themselves have implemented his educational ethos in their own classrooms and labs.   Throughout his career, Flowers received numerous awards and accolades for his vast contributions to engineering education. The American Society of Mechanical Engineers honored him with both the Ruth and Joel Spira Outstanding Design Educator Award and the Edwin F. Church Medal. Flowers received the J.P. Den Hartog Distinguished Educator Award, was a MacVicar Fellow, and was elected to the National Academy of Engineering. He also served as a distinguished partner and a member of the President's Council at Olin College of Engineering. Flowers is survived by his beloved wife Margaret Flowers of Weston, Massachusetts, his sister, Kay Wells of St. Augustine, Florida, his niece Catherine Calabria, also of St. Augustine, his nephew, David Morrison of Arlington, Virginia, as well as generations of grateful and adoring students. Memorial donations to FIRST and memories of Flowers may be delivered via this website or mailed to FIRST c/o Director Dia Stolnitz, 200 Bedford Street, Manchester, New Hampshire, 03101. This article will be updated with information about memorial services as it becomes available. ",Computer Science,0.15863115252616197
28,Science Daily,'Electroadhesive' Stamp Picks Up and Puts Down Microscopic Structures,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011144619.htm,"   As circuit boards are packed with ever smaller components, however, robotic grippers' ability to manipulate these objects is approaching a limit. ""Electronics manufacturing requires handling and assembling small components in a size similar to or smaller than grains of flour,"" says Sanha Kim, a former MIT postdoc and research scientist who worked in the lab of mechanical engineering associate professor John Hart. ""So a special pick-and-place solution is needed, rather than simply miniaturizing [existing] robotic grippers and vacuum systems."" Now Kim, Hart, and others have developed a miniature ""electroadhesive"" stamp that can pick up and place down objects as small as 20 nanometers wide -- about 1,000 times finer than a human hair. The stamp is made from a sparse forest of ceramic-coated carbon nanotubes arranged like bristles on a tiny brush. When a small voltage is applied to the stamp, the carbon nanotubes become temporarily charged, forming prickles of electrical attraction that can attract a minute particle. By turning the voltage off, the stamp's ""stickiness"" goes away, enabling it to release the object onto a desired location. Hart says the stamping technique can be scaled up to a manufacturing setting to print micro- and nanoscale features, for instance to pack more elements onto ever smaller computer chips. The technique may also be used to pattern other small, intricate features, such as cells for artificial tissues. And, the team envisions macroscale, bioinspired electroadhesive surfaces, such as voltage-activated pads for grasping everyday objects and for gecko-like climbing robots.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Simply by controlling voltage, you can switch the surface from basically having zero adhesion to pulling on something so strongly, on a per unit area basis, that it can act somewhat like a gecko's foot,"" Hart says. The team has published its results today in the journal Science Advances. Like dry Scotch tape Existing mechanical grippers are unable to pick up objects smaller than about 50 to 100 microns, mainly because at smaller scales surface forces tend to win over gravity. You may see this when pouring flour from a spoon -- inevitably, some tiny particles stick to the spoon's surface, rather than letting gravity drag them off. ""The dominance of surface forces over gravity forces becomes a problem when trying to precisely place smaller things -- which is the foundational process by which electronics are assembled into integrated systems,"" Hart says.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     He and his colleagues noted that electroadhesion, the process of adhering materials via an applied voltage, has been used in some industrial settings to pick and place large objects, such as fabrics, textiles, and whole silicon wafers. But this same electroadhesion had never been applied to objects at the microscopic level, because a new material design for controlling electroadhesion at smaller scales was needed. Hart's group has previously worked with carbon nanotubes (CNTs) -- atoms of carbon linked in a lattice pattern and rolled into microscopic tubes. CNTs are known for their exceptional mechanical, electrical, and chemical properties, and they have been widely studied as dry adhesives. ""Previous work on CNT-based dry adhesives focused on maximizing the contact area of the nanotubes to essentially create a dry Scotch tape,"" Hart says. ""We took the opposite approach, and said, 'let's design a nanotube surface to minimize the contact area, but use electrostatics to turn on adhesion when we need it.'"" A sticky on/off switch The team found that if they coated CNTs with a thin dielectric material such as aluminum oxide, when they applied a voltage to the nanotubes, the ceramic layer became polarized, meaning its positive and negative charges became temporarily separated. For instance, the positive charges of the tips of the nanotubes induced an opposite polarization in any nearby conducting material, such as a microscopic electronic element. As a result, the nanotube-based stamp adhered to the element, picking it up like tiny, electrostatic fingers. When the researchers turned the voltage off, the nanotubes and the element depolarized, and the ""stickiness"" went away, allowing the stamp to detach and place the object onto a given surface. The team explored various formulations of stamp designs, altering the density of carbon nanotubes grown on the stamp, as well as the thickness of the ceramic layer that they used to coat each nanotube. They found that the thinner the ceramic layer and the more sparsely spaced the carbon nanotubes were, the greater the stamp's on/off ratio, meaning the greater the stamp's stickiness was when the voltage was on, versus when it was off. In their experiments, the team used the stamp to pick up and place down films of nanowires, each about 1,000 times thinner than a human hair. They also used the technique to pick and place intricate patterns of polymer and metal microparticles, as well as micro-LEDs. Hart says the electroadhesive printing technology could be scaled up to manufacture circuit boards and systems of miniature electronic chips, as well as displays with microscale LED pixels. ""With ever-advancing capabilities of semiconductor devices, an important need and opportunity is to integrate smaller and more diverse components, such as microprocessors, sensors, and optical devices,"" Hart says. ""Often, these are necessarily made separately but must be integrated together to create next-generation electronic systems. Our technology possibly bridges the gap necessary for scalable, cost-effective assembly of these systems."" This research was supported in part by the Toyota Research Insititute, the National Science Foundation, and the MIT-Skoltech Next Generation Program. ",Electronics and Technology,0.15857375464774692
29,IEEE,Next-Gen AR Glasses Will Require New Chip Designs,Electronics and Technology,2019-10-09,-,https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/dramatic-changes-in-chip-design-will-be-necessary-to-make-ar-glasses-a-reality,"      What seems like a simple task—building a useful form of augmented reality into comfortable, reasonably stylish, eyeglasses—is going to need significant technology advances on many fronts, including displays, graphics, gesture tracking, and low-power processor design. That was the message of Sha Rabii, Facebook’s head of silicon and technology engineering. Rabii, speaking at Arm TechCon 2019 in San Jose, Calif., on Tuesday, described a future with AR glasses that enable wearers to see at night, improve overall eyesight, translate signs on the fly, prompt wearers with the names of people they meet, create shared whiteboards, encourage healthy food choices, and allow selective hearing in crowded rooms. This type of AR will be, he said, “an assistant, connected to the Internet, sitting on your shoulders, and feeding you useful information to your ears and eyes when you need it.” This vision, he indicated, isn’t arriving anytime soon, but it is achievable. The biggest roadblock, he said, is lowering the energy consumption of the hardware, along with reducing the heat that today’s processors emit. “The low-power design community is uniquely positioned to take the mantle and create the tools that let us realize this vision,” he said. Rabii had a couple of suggestions for approaches that chip developers could take. For one, he said, chips have to be better tailored to their intended uses. “Our use case,” he said, speaking about AR glasses, “is moderate performance but high-power efficiency, form factors that support stylish and lightweight designs, [and chip designs that are] mindful of temperature for user comfort.” Designers also need to think more realistically about how chips use energy. Energy consumption, he says, is mostly “determined by memory access and data movement. Data transfer is far more expensive than compute.” For example, he indicated, “fetching 1 byte from DRAM takes 12,000 times more energy than performing an 8-bit addition; sending 1 byte wirelessly takes 300,000 times more energy.” Hardware designers need to keep these differences in mind in the way they implement AI, Rabii says. “The prevalent model is to have a monolithic accelerator as a discrete compute element, with all AI workloads transferred to this element,” he said. “But this is a data transfer intensive architecture, which has implications for power consumption.” Better, he suggested, would be to “treat AI as a deeply embedded function and distribute it across all the compute” in a system. This type of architecture, he said, brings compute to data, so data doesn’t have to move around as much, dramatically saving power. There are other ways AI can be designed to use less energy, Rabii says. “Not every AI function needs the same precision,” he says. “A large percentage of the computational effort is required for the last percents of accuracy,” so breaking up workloads and reducing precision when possible can make AI systems far more efficient. That’s what designers can do now. In the future, he said, Facebook is looking forward to improvements in semiconductor process technologies that will lead to better performance per watt, as well as specialized accelerators that focus on specific types of AI for higher performance and better energy efficiency. Some of those advances, he hopes, will come from Arm Holdings and the Arm ecosystem.  IEEE Spectrum’s blog featuring the people, places, and passions of the world of technologists in Silicon Valley and its environs. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Computer Science,0.15778018995406995
30,MIT News,Controlling 2-D magnetism with stacking order,Electronics and Technology,2019-09-30,-,http://news.mit.edu/2019/controlling-2d-magnetism-stacking-order-0930,"  Researchers led by MIT Department of Physics Professor Pablo Jarillo-Herrero last year showed that rotating layers of hexagonally structured graphene at a particular “magic angle” could change the material’s electronic properties from an insulating state to a superconducting state. Now researchers in the same group and their collaborators have demonstrated that in a different ultra-thin material that also features a honeycomb-shaped atomic structure — chromium trichloride (CrCl3) — they can alter the material’s magnetic properties by shifting the stacking order of layers. The researchers peeled away two-dimensional (2-D) layers of chromium trichloride using tape in the same way researchers peel away graphene from graphite. Then they studied the 2-D chromium trichloride’s magnetic properties using electron tunneling. They found that the magnetism is different in 2-D and 3-D crystals due to different stacking arrangements between atoms in adjacent layers. At high temperatures, each chromium atom in chromium trichloride has a magnetic moment that fluctuates like a tiny compass needle. Experiments show that as the temperature drops below 14 kelvins (-434.47 degrees Fahrenheit), deep in the cryogenic temperature range, these magnetic moments freeze into an ordered pattern, pointing in opposite directions in alternating layers (antiferromagnetism). The magnetic direction of all the layers of chromium trichloride can be aligned by applying a magnetic field. But the researchers found that in its 2-D form, this alignment needs a magnetic force 10 times stronger than in the 3-D crystal. The results were recently published online in Nature Physics. “What we’re seeing is that it’s 10 times harder to align the layers in the thin limit compared to the bulk, which we measure using electron tunneling in a magnetic field,” says MIT physics graduate student Dahlia R. Klein, a National Science Foundation graduate research fellow and one of the paper’s lead authors. Physicists call the energy required to align the magnetic direction of opposing layers the interlayer exchange interaction. “Another way to think of it is that the interlayer exchange interaction is how much the adjacent layers want to be anti-aligned,” fellow lead author and MIT postdoc David MacNeill suggests. The researchers attribute this change in energy to the slightly different physical arrangement of the atoms in 2-D chromium chloride. “The chromium atoms form a honeycomb structure in each layer, so it’s basically stacking the honeycombs in different ways,” Klein says. “The big thing is we’re proving that the magnetic and stacking orders are very strongly linked in these materials.” ""Our work highlights how the magnetic properties of 2-D magnets can differ very substantially from their 3-D counterparts,” says senior author Pablo Jarillo-Herrero, the Cecil and Ida Green Professor of Physics. “This means that we have now a new generation of highly tunable magnetic materials, with important implications for both new fundamental physics experiments and potential applications in spintronics and quantum information technologies."" Layers are very weakly coupled in these materials, known as van der Waals magnets, which is what makes it easy to remove a layer from the 3-D crystal with adhesive tape. “Just like with graphene, the bonds within the layers are very strong, but there are only very weak interactions between adjacent layers, so you can isolate few-layer samples using tape,” Klein says. MacNeill and Klein grew the chromium chloride samples, built and tested nanoelectronic devices, and analyzed their results. The researchers also found that as chromium trichloride is cooled from room temperature to cryogenic temperatures, 3-D crystals of the material undergo a structural transition that the 2-D crystals do not. This structural difference accounts for the higher energy required to align the magnetism in the 2-D crystals. The researchers measured the stacking order of 2-D layers through the use of Raman spectroscopy and developed a mathematical model to explain the energy involved in changing the magnetic direction. Co-author and Harvard University postdoc Daniel T. Larson says he analyzed a plot of Raman data that showed variations in peak location with the rotation of the chromium trichloride sample, determining that the variation was caused by the stacking pattern of the layers. “Capitalizing on this connection, Dahlia and David have been able to use Raman spectroscopy to learn details about the crystal structure of their devices that would be very difficult to measure otherwise,” Larson explains. “I think this technique will be a very useful addition to the toolbox for studying ultra-thin structures and devices.” Department of Materials Science and Engineering graduate student Qian Song carried out the Raman spectroscopy experiments in the lab of MIT assistant professor of physics Riccardo Comin. Both also are co-authors of the paper. “This research really highlights the importance of stacking order on understanding how these van der Waals magnets behave in the thin limit,” Klein says. MacNeill adds, “The question of why the 2-D crystals have different magnetic properties had been puzzling us for a long time. We were very excited to finally understand why this is happening, and it’s because of the structural transition.” This work builds on two years of prior research into 2-D magnets in which Jarillo-Herrero’s group collaborated with researchers at the University of Washington, led by Professor Xiaodong Xu, who holds joint appointments in the departments of Materials Science and Engineering, Physics, and Electrical and Computer Engineering, and others. Their work, which was published in a Nature letter in June 2017, showed for the first time that a different material with a similar crystal structure — chromium triiodide (CrI3) — also behaved differently in the 2-D form than in the bulk, with few-layer samples showing antiferromagnetism unlike the ferromagnetic 3-D crystals. Jarillo-Herrero’s group went on to show in a May 2018 Science paper that chromium triiodide exhibited a sharp change in electrical resistance in response to an applied magnetic field at low temperature. This work demonstrated that electron tunneling is a useful probe for studying magnetism of 2-D crystals. Klein and MacNeill were also the first authors of this paper. University of Washington Professor Xiaodong Xu says of the latest findings, “The work presents a very clever approach, namely the combined tunneling measurements with polarization resolved Raman spectroscopy. The former is sensitive to the interlayer antiferromagnetism, while the latter is a sensitive probe of crystal symmetry. This approach gives a new method to allow others in the community to uncover the magnetic properties of layered magnets.” “This work is in concert with several other recently published works,” Xu says. “Together, these works uncover the unique opportunity provided by layered van der Waals magnets, namely engineering magnetic order via controlling stacking order. It is useful for arbitrary creation of new magnetic states, as well as for potential application in reconfigurable magnetic devices.” Other authors contributing to this work include Efthimious Kaxiras, the John Hasbrouck Van Vleck Professor of Pure and Applied Physics at Harvard University; Harvard graduate student Shiang Fang; Iowa State University Distinguished Professor (Condensed Matter Physics) Paul C. Canfield; Iowa State graduate student Mingyu Xu; and Raquel A. Ribeiro, of Iowa State University and the Federal University of ABC, Santo André, Brazil. This work was supported in part by the Center for Integrated Quantum Materials, the U.S. Department of Energy Office of Science Basic Energy Sciences Program, the Gordon and Betty Moore Foundation’s EPiQS Initiative, and the Alfred P. Sloan Foundation. ",Electronics and Technology,0.15725626381623065
31,Science Daily,The Future of 'Extremely' Energy-Efficient Circuits,Society,2019-09-18,-,https://www.sciencedaily.com/releases/2019/09/190918105637.htm,"   To answer this demand, a team of researchers from Japan and the United States have developed a framework to reduce energy consumption while improving efficiency. They published their results on July 19 in Scientific Reports, a Nature journal. ""The significant amount of energy consumption has become a critical problem in modern society,"" said Olivia Chen, corresponding author of the paper and assistant professor in the Institute of Advanced Sciences at Yokohama National University. ""There is an urgent requirement for extremely energy-efficient computing technologies."" The research team used a digital logic process called Adiabatic Quantum-Flux-Parametron (AQFP). The idea behind the logic is that direct current should be replaced with alternating current. The alternating current acts as both the clock signal and the power supply -- as the current switches directions, it signals the next time phase for computing. The logic, according to Chen, could improve conventional communication technologies with currently available fabrication processes.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""However, there lacks a systematic, automatic synthesis framework to translate from high-level logic description to Adiabatic Quantum-Flux-Parametron circuit netlist structures,"" Chen said, referring to the individual processors within the circuit. ""In this paper, we mitigate that gap by presenting an automatic flow. We also demonstrate that AQFP can achieve a reduction in energy use by several orders of magnitude compared to traditional technologies."" The researchers proposed a top-down framework for computing decisions that can also analyze its own performance. To do this, they used logic synthesis, a process by which they direct the passage of information through logic gates within the processing unit. Logic gates can take in a little bit of information and output a yes or no answer. The answer can trigger other gates to respond and move the process forward, or stop it completely. With this basis, the researchers developed a computation logic that takes the high-level understanding of processing and how much energy a system uses and dissipates and describes it as an optimized map for each gate within the circuit model. From this, Chen and the research team can balance the estimation of power needed to process through the system and the energy that the system dissipates. According to Chen, this approach also compensates for the cooling energy needed for superconducting technologies and reduces the energy dissipation by two orders of magnitude. ""These results demonstrate the potential of AQFP technology and applications for large-scale, high-performance and energy-efficient computations,"" Chen said. Ultimately, the researchers plan to develop a fully automated framework to generate the most efficient AQFP circuit layout. ""The synthesis results of AQFP circuits are highly promising in terms of energy-efficient and high-performance computing,"" Chen said. ""With the future advancing and maturity of AQFP fabrication technology, we anticipate broader applications ranging from space applications and large-scale computing facilities such as data centers."" ",Computer Science,0.15383373516894788
32,MIT News,Scientists observe a single quantum vibration under ordinary conditions,Research,2019-10-06,-,http://news.mit.edu/2019/single-quantum-vibration-normal-1007,"  When a guitar string is plucked, it vibrates as any vibrating object would, rising and falling like a wave, as the laws of classical physics predict. But under the laws of quantum mechanics, which describe the way physics works at the atomic scale, vibrations should behave not only as waves, but also as particles. The same guitar string, when observed at a quantum level, should vibrate as individual units of energy known as phonons. Now scientists at MIT and the Swiss Federal Institute of Technology have for the first time created and observed a single phonon in a common material at room temperature. Until now, single phonons have only been observed at ultracold temperatures and in precisely engineered, microscopic materials that researchers must probe in a vacuum. In contrast, the team has created and observed single phonons in a piece of diamond sitting in open air at room temperature. The results, the researchers write in a paper published today in Physical Review X, “bring quantum behavior closer to our daily life.” “There is a dichotomy between our daily experience of what a vibration is — a wave — and what quantum mechanics tells us it must be — a particle,” says Vivishek Sudhir, a postdoc in MIT’s Kavli Institute for Astrophysics and Space Research. “Our experiment, because it is conducted at very tangible conditions, breaks this tension between our daily experience and what physics tells us must be the case.” The technique the team developed can now be used to probe other common materials for quantum vibrations. This may help researchers characterize the atomic processes in solar cells, as well as identify why certain materials are superconducting at high temperatures. From an engineering perspective, the team’s technique can be used to identify common phonon-carrying materials that may make ideal interconnects, or transmission lines, between the quantum computers of the future. “What our work means is that we now have access to a much wider palette of systems to choose from,” says Sudhir, one of the paper’s lead authors. Sudhir’s co-authors are Santiago Tarrago Velez, Kilian Seibold, Nils Kipfer, Mitchell Anderson, and Christophe Galland, of the Swiss Federal Institute of Technology. “Democratizing quantum mechanics” Phonons, the individual particles of vibration described by quantum mechanics, are also associated with heat. For instance, when a crystal, made from orderly lattices of interconnected atoms, is heated at one end, quantum mechanics predicts that heat travels through the crystal in the form of phonons, or individual vibrations of the bonds between molecules. Single phonons have been extremely difficult to detect, mainly because of their sensitivity to heat. Phonons are susceptible to any thermal energy that is greater than their own. If phonons are inherently low in energy, then exposure to higher thermal energies could trigger a material’s phonons to excite en masse, making detection of a single photon a needle-in-a-haystack endeavor. The first efforts to observe single phonons did so with materials specially engineered to harbor very few phonons, at relatively high energies. These researchers then submerged the materials in near-absolute-zero refrigerators Sudhir describes as “brutally, aggressively cold,” to ensure that the surrounding thermal energy was lower than the energy of the phonons in the material. “If that’s the case, then the [phonon] vibration cannot borrow energy from the thermal environment to excite more than one phonon,” Sudhir explains. The researchers then shot a pulse of photons (particles of light) into the material, hoping that one photon would interact with a single phonon. When that happens, the photon, in a process known as Raman scattering, should reflect back out at a different energy imparted to it by the interacting phonon. In this way, researchers were able to detect single phonons, though at ultracold temperatures, and in carefully engineered materials. “What we’ve done here is to ask the question, how do you get rid of this complicated environment you’ve created around this object, and bring this quantum effect to our setting, to see it in more common materials,” Sudhir says. “It’s like democratizing quantum mechanics in some sense.” One in a million For the new study, the team looked to diamond as a test subject. In diamond, phonons naturally operate at high frequencies, of tens of terahertz — so high that, at room temperature, the energy of a single phonon is higher than the surrounding thermal energy. “When this crystal of diamond sits at room temperature, phonon motion does not even exist, because there’s no energy at room temperature to excite anything,” Sudhir says. Within this vibrationally quiet mix of phonons, the researchers aimed to excite just a single phonon. They sent high-frequency laser pulses, consisting of 100 million photons each, into the diamond — a crystal made up of carbon atoms — on the off chance that one of them would interact and reflect off a phonon. The team would then measure the decreased frequency of the photon involved in the collision — confirmation that it had indeed hit upon a phonon, though this operation wouldn’t be able to discern whether one or more phonons were excited in the process. To decipher the number of phonons excited, the researchers sent a second laser pulse into the diamond, as the phonon’s energy gradually decayed. For each phonon excited by the first pulse, this second pulse can de-excite it, taking away that energy in the form of a new, higher-energy photon. If only one phonon was initially excited, then one new, higher-frequency photon should be created. To confirm this, the researchers placed a semitransparent glass through which this new, higher-frequency photon would exit the diamond, along with two detectors on either side of the glass. Photons do not split, so if multiple phonons were excited then de-excited, the resulting photons should pass through the glass and scatter randomly into both detectors. If just one detector “clicks,” indicating the detection of a single photon, the team can be sure that that photon interacted with a single phonon. “It’s a clever trick we play to make sure we are observing just one phonon,” Sudhir says. The probability of a photon interacting with a phonon is about one in 10 billion. In their experiments, the researchers blasted the diamond with 80 million pulses per second — what Sudhir describes as a “train of millions of billions of photons” over several hours, in order to detect about 1 million photon-phonon interactions. In the end, they found, with statistical significance, that they were able to create and detect a single quantum of vibration. “This is sort of an ambitious claim, and we have to be careful the science is rigorously done, with no room for reasonable doubt,” Sudhir says. When sending in their second laser pulse to verify that single phonons were indeed being created, the researchers delayed this pulse, sending in into the diamond as the excited phonon was beginning to ebb in energy. In this way, they were able to glean the manner in which the phonon itself decayed. “So, not only are we able to probe the birth of a single phonon, but also we’re able to probe its death,” Sudhir says. “Now we can say, ‘go use this technique to study how long it takes for a single phonon to die out in your material of choice.’ That number is very useful. If the time it takes to die is very long, then that material can support coherent phonons. If that’s the case, you can do interesting things with it, like thermal transport in solar cells, and interconnects between quantum computers.” ",Electronics and Technology,0.15074652795659813
33,ACM,Algorithm Helps Autonomous Vehicles Avoid Pedestrians,ACM,2019-10-10,-,https://www.theengineer.co.uk/algorithm-av-safety-pedestrians/,"       Algorithm Helps Autonomous Vehicles Avoid Pedestrians     The Engineer (UK)October 10, 2019   Researchers at the University of Waterloo in Canada have developed a decision-making and motion-planning algorithm to help autonomous vehicles (AVs) minimize injuries and damage when accidents with pedestrians are unavoidable. When such a collision is anticipated, the model predictive control (MPC) algorithm evaluates all available actions, selecting the one with the least serious consequences. Waterloo's Amir Khajepour added that the system is a necessity, disputing the notion that AVs can avoid all collisions. The MPC algorithm determines the most appropriate AV response in emergency situations, in accordance with pre-defined mathematical calculations accounting for injury severity and damage.                 ",Computer Science,0.14969072471228104
34,Science Daily,"That New Yarn? Wearable, Washable Textile Devices Are Possible With MXene-Coated Yarns",Computers & Math,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010161548.htm,"   Two groups of researchers at Drexel University -- one, who is leading the development of industrial functional fabric production techniques, and the other, a pioneer in the study and application of one of the strongest, most electrically conductive super materials in use today -- believe they have a solution. They've improved a basic element of textiles: yarn. By adding technical capabilities to the fibers that give textiles their character, fit and feel, the team has shown that it can knit new functionality into fabrics without limiting their wearability. In a paper recently published in the journal Advanced Functional Materials, the researchers, led by Yury Gogotsi, PhD, Distinguished University and Bach professor in Drexel's College of Engineering, and Genevieve Dion, an associate professor in Westphal College of Media Arts & Design and director of Drexel's Center for Functional Fabrics, showed that they can create a highly conductive, durable yarn by coating standard cellulose-based yarns with a type of conductive two-dimensional material called MXene. Hitting snags ""Current wearables utilize conventional batteries, which are bulky and uncomfortable, and can impose design limitations to the final product,"" they write. ""Therefore, the development of flexible, electrochemically and electromechanically active yarns, which can be engineered and knitted into full fabrics provide new and practical insights for the scalable production of textile-based devices."" The team reported that its conductive yarn packs more conductive material into the fibers and can be knitted by a standard industrial knitting machine to produce a textile with top-notch electrical performance capabilities. This combination of ability and durability stands apart from the rest of the functional fabric field today.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Most attempts to turn textiles into wearable technology use stiff metallic fibers that alter the texture and physical behavior of the fabric. Other attempts to make conductive textiles using silver nanoparticles and graphene and other carbon materials raise environmental concerns and come up short on performance requirements. And the coating methods that are successfully able to apply enough material to a textile substrate to make it highly conductive also tend to make the yarns and fabrics too brittle to withstand normal wear and tear. ""Some of the biggest challenges in our field are developing innovative functional yarns at scale that are robust enough to be integrated into the textile manufacturing process and withstand washing,"" Dion said. ""We believe that demonstrating the manufacturability of any new conductive yarn during experimental stages is crucial. High electrical conductivity and electrochemical performance are important, but so are conductive yarns that can be produced by a simple and scalable process with suitable mechanical properties for textile integration. All must be taken into consideration for the successful development of the next-generation devices that can be worn like everyday garments."" The winning combination Dion has been a pioneer in the field of wearable technology, by drawing on her background on fashion and industrial design to produce new processes for creating fabrics with new technological capabilities. Her work has been recognized by the Department of Defense, which included Drexel, and Dion, in its Advanced Functional Fabrics of America effort to make the country a leader in the field. She teamed with Gogotsi, who is a leading researcher in the area of two-dimensional conductive materials, to approach the challenge of making a conductive yarn that would hold up to knitting, wearing and washing.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Gogotsi's group was part of the Drexel team that discovered highly conductive two-dimensional materials, called MXenes, in 2011 and have been exploring their exceptional properties and applications for them ever since. His group has shown that it can synthesize MXenes that mix with water to create inks and spray coatings without any additives or surfactants -- a revelation that made them a natural candidate for making conductive yarn that could be used in functional fabrics. ""Researchers have explored adding graphene and carbon nanotube coatings to yarn, our group has also looked at a number of carbon coatings in the past,"" Gogotsi said. ""But achieving the level of conductivity that we demonstrate with MXenes has not been possible until now. It is approaching the conductivity of silver nanowire-coated yarns, but the use of silver in the textile industry is severely limited due to its dissolution and harmful effect on the environment. Moreover, MXenes could be used to add electrical energy storage capability, sensing, electromagnetic interference shielding and many other useful properties to textiles."" In its basic form, titanium carbide MXene looks like a black powder. But it is actually composed of flakes that are just a few atoms thick, which can be produced at various sizes. Larger flakes mean more surface area and greater conductivity, so the team found that it was possible to boost the performance of the yarn by infiltrating the individual fibers with smaller flakes and then coating the yarn itself with a layer of larger-flake MXene. Putting it to the test The team created the conductive yarns from three common, cellulose-based yarns: cotton, bamboo and linen. They applied the MXene material via dip-coating, which is a standard dyeing method, before testing them by knitting full fabrics on an industrial knitting machine -- the kind used to make most of the sweaters and scarves you'll see this fall. Each type of yarn was knit into three different fabric swatches using three different stitch patterns -- single jersey, half gauge and interlock -- to ensure that they are durable enough to hold up in any textile from a tightly knit sweater to a loose-knit scarf. ""The ability to knit MXene-coated cellulose-based yarns with different stitch patterns allowed us to control the fabric properties, such as porosity and thickness for various applications,"" the researchers write. To put the new threads to the test in a technological application, the team knitted some touch-sensitive textiles -- the sort that are being explored by Levi's and Yves Saint Laurent as part of Google's Project Jacquard. Not only did the MXene-based conductive yarns hold up against the wear and tear of the industrial knitting machines, but the fabrics produced survived a battery of tests to prove its durability. Tugging, twisting, bending and -- most importantly -- washing, did not diminish the touch-sensing abilities of the yarn, the team reported -- even after dozens of trips through the spin cycle. Pushing forward But the researchers suggest that the ultimate advantage of using MXene-coated conductive yarns to produce these special textiles is that all of the functionality can be seamlessly integrated into the textiles. So instead of having to add an external battery to power the wearable device, or wirelessly connect it to your smartphone, these energy storage devices and antennas would be made of fabric as well -- an integration that, though literally seamed, is a much smoother way to incorporate the technology. ""Electrically conducting yarns are quintessential for wearable applications because they can be engineered to perform specific functions in a wide array of technologies,"" they write. Using conductive yarns also means that a wider variety of technological customization and innovations are possible via the knitting process. For example, ""the performance of the knitted pressure sensor can be further improved in the future by changing the yarn type, stitch pattern, active material loading and the dielectric layer to result in higher capacitance changes,"" according to the authors. Dion's team at the Center for Functional Fabrics is already putting this development to the test in a number of projects, including a collaboration with textile manufacturer Apex Mills -- one of the leading producers of material for car seats and interiors. And Gogotsi suggests the next step for this work will be tuning the coating process to add just the right amount of conductive MXene material to the yarn for specific uses. ""With this MXene yarn, so many applications are possible,"" Gogotsi said. ""You can think about making car seats with it so the car knows the size and weight of the passenger to optimize safety settings; textile pressure sensors could be in sports apparel to monitor performance, or woven into carpets to help connected houses discern how many people are home -- your imagination is the limit."" ",Electronics and Technology,0.14908198098849618
35,MIT News,SMART develops a way to commercially manufacture integrated silicon III-V chips,Research,2019-10-03,-,http://news.mit.edu/2019/mit-singapore-smart-way-to-manufacture-integrated-silicon-iii-v-chips-1003,"  The Singapore-MIT Alliance for Research and Technology (SMART), MIT’s research enterprise in Singapore, has announced the successful development of a commercially viable way to manufacture integrated silicon III-V chips with high-performance III-V devices inserted into their design. In most devices today, silicon-based CMOS chips are used for computing, but they are not efficient for illumination and communications, resulting in low efficiency and heat generation. This is why current 5G mobile devices on the market get very hot upon use and can shut down after a short time. This is where III-V semiconductors are valuable. III-V chips are made with compounds including elements in the third and fifth columns of the periodic table, such as gallium nitride (GaN) and indium gallium arsenide (InGaAs). Due to their unique properties, they are exceptionally well-suited for optoelectronics (such as LEDs) and communications (such as 5G wireless), boosting efficiency substantially. “By integrating III-V into silicon, we can build upon existing manufacturing capabilities and low-cost volume production techniques of silicon and include the unique optical and electronic functionality of III-V technology,” says Eugene Fitzgerald, CEO and director of SMART and the Merton C. Flemings-SMA Professor of Materials Science and Engineering at MIT. “The new chips will be at the heart of future product innovation and power the next generation of communications devices, wearables, and displays.” Kenneth Lee, senior scientific director of the SMART Low Energy Electronic Systems (LEES) research program, adds: “Integrating III-V semiconductor devices with silicon in a commercially viable way is one of the most difficult challenges faced by the semiconductor industry, even though such integrated circuits have been desired for decades. Current methods are expensive and inefficient, which is delaying the availability of the chips the industry needs. With our new process, we can leverage existing capabilities to manufacture these new integrated silicon III-V chips cost-effectively and accelerate the development and adoption of new technologies that will power economies.” The new technology developed by SMART builds two layers of silicon and III-V devices on separate substrates and integrates them vertically together within a micron, which is 1/50th the diameter of a human hair. The process can use existing 200 micrometer manufacturing tools, which will allow semiconductor manufacturers in Singapore and around the world to make new use of their current equipment. Today, the cost of investing in a new manufacturing technology is in the range of tens of billions of dollars; the new integrated circuit platform is highly cost-effective, and will result in much lower-cost novel circuits and electronic systems. SMART is focusing on creating new chips for pixelated illumination/display and 5G markets, which has a combined potential market of over $100 billion. Other markets that SMART’s new integrated silicon III-V chips will disrupt include wearable mini-displays, virtual reality applications, and other imaging technologies. The patent portfolio has been exclusively licensed by New Silicon Corporation (NSC), a Singapore-based spinoff from SMART. NSC is the first fabless silicon integrated circuit company with proprietary materials, processes, devices, and design for monolithic integrated silicon III-V circuits. SMART’s new integrated Silicon III-V chips will be available next year and expected in products by 2021. SMART’s LEES Interdisciplinary Research Group is creating new integrated circuit technologies that result in increased functionality, lower power consumption, and higher performance for electronic systems. These integrated circuits of the future will impact applications in wireless communications, power electronics, LED lighting, and displays. LEES has a vertically-integrated research team possessing expertise in materials, devices, and circuits, comprising multiple individuals with professional experience within the semiconductor industry. This ensures that the research is targeted to meet the needs of the semiconductor industry both within Singapore and globally. ",Electronics and Technology,0.14859993653430115
36,MIT News,Notation system allows scientists to communicate polymers more easily,Computer Science,2019-09-18,-,http://news.mit.edu/2019/bigsmiles-notation-system-allows-scientists-communicate-polymers-more-easily-0918,"  Having a compact, yet robust, structurally-based identifier or representation system for molecular structures is a key enabling factor for efficient sharing and dissemination of results within the research community. Such systems also lay down the essential foundations for machine learning and other data-driven research. While substantial advances have been made for small molecules, the polymer community has struggled in coming up with an efficient representation system. For small molecules, the basic premise is that each distinct chemical species corresponds to a well-defined chemical structure. This does not hold for polymers. Polymers are intrinsically stochastic molecules that are often ensembles with a distribution of chemical structures. This difficulty limits the applicability of all deterministic representations developed for small molecules. In a paper published Sept. 12 in ACS Central Science, researchers at MIT, Duke University, and Northwestern University report a new representation system that is capable of handling the stochastic nature of polymers, called BigSMILES. “BigSMILES addresses a significant challenge in the digital representation of polymers,” explains Connor Coley PhD ’19, co-author of the paper. “Polymers are almost always ensembles of multiple chemical structures, generated through stochastic processes, so we can't use the same strategies for writing down their structures as for small molecules.” Co-authors are Coley; associate professor of chemical engineering Bradley D. Olsen at MIT; Warren K. Lewis Professor of Chemical Engineering Klavs F. Jensen at MIT; assistant professor of chemistry Julia A. Kalow at Northwestern University; associate professor of chemistry Jeremiah A. Johnson at MIT; William T. Miller Professor of Chemistry Stephen L. Craig at Duke University; graduate student Eliot Woods at Northwestern University; graduate student Zi Wang at Duke University; graduate student Wencong Wang at MIT; graduate student Haley K. Beech at MIT; visiting researcher Hidenobu Mochigase at MIT; and graduate student Tzyy-Shyang Lin at MIT. There are several line notations to communicate molecular structure, with simplified molecular-input line-entry system (SMILES) being the most popular. SMILES is generally considered the most human-readable variant, with by far the widest software support. In practice, SMILES provides a simple set of representations that are suitable as labels for chemical data and as a memory-compact identifier for data exchange between researchers. As a text-based system, SMILES is also a natural fit to many text-based machine learning algorithms. These characteristics have made SMILES a perfect tool for translating chemistry knowledge into a machine-friendly form, and it has been successfully applied for small molecule property prediction and computer-aided synthesis planning. Polymers, however, have resisted description by this and other structural languages. This is because most structural languages such as SMILES have been designed to describe molecules or chemical fragments that are well-defined atomistic graphs. Since polymers are stochastic molecules, they do not have unique SMILES representations. This lack of a unified naming or identifier convention for polymer materials is one of the major hurdles slowing down the development of the polymer informatics field. While pioneering efforts on polymer informatics, such as the Polymer Genome Project, have demonstrated the usefulness of SMILES extensions in polymer informatics, the fast development of new chemistry and the rapid development of materials informatics and data-driven research make the need for a universally applicable naming convention for polymers important. “Machine learning presents an enormous opportunity to accelerate chemical development and discovery,” says Lin He, acting deputy division director for the National Science Foundation (NSF) Division of Chemistry. “This expanded tool to label structures, specifically devised to address the unique challenges inherent to polymers, greatly enhances the searchability of chemical structural data, and brings us one step closer to harnessing the data revolution.” The researchers have created a new structurally-based construct as an addition to the highly successful SMILES representation that can treat the random nature of polymer materials. Since polymers are high molar mass molecules, this construct is named BigSMILES. In BigSMILES, polymeric fragments are represented by a list of repeating units enclosed by curly brackets. The chemical structures of the repeating units are encoded using normal SMILES syntax, but with additional bonding descriptors that specify how different repeating units are connected to form polymers. This simple design of syntax would enable the encoding of macromolecules over a wide range of different chemistries, including homopolymer, random copolymers and block copolymers, and a variety of molecular connectivity, ranging from linear polymers to ring polymers to even branched polymers. As in SMILES, BigSMILES representations are compact, self-contained text strings. “Standardizing the digital representation of polymeric structures with BigSMILES will encourage the sharing and aggregation of polymer data, improving model quality over time and reinforcing the benefits of its use,” says Jason Clark, the materials lead in Open Innovation for Renewable Chemicals and Materials at Braskem, who was not associated with the research. “BigSMILES is a significant contribution to the field in that it addresses the need for a flexible system to represent complex polymer structures digitally.”   Clark adds, “The challenges faced by the plastics industry in the context of the circular economy begins with the source of raw materials and continues all the way through end-of-life management. Addressing these challenges requires the innovative design of polymer-based materials, which has traditionally suffered from lengthy development cycles. Advances in artificial intelligence and machine learning have shown promise to accelerate the development cycle for applications utilizing metal alloys and small organic molecules, motivating the plastics industry to seek a parallel approach.” BigSMILES digital representations facilitate the evaluation of structure-performance relationships by application of data science methods, he says, ultimately accelerating the convergence to the polymer structures or compositions that will help enable the circular economy. “A multitude of complicated polymer structures can be constructed through the composition of three new basic operators and original SMILES symbols,” says Olsen, “Entire fields of chemistry, materials science, and engineering, including polymer science, biomaterials, materials chemistry, and much of biochemistry, are based upon macromolecules which have stochastic structures. This can basically be thought of as a new language for how to write the structure of large molecules.” “One of the things I’m excited about is how the data entry might eventually be tied directly to the synthetic methods used to make a particular polymer,” says Craig, “Because of that, there is an opportunity to actually capture and process more information about the molecules than is typically available from standard characterizations. If this can be done, it will enable all sorts of discoveries.” This work was funded by the NSF through the Center for the Chemistry of Molecularly Optimized Networks, an NSF Center for Chemical Innovation. ",Electronics and Technology,0.14414192775695323
37,IEEE,X-Ray Tech Lays Chip Secrets Bare,Electronics and Technology,2019-10-07,-,https://spectrum.ieee.org/nanoclast/semiconductors/design/xray-tech-lays-chip-secrets-bare,"      Scientists and engineers in Switzerland and California have come up with a technique that can reveal the 3D design of a modern microprocessor without destroying it. Typically today, such reverse engineering is a time-consuming process that involves painstakingly removing each of a chip’s many nanometers-thick interconnect layers and mapping them using a hierarchy of different imaging techniques, from optical microscopy for the larger features to electron microscopy for the tiniest features.  The inventors of the new technique, called ptychographic X-ray laminography, say it could be used by integrated circuit designers to verify that manufactured chips match their designs, or by government agencies concerned about “kill switches” or hardware trojans that could have secretly been added to ICs they depend on. “It’s the only approach to non-destructive reverse engineering of electronic chips—[and] not just reverse engineering but assurance that chips are manufactured according to design,” says Anthony F. J. Levi, professor of electrical and computer engineering at University of Southern California, who led the California side of the team. “You can identify the foundry, aspects of the design, who did the design. It’s like a fingerprint.” The new technique is an improvement on technology unveiled by the same team in 2017 called ptychographic computed tomography. That process used a coherent beam of X-rays from a synchrotron to illuminate a 10-micrometer pillar that had been cut away from the rest of the chip. The team then recorded how the X-rays diffract and scatter from the pillar at a variety of angles and computed what the internal structures must be in order to create that pattern. With the new technique, “the goal was obviously to avoid having to do any cutting at all,” explains Gabriel Aeppli, head of the photonic science division at the Paul Scherrer Institute (PSI) in Switzerland and professor of physics at the Swiss Federal Institutes of Technology in Zürich and Lausanne, who led the research. “A modern chip with a billion transistors is a bigger footprint than 10 microns.” The group wanted a single technology that would allow them to image a whole chip and also zoom in on points of interest. The prior technique needed the pillar, because trying to see through a whole chip, edge-on, absorbs too many of the X-rays to produce a useful diffraction pattern. But shooting the X-rays through the chip at an angle creates a small enough cross-section. However, it also produces a gap in the information. Some of that information can be regained by making some assumptions about what you’re looking at, explains Aeppli. For example, we know that real interconnects can’t have certain shapes. Finding the right angle for the X-rays—which turned out to be 61 degrees—was a matter of balancing absorption and information loss, says Aeppli. In the new technique, the bare chip is polished down to a thickness of 20 micrometers and then placed on a scanning stage at a 61-degree tilt. The stage then rotates the chip as the X-ray beam is focused on it. A photon-counting camera receives the resulting diffraction pattern. Using the technique in low-resolution mode, the team scanned a 300-by-300-micrometer area in 30 hours. They then used it to zoom in on a 40-micrometer-diameter section to produce a 3D image with 18.9-nanometer resolution, requiring another 60 hours. Using the high-resolution mode, the researchers could identify the parts of an individual inverter circuit in a chip made using 16-nanometer node technology. This first laminography microscope engineered by PSI’s Mirko Holler, can image a maximum of 12 by 12 millimeters—easily accommodating many chips, such as the iPhone processor Apple A12, but not large enough for an entire Nvidia Volta GPU. Though the group tested the technique on a chip made using a 16-nanometer process technology, it will be able to comfortably handle those made using the new 7-nanometer process technology, where the minimum distance between metal lines is around 35 to 40 nanometers. Future versions of the laminography technique could reach a resolution of just 2 nanometers or reduce the time for a low-resolution inspection of that 300-by-300-micrometer segment to less than an hour, the researchers say. Those improvements will come from a new generation of synchrotron light sources. The synchrotron at PSI is considered a 3rd generation machine. But 4th generation machines are already starting up, such as Sweden’s MAX IV. With a higher flux of X-ray photons flowing through the chip, the system can collect more usable data per unit of time, leading to higher resolution and faster processing. “We’re looking at improvements of 1,000 to 10,000 over the next five or six years in terms of the pixels we’re collecting per unit [of] time,” says Aeppli. Ptychographic X-ray laminography can be further sped up by starting with more information about the chip. Knowing the design rules ahead of time allows the system to come to a conclusion about what it’s seeing with fewer photons. In fact, Aeppli suspects one of the main uses of the technology will be to look for deviations from the design that could be indicative of manufacturing errors or something more sinister. “Looking for deviation from design is an easier problem than reverse engineering the entire design,” he says. The team is seeing “a lot of interest from the [United States] on the national security side.” However, Aeppli expects chipmakers to use the laminography technique, as well. “Every region that has major chip foundries nearby has some national lab with a synchrotron,” he points out. Aeppli, Levi, and their teams reported the technique this week in Nature Electronics.  Monthly newsletter about how new materials, designs, and processes drive the chip industry.  IEEE Spectrum’s nanotechnology blog, featuring news and analysis about the development, applications, and future of science and technology at the nanoscale. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Electronics and Technology,0.14258201235599108
38,MIT News,This flat structure morphs into shape of a human face when temperature changes,Robotics,2019-09-30,-,http://news.mit.edu/2019/mesh-structure-shape-temperature-changes-0930,"  Researchers at MIT and elsewhere have designed 3-D printed mesh-like structures that morph from flat layers into predetermined shapes, in response to changes in ambient temperature. The new structures can transform into configurations that are more complex than what other shape-shifting materials and structures can achieve. As a demonstration, the researchers printed a flat mesh that, when exposed to a certain temperature difference, deforms into the shape of a human face. They also designed a mesh embedded with conductive liquid metal, that curves into a dome to form an active antenna, the resonance frequency of which changes as it deforms. The team’s new design method can be used to determine the specific pattern of flat mesh structures to print, given the material’s properties, in order to make the structure transform into a desired shape. The researchers say that down the road, their technique may be used to design deployable structures, such as tents or coverings that automatically unfurl and inflate in response to changes in temperature or other ambient conditions. Such complex, shape-shifting structures could also be of use as stents or scaffolds for artificial tissue, or as deformable lenses in telescopes. Wim van Rees, assistant professor of mechanical engineering at MIT, also sees applications in soft robotics. “I’d like to see this incorporated in, for example, a robotic jellyfish that changes shape to swim as we put it in water,” says van Rees. “If you could use this as an actuator, like an artificial muscle, the actuator could be any arbitrary shape that transforms into another arbitrary shape. Then you’re entering an entirely new design space in soft robotics.” Van Rees and his colleagues are publishing their results this week in the Proceedings of the National Academy of Sciences. His co-authors are J. William Boley of Boston University; Ryan Truby, Arda Kotikian, Jennifer Lewis, and L. Mahadevan of Harvard University; Charles Lissandrello of Draper Laboratory; and Mark Horenstein of Boston University. Gift wrap’s limit Two years ago, van Rees came up with a theoretical design for how to transform a thin flat sheet into a complex shape such as a human face. Until then, researchers in the field of 4-D materials — materials designed to deform over time — had developed ways for certain materials to change, or morph, but only into relatively simple structures. “My goal was to start with a complex 3-D shape that we want to achieve, like a human face, and then ask, ‘How do we program a material so it gets there?’” van Rees says. “That’s a problem of inverse design.” He came up with a formula to compute the expansion and contraction that regions of a bilayer material sheet would have to achieve in order to reach a desired shape, and developed a code to simulate this in a theoretical material. He then put the formula to work, and visualized how the method could transform a flat, continuous disc into a complex human face. But he and his collaborators quickly found that the method wouldn’t apply to most physical materials, at least if they were trying to work with continuous sheets. While van Rees used a continuous sheet for his simulations, it was of an idealized material, with no physical constraints on the amount of expansion and contraction it could achieve. Most materials, in contrast, have very limited growth capabilities. This limitation has profound consequences on a property known as double curvature, meaning a surface that can curve simultaneously in two perpendicular directions — an effect that is described in an almost 200-year-old theorem by Carl Friedrich Gauss called the Theorema Egregium, Latin for “Remarkable Theorem.” If you’ve ever tried to gift wrap a soccer ball, you’ve experienced this concept in practice: To transform paper, which has no curvature at all, to the shape of a ball, which has positive double curvature, you have to crease and crumple the paper at the sides and bottom to completely wrap the ball. In other words, for the paper sheet to adapt to a shape with double curvature, it would have to stretch or contract, or both, in the necessary places to wrap a ball uniformly. To impart double curvature to a shape-shifting sheet, the researchers switched the basis of the structure from a continuous sheet to a lattice, or mesh. The idea was twofold: first, a temperature-induced bending of the lattice’s ribs would result in much larger expansions and contractions of the mesh nodes, than could be achieved in a continuous sheet. Second, the voids in the lattice can easily accommodate large changes in surface area when the ribs are designed to grow at different rates across the sheet. The researchers also designed each individual rib of the lattice to bend by a predetermined degree in order to create the shape of, say, a nose rather than an eye-socket. For each rib, they incorporated four skinnier ribs, arranging two to line up atop the other two. All four miniribs were made from carefully selected variations of the same base material, to calibrate the required different responses to temperature. When the four miniribs were bonded together in the printing process to form one larger rib, the rib as a whole could curve due to the difference in temperature response between the materials of the smaller ribs: If one material is more responsive to temperature, it may prefer to elongate. But because it is bonded to a less responsive rib, which resists the elongation, the whole rib will curve instead. The researchers can play with the arrangement of the four ribs to “preprogram” whether the rib as a whole curves up to form part of a nose, or dips down as part of an eye socket. Shapes unlocked To fabricate a lattice that changes into the shape of a human face, the researchers started with a 3-D image of a face — to be specific, the face of Gauss, whose principles of geometry underly much of the team’s approach. From this image, they created a map of the distances a flat surface would require to rise up or dip down to conform to the shape of the face. Van Rees then devised an algorithm to translate these distances into a lattice with a specific pattern of ribs, and ratios of miniribs within each rib. The team printed the lattice from PDMS, a common rubbery material which naturally expands when exposed to an increase in temperature. They adjusted the material’s temperature responsiveness by infusing one solution of it with glass fibers, making it physically stiffer and more resistant to a change in temperature. After printing lattice patterns of the material, they cured the lattice in a 250-degree-Celsius oven, then took it out and placed it in a saltwater bath, where it cooled to room temperature and morphed into the shape of a human face.   Courtesy of the researchers The team also printed a latticed disc made from ribs embedded with a liquid metal ink — an antenna of sorts, that changed its resonant frequency as the lattice transformed into a dome. Van Rees and his colleagues are currently investigating ways to apply the design of complex shape-shifting to stiffer materials, for sturdier applications, such as temperature-responsive tents and self-propelling fins and wings. This research was supported, in part, by the National Science Foundation, and Draper Laboratory. ",Electronics and Technology,0.14241095992278077
39,Science Daily,Scientists Reveal Mechanism of Electron Charge Exchange in Molecules,Computers & Math,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111714.htm,"   With this technique, the UCI scientists were able to observe electron distribution between atoms and molecules and uncover clues to the origins of ferroelectricity, the capacity of certain crystals to possess spontaneous electric polarization that can be switched by the application of an electric field. The research, which is highlighted in a study published today in Nature, also revealed the mechanism of charge transfer between two materials. ""This method is an advancement in electron microscopy -- from detecting atoms to imaging electrons -- that could help us engineer new materials with desired properties and functionalities for devices used in data storage, energy conversion and quantum computing,"" said team leader Xiaoqing Pan, UCI's Henry Samueli Endowed Chair in Engineering and a professor of both materials science & engineering and physics & astronomy. Employing a new aberration-corrected scanning transmission electron microscope with a fine electron probe measuring half an angstrom and a fast-direct electron detection camera, his group was able to acquire a 2D raster image of diffraction patterns from a region of interest in the sample. As obtained, the data sets are 4D, since they consist of 2D diffraction patterns from each probe location in a 2D scanning area. ""With our new microscope, we can routinely form an electron probe as small as 0.6 angstrom, and our high-speed camera with angular resolution can acquire 4D STEM images with 512 x 512 pixels at greater than 300 frames per second,"" Pan said. ""Using this technique, we can see the electron charge distribution between atoms in two different perovskite oxides, non-polar strontium titanate and ferroelectric bismuth ferrite."" Electron charge density in bulk materials can be measured by X-ray or electron diffraction techniques by assuming a perfectly defect-free structure within the beam-illuminated area. But, Pan said, there remains a challenge in resolving electron charge density in nanostructured materials consisting of interfaces and defects. ""In principle, local electric field and charge density can be determined by electron diffraction imaging using an aberration-corrected scanning transmission electron microscope with a sub-angstrom electron probe,"" he said. ""While penetrating through a specimen, the electron beam interacts with the internal electric field of material in its pathway, resulting in a change in its momentum reflected in the diffraction pattern. By measuring this change, the electric field in a local region of the specimen can be delineated, and the charge density can be derived."" Pan added that although this principle has been demonstrated in simulations, no experiment has been successful until now. ""The electron charge density maps obtained using the 4D STEM method match with theoretical results from the first-principle calculations,"" said lead author Wenpei Gao, a UCI postdoctoral researcher in materials science & engineering. ""The study of the ferroelectric/insulator interface between bismuth ferrite and strontium titanate using this technique directly shows how features of the bismuth compound's polar atomic structure leak across the interface, appearing in the normally non-polar strontium titanate. As a result, the interface hosts excess electrons confined to a small region less than 1 nanometer thick."" Pan said that this project gives materials scientists and engineers new tools for evaluating structures, defects and interfaces in functional materials and nanodevices. He noted that it may soon be possible to conduct high-throughput mapping of the charge density of materials and molecules to add to the database of properties aiding in the Materials Genome Initiative. ""As electron microscopy advances from imaging atoms to probing electrons, it will lead to new understanding and discovery in materials research,"" said co-author Ruqian Wu, UCI professor of physics & astronomy, who led the study's theoretical work. ""The ability to image the charge density distribution around atoms near interfaces, grain boundaries or other planar defects opens new fields for electron microscopy and materials science."" ",Electronics and Technology,0.1422193371147304
40,Science Daily,Super Light Dampers for Low Tones,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103404.htm,"   Some of these properties are retained even if the atomic crystal structures are enlarged about 100,000,000 times and the crystals are replicated on a large scale. Physicists have been taking ad-vantage of this for several years now: If the original crystals scatter X-rays with very short wavelengths, the enlarged copies can scatter oscillations with long wavelengths in all directions. A very elegant way for vibration damping has thus been found. Enlarged crystal structures with such acoustic properties are called phononic crystals. Internally rotating crystal structures Andrea Bergamini and his team from Empa's Acoustics / Noise Reduction Department have now succeeded in integrating additional properties into the crystals that are not present in the originals. The researchers built small, rotating plates into the crystal structures, which are able to convert oscillations along the longitudinal axis into torsional movements. For the first time, an undesirable oscillation can not only be scattered in different spatial directions, but can also be converted into thermal energy. AAAA or ABAB arrangement In a next step, Bergamini and his research colleagues coupled several of the rotating disks in the crystal together. This can be done in two different ways: either all disks rotate in the same direction (isotactic arrangement) or they are alternately connected to each other with their rotational directions (syndiotactic arrangement). The effect differs dramatically: the syndiotactic ABAB arrangement of the direction of rotation creates a so-called frequency band gap. A wide range of oscillations is ""swallowed"" by the ro-tation mechanism and not passed through the crystal. On the other hand, the isotactic AAAA arrangement of the rotary motions generates new waves with similar frequencies, which are transmitted through the crystal. A mechanical component with certain geometry therefore determines whether the crystal is insulating or conducting. The team has now published the research results in the current issue of the journal Nature Communications. The ""Cryptography Window"" But how can such oscillation frequency band gaps be used? In the meantime, a first model has been developed in the laboratory showing a possible function of phononic crystals: Bergamini built a window from two Plexiglas plates in which rotating discs in syndiotactic arrangement are integrated. The size of the discs is tuned to the frequency of human speech. The idea: when certain frequencies are filtered out of speech, the spoken content becomes incomprehensible to the listener. The human brain can no longer assemble the acoustic information into a meaning. First tests in the acoustics laboratory show: the approach is very promising. You can clearly see the talking people and also hear who is talking in a muffled way. But not a single word can be understood clearly from the spoken text. Soundproofing elements that are a hundred times lighter? Bergamini and his colleagues expect that transparent, phononic crystals could be interesting for architects and interior designers. This physical trick makes it possible to produce rigid building materials with a stable shape that insulate sound very well and can be up to 100 times lighter than other phononic insulating materials which have the same effect. In mechanical engineering, aircraft construction and lightweight automotive construction, too, the filtering out of interfering frequencies with lightweight de-signer insulating materials could soon play a major role. ",Electronics and Technology,0.14100615329245236
41,Science Daily,How to Control Friction in Topological Insulators,Matter & Energy,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111735.htm,"   Thanks to their unique electrical properties, topological insulators promise many innovations in the electronics and computer industries, as well as in the development of quantum computers. The thin surface layer can conduct electricity almost without resistance, resulting in less heat than traditional materials. This makes them of particular interest for electronic components. Furthermore, in topological insulators, the electronic friction -- i.e. the electron-mediated conversion of electrical energy into heat -- can be reduced and controlled. Researchers of the University of Basel, the Swiss Nanoscience Institute (SNI) and the Istanbul Technical University have now been able to experimentally verify and demonstrate exactly how the transition from energy to heat through friction behaves -- a process known as dissipation. Measuring friction with a pendulum The team headed by Professor Ernst Meyer at the Department of Physics of the University of Basel investigated the effects of friction on the surface of a bismuth telluride topological insulator. The scientists used an atomic force microscope in pendulum mode. Here, the conductive microscope tip made of gold oscillates back and forth just above the two-dimensional surface of the topological insulator. When a voltage is applied to the microscope tip, the movement of the pendulum induces a small electrical current on the surface. In conventional materials, some of this electrical energy is converted into heat through friction. The result on the conductive surface of the topological insulator looks very different: the loss of energy through the conversion to heat is significantly reduced. ""Our measurements clearly show that at certain voltages there is virtually no heat generation caused by electronic friction,"" explains Dr. Dilek Yildiz, who carried out this work within the SNI PhD School. A novel mechanism The researchers were also able to observe for the first time a new quantum-mechanical dissipation mechanism that occurs only at certain voltages. Under these conditions, the electrons migrate from the tip through an intermediate state into the material -- similar to the tunneling effect in scanning tunneling microscopes. By regulating the voltage, the scientists were able to influence the dissipation. ""These measurements confirm the great potential of topological insulators, since electronic friction can be controlled in a targeted manner,"" adds Meyer. ",Electronics and Technology,0.13993801422703533
42,Science Daily,Overlap Allows Nanoparticles to Enhance Light-Based Detection,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015113318.htm,"   The labs of Rice chemists Christy Landes and Stephan Link demonstrated how to optimize a method that can sense small concentrations of molecules by amplifying the light they emit when their spectral frequencies overlap with those of nearby plasmonic nanoparticles. The surface plasmons, coherent electron waves that ripple across the surface of a metallic nanoparticle, act as antennas and enhance the molecules' emitted light up to 10 times when they're in the ""sweet spot"" near a particle. Their technique is the topic of a paper in a special edition of the Journal of Chemical Physics focused on emerging directions in plasmonics. The work at Rice could help researchers analyze the active surfaces of catalysts and other materials at the nanoscale, an important step in improving their efficiency. The discovery relies on the phenomenon of electrochemiluminescence (ECL), by which electricity drives chemical reactions that prompt molecules to emit light, said Thomas Heiderscheit, a Rice graduate student and the paper's lead author. It is often used to detect trace materials like heavy metals in water or the Zika virus in biological fluids. Previous studies inferred that spectral overlap of the nanoparticle and molecules would enhance the signal, but those studies could not account for the innate differences in nanoparticle sizes and shapes that could mask the effects. The Rice researchers had set out to minimize these other impacts to focus only on the role of spectral frequency overlap on signal enhancement. ""This study looks at what type of antenna is the best to use, because the properties of the nanoparticle dictate the spectrum and its overlap with the molecule,"" said Miranda Gallagher, a Rice postdoctoral research associate and co-author of the paper. ""Should it be round or should it have sharp edges? Should it be smaller or larger?"" In experiments, the researchers combined either gold nanospheres or sharp-tipped gold nanotriangles with a ruthenium-based dye molecule in a polymer shell that kept the molecules from migrating too far from the particles. ""That's essentially our electrode,"" Heiderscheit said. ""If we didn't have the polymer, the dye molecules would be free to move and we'd see light diffused across the sample."" With the molecules constrained by the polymer, they could clearly see molecules emitting near particles. They determined signal enhancement is controlled by a combination of size and frequency matching between the dye molecule and the nanospheres, and just frequency matching for nanotriangles. Single-molecule imaging is still a stretch for the nascent technique, Heiderscheit said. ""Essentially, we're imaging how active a surface is,"" he said. ""The Department of Energy (the main sponsor of the project) cares about this research because it could achieve super-resolution mapping of reactivity on a surface."" Super-resolution enables the capture of images below the diffraction limit of light. ""For instance, if you have nanoparticles in a battery system, you can use ECL to map where the reactions are most chemically active,"" Heiderscheit said. ""You're essentially determining what nanoparticles make a good catalyst, and we can use this tool to design better ones."" ",Electronics and Technology,0.13840175592585036
43,ACM,Drones as Detectives: Surveying Crime Scenes for Evidence,ACM,2019-10-08,-,https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence,"       Drones as Detectives: Surveying Crime Scenes for Evidence     IEEE SpectrumMichelle HampsonOctober 8, 2019   Researchers at the Federal University of Bahia in Brazil are designing drones to perform forensic examinations at crime scenes by recording evidence from multiple angles. The AirCSI drone first sweeps the scene, scanning the environs with a stereo camera, as a visual self-localization and mapping system tracks the drone's position. A secondary system with another camera captures images from multiple angles. The drone then plots a zigzag path, making additional sweeps to gather finer information on each piece of evidence. Tests with simulation software found that using multiple angles to detect evidence is up to 18% more effective than using a single angle.                 ",Computer Science,0.13706842348613776
44,MIT News,Photovoltaic-powered sensors for the “internet of things”,Computer Science,2019-09-27,-,http://news.mit.edu/2019/photovoltaic-rfid-sensors-iot-0927,"  By 2025, experts estimate the number of “internet of things” devices — including sensors that gather real-time data about infrastructure and the environment — could rise to 75 billion worldwide. As it stands, however, those sensors require batteries that must be replaced frequently, which can be problematic for long-term monitoring.   MIT researchers have designed photovoltaic-powered sensors that could potentially transmit data for years before they need to be replaced. To do so, they mounted thin-film perovskite cells — known for their potential low cost, flexibility, and relative ease of fabrication — as energy-harvesters on inexpensive radio-frequency identification (RFID) tags. The cells could power the sensors in both bright sunlight and dimmer indoor conditions. Moreover, the team found the solar power actually gives the sensors a major power boost that enables greater data-transmission distances and the ability to integrate multiple sensors onto a single RFID tag. “In the future, there could be billions of sensors all around us. With that scale, you’ll need a lot of batteries that you’ll have to recharge constantly. But what if you could self-power them using the ambient light? You could deploy them and forget them for months or years at a time,” says Sai Nithin Kantareddy, a PhD student in the MIT Auto-ID Laboratory. “This work is basically building enhanced RFID tags using energy harvesters for a range of applications.” In a pair of papers published in the journals Advanced Functional Materials and IEEE Sensors, MIT Auto-ID Laboratory and MIT Photovoltaics Research Laboratory researchers describe using the sensors to continuously monitor indoor and outdoor temperatures over several days. The sensors transmitted data continuously at distances five times greater than traditional RFID tags — with no batteries required. Longer data-transmission ranges mean, among other things, that one reader can be used to collect data from multiple sensors simultaneously. Depending on certain factors in their environment, such as moisture and heat, the sensors can be left inside or outside for months or, potentially, years at a time before they degrade enough to require replacement. That can be valuable for any application requiring long-term sensing, indoors and outdoors, including tracking cargo in supply chains, monitoring soil, and monitoring the energy used by equipment in buildings and homes. Joining Kantareddy on the papers are: Department of Mechanical Engineering (MechE) postdoc Ian Mathews, researcher Shijing Sun, chemical engineering student Mariya Layurova, researcher Janak Thapa, researcher Ian Marius Peters, and Georgia Tech Professor Juan-Pablo Correa-Baena, who are all members of the Photovoltaics Research Laboratory; Rahul Bhattacharyya, a researcher in the AutoID Lab; Tonio Buonassisi, a professor in MechE; and Sanjay E. Sarma, the Fred Fort Flowers and Daniel Fort Flowers Professor of Mechanical Engineering. Combining two low-cost technologies  In recent attempts to create self-powered sensors, other researchers have used solar cells as energy sources for internet of things (IoT) devices. But those are basically shrunken-down versions of traditional solar cells — not perovskite. The traditional cells can be efficient, long-lasting, and powerful under certain conditions “but are really infeasible for ubiquitous IoT sensors,” Kantareddy says. Traditional solar cells, for instance, are bulky and expensive to manufacture, plus they are inflexible and cannot be made transparent, which can be useful for temperature-monitoring sensors placed on windows and car windshields. They’re also really only designed to efficiently harvest energy from powerful sunlight, not low indoor light. Perovskite cells, on the other hand, can be printed using easy roll-to-roll manufacturing techniques for a few cents each; made thin, flexible, and transparent; and tuned to harvest energy from any kind of indoor and outdoor lighting. The idea, then, was combining a low-cost power source with low-cost RFID tags, which are battery-free stickers used to monitor billions of products worldwide. The stickers are equipped with tiny, ultra-high-frequency antennas that each cost around three to five cents to make. RFID tags rely on a communication technique called “backscatter,” that transmits data by reflecting modulated wireless signals off the tag and back to a reader. A wireless device called a reader — basically similar to a Wi-Fi router — pings the tag, which powers up and backscatters a unique signal containing information about the product it’s stuck to. Traditionally, the tags harvest a little of the radio-frequency energy sent by the reader to power up a little chip inside that stores data, and uses the remaining energy to modulate the returning signal. But that amounts to only a few microwatts of power, which limits their communication range to less than a meter. The researchers’ sensor consists of an RFID tag built on a plastic substrate. Directly connected to an integrated circuit on the tag is an array of perovskite solar cells. As with traditional systems, a reader sweeps the room, and each tag responds. But instead of using energy from the reader, it draws harvested energy from the perovskite cell to power up its circuit and send data by backscattering RF signals. Efficiency at scale The key innovations are in the customized cells. They’re fabricated in layers, with perovskite material sandwiched between an electrode, cathode, and special electron-transport layer materials. This achieved about 10 percent efficiency, which is fairly high for still-experimental perovskite cells. This layering structure also enabled the researchers to tune each cell for its optimal “bandgap,” which is an electron-moving property that dictates a cell’s performance in different lighting conditions. They then combined the cells into modules of four cells. In the Advanced Functional Materials paper, the modules generated 4.3 volts of electricity under one sun illumination, which is a standard measurement for how much voltage solar cells produce under sunlight. That’s enough to power up a circuit — about 1.5 volts — and send data around 5 meters every few seconds. The modules had similar performances in indoor lighting. The IEEE Sensors paper primarily demonstrated wide‐bandgap perovskite cells for indoor applications that achieved between 18.5 percent and 21. 4 percent efficiencies under indoor fluorescent lighting, depending on how much voltage they generate. Essentially, about 45 minutes of any light source will power the sensors indoors and outdoors for about three hours.   The RFID circuit was prototyped to only monitor temperature. Next, the researchers aim to scale up and add more environmental-monitoring sensors to the mix, such as humidity, pressure, vibration, and pollution. Deployed at scale, the sensors could especially aid in long-term data-collection indoors to help build, say, algorithms that help make smart buildings more energy efficient. “The perovskite materials we use have incredible potential as effective indoor-light harvesters. Our next step is to integrate these same technologies using printed electronics methods, potentially enabling extremely low-cost manufacturing of wireless sensors,"" Mathews says. ",Electronics and Technology,0.13628198877539746
45,IEEE,Forget Moore’s Law—Chipmakers Are More Worried About Heat and Power Issues,Electronics and Technology,2019-10-11,-,https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/power-problems-might-drive-chip-specialization,"      Power consumption and heat generation: these hurdles impede progress toward faster, cheaper chips, and are worrying semiconductor industry veterans far more than the slowing of Moore’s Law. That was the takeaway from several discussions about current and future chip technologies held in Silicon Valley this week. John Hennessy—president emeritus of Stanford University, Google chairman, and MIPS Computer Systems founder—says Moore’s Law “was an ambition, a goal. It wasn’t a law; it was something to shoot for.”  “It is definitely slowing down,” he says, “but to say it’s dead is premature.” That slowing, at this point, isn’t his biggest concern. The real problem, Hennessy says, is the failure of Dennard scaling, an observation that as transistors get smaller and circuits become faster, a chip’s power consumption stays the same. “Who would have thought,” he says, “that microprocessors would have to slow down clock speeds or turn off cores to keep from burning up?” Hennessy spoke as part of a panel at a Churchill Club forum held Monday in Menlo Park. The power consumption of microprocessors was also a hot topic at Arm TechCon 2019 on Tuesday, with Sha Rabii, Facebook’s head of silicon and technology engineering, indicating that the energy used by microprocessors and the heat that chips give off is a major roadblock on the road to augmented reality glasses. How to tackle the problem? The key, several industry veterans suggested, may be specialization. “Either we keep going on [an] evolutionary path, with faster CPUs and everything happening in software, or we look at it as a systems problem and think about what we would do differently” if the industry were designing from the ground up, said Navin Chaddha, managing director of the Mayfield Fund, speaking at the Churchill Club event. “I believe the world is moving to specialization,” Chaddha says, instead of focusing on doubling raw processing power every 18 to 24 months. The recent rush of startups producing processors designed to do deep learning—such as Cerebras Systems, Mythic, and Syntiant—are examples of this kind of thinking. But there may be limits to just how much specialization can help.   Arm Holdings CEO Simon Segars would seem to agree with Chaddha. Segars kicked off Arm TechCon by announcing that the company will, for the first time, allow developers to insert custom instructions into the core of Arm chips, allowing more efficient processing of algorithms. This capability will permit at least some degree of specialization, which is a big departure for a company that has always focused on standardized products.   IEEE Spectrum’s blog featuring the people, places, and passions of the world of technologists in Silicon Valley and its environs. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Computer Science,0.1359046030505886
46,ACM,International Team Uses Deep Learning to Create Virtual ‘Super Instrument’,ACM,2019-10-07,-,https://www.swri.org/press-release/deep-learning-virtual-super-instrument-algorithm,"    International Team Uses Deep Learning to Create Virtual ‘Super Instrument’     Southwest Research InstituteOctober 7, 2019   Researchers at the Southwest Research Institute (SwRI) and nine other institutions, in collaboration with the National Aeronautics and Space Administration's Frontier Development Laboratory, have developed a deep learning algorithm that analyzes ultraviolet images of the Sun and measures the energy the Sun emits as ultraviolet light. The algorithm is already in use as part of a Frontier Development Laboratory project for forecasting ionospheric disturbances. Said SwRI's Andrés Muñoz-Jaramillo, ""In essence, deep learning involves sophisticated transformation of data. We can make these transformations into scientifically useful data and modernize the way we view not just the Sun, but a great number of scientific questions.""                 ",Computer Science,0.13331487346069568
47,Science Daily,Graphene Turns 15 on Track to Deliver on Its Promises,Society,2019-10-04,-,https://www.sciencedaily.com/releases/2019/10/191004105625.htm,"   In a world dominated by the immediacy of social media and digital technologies, it is hard to take a step back and think about how long materials take to develop. The silicon transistor, at the heart of all our beloved gadgets, was engineered in 1958. However, scientists had known of silicon for over 120 years -- it was discovered in 1824. Although expecting broad market penetration for graphene today would not be realistic, the truth is that one can already find graphene-enabled products on the market. A number of these commercial applications have been enabled by the Graphene Flagship, a project funded by the European Commission that kicked off in 2013. Bringing together nearly 150 partners from 23 countries, it created the perfect breeding ground for innovation, which could not emerge without an intricate web of collaborations between academics, researchers, and industries. The Graphene Flagship also acted as inspiration for many programmes on graphene and related layered materials in many other countries. The Graphene Flagship expects short-term applications in the materials sector, with graphene-enabled inks, composites, and coatings, for applications ranging from food packaging to textiles and sports goods. In the mid-term, graphene could be crucial for the energy sector, and market analyses agree on a high potential for graphene-enabled batteries and supercapacitors. With the first graphene-enabled solar farm to be installed in Crete next year, the Graphene Flagship will showcase how graphene can enable more sustainable energy generation, in line with Europe's commitment to renewable energies. A host of applications for graphene are expected to hit the market 10 to 15 years from now. These are related to (opto)electronics, where graphene can deliver performances orders of magnitude higher than current technologies. The developments in this area could trigger the next-generation of (opto)electronic devices, bringing the 'more-than-Moore' devices to reality. To secure its most valuable strength -- bridging the gap between basic and applied research -- the Graphene Flagship has also announced the creation of the first graphene foundry. With a budget of almost €20 million over four years, this experimental pilot line will pave the way towards commercially competitive graphene products, such as transceivers, photodetectors, and sensors. The Graphene Flagship foundry will also develop a process design kit: a set of 'instructions' to support product tape-out and guarantee that the finalised designs are high-quality and consistent. The foundry will be accessible by academia and industry stakeholders worldwide. Kari Hjelt, Graphene Flagship Head of Innovation stated: ""We are now seeing the first wave of graphene-enabled products on the market. The commercialisation activities of graphene are moving from materials development towards components and system level integration. In the future we will see a growing number of high-value add products for various application domains."" Thomas Reiss, Graphene Flagship Work Package Leader for Industrialisation, adds: ""Key factors facilitating the further commercialisation of graphene comprise establishing innovation ecosystems and providing holistic innovation support. This includes elaborating innovation roadmaps and creating trust and confidence in graphene among industry by trusted validation and standardization services."" Andrea C. Ferrari, Graphene Flagship Science and Technology Officer and Chair of its Management Panel, concludes: ""Graphene and related materials are progressing towards commercialization at the expected pace. The Graphene Flagship is not about hype, but about concrete and tangible results and progress. The Flagship Foundry will strengthen the EU position as world leader and pioneer in graphene technology and facilitate incorporation of graphene devices in various industries."" ",Electronics and Technology,0.13323085072583374
48,MIT News,Engineers put Leonardo da Vinci’s bridge design to the test,Research,2019-10-09,-,http://news.mit.edu/2019/leonardo-da-vinci-bridge-test-1010,"  In 1502 A.D., Sultan Bayezid II sent out the Renaissance equivalent of a government RFP (request for proposals), seeking a design for a bridge to connect Istanbul with its neighbor city Galata. Leonardo da Vinci, already a well-known artist and inventor, came up with a novel bridge design that he described in a letter to the Sultan and sketched in a small drawing in his notebook. He didn’t get the job. But 500 years after his death, the design for what would have been the world’s longest bridge span of its time intrigued researchers at MIT, who wondered how thought-through Leonardo’s concept was and whether it really would have worked. Spoiler alert: Leonardo knew what he was doing. To study the question, recent graduate student Karly Bast MEng ’19, working with professor of architecture and of civil and environmental engineering John Ochsendorf and undergraduate Michelle Xie, tackled the problem by analyzing the available documents, the possible materials and construction methods that were available at the time, and the geological conditions at the proposed site, which was a river estuary called the Golden Horn. Ultimately, the team built a detailed scale model to test the structure’s ability to stand and support weight, and even to withstand settlement of its foundations. The results of the study were presented in Barcelona this week at the conference of the International Association for Shell and Spatial Structures. They will also be featured in a talk at Draper in Cambridge, Massachusetts, later this month and in an episode of the PBS program NOVA, set to air on Nov. 13. A flattened arch In Leonardo’s time, most masonry bridge supports were made in the form of conventional semicircular arches, which would have required 10 or more piers along the span to support such a long bridge. Leonardo’s bridge concept was dramatically different — a flattened arch that would be tall enough to allow a sailboat to pass underneath with its mast in place, as illustrated in his sketch, but that would cross the wide span with a single enormous arch. The bridge would have been about 280 meters long (though Leonardo himself was using a different measurement system, since the metric system was still a few centuries off), making it the longest span in the world at that time, had it been built. “It’s incredibly ambitious,” Bast says. “It was about 10 times longer than typical bridges of that time.” The design also featured an unusual way of stabilizing the span against lateral motions — something that has resulted in the collapse of many bridges over the centuries. To combat that, Leonardo proposed abutments that splayed outward on either side, like a standing subway rider widening her stance to balance in a swaying car. In his notebooks and letter to the Sultan, Leonardo provided no details about the materials that would be used or the method of construction. Bast and the team analyzed the materials available at the time and concluded that the bridge could only have been made of stone, because wood or brick could not have carried the loads of such a long span. And they concluded that, as in classical masonry bridges such as those built by the Romans, the bridge would stand on its own under the force of gravity, without any fasteners or mortar to hold the stone together. To prove that, they had to build a model and demonstrate its stability. That required figuring out how to slice up the complex shape into individual blocks that could be assembled into the final structure. While the full-scale bridge would have been made up of thousands of stone blocks, they decided on a design with 126 blocks for their model, which was built at a scale of 1 to 500 (making it about 32 inches long). Then the individual blocks were made on a 3D printer, taking about six hours per block to produce. “It was time-consuming, but 3D printing allowed us to accurately recreate this very complex geometry,” Bast says. Testing the design’s feasibility This is not the first attempt to reproduce Leonardo’s basic bridge design in physical form. Others, including a pedestrian bridge in Norway, have been inspired by his design, but in that case modern materials — steel and concrete — were used, so that construction provided no information about the practicality of Leonardo’s engineering. “That was not a test to see if his design would work with the technology from his time,” Bast says. But because of the nature of gravity-supported masonry, the faithful scale model, albeit made of a different material, would provide such a test. “It’s all held together by compression only,” she says. “We wanted to really show that the forces are all being transferred within the structure,” which is key to ensuring that the bridge would stand solidly and not topple. As with actual masonry arch bridge construction, the “stones” were supported by a scaffolding structure as they were assembled, and only after they were all in place could the scaffolding be removed to allow the structure to support itself. Then it came time to insert the final piece in the structure, the keystone at the very top of the arch. “When we put it in, we had to squeeze it in. That was the critical moment when we first put the bridge together. I had a lot of doubts” as to whether it would all work, Bast recalls. But “when I put the keystone in, I thought, ‘this is going to work.’ And after that, we took the scaffolding out, and it stood up.” “It’s the power of geometry” that makes it work, she says. “This is a strong concept. It was well thought out.” Score another victory for Leonardo. “Was this sketch just freehanded, something he did in 50 seconds, or is it something he really sat down and thought deeply about? It’s difficult to know” from the available historical material, she says. But proving the effectiveness of the design suggests that Leonardo really did work it out carefully and thoughtfully, she says. “He knew how the physical world works.” He also apparently understood that the region was prone to earthquakes, and incorporated features such as the spread footings that would provide extra stability. To test the structure’s resilience, Bast and Xie built the bridge on two movable platforms and then moved one away from the other to simulate the foundation movements that might result from weak soil. The bridge showed resilience to the horizontal movement, only deforming slightly until being stretched to the point of complete collapse. The design may not have practical implications for modern bridge designers, Bast says, since today’s materials and methods provide many more options for lighter, stronger designs. But the proof of the feasibility of this design sheds more light on what ambitious construction projects might have been possible using only the materials and methods of the early Renaissance. And it once again underscores the brilliance of one of the world’s most prolific inventors. It also demonstrates, Bast says, that “you don’t necessarily need fancy technology to come up with the best ideas.” ",Computer Science,0.13223612738447113
49,Science Daily,Quantum Paradox Experiment May Lead to More Accurate Clocks and Sensors,Computers & Math,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092242.htm,"   University of Queensland physicist Dr Magdalena Zych said the international collaboration aimed to test Einstein's twin paradox using quantum particles in a 'superposition' state. ""The twin paradox is one of the most counterintuitive predictions of relativity theory,"" Dr Zych said. ""It says that time can pass at different speeds for people at different distances to an enormous mass or travelling with different velocities. ""For example, relative to a reference clock far from any massive object, a clock closer to a mass or moving at high speed will tick slower. ""This creates a 'twin paradox', where one of a pair of twins departs on a fast-speed journey while the other stays behind.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""When the twins reunite, the travelling twin would be much younger, as different amounts of time have passed for each of them. ""It's a mind-blowing effect -- featured in popular movies like Interstellar -- but it's also been verified by real world experiments, and is even taken into consideration in order for everyday GPS technology to work."" The team included researchers from the University of Ulm and Leibniz University Hannover and found how one could use advanced laser technology to realise a quantum version of the Einstein's twin paradox. In the quantum version, rather than twins there will be only one particle travelling in a quantum superposition. ""A quantum superposition means the particle is in two locations at the same time, in each of them with some probability, and yet this is different to placing the particle in one or the other location randomly,"" Dr Zych said.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""It's another way for an object to exist, only allowed by the laws of quantum physics. ""The idea is to put one particle in superposition on two trajectories with different speeds, and see if a different amount of time passes for each of them, as in the twin paradox. ""If our understanding of quantum theory and relativity is right, when the superposed trajectories meet, the quantum traveller will be in superposition of being older and younger than itself. ""This would leave an unmistakeable signature in the results of the experiment, and that's what we hope will be found when the experiment is realised in the future. ""It could lead to advanced technologies that will allow physicists to build more precise sensors and clocks -- potentially, a key part of future navigation systems, autonomous vehicles and earthquake early-warning networks."" The experiment itself will also answer some open questions in modern physics. ""A key example is, can time display quantum behaviour or is it fundamentally classical?"" Dr Zych said. ""This question is likely crucial for the 'holy grail' of theoretical physics: finding a joint theory of quantum and gravitational phenomena. ""We're looking forward to helping answer this question, and tackling many more."" ",Space & Time,0.1314146619192788
50,MIT News,A new mathematical approach to understanding zeolites,Electronics and Technology,2019-10-07,-,http://news.mit.edu/2019/identify-zeolites-transform-1007,"  Zeolites are a class of natural or manufactured minerals with a sponge-like structure, riddled with tiny pores that make them useful as catalysts or ultrafine filters. But of the millions of zeolite compositions that are theoretically possible, so far only about 248 have ever been discovered or made. Now, research from MIT helps explain why only this small subset has been found, and could help scientists find or produce more zeolites with desired properties. The new findings are being reported this week in the journal Nature Materials, in a paper by MIT graduate students Daniel Schwalbe-Koda and Zach Jensen, and professors Elsa Olivetti and Rafael Gomez-Bombarelli. Previous attempts to figure out why only this small group of possible zeolite compositions has been identified, and to explain why certain types of zeolites can be transformed into specific other types, have failed to come up with a theory that matches the observed data. Now, the MIT team has developed a mathematical approach to describing the different molecular structures. The approach is based on graph theory, which can predict which pairs of zeolite types can be transformed from one to the other. This could be an important step toward finding ways of making zeolites tailored for specific purposes. It could also lead to new pathways for production, since it predicts certain transformations that have not been previously observed. And, it suggests the possibility of producing zeolites that have never been seen before, since some of the predicted pairings would lead to transformations into new types of zeolite structures. Interzeolite tranformations Zeolites are widely used today in applications as varied as catalyzing the “cracking” of petroleum in refineries and absorbing odors as components in cat litterbox filler. Even more applications may become possible if researchers can create new types of zeolites, for example with pore sizes suited to specific types of filtration. All kinds of zeolites are silicate minerals, similar in chemical composition to quartz. In fact, over geological timescales, they will all eventually turn into quartz — a much denser form of the mineral — explains Gomez-Bombarelli, who is the Toyota Assistant Professor in Materials Processing. But in the meantime, they are in a “metastable” form, which can sometimes be transformed into a different metastable form by applying heat or pressure or both. Some of these transformations are well-known and already used to produce desired zeolite varieties from more readily available natural forms. Currently, many zeolites are produced by using chemical compounds known as OSDAs (organic structure-directing agents), which provide a kind of template for their crystallization. But Gomez-Bombarelli says that if instead they can be produced through the transformation of another, readily available form of zeolite, “that’s really exciting. If we don’t need to use OSDAs, then it’s much cheaper [to produce the material].The organic material is pricey. Anything we can make to avoid the organics gets us closer to industrial-scale production.” Traditional chemical modeling of the structure of different zeolite compounds, researchers have found, provides no real clue to finding the pairs of zeolites that can readily transform from one to the other. Compounds that appear structurally similar sometimes are not subject to such transformations, and other pairs that are quite dissimilar turn out to easily interchange. To guide their research, the team used an artificial intelligence system previously developed by the Olivetti group to “read” more than 70,000 research papers on zeolites and select those that specifically identify interzeolite transformations. They then studied those pairs in detail to try to identify common characteristics. What they found was that a topological description based on graph theory, rather than traditional structural modeling, clearly identified the relevant pairings. These graph-based descriptions, based on the number and locations of chemical bonds in the solids rather than their actual physical arrangement, showed that all the known pairings had nearly identical graphs. No such identical graphs were found among pairs that were not subject to transformation. The finding revealed a few previously unknown pairings, some of which turned out to match with preliminary laboratory observations that had not previously been identified as such, thus helping to validate the new model. The system also was successful at predicting which forms of zeolites can intergrow — forming combinations of two types that are interleaved like the fingers on two clasped hands. Such combinations are also commercially useful, for example for sequential catalysis steps using different zeolite materials. Ripe for further research   The new findings might also help explain why many of the theoretically possible zeolite formations don’t seem to actually exist. Since some forms readily transform into others, it may be that some of them transform so quickly that they are never observed on their own. Screening using the graph-based approach may reveal some of these unknown pairings and show why those short-lived forms are not seen. Some zeolites, according to the graph model, “have no hypothetical partners with the same graph, so it doesn’t make sense to try to transform them, but some have thousands of partners” and thus are ripe for further research, Gomez-Bombarelli says. In principle, the new findings could lead to the development of a variety of new catalysts, tuned to the exact chemical reactions they are intended to promote. Gomez-Bombarelli says that almost any desired reaction could hypothetically find an appropriate zeolite material to promote it. “Experimentalists are very excited to find a language to describe their transformations that is predictive,” he says. This work is “a major advancement in the understanding of interzeolite transformations, which has become an increasingly important topic owing to the potential for using these processes to improve the efficiency and economics of commercial zeolite production,” says Jeffrey Rimer, an associate professor of chemical and biomolecular engineering at the University of Houston, who was not involved in this research. Manuel Moliner, a tenured scientist at the Technical University of Valencia, in Spain, who also was not connected to this research, says: “Understanding the pairs involved in particular interzeolite transformations, considering not only known zeolites but also hundreds of hypothetical zeolites that have not ever been synthesized, opens extraordinary practical opportunities to rationalize and direct the synthesis of target zeolites with potential interest as industrial catalysts.” This research was supported, in part, by the National Science Foundation and the Office of Naval Research. ",Electronics and Technology,0.13113715384611455
51,MIT News,Computing and the search for new planets,Space & Time,2019-09-23,-,http://news.mit.edu/2019/computing-and-search-for-new-planets-tess-mit-0923,"  When MIT launched the MIT Stephen A. Schwarzman College of Computing this fall, one of the goals was to drive further innovation in computing across all of MIT’s schools. Researchers are already expanding beyond traditional applications of computer science and using these techniques to advance a range of scientific fields, from cancer medicine to anthropology to design — and to the discovery of new planets. Computation has already proven useful for the Transiting Exoplanet Survey Satellite (TESS), a NASA-funded mission led by MIT. Launched from Cape Canaveral in April 2018, TESS is a satellite that takes images of the sky as it orbits the Earth. These images can help researchers find planets orbiting stars beyond our sun, called exoplanets. This work, which is now halfway complete, will reveal more about the other planets within what NASA calls our “solar neighborhood.”  “TESS just completed the first of its two-year prime mission, surveying the southern night sky,” says Sara Seager, an astrophysicist and planetary scientist at MIT and deputy director of science for TESS. “TESS found over 1,000 planet candidates and about 20 confirmed planets, some in multiple-planet systems.” While TESS has enabled some impressive discoveries so far, finding these exoplanets is no simple task. TESS is collecting images of more than 200,000 distant stars, saving an image of these planets every two minutes, as well as saving an image of a large swath of sky every 30 minutes. Seager says every two weeks, which is how long it takes the satellite to orbit the Earth, TESS sends about 350 gigabytes of data (once uncompressed) to Earth. While Seager says this is not as much data as people might expect (a 2019 Macbook Pro has up to 512 gigabytes of storage), analyzing the data involves taking many complex factors into consideration. Seager, who says she has long been interested in how computation can be used as a tool for science, began discussing the project with Victor Pankratius, a former principal research scientist in MIT’s Kavli Institute for Astrophysics and Space Research, who is now the director and head of global software engineering at Bosch Sensortec. A trained computer scientist, Pankratius says that after arriving at MIT in 2013, he started thinking about scientific fields that produce big data, but that have not yet fully benefited from computing techniques. After speaking with astronomers like Seager, he learned more about the data their instruments collect and became interested in applying computer-aided discovery techniques to the search for exoplanets. “The universe is a big place,” Pankratius says. “So I think leveraging what we have on the computer science side is a great thing.”  The basic idea underlying TESS’ mission is that like our own solar system, in which the Earth and other planets revolve around a central star (the sun), there are other planets beyond our solar system revolving around different stars. The images TESS collects produce light curves — data that show how the brightness of the star changes over time. Researchers are analyzing these light curves to find drops in brightness, which could indicate that a planet is passing in front of the star and temporarily blocking some of its light.  “Every time a planet orbits, you would see this brightness go down,” Pankratius says. “It's almost like a heartbeat.”  The trouble is that not every dip in brightness is necessarily caused by a passing planet. Seager says machine learning currently comes into play during the “triage” phase of their TESS data analysis, helping them distinguish between potential planets and other things that could cause dips in brightness, like variable stars, which naturally vary in their brightness, or instrument noise. Analysis on planets that pass through triage is still done by scientists who have learned how to “read” light curves. But the team is now using thousands of light curves that have been classified by eye to teach neural networks how to identify exoplanet transits. Computation is helping them narrow down which light curves they should examine in more detail. Liang Yu PhD ’19, a recent physics graduate, built upon an existing code to write the machine learning tool that the team is now using. While helpful for homing in on the most relevant data, Seager says machine learning cannot yet be used to simply find exoplanets. “We still have a lot of work to do,” she says. Pankratius agrees. “What we want to do is basically create computer-aided discovery systems that do this for all [stars] all the time,” he says. “You want to just press a button and say, show me everything. But right now it's still people with some automation vetting all of these light curves.” Seager and Pankratius also co-taught a course that focused on various aspects of computation and artificial intelligence (AI) development in planetary science. Seager says inspiration for the course arose from a growing interest from students to learn about AI and its applications to cutting-edge data science. In 2018, the course allowed students to use actual data collected by TESS to explore machine learning applications for this data. Modeled after another course Seager and Pankratius taught, students in the course were able to choose a scientific problem and learn the computation skills to solve that problem. In this case, students learned about AI techniques and applications to TESS. Seager says students had a great response to the unique class.  “As a student, you could actually make a discovery,” Pankratius says. “You can build a machine learning algorithm, run it on this data, and who knows, maybe you will find something new.” Much of the data TESS collects is also readily available as part of a larger citizen science project. Pankratius says anyone with the right tools could start making discoveries of their own. Thanks to cloud connectivity, this is even possible on a cell phone.  “If you get bored on your bus ride home, why not search for planets?” he says. Pankratius says this type of collaborative work allows experts in each domain to share their knowledge and learn from each other, rather than each trying to get caught up in the other’s field.   “Over time, science has become more specialized, so we need ways to integrate the specialists better,” Pankratius says. The college of computing could help forge more such collaborations, he adds. Pankratius also says it could attract researchers who work at the intersection of these disciplines, who can bridge gaps in understanding between experts. This type of work integrating computer science is already becoming increasingly common across scientific fields, Seager notes. “Machine learning is ‘in vogue’ right now,” she says.  Pankratius says that is in part because there is more evidence that leveraging computer science techniques is an effective way to address various types of problems and growing data sets. “We now have demonstrations in different areas that the computer-aided discovery approach doesn’t just work,” Pankratius says. “It actually leads to new discoveries.” ",Space & Time,0.12993678883409154
52,Science Daily,Controlling Superconducting Regions Within an Exotic Metal,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011074717.htm,"   EPFL's Laboratory of Quantum Materials (QMAT), headed by Philip Moll, has been working on a specific group of unconventional superconductors known as heavy fermion materials. The QMAT scientists, as part of a broad international collaboration between EPFL, the Max Planck Institute for Chemical Physics of Solids, the Los Alamos National Laboratory and Cornell University, made a surprising discovery about one of these materials, CeIrIn5. CeIrIn5 is a metal that superconducts at a very low temperature, only 0.4°C above absolute zero (around -273°C). The QMAT scientists, together with Katja C. Nowack from Cornell University, have now shown that this material could be produced with superconducting regions coexisting alongside regions in a normal metallic state. Better still, they produced a model that allows researchers to design complex conducting patterns and, by varying the temperature, to distribute them within the material in a highly controlled way. Their research has just been published in Science. To achieve this feat, the scientists sliced very thin layers of CeIrIn5 -- only around a thousandth of a millimeter thick -- that they joined to a sapphire substrate. When cooled, the material contracts significantly whereas the sapphire contracts very little. The resulting interaction puts stress on the material, as if it were being pulled in all directions, thus slightly distorting the atomic bonds in the slice. As the superconductivity in CeIrIn5 is unusually sensitive to the material's exact atomic configuration, engineering a distortion pattern is all it takes to achieve a complex pattern of superconductivity. This new approach allows researchers to ""draw"" superconducting circuitry on a single crystal bar, a step that paves the way for new quantum technologies. This discovery represents a major step forward in controlling superconductivity in heavy fermion materials. But that's not the end of the story. Following on from this project, a post-doc researcher has just begun exploring possible technological applications. ""We could, for example, change the regions of superconductivity by modifying the material's distortion using a microactuator,"" says Moll. ""The ability to isolate and connect superconducting regions on a chip could also create a kind of switch for future quantum technologies, a little like the transistors used in today's computing."" ",Electronics and Technology,0.12960892532849178
53,MIT News,Meet the 2019-20 MLK Visiting Professors and Scholars,Computer Science,2019-10-08,-,http://news.mit.edu/2019/dr-martin-luther-king-jr-visiting-professors-and-scholars-program-1008,"  Founded in 1990, the Martin Luther King Jr. (MLK) Visiting Professors and Scholars Program honors the life and legacy of Martin Luther King by increasing the presence of, and recognizing the contributions of, underrepresented minority scholars at MIT. MLK Visiting Professors and Scholars enhance their scholarship through intellectual engagement with the MIT community and enrich the cultural, academic, and professional experience of students. The program hosts between four and eight scholars each year with financial and institutional support from the Office of the Provost and oversight from the Institute Community and Equity Office. Six scholars are visiting MIT this academic year as part of the program. Kasso Okoudjou is returning for a second year as an MLK Visiting Professor in the Department of Mathematics. Originally from Benin, he moved to the United States in 1998 and earned a PhD in mathematics from Georgia Tech. Okoudjou joins MIT from the University of Maryland College Park, where he is a professor. His research interests include applied and pure harmonic analysis, especially time-frequency and time-scale analysis; frame theory; and analysis and differential equations on fractals. He is interested in broadening the participation of underrepresented minorities in (undergraduate) research in the mathematical sciences. Matthew Schumaker joins MIT for another year in the Music and Theater Arts Section within the School of Humanities, Arts, and Social Sciences. Schumaker received his doctorate in music composition from the University of California at Berkeley. At MIT, he teaches a new course, 21M.380 (Composing for Solo Instrument and Live Electronics), a hands-on music technology composition seminar combining instrumental writing with real-time computer music. Additionally, The Radius Ensemble in Cambridge, Massachusetts has commissioned Schumaker to write a new piece of music that seeks to translate into music the vibrant, curved gestures and slashed markings in the abstract landscapes of celebrated Ethiopian-born painter Julie Mehretu. Jamie Macbeth is visiting from Smith College, where he is an assistant professor in computer science. He received his PhD in computer science from University of California at Los Angeles. Although this is his first year as an MLK Visiting Scholar, he is not new to MIT, since he has been a visiting scientist since 2017. He is hosted by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). Macbeth’s research is focused on building and studying intelligent computing systems that demonstrate a human-like capability for in-depth understanding and production of natural language, and thus can achieve richer interactions with human users. He is especially keen on building systems that decompose the meaning of language into complex conceptual structures that reflect humans’ embodied cognition, memory, imagery and knowledge about social situations. Ben McDonald has been a postdoc in the Department of Chemistry since 2018 and is now an MLK Visiting Scholar. McDonald received his PhD in synthetic organic chemistry from Northwestern University. His research focused on the total synthesis of flavonolignan natural products and the development of reverse-polarity carbon-carbon bond forming reactions. As a member of the department’s Chemistry Alliance for Inclusion and Diversity, he is focused on advancing diversity, equity and inclusion efforts. One of the initiatives he seeks to establish is a summer research program, which recruits talented future scientists from underrepresented backgrounds. Tina Opie is an associate professor in the Management Division at Babson College. Opie obtained her PhD in management (with a concentration in organizational behavior) from New York University’s Stern School of Business. As an MLK Visiting Scholar in MIT Sloan School of Management, along with access to MIT’s Behavioral Research Lab, she is conducting research to develop the construct of Shared Sisterhood. “Shared Sisterhood examines how high-quality relationships (e.g., relationships characterized by trust, emotional vulnerability) between black, white, and Latinx women at work facilitate workplace inclusion and equity.” Though her work has a specific focus, people of all genders and racioethnic backgrounds can be “sisters” and can contribute to fostering a more inclusive work environment. Opie established Opie Consulting Group, a diversity-and-inclusion consultancy that incorporates Shared Sisterhood in creating inclusive workplaces.   Rhonda Williams, an MLK Visiting Professor hosted by the Department of History, joins MIT from Vanderbilt University, where she was recently appointed the John L. Seigenthaler Chair in American History. She is the founder of the Social Justice Institute at Case Western Reserve University. Her essay titled “Black Women Who Educate for Justice and Put Their Time, Lives, and Spirits on the Line"" was recently published in ""Black Women And Social Justice Education: Legacies and Lessons"" (2019, SUNY Press). On Oct. 25, Williams will deliver a social justice-related performance-lecture called “The Things That Divide Us: Meditations” at MIT. In spring 2020, she will facilitate a social justice workshop for students, faculty and staff. For more information about our scholars and the program, visit mlkscholars.mit.edu. ",Computer Science,0.12670888132014183
54,MIT News,Funding for sustainable concrete cemented for five more years,Computer Science,2019-10-04,-,http://news.mit.edu/2019/renewed-funding-sustainable-concrete-1004,"  The MIT Concrete Sustainability Hub (CSHub), an interdisciplinary team of researchers dedicated to concrete and infrastructure science, engineering, and economics, has renewed its relationship with its industry partners for another five years. Founded in 2009, CSHub has spent a decade over two five-year phases collaborating with the Portland Cement Association (PCA) and the Ready Mixed Concrete Research & Education Foundation (RMC) to achieve durable and sustainable buildings and infrastructure in ever-more-demanding environments. Over its next five-year phase, CSHub will receive $10 million of additional funding from its partners to continue its research efforts. “Taking CSHub’s work to the next level will not only help us achieve our goal of making concrete more sustainable, but will also continue to strengthen our communities by providing designers, owners, and policymakers with the best information and tools available to make the best choices for their construction projects,” says Julia Garbini, the executive director of RMC. According to Michael Ireland, PCA president and CEO, CSHub’s past research has also allowed the industry to investigate the unique properties of concrete and cement. “For 10 years and counting, the MIT CSHub has helped the cement and concrete industry to identify and study the myriad benefits of its products,” he says. Concrete, the world’s most-used building material, is made by mixing cement with abundant aggregate materials like sand and gravel. The result is an extremely strong and stiff material that can be produced nearly anywhere from readily available ingredients using relatively inexperienced labor. Concrete also offers numerous properties such as durability, formability, and thermal mass that can reduce energy consumption. “On a per-unit-weight basis, concrete is a low environmental impact material,” says Jeremy Gregory, CSHub’s executive director. “It’s essential to our built environment due to its durability, strength, and affordability. As a consequence, it’s the most-used building material in the world and hence, there is a significant opportunity to look at how we balance both its role in sustainable development and lower its environmental impact.” To do this, CSHub has taken a bottom-up approach, studying concrete from its nanoscale to its application in pavements and buildings, all the way to its role in urban environments and broader economic systems. “Classical concrete science and structural engineering often use top-down approaches,” says CSHub Faculty Director and MIT Professor Franz-Josef Ulm. “You identify weaknesses at a large scale, go to a smaller scale, make a change, and then observe the response. It is different when you go from the bottom-up — you have all of the possibilities in front of you.” Over the past decade, CSHub researchers have used this bottom-up approach to develop tools that measure the costs, environmental impacts, and hazard resilience of infrastructure and construction projects. In 2018, they developed the Break-Even Mitigation Percentage dashboard to provide developers with data on the costs of hazard mitigation. The dashboard shows the return on investment for hazard-resistant construction. In some communities, researchers found that that return can come as early as two years. Their investigation into the life cycle of buildings has also led to the creation of the Building Attribute to Impact Algorithm (BAIA), which informs designers of which aspects of a building will have the strongest impact on its life cycle cost and environmental impact. Researchers have applied these same life cycle perspectives to pavements. A case study conducted with North Carolina’s Department of Transportation highlighted actions that could reduce spending on pavements by tens of millions of dollars while meeting or exceeding performance and emissions targets. Recent CSHub materials science research has also informed the discovery of novel solutions to longstanding durability issues in concrete. In particular, researchers identified new explanations for two major causes of damage in concrete — freeze-thaw cycles and alkali-silica reaction. “Whenever you touch old problems, there are perceptions that they are very difficult to change,” says Ulm. “However, here we applied a bottom-up approach to an old problem and found solutions that have not been looked at before.” In the next phase of collaboration, CSHub will expand its scope to investigate concrete’s role in solving economic, environmental, and social challenges. “We have done a lot of work in the past two phases on the technical aspects of concrete,” says Gregory. “What we are trying to do in this next phase is to conduct research that will engage the broader public by leveraging crowdsourced data, artificial intelligence, and the latest tools of data science.” One Phase III project is already in development. Using their past work on pavements, CSHub researchers have created Carbin, an app that uses a smartphone to record pavement quality from within a moving vehicle. Through crowdsourcing, the app has recorded data on over 130,000 miles of roads across the world. The data will eventually support decisions on infrastructure maintenance at a far lower cost than that of traditional technologies, like laser scanning. “With the CSHub now entering its third phase, we are excited about the opportunities this close industry-academia collaboration brings to MIT, the concrete industry, and society at large,” says Markus Buehler, Jerry McAfee Professor in Engineering and MIT Department of Civil and Environmental Engineering head. “Applying cutting-edge fundamental research to problems in industry has the potential for large-scale impact.” ",Computer Science,0.12648163639238114
55,MIT News,New capsule can orally deliver drugs that usually have to be injected,Research,2019-10-07,-,http://news.mit.edu/2019/orally-deliver-drugs-injected-1007,"  Many drugs, especially those made of proteins, cannot be taken orally because they are broken down in the gastrointestinal tract before they can take effect. One example is insulin, which patients with diabetes have to inject daily or even more frequently. In hopes of coming up with an alternative to those injections, MIT engineers, working with scientists from Novo Nordisk, have designed a new drug capsule that can carry insulin or other protein drugs and protect them from the harsh environment of the gastrointestinal tract. When the capsule reaches the small intestine, it breaks down to reveal dissolvable microneedles that attach to the intestinal wall and release drug for uptake into the bloodstream. “We are really pleased with the latest results of the new oral delivery device our lab members have developed with our collaborators, and we look forward to hopefully seeing it help people with diabetes and others in the future,” says Robert Langer, the David H. Koch Institute Professor at MIT and a member of the Koch Institute for Integrative Cancer Research. In tests in pigs, the researchers showed that this capsule could load a comparable amount of insulin to that of an injection, enabling fast uptake into the bloodstream after the microneedles were released. Langer and Giovanni Traverso, an assistant professor in MIT’s Department of Mechanical Engineering and a gastroenterologist at Brigham and Women’s Hospital, are the senior authors of the study, which appears today in Nature Medicine. The lead authors of the paper are recent MIT PhD recipient Alex Abramson and former MIT postdoc Ester Caffarel-Salvador. Microneedle delivery Langer and Traverso have previously developed several novel strategies for oral delivery of drugs that usually have to be injected. Those efforts include a pill coated with many tiny needles, as well as star-shaped structures that unfold and can remain in the stomach from days to weeks while releasing drugs. “A lot of this work is motivated by the recognition that both patients and health care providers prefer the oral route of administration over the injectable one,” Traverso says. Earlier this year, they developed a blueberry-sized capsule containing a small needle made of compressed insulin. Upon reaching the stomach, the needle injects the drug into the stomach lining. In the new study, the researchers set out to develop a capsule that could inject its contents into the wall of the small intestine. Most drugs are absorbed through the small intestine, Traverso says, in part because of its extremely large surface area --- 250 square meters, or about the size of a tennis court. Also, Traverso noted that pain receptors are lacking in this part of the body, potentially enabling pain-free micro-injections in the small intestine for delivery of drugs like insulin. To allow their capsule to reach the small intestine and perform these micro-injections, the researchers coated it with a polymer that can survive the acidic environment of the stomach, which has a pH of 1.5 to 3.5. When the capsule reaches the small intestine, the higher pH (around 6) triggers it to break open, and three folded arms inside the capsule spring open. Each arm contains patches of 1-millimeter-long microneedles that can carry insulin or other drugs. When the arms unfold open, the force of their release allows the tiny microneedles to just penetrate the topmost layer of the small intestine tissue. After insertion, the needles dissolve and release the drug. “We performed numerous safety tests on animal and human tissue to ensure that the penetration event allowed for drug delivery without causing a full thickness perforation or any other serious adverse events,” Abramson says. To reduce the risk of blockage in the intestine, the researchers designed the arms so that they would break apart after the microneedle patches are applied. The new capsule represents an important step toward achieving oral delivery of protein drugs, which has been very difficult to do, says David Putnam, a professor of biomedical engineering and chemical and biomolecular engineering at Cornell University.   “It’s a compelling paper,” says Putnam, who was not involved in the study. “Delivering proteins is the holy grail of drug delivery. People have been trying to do it for decades.” Insulin demonstration In tests in pigs, the researchers showed that the 30-millimeter-long capsules could deliver doses of insulin effectively and generate an immediate blood-glucose-lowering response. They also showed that no blockages formed in the intestine and the arms were excreted safely after applying the microneedle patches. “We designed the arms such that they maintained sufficient strength to deliver the insulin microneedles to the small intestine wall, while still dissolving within several hours to prevent obstruction of the gastrointestinal tract,” Caffarel-Salvador says. Although the researchers used insulin to demonstrate the new system, they believe it could also be used to deliver other protein drugs such as hormones, enzymes, or antibodies, as well as RNA-based drugs. “We can deliver insulin, but we see applications for many other therapeutics and possibly vaccines,” Traverso says. “We’re working very closely with our collaborators to identify the next steps and applications where we can have the greatest impact.” The research was funded by Novo Nordisk and the National Institutes of Health. Other authors of the paper include Vance Soares, Daniel Minahan, Ryan Yu Tian, Xiaoya Lu, David Dellal, Yuan Gao, Soyoung Kim, Jacob Wainer, Joy Collins, Siddartha Tamang, Alison Hayward, Tadayuki Yoshitake, Hsiang-Chieh Lee, James Fujimoto, Johannes Fels, Morten Revsgaard Frederiksen, Ulrik Rahbek, and Niclas Roxhed. ",Electronics and Technology,0.12549281129548404
56,ACM,Doctors Look to Eye-Tracking to Improve Care,ACM,2019-10-10,-,https://www.wsj.com/articles/doctors-look-to-eye-tracking-to-improve-care-11570716001,"       Doctors Look to Eye-Tracking to Improve Care     The Wall Street JournalDavid M. EwaltOctober 10, 2019   Eye-tracking technology is increasingly being used by healthcare professionals for a variety of purposes, including teaching medical students, improving surgeries, and diagnosing eye-related issues such as nearsightedness. Companies like Sweden’s Tobii are selling technology that allows other companies to add eye-tracking capabilities to their own products. For example, a system made by Tobii client ControlRad employs eye-tracking hardware to direct x-ray scanners, lowering radiation dosage for patients and doctors during surgery. Israeli company NovaSight's vision-assessment system has patients wear liquid-crystal display eyewear to diagnose visual impairments. Gartner’s Werner Goertz anticipates more eye-tracking products will hit the market next year, as availability grows of low-cost hardware and safe, reliable technology.               *May Require Paid Registration  ",Computer Science,0.12476946614836149
57,MIT News,Oobleck’s weird behavior is now predictable,Research,2019-10-05,-,http://news.mit.edu/2019/oobleck-behavior-predict-cornstarch-1006,"  It’s a phenomenon many preschoolers know well: When you mix cornstarch and water, weird things happen. Swish it gently in a bowl, and the mixture sloshes around like a liquid. Squeeze it, and it starts to feel like paste. Roll it between your hands, and it solidifies into a rubbery ball. Try to hold that ball in the palm of your hand, and it will dribble away as a liquid. Most of us who have played with this stuff know it as “oobleck,” named after a sticky green goo in Dr. Seuss’ “Bartholomew and the Oobleck.” Scientists, on the other hand, refer to cornstarch and water as a “non-Newtonian fluid” — a material that appears thicker or thinner depending on how it is physically manipulated. Now MIT engineers have developed a mathematical model that predicts oobleck’s weird behavior. Using their model, the researchers accurately simulated how oobleck turns from a liquid to a solid and back again, under various conditions. Aside from predicting what the stuff might do in the hands of toddlers, the new model can be useful in predicting how oobleck and other solutions of ultrafine particles might behave for military and industrial applications. Could an oobleck-like substance fill highway potholes and temporarily harden as a car drives over it? Or perhaps the slurry could pad the lining of bulletproof vests, morphing briefly into an added shield against sudden impacts. With the team’s new oobleck model, designers and engineers can start to explore such possibilities. “It’s a simple material to make — you go to the grocery store, buy cornstarch, then turn on your faucet,” says Ken Kamrin, associate professor of mechanical engineering at MIT. “But it turns out the rules that govern how this material flows are very nuanced.” Kamrin, along with graduate student Aaron Baumgarten, have published their results today in the Proceedings of the National Academy of Sciences.         A clumpy model Kamrin’s primary work focuses on characterizing the flow of granular material such as sand. Over the years, he’s developed a mathematical model that accurately predicts the flow of dry grains under a number of different conditions and environments. When Baumgarten joined the group, the researchers started work on a model to describe how saturated wet sand moves. It was around this time that Kamrin and Baumgarten saw a scientific talk on oobleck. “We’d seen this talk, and we had a lengthy debate over what is oobleck, and how is it different from wet sand,” Kamrin says. “After some vigorous back and forth with Aaron, he decided to see if we could turn this wet sand model into one for oobleck.” Granular material in oobleck is much finer than sand: A single particle of cornstarch is about 1 to 10 microns wide and about one-hundredth the size of a grain of sand. Kamrin says particles at such a small scale experience effects that larger particles such as sand do not. For instance, because cornstarch particles are so small, they can be influenced by temperature, and by electric charges that build up between particles, causing them to slightly repel against each other. “As long as you squish slowly, the grains will repel, keeping a layer of fluid between them, and just slide past each other, like a fluid,” Kamrin says. “But if you do anything too fast, you’ll overcome that little repulsion, the particles will touch, there will be friction, and it’ll act as a solid.” This repulsion happening at the small scale brings out a key difference between large and ultrafine grain mixtures at the lab scale: The viscosity, or consistency of wet sand at a given packing density remains the same, whether you stir it gently or slam a fist into it. In contrast, oobleck has a low, liquid-like viscosity when slowly stirred. But if its surface is punched, a rapidly growing zone of the slurry adjacent to the contact point becomes more viscous, causing oobleck’s surface to bounce back and resist the impact, like a solid trampoline. They found that stress was the main factor in determining whether a material was more or less viscous. For instance, the faster and more forcefully oobleck is disturbed, the “clumpier” it is — that is, the more the underlying particles make frictional, as opposed to lubricated, contact. If it is slowly and gently deformed, oobleck is less viscous, with particles that are more evenly distributed and that repel against each other, like a liquid. The team looked to model the effect of repulsion of fine particles, with the idea that perhaps a new “clumpiness variable” could be added to their model of wet sand to make an accurate model of oobleck. In their model, they included mathematical terms to describe how this variable would grow and shrink under a certain stress or force. “Now we have a robust way of modeling how clumpy any chunk of the material in the body will be as you deform it in an arbitrary way,” Baumgarten says. Wheels spinning The researchers incorporated this new variable into their more general model for wet sand, and looked to see whether it would predict oobleck’s behavior. They used their model to simulate previous experiments by others, including a simple setup of oobleck being squeezed and sheared between two plates, and a set of experiments in which a small projectile is shot into a tank of oobleck at different speeds. In all scenarios, the simulations matched the experimental data and reproduced the motion of the oobleck, replicating the regions where it morphed from liquid to solid, and back again. To see how their model could predict oobleck’s behavior in more complex conditions, the team simulated a pronged wheel driving at different speeds over a deep bed of the slurry. They found the faster the wheel spun, the more the mixture formed what Baumgarten calls a “solidification front” in the oobleck, that momentarily supports the wheel so that it can roll across without sinking. Kamrin and Baumgarten say the new model can be used to explore how various ultrafine-particle solutions such as oobleck behave when put to use as, for instance, fillings for potholes, or bulletproof vests. They say the model could also help to identify ways to redirect slurries through systems such as industrial plants. “With industrial waste products, you could get fine particle suspensions that don’t flow the way you expect, and you have to move them from this vat to that vat, and there may be best practices that people don’t know yet, because there’s no model for it,” Kamrin says. “Maybe now there is.” This research was supported, in part, by the Army Research Office and the National Science Foundation. ",Electronics and Technology,0.12034842909769573
58,Science Daily,Why the Language-Ready Brain Is So Complex,Society,2019-10-03,-,https://www.sciencedaily.com/releases/2019/10/191003141149.htm,"   In the classical view, there are two major language areas in the left half of our brain. Broca's area (in the frontal lobe) is responsible for the production of language (speaking and writing), while Wernicke's area (in the temporal lobe) supports the comprehension of language (listening and reading). A large fibre tract (the arcuate fasciculus) connects these two 'perisylvian' areas (around the Sylvian fissure, the split which divides the two lobes). ""The classical view is largely wrong,"" says Hagoort. Language is infinitely more complex than speaking or understanding single words, which is what the classical model was based on. While words are among the elementary 'building blocks' of language, we also need 'operations' to combine words into structured sentences, such as 'the editor of the newspaper loved the article'. To understand and interpret such an utterance, knowing the speech sounds (or letters) and meaning of the individual words is not enough. For instance, we also need information about the context (who is the speaker?), the intonation (is the tone cynical?), and knowledge of the world (what does an editor do?). Multiple language areas In recent years neuroanatomists discovered that Broca's and Wernicke's regions actually contain multiple neuroanatomical areas. Also, newly discovered language areas extend beyond the classical areas, even into the parietal lobe, with more connections between these areas than previously thought. Moreover, the traditional areas are involved in language comprehension as well as production. Scientist also learned that other regions of the brain are more important for language than once thought, including the right hemisphere and the cerebellum. Interestingly, language areas also turn out to be somewhat variable. For instance, in people who are born blind, language can spread to the occipital lobe (or visual brain). Our brains process language with astonishing speed and 'immediacy', in a dynamic network of interacting brain areas. All the relevant information becomes available immediately, as we start combining the meanings of individual words, unifying the different sources of information. To speed up this process, our brain actively predicts what is coming next (for instance, we might expect 'newspaper' to follow 'the editor of the ...'). As most utterances are part of a conversation, some information is usually already shared between the speaker and the listener. Speakers make sure that they mark 'new information', using the order of the words or pitch to focus the listener's attention (after hearing that readers of the newspaper did not like the article, one could say 'the EDITOR of the newspaper loved the article'). Only when relevant 'new' information is unexpected or ungrammatical, people's brains are shown to react. Listeners likely process 'old' information in a 'good-enough' manner, ignoring some of the details, explains Hagoort, which is why they do not seem to notice unexpected 'old' information. To make matters even more complex, language is often indirect. To know what a speaker really means, listeners need to infer a speaker's intention. For instance, 'It is hot here' could well be intended as a request to open the window, rather than a statement about the temperature. Neuroimaging studies show that such 'pragmatic' inferences depend on brain areas that are involved in 'Theory of Mind', or thinking about other people's beliefs, emotions and desires. Language is a ""complex biocultural hybrid,"" concludes Hagoort. But what is the essence of human language? Is it syntax, to be found in Broca's area? Hagoort challenges this old notion: ""Accounting for the full picture of human language skills is not helped by a distinction between essential and nonessential aspects of speech and language."" Instead, the neuroscientist argues for a multiple brain-network view of language, in which some operations might well be shared with other cognitive domains, such as music and arithmetic. Language being the multi-layered system that it is, no wonder that the language-ready brain is so enormously complex,"" says Hagoort. ",Computer Science,0.11975331991852696
59,ACM,Biologically Inspired Artificial Skin Improves Sensory Ability of Robots,ACM,2019-10-10,-,https://www.tum.de/nc/en/about-tum/news/press-releases/details/35732-1/,"       Biologically Inspired Artificial Skin Improves Sensory Ability of Robots     Technical University of Munich (Germany)October 10, 2019   Researchers at the Technical University of Munich in Germany have designed a system integrating artificial skin with control algorithms, which they used to create the first autonomous humanoid robot with full-body synthetic skin. The skin is composed of hexagonal cells about an inch in diameter, each with a microprocessor and sensors to measure pressure, acceleration, proximity, and temperature. The researchers use an event-based system to track the cells instead of continuous monitoring, with individual cells only sending data when values change; this cuts the processing load by up to 90%. Said the university’s Gordon Cheng, “Our system is designed to work trouble-free and quickly with all kinds of robots. Now we're working to create smaller skin cells with the potential to be produced in larger numbers.”                 ",Robotics,0.11966834269788348
60,Science Daily,Researchers Build a Soft Robot With Neurologic Capabilities,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110650.htm,"   Cunjiang Yu, Bill D. Cook Associate Professor of Mechanical Engineering at the University of Houston, said the work represents a significant step toward the development of prosthetics that could directly connect with the peripheral nerves in biological tissues, offering neurological function to artificial limbs, as well as toward advances in soft neurorobots capable of thinking and making judgments. Yu is corresponding author for a paper describing the work, published in Science Advances. He is also a principal investigator with the Texas Center for Superconductivity at the University of Houston. ""When human skin is touched, you feel it,"" Yu said to describe the human capabilities the new device can mimic. ""The feeling originates in your brain, through neural pathways from your skin to the brain."" The findings have implications for neuroprosthetics, as well as for neuromorphic computing, an emerging technology with the potential to allow high volume information processing using small amounts of energy through devices that mimic the electric behavior of neural networks. Inspired by Nature Inspired by nature, the researchers designed artificial synaptic transistors -- that is, transistors that function similarly to neurons -- which continue to work even after being stretched as much as 50%. While the resulting neurological function is less sophisticated than that exhibited by those of its living counterparts, they said it marks an important first step toward more powerful engineering systems in the future. The transistor, described by researchers as having stretching characteristics similar to those in a rubber band, exhibited functions similar to those of biological synapses, including excitatory postsynaptic potential, current, facilitation, and short-term memory and long-term memory. The soft neurorobot was equipped with a neurologically integrated tactile sensory skin, allowing it to sense the interaction with the external environment and respond accordingly. ""The neurorobot senses physical tapping and locomotes adaptively in a programmed manner through synapse memory encoded signals,"" the researchers wrote. ",Health,0.11767890846198117
61,Science Daily,New Soft Actuators Could Make Soft Robots Less Bulky,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011142000.htm,"   As a proof of concept, engineers used these new actuators to build a soft, battery-powered robot that can walk untethered on flat surfaces and move objects. They also built a soft gripper that can grasp and pick up small objects. The team, led by UC San Diego mechanical and aerospace engineering professor Shengqiang Cai, published the work Oct. 11 in Science Advances. A problem with most soft actuators is that they come with bulky setups. That's because their movements are controlled by pumping either air or fluids through chambers inside. So building robots with these types of actuators would require tethering them to pumps, large power sources and other specialized equipment. In the current study, UC San Diego engineers created soft actuators that are controlled with electricity. ""This feature makes our tubular actuators compatible with most low-cost, commercially available electronic devices and batteries,"" Cai said. The actuators are made from a type of material used for artificial muscles in robots, called liquid crystal elastomers. They are composed of liquid crystal molecules embedded in a stretchy polymer network. What's special about these materials is they change shape, move and contract in response to stimuli such as heat or electricity -- similar to how muscles contract in response to signals from nerve cells.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     To construct each actuator, engineers sandwiched three heating wires between two thin films of liquid crystal elastomer. The material is then rolled into a tube, pre-stretched and exposed to UV light. Each heating wire can be controlled independently to make the tube bend in six different directions. When an electric current is passed through one or two of the wires, it heats up part of the tube and makes it bend in the direction of those wires. When a current is sent through all three wires, the entire tube contracts, shortening in length. When the electricity is turned off, the tube slowly cools down and returns to its original shape. ""Using an externally applied electrical potential makes it easy to program the position of each tubular actuator,"" said first author Qiguang He, a mechanical and aerospace engineering Ph.D. student at the UC San Diego Jacobs School of Engineering. Combining multiple actuators together enabled engineers to build different types of soft robots. They built an untethered, walking robot using four actuators as legs. This robot is powered by a small lithium/polymer battery on board. They also built a soft gripper using three actuators as fingers. Each robot has an on-board microcontroller in which engineers programmed a sequence of electrically controlled motions for the actuators. This allows the robots to move independently. The team is now working on making soft actuators that can move faster. The current actuators take about 30 seconds to fully bend and contract, and up to four minutes to return to their original shapes. That's because the material takes a bit of time to fully heat up and cool down. The ultimate goal is to make actuators that can contract and relax as quickly as human muscles, He said. This work was supported by the Office of Naval Research (grant N00014-17-1-2062) and the National Science Foundation (grant CMMI-1554212). This work was performed in part at the San Diego Nanotechnology Infrastructure (SDNI) at UC San Diego, a member of the National Nanotechnology Coordinate Infrastructure, which is supported by the National Science Foundation (grant ECCS-1542148). ",Robotics,0.11624490672576027
62,IEEE,From Mainframes to PCs: What Robot Startups Can Learn From the Computer Revolution,Robotics,2019-10-09,-,https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution,"      This is a guest post. The views expressed here are solely those of the author and do not represent positions of IEEE Spectrum or the IEEE. Autonomous robots are coming around slowly. We already got autonomous vacuum cleaners, autonomous lawn mowers, toys that bleep and blink, and (maybe) soon autonomous cars. Yet, generation after generation, we keep waiting for the robots that we all know from movies and TV shows. Instead, businesses seem to get farther and farther away from the robots that are able to do a large variety of tasks using general-purpose, human anatomy-inspired hardware. Although these are the droids we have been looking for, anything that came close, such as Willow Garage’s PR2 or Rethink Robotics’ Baxter has bitten the dust. With building a robotic company being particularly hard, compounding business risk with technological risk, the trend goes from selling robots to selling actual services like mowing your lawn, provide taxi rides, fulfilling retail orders, or picking strawberries by the pound. Unfortunately for fans of R2-D2 and C-3PO, these kind of business models emphasize specialized, room- or fridge-sized hardware that is optimized for one very specific task, but does not contribute to a general-purpose robotic platform.  We have actually seen something very similar in the personal computer (PC) industry. In the 1950s, even though computers could be as big as an entire room and were only available to a selected few, the public already had a good idea of what computers would look like. A long list of fictional computers started to populate mainstream entertainment during that time. In a 1962 New York Times article titled “Pocket Computer to Replace Shopping List,” visionary scientist John Mauchly stated that “there is no reason to suppose the average boy or girl cannot be master of a personal computer.” In 1968, Douglas Engelbart gave us the “mother of all demos,” browsing hypertext on a graphical screen and a mouse, and other ideas that have become standard only decades later. Now that we have finally seen all of this, it might be helpful to examine what actually enabled the computing revolution to learn where robotics is really at and what we need to do next.  In the 1970s, mainframes were about to be replaced by the emerging class of mini-computers, fridge-sized devices that cost less than US $25,000 ($165,000 in 2019 dollars). These computers did not use punch-cards, but could be programmed in Fortran and BASIC, dramatically expanding the ease with which potential applications could be created. Yet it was still unclear whether mini-computers could ever replace big mainframes in applications that require fast and efficient processing of large amounts of data, let alone enter every living room. This is very similar to the robotics industry right now, where large-scale factory robots (mainframes) that have existed since the 1960s are seeing competition from a growing industry of collaborative robots that can safely work next to humans and can easily be installed and programmed (minicomputers). As in the ’70s, applications for these devices that reach system prices comparable to that of a luxury car are quite limited, and it is hard to see how they could ever become a consumer product.  Yet, as in the computer industry, successful architectures are quickly being cloned, driving prices down, and entirely new approaches on how to construct or program robotic arms are sprouting left and right. Arm makers are joined by manufacturers of autonomous carts, robotic grippers, and sensors. These components can be combined, paving the way for standard general purpose platforms that follow the model of the IBM PC, which built a capable, open architecture relying as much on commodity parts as possible.  General purpose robotic systems have not been successful for similar reasons that general purpose, also known as “personal,” computers took decades to emerge. Mainframes were custom-built for each application, while typewriters got smarter and smarter, not really leaving room for general purpose computers in between. Indeed, given the cost of hardware and the relatively little abilities of today’s autonomous robots, it is almost always smarter to build a special purpose machine than trying to make a collaborative mobile manipulator smart.  A current example is e-commerce grocery fulfillment. The current trend is to reserve underutilized parts of a brick-and-mortar store for a micro-fulfillment center that stores goods in little crates with an automated retrieval system and a (human) picker. A number of startups like Alert Innovation, Fabric, Ocado Technology, TakeOff Technologies, and Tompkins Robotics, to just name a few, have raised hundreds of millions of venture capital recently to build mainframe equivalents of robotic fulfillment centers. This is in contrast with a robotic picker, which would drive through the aisles to restock and pick from shelves. Such a robotic store clerk would come much closer to our vision of a general purpose robot, but would require many copies of itself that crowd the aisles to churn out hundreds of orders per hour as a microwarehouse could. Although eventually more efficient, the margins in retail are already low and make it unlikely that this industry will produce the technological jump that we need to get friendly C-3POs manning the aisles.   Mainframes were also attacked from the bottom. Fascination with the new digital technology has led to a hobbyist movement to create microcomputers that were sold via mail order or at RadioShack. Initially, a large number of small businesses was selling tens, at most hundreds, of devices, usually as a kit and with wooden enclosures. This trend culminated into the “1977 Trinity” in the form of the Apple II, the Commodore PET, and the Tandy TRS-80, complete computers that were sold for prices around $2500 (TRS) to $5000 (Apple) in today’s dollars. The main application of these computers was their programmability (in BASIC), which would enable consumers to “learn to chart your biorhythms, balance your checking account, or even control your home environment,” according to an original Apple advertisement. Similarly, there exists a myriad of gadgets that explore different aspects of robotics such as mobility, manipulation, and entertainment.  As in the fledgling personal computing industry, the advertised functionality was at best a model of the real deal. A now-famous milestone in entertainment robotics was the original Sony’s Aibo, a robotic dog that was advertised to have many properties that a real dog has such as develop its own personality, play with a toy, and interact with its owner. Released in 1999, and re-launched in 2018, the platform has a solid following among hobbyists and academics who like its programmability, but probably only very few users who accept the device as a pet stand-in. There also exist countless “build-your-own-robotic-arm” kits. One of the more successful examples is the uArm, which sells for around $800, and is advertised to perform pick and place, assembly, 3D printing, laser engraving, and many other things that sound like high value applications. Using compelling videos of the robot actually doing these things in a constrained environment has led to two successful crowd-funding campaigns, and have established the robot as a successful educational tool. Finally, there exist platforms that allow hobbyist programmers to explore mobility to construct robots that patrol your house, deliver items, or provide their users with telepresence abilities. An example of that is the Misty II. Much like with the original Apple II, there remains a disconnect between the price of the hardware and the fidelity of the applications that were available.   For computers, this disconnect began to disappear with the invention of the first electronic spreadsheet software VisiCalc that spun out of Harvard in 1979 and prompted many people to buy an entire microcomputer just to run the program. VisiCalc was soon joined by WordStar, a word processing application, that sold for close to $2000 in today’s dollars. WordStar, too, would entice many people to buy the entire hardware just to use the software. The two programs are early examples of what became known as “killer application.”  With factory automation being mature, and robots with the price tag of a minicomputer being capable of driving around and autonomously carrying out many manipulation tasks, the robotics industry is somewhere where the PC industry was between 1973—the release of the Xerox Alto, the first computer with a graphical user interface, mouse, and special software—and 1979—when microcomputers in the under $5000 category began to take off. So what would it take for robotics to continue to advance like computers did? The market itself already has done a good job distilling what the possible killer apps are. VCs and customers alike push companies who have set out with lofty goals to reduce their offering to a simple value proposition. As a result, companies that started at opposite ends often converge to mirror images of each other that offer very similar autonomous carts, (bin) picking, palletizing, depalletizing, or sorting solutions. Each of these companies usually serves a single application to a single vertical—for example bin-picking clothes, transporting warehouse goods, or picking strawberries by the pound. They are trying to prove that their specific technology works without spreading themselves too thin.  Very few of these companies have really taken off. One example is Kiva Systems, which turned into the logistic robotics division of Amazon. Kiva and others are structured around sound value propositions that are grounded in well-known user needs. As these solutions are very specialized, however, it is unlikely that they result into any economies of scale of the same magnitude that early computer users who bought both a spreadsheet and a word processor application for their expensive minicomputer could enjoy. What would make these robotic solutions more interesting is when functionality becomes stackable. Instead of just being able to do bin picking, palletizing, and transportation with the same hardware, these three skills could be combined to model entire processes. A skill that is yet little addressed by startups and is historically owned by the mainframe equivalent of robotics is assembly of simple mechatronic devices. The ability to assemble mechatronic parts is equivalent to other tasks such as changing a light bulb, changing the batteries in a remote control, or tending machines like a lever-based espresso machine. These tasks would involve the autonomous execution of complete workflows possible using a single machine, eventually leading to an explosion of industrial productivity across all sectors.  For example, picking up an item from a bin, arranging it on the robot, moving it elsewhere, and placing it into a shelf or a machine is a process that equally applies to a manufacturing environment, a retail store, or someone’s kitchen. Even though many of the above applications are becoming possible, it is still very hard to get a platform off the ground without added components that provide “killer app” value of their own. Interesting examples are Rethink Robotics or the Robot Operating System (ROS). Rethink Robotics’ Baxter and Sawyer robots pioneered a great user experience (like the 1973 Xerox Alto, really the first PC), but its applications were difficult to extend beyond simple pick-and-place and palletizing and depalletizing items. ROS pioneered interprocess communication software that was adapted to robotic needs (multiple computers, different programming languages) and the idea of software modularity in robotics, but—in the absence of a common hardware platform—hasn’t yet delivered a single application, e.g. for navigation, path planning, or grasping, that performs beyond research-grade demonstration level and won’t get discarded once developers turn to production systems. At the same time, an increasing number of robotic devices, such as robot arms or 3D perception systems that offer intelligent functionality, provide other ways to wire them together that do not require an intermediary computer, while keeping close control over the real-time aspects of their hardware.  Robotic Materials GPR-1 combines a MIR-100 autonomous cart with an UR-5 collaborative robotic arm, an onRobot force/torque sensor and Robotic Materials’ SmartHand to perform out-of-the-box mobile assembly, bin picking, palletizing, and depalletizing tasks.  At my company, Robotic Materials Inc., we have made strides to identify a few applications such as bin picking and assembly, making them configurable with a single click by combining machine learning and optimization with an intuitive user interface. Here, users can define object classes and how to grasp them using a web browser, which then appear as first-class objects in a robot-specific graphical programming language. We have also done this for assembly, allowing users to stack perception-based picking and force-based assembly primitives by simply dragging and dropping appropriate commands together. While such an approach might answer the question of a killer app for robots priced in the “minicomputer” range, it is unclear how killer app-type value can be generated with robots in the less-than-$5000 category. A possible answer is two-fold: First, with low-cost arms, mobility platforms, and entertainment devices continuously improving, a confluence of technology readiness and user innovation, like with the Apple II and VisiCalc, will eventually happen. For example, there is not much innovation needed to turn Misty into a home security system; the uArm into a low-cost bin-picking system; or an Aibo-like device into a therapeutic system for the elderly or children with autism. Second, robots and their components have to become dramatically cheaper. Indeed, computers have seen an exponential reduction in price accompanied by an exponential increase in computational power, thanks in great part to Moore’s Law. This development has helped robotics too, allowing us to reach breakthroughs in mobility and manipulation due to the ability to process massive amounts of image and depth data in real-time, and we can expect it to continue to do so. One might ask, however, how a similar dynamics might be possible for robots as a whole, including all their motors and gears, and what a “Moore’s Law” would look like for the robotics industry. Here, it helps to remember that the perpetuation of  Moore’s Law is not the reason, but the result of the PC revolution. Indeed, the first killer apps for bookkeeping, editing, and gaming were so good that they unleashed tremendous consumer demand, beating the benchmark on what was thought to be physically possible over and over again. (I vividly remember 56 kbps to be the absolute maximum data rate for copper phone lines until DSL appeared.) That these economies of scale are also applicable to mechatronics is impressively demonstrated by the car industry. A good example is the 2020 Prius Prime, a highly computerized plug-in hybrid, that is available for one third of the cost of my company’s GPR-1 mobile manipulator while being orders of magnitude more complex, sporting an electrical motor, a combustion engine, and a myriad of sensors and computers. It is therefore very well conceivable to produce a mobile manipulator that retails at one tenth of the cost of a modern car, once robotics enjoy similar mass-market appeal. Given that these robots are part of the equation, actively lowering cost of production, this might happen as fast as never before in the history of industrialization.  There is one more driver that might make robots exponentially more capable: the cloud. Once a general purpose robot has learned or was programmed with a new skill, it could share it with every other robot. At some point, a grocer who buys a robot could assume that it already knows how to recognize and handle 99 percent of the retail items in the store. Likewise, a manufacturer can assume that the robot can handle and assemble every item available from McMaster-Carr and Misumi. Finally, families could expect a robot to know every kitchen item that Ikea and Pottery Barn is selling. Sounds like a labor intense problem, but probably more manageable than collecting footage for Google’s Street View using cars, tricycles, and snowmobiles, among other vehicles. While we are waiting for these two trends—better and better applications and hardware with decreasing cost—to converge, we as a community have to keep exploring what the canonical robotic applications beyond mobility, bin picking, palletizing, depalletizing, and assembly are. We must also continue to solve the fundamental challenges that stand in the way of making these solutions truly general and robust.  For both questions, it might help to look at the strategies that have been critical in the development of the personal computer, which might equally well apply to robotics:  Start with a solution to a problem your customers have. Unfortunately, their problem is almost never that they need your sensor, widget, or piece of code, but something that already costs them money or negatively affects them in some other way. Example: There are many more people who had a problem calculating their taxes (and wanted to buy VisiCalc) than writing their own solution in BASIC.  Build as little of your own hardware as necessary. Your business model should be stronger than the margin you can make on the hardware. Why taking the risk? Example: Why build your own typewriter if you can write the best typewriting application that makes it worth buying a computer just for that?   If your goal is a platform, make sure it comes with a killer application, which alone justifies the platform cost. Example: Microcomputer companies came and went until the “1977 Trinity” intersected with the killer apps spreadsheet and word processors. Corollary: You can also get lucky.   Use an open architecture, which creates an ecosystem where others compete on creating better components and peripherals, while allowing others to integrate your solution into their vertical and stack it with other devices. Example: Both the Apple II and the IBM PC were completely open architectures, enabling many clones, thereby growing the user and developer base.   It’s worthwhile pursuing this. With most business processes already being digitized, general purpose robots will allow us to fill in gaps in mobility and manipulation, increasing productivity at levels only limited by the amount of resources and energy that are available, possibly creating a utopia in which creativity becomes the ultimate currency. Maybe we’ll even get R2-D2.   Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11521372636131275
63,IEEE,OpenAI Teaches Robot Hand to Solve Rubik's Cube,Robotics,2019-10-15,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving,"      In-hand manipulation is a skill that, as far as I’m aware, humans in general don’t actively learn. We just sort of figure it out by doing other, more specific tasks with our fingers and hands. This makes it particularly tricky to teach robots to solve in-hand manipulation tasks because the way we do it is through experimentation and trial and error. Robots can learn through trial and error as well, but since it usually ends up being mostly error, it takes a very, very long time. Last June, we wrote about OpenAI’s approach to teaching a five-fingered robot hand to manipulate a cube. The method that OpenAI used leveraged the same kind of experimentation and trial and error, but in simulation rather than on robot hardware. For complex tasks that take a lot of finesse, simulation generally translates poorly into real-world skills, but OpenAI made their system super robust by introducing a whole bunch of randomness into the simulation during the training process. That way, even if the simulation didn’t perfectly match reality (which it didn’t), the system could still handle the kinds of variations that it experienced on the real-world hardware. In a preprint paper published online today, OpenAI has managed to teach its robot hand to solve a much more difficult version of in-hand cube manipulation: single-handed solving of a 3x3 Rubik’s cube. The new work is also based on the idea of solving a problem using advanced simulations and then transferring the solution to a real-world system, or what researchers call “sim2real.” In the paper, OpenAI says the new approach “vastly improved sim2real transfer.”  The initial step was to break down the robot manipulation of the Rubik’s cube into two different tasks: 1. rotating a single face of the cube 90 degrees in either direction, and 2. flipping the cube to bring a different face to the top. Since rotating the top face is much simpler for the robot than rotating other faces, the most reliable strategy is to just do a 90-degree flip to get the face you want to rotate on top. The actual process of solving the cube is computationally straightforward, although the solving process is optimized for the motions that the robot can perform rather than the solve that would take the least number of steps. The physical setup that’s doing the real-world cube solving is a Shadow Dexterous E Series Hand with a PhaseSpace motion capture system, plus RGB cameras for visual pose estimation. The cube that’s being manipulated is also pretty fancy: It’s stuffed with sensors that report the orientation of each face with an accuracy of five degrees, which is necessary because it’s otherwise very difficult to know the state of a Rubik’s cube when some of its faces are occluded.  While the video makes it easy to focus on the physical robot, the magic is mostly happening in simulation, and transferring things learned in simulation to the real world. Again, the key to this is domain randomization—jittering parts of the simulation around so that your system has to adapt to different situations similar to those that might be encountered in the real-world. For example, maybe you slightly alter the weight of the cube, or change the friction of the fingertips a little bit, or turn down the lighting. If your system can handle these simulated variations, it’ll be more robust to real-world operation. The physical setup includes the Shadow Dexterous Hand, a PhaseSpace motion capture system, and RGB cameras. OpenAI modified the Shadow Dexterous Hand by moving the PhaseSpace LEDs and cables inside the fingers and by adding rubber to the fingertips. When we spoke to last year to Jonas Schneider (one of the authors of the cube manipulation work) and asked him where he thought that system was the weakest, he said that the biggest problem at that point was that the randomizations were both task-specific and hand designed. It’s probably not surprising, then, that one of the big contributions of the Rubik’s cube work is “a novel method for automatically generating a distribution over randomized environments for training reinforcement learning policies and vision state estimators,” which the researchers call automatic domain randomization (ADR). Here’s why ADR is important, according to the paper: Our main hypothesis that motivates ADR is that training on a maximally diverse distribution over environments leads to transfer via emergent meta-learning. More concretely, if the model has some form of memory, it can learn to adjust its behavior during deployment to improve performance on the current environment over time, i.e. by implementing a learning algorithm internally. We hypothesize that this happens if the training distribution is so large that the model cannot memorize a special-purpose solution per environment due to its finite capacity. ADR is a first step in this direction of unbounded environmental complexity: it automates and gradually expands the randomization ranges that parameterize a distribution over environments.   Special-purpose solutions per environment are bad, because they work for that environment, but not for other environments. You can think of each little tweak to a simulation as creating a new environment, and the idea behind ADR is to automate these tweaks to create so many new environments that the system is forced to instead come up with general solutions that can work for many different environments all at once. This reflects the robustness required for real-world operation, where no two environments are ever exactly alike. It turns out that ADR is both better and more efficient than the previous manual tuning, say the researchers: ADR clearly leads to improved transfer with much less need for hand-engineered randomizations. We significantly outperformed our previous best results, which were the result of multiple months of iterative manual tuning. In terms of results, the researchers were mostly concerned with how many flips and rotations the system could do in a row without failing, rather than how many complete solves it was capable of. It sounds like a complete solve was a bit of an outlier—the starting configuration of the cube could be solved by the system in 43 successful moves, while the average successful run of the best trained policy (continuously trained over multiple months) was about 27 moves. Sixty percent of the time, the system could get halfway to a complete solve, and it made it the entire way 20 percent of the time. The researchers point out that the method they’ve developed here is general purpose, and you can train a real-world robot to do pretty much any task that you can adequately simulate. You don’t need any real-world training at all, as long as your simulations are diverse enough, which is where the automatic domain randomization comes in. The long-term goal is to reduce the task specialization that’s inherent to most robots, which will help them be more useful and adaptable in real-world applications. Lastly, just for reference, here’s what (I think) is the current 3x3 cube world record is, set just a few days ago by Max Park:  Wow. It’s interesting that it appears to be faster and/or more efficient for a human to use table contact to augment their own dexterity. We’ve seen other robots make use of environmental contact for manipulation; it would be cool if OpenAI threw a surface into the simulation to see if their system could make use of it. [ OpenAI ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11487014868718694
64,IEEE,Labrador Systems Developing Affordable Assistive Robots for the Home,Robotics,2019-10-15,-,https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home,"      Developing robots for the home is still a challenge, especially if you want those robots to interact with people and help them do practical, useful things. However, the potential markets for home robots are huge, and one of the most compelling markets is for home robots that can assist humans who need them. Today, Labrador Systems, a startup based in California, is announcing a pre-seed funding round of $2 million (led by SOSV’s hardware accelerator HAX with participation from Amazon’s Alexa Fund and iRobot Ventures, among others) with the goal of expanding development and conducting pilot studies of  “a new [assistive robot] platform for supporting home health.” Labrador was founded two years ago by Mike Dooley and Nikolai Romanov. Both Mike and Nikolai have backgrounds in consumer robotics at Evolution Robotics and iRobot, but as an ’80s gamer, Mike’s bio (or at least the parts of his bio on LinkedIn) caught my attention: From 1995 to 1997, Mike worked at Brøderbund Software, helping to manage play testing for games like Myst and Riven and the Where in the World is Carmen San Diego series. He then spent three years at Lego as the product manager for MindStorms. After doing some marginally less interesting things, Mike was the VP of product development at Evolution Robotics from 2006 to 2012, where he led the team that developed the Mint floor sweeping robot. Evolution was acquired by iRobot in 2012, and Mike ended up as the VP of product development over there until 2017, when he co-founded Labrador. I was pretty much sold at Where in the World is Carmen San Diego (the original version of which I played from a 5.25” floppy on my dad’s Apple IIe)*, but as you can see from all that other stuff, Mike knows what he’s doing in robotics as well. And according to Labrador’s press release, what they’re doing is this: Labrador Systems is an early stage technology company developing a new generation of assistive robots to help people live more independently. The company’s core focus is creating affordable solutions that address practical and physical needs at a fraction of the cost of commercial robots. … Labrador’s technology platform offers an affordable solution to improve the quality of care while promoting independence and successful aging.  Labrador’s personal robot, the company’s first offering, will enter pilot studies in 2020. That’s about as light on detail as a press release gets, but there’s a bit more on Labrador’s website, including: The only hardware we’ve actually seen from Labrador at this point is a demo that they put together for Amazon’s re:MARS conference, which took place a few months ago, showing a “demonstration project” called Smart Walker:  This isn’t the home assistance robot that Labrador got its funding for, but rather a demonstration of some of their technology. So of course, the question is, what’s Labrador working on, then? It’s still a secret, but Mike Dooley was able to give us a few more details. IEEE Spectrum: Your website shows a smart walker concept—how is that related to the assistive robot that you’re working on? Mike Dooley: The smart walker was a request from a major senior living organization to have our robot (which is really good at navigation) guide residents from place to place within their communities. To test the idea with residents, it turned out to be much quicker to take the navigation system from the robot and put it on an existing rollator walker. So when you see the clips of the technology in the smart walker video on our website, that’s actually the robot’s navigation system localizing in real time and path planning in an environment. “Assistive robot” can cover a huge range of designs and capabilities—can you give us any more detail about your robot, and what it’ll be able to do? One of the core features of our robot is to help people move things where they have difficulty moving themselves, particularly in the home setting. That may sound trivial, but to someone who has impaired mobility, it can be a major daily challenge and negatively impact their life and health in a number of ways. Some examples we repeatedly hear are people not staying hydrated or taking their medication on time simply because there is a distance between where they are and the items they need. Once we have those base capabilities, i.e. the ability to navigate around a home and move things within it, then the robot becomes a platform for a wider variety of applications. What made you decide to develop assistive robots, and why are robots a good solution for seniors who want to live independently? Supporting independent living has been seen as a massive opportunity in robotics for some time, but also as something off in the future. The turning point for me was watching my mother enter that stage in her life and seeing her transition to using a cane, then a walker, and eventually to a wheelchair. That made the problems very real for me. It also made things much clearer about how we could start addressing specific needs with the tools that are becoming available now. In terms of why robots can be a good solution, the basic answer is the level of need is so overwhelming that even helping with “basic” tasks can make an appreciable difference in the quality of someone’s daily life. It’s also very much about giving individuals a degree of control back over their environment. That applies to seniors as well as others whose world starts getting more complex to manage as their abilities become more impaired. What are the particular challenges of developing assistive robots, and how are you addressing them? Why do you think there aren’t more robotics startups in this space? The setting (operating in homes and personal spaces) and the core purpose of the product (aiding a wide variety of individuals) bring a lot of complexity to any capability you want to build into an assistive robot. Our approach is to put as much structure as we can into the system to make it functional, affordable, understandable and reliable. I think one of the reasons you don’t see more startups in the space is that a lot of roboticists want to skip ahead and do the fancy stuff, such as taking on human-level capabilities around things like manipulation. Those are very interesting research topics, but we think those are also very far away from being practical solutions you can productize for people to use in their homes. How do you think assistive robots and human caregivers should work together? The ideal scenario is allowing caregivers to focus more of their time on the high-touch, personal side of care. The robot can offload the more basic support tasks as well as extend the impact of the caregiver for the long hours of the day they can’t be with someone at their home. We see that applying to both paid care providers as well as the 40 million unpaid family members and friends that provide assistance. The robot is really there as a tool, both for individuals in need and the people that help them. What’s promising in the research discussions we’ve had so far, is that even when a caregiver is present, giving control back to the individual for simple things can mean a lot in the relationship between them and the caregiver. What should we look forward to from Labrador in 2020? Our big goal in 2020 is to start placing the next version of the robot with individuals with different types of needs to let them experience it naturally in their own homes and provide feedback on what they like, what don’t like and how we can make it better. We are currently reaching out to companies in the healthcare and home health fields to participate in those studies and test specific applications related to their services. We plan to share more detail about those studies and the robot itself as we get further into 2020. If you’re an organization (or individual) who wants to possibly try out Labrador’s prototype, the company encourages you to connect with them through their website. And as we learn more about what Labrador is up to, we’ll have updates for you, presumably in 2020. [ Labrador Systems ] * I just lost an hour of my life after finding out that you can play Where in the World is Carmen San Diego in your browser for free.  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11326359890994162
65,ACM,"Caltech's Latest Creation: A Hovering, Bird-Like Robot That Could Someday Explore Mars",ACM,2019-10-09,-,https://www.washingtonpost.com/technology/2019/10/09/caltechs-latest-creation-hovering-bird-like-robot-that-could-someday-explore-mars/,"    Caltech's Latest Creation: A Hovering, Bird-Like Robot That Could Someday Explore Mars     The Washington PostPeter HolleyOctober 9, 2019   California Institute of Technology (Caltech) researchers have developed a robot drone that can traverse rough terrain. A team at Caltech's Center for Autonomous Systems and Technology designed Leonardo (LEg ON Aerial Robotic DrOne) as a bipedal/aerial hybrid, taking birds' ability to walk on two legs and fly as inspiration. Leonardo features two torso rotors enabling the robot to lift off when it loses balance, then recover via thrust-facilitated weight shifting. Caltech's Morteza Gharib said Leonardo's stabilizing legs also would allow the machine to carry bulkier batteries, expanding its power and versatility. Said Gharib, ""They'd be able to carry equipment and then fly overhead and take steady images, becoming a scientists or an engineer's companion.""               *May Require Paid Registration  ",Robotics,0.11188917971571781
66,IEEE,Video Friday: This Humanoid Robot Will Serve You Ice Cream,Robotics,2019-10-11,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream,"      Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (send us your events!): Let us know if you have suggestions for next week, and enjoy today’s videos. What’s better than a robotics paper with “dynamic” in the title? A robotics paper with “highly dynamic” in the title. From Sangbae Kim’s lab at MIT, the latest exploits of Mini Cheetah:    Yes I’d very much like one please. Full paper at the link below. [ Paper ] via [ MIT ] A humanoid robot serving you ice cream—on his own ice cream bike: What a delicious vision!    [ Roboy ]  The Roomba “i” series and “s” series vacuums have just gotten an update that lets you set “keep out” zones, which is super useful. Tell your robot where not to go!    I feel bad, that Roomba was probably just hungry :( [ iRobot ]   We wrote about Voliro’s tilt-rotor hexcopter a couple years ago, and now it’s off doing practical things, like spray painting a building pretty much the same color that it was before.    [ Voliro ] Thanks Mina!   Here’s a clever approach for bin-picking problematic objects, like shiny things: Just grab a whole bunch, and then sort out what you need on a nice robot-friendly table.    It might take a little bit longer, but what do you care, you’re probably off sipping a cocktail with a little umbrella in it on a beach somewhere. [ Harada Lab ]   A unique combination of the IRB 1200 and YuMi industrial robots that use vision, AI and deep learning to recognize and categorize trash for recycling.    [ ABB ]   Measuring glacial movements in-situ is a challenging, but necessary task to model glaciers and predict their future evolution. However, installing GPS stations on ice can be dangerous and expensive when not impossible in the presence of large crevasses. In this project, the ASL develops UAVs for dropping and recovering lightweight GPS stations over inaccessible glaciers to record the ice flow motion. This video shows the results of first tests performed at Gorner glacier, Switzerland, in July 2019.    [ EPFL ]   Turns out Tertills actually do a pretty great job fighting weeds.    Plus, they leave all those cute lil’ Tertill tracks. [ Franklin Robotics ]   The online autonomous navigation and semantic mapping experiment presented [below] is conducted with the Cassie Blue bipedal robot at the University of Michigan. The sensors attached to the robot include an IMU, a 32-beam LiDAR and an RGB-D camera. The whole online process runs in real-time on a Jetson Xavier and a laptop with an i7 processor. The resulting map is so precise that it looks like we are doing real-time SLAM (simultaneous localization and mapping). In fact, the map is based on dead-reckoning via the InvEKF.    [ GTSAM ] via [ University of Michigan ]   UBTECH has announced an upgraded version of its Meebot, which is 30 percent bigger and comes with more sensors and programmable eyes.    [ UBTECH ]   ABB’s research team will be working with medical staff, scientist and engineers to develop non-surgical medical robotics systems, including logistics and next-generation automated laboratory technologies. The team will develop robotics solutions that will help eliminate bottlenecks in laboratory work and address the global shortage of skilled medical staff.    [ ABB ]     In this video, Ian and Chris go through Misty’s SDK, discussing the languages we’ve included, the tools that make it easy for you to get started quickly, a quick rundown of how to run the skills you build, plus what’s ahead on the Misty SDK roadmap.     [ Misty Robotics ]   My guess is that this was not one of iRobot’s testing environments for the Roomba.    You know, that’s actually super impressive. And maybe if they threw one of the self-emptying Roombas in there, it would be a viable solution to the entire problem. [ How Farms Work ]   Part of WeRobotics’ Flying Labs network, Panama Flying Labs is a local knowledge hub catalyzing social good and empowering local experts. Through training and workshops, demonstrations and missions, the Panama Flying Labs team leverages the power of drones, data, and AI to promote entrepreneurship, build local capacity, and confront the pressing social challenges faced by communities in Panama and across Central America.    [ Panama Flying Labs ]   Go on a virtual flythrough of the NIOSH Experimental Mine, one of two courses used in the recent DARPA Subterranean Challenge Tunnel Circuit Event held 15-22 August, 2019. The data used for this partial flythrough tour were collected using 3D LIDAR sensors similar to the sensors commonly used on autonomous mobile robots.    [ SubT ]   Special thanks to PBS, Mark Knobil, Joe Seamans and Stan Brandorff and many others who produced this program in 1991. It features Reid Simmons (and his 1 year old son), David Wettergreen, Red Whittaker, Mac Macdonald, Omead Amidi, and other Field Robotics Center alumni building the planetary walker prototype called Ambler. The team gets ready for an important demo for NASA.    [ CMU RI ]   As art and technology merge, roboticist Madeline Gannon explores the frontiers of human-robot interaction across the arts, sciences and society, and explores what this could mean for the future.    [ Sonar+D ]    Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11025859399495987
67,IEEE,Agility Robotics Unveils Upgraded Digit Walking Robot,Robotics,2019-10-14,-,https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot,"      Last time we saw Agility Robotics’ Digit biped, it was picking up a box from a Ford delivery van and autonomously dropping it off on a porch, while at the same time managing to not trip over stairs, grass, or small children. As a demo, it was pretty impressive, but of course there’s an enormous gap between making a video of a robot doing a successful autonomous delivery and letting that robot out into the semi-structured world and expecting it to reliably do a good job. Agility Robotics is aware of this, of course, and over the last six months they’ve been making substantial improvements to Digit to make it more capable and robust. A new video posted today shows what’s new with the latest version of Digit—Digit v2.  We appreciate Agility Robotics foregoing music in the video, which lets us hear exactly what Digit sounds like in operation. The most noticeable changes are in Digit’s feet, torso, and arms, and I was particularly impressed to see Digit reposition the box on the table before grasping it to make sure that it could get a good grip. Otherwise, it’s hard to tell what’s new, so we asked Agility Robotics’ CEO Damion Shelton to get us up to speed. IEEE Spectrum: Can you summarize the differences between Digit v1 and v2? We’re particularly interested in the new feet. Damion Shelton: The feet now include a roll degree of freedom, so that Digit can resist lateral forces without needing to side step. This allows Digit v2 to balance on one foot statically, which Digit v1 and Cassie could not do. The larger foot also dramatically decreases load per unit area, for improved performance on very soft surfaces like sand. The perception stack includes four Intel RealSense cameras used for obstacle detection and pick/place, plus the lidar. In Digit v1, the perception systems were brought up incrementally over time for development purposes. In Digit v2, all perception systems are active from the beginning and tied to a dedicated computer. The perception system is used for a number of additional things beyond manipulation, which we’ll start to show in the next few weeks. The torso changes are a bit more behind-the-scenes. All of the electronics in it are now fully custom, thermally managed, and environmentally sealed. We’ve also included power and ethernet to a payload bay that can fit either a NUC or Jetson module (or other customer payload). What exactly are we seeing in the video in terms of Digit’s autonomous capabilities? At the moment this is a demonstration of shared autonomy. Picking and placing the box is fully autonomous. Balance and footstep placement are fully autonomous, but guidance and obstacle avoidance are under local teleop. It’s no longer a radio controller as in early videos; we’re not ready to reveal our current controller design but it’s a reasonably significant upgrade. This is v2 hardware, so there’s one more full version in development prior to the 2020 launch, which will expand the autonomy envelope significantly. What are some unique features or capabilities of Digit v2 that might not be obvious from the video? For those who’ve used Cassie robots, the power-up and power-down ergonomics are a lot more user friendly. Digit can be disassembled into carry-on luggage sized pieces (give or take) in under 5 minutes for easy transport. The battery charges in-situ using a normal laptop-style charger. I’m curious about this “stompy” sort of gait that we see in Digit and many other bipedal robots—are there significant challenges or drawbacks to implementing a more human-like (and presumably quieter) heel-toe gait? There are no drawbacks other than increased complexity in controls and foot design. With Digit v2, the larger surface area helps with the noise, and v2 has similar or better passive-dynamic performance as compared to Cassie or Digit v1. The foot design is brand new, and new behaviors like heel-toe are an active area of development. How close is Digit v2 to a system that you’d be comfortable operating commercially? We’re on track for a 2020 launch for Digit v3. Changes from v2 to v3 are mostly bug-fix in nature, with a few regulatory upgrades like full battery certification. Safety is a major concern for us, and we have launch customers that will be operating Digit in a safe environment, with a phased approach to relaxing operational constraints. Digit operates almost exclusively under force control (as with cobots more generally), but at the moment we’ll err on the side of caution during operation until we have the stats to back up safety and reliability. The legged robot industry has too much potential for us to screw it up by behaving irresponsibly. It will be a while before Digit (or any other humanoid robot) is operating fully autonomously in crowds of people, but there are so many large market opportunities (think indoor factory/warehouse environments) to address prior to that point that we expect to mature the operational safety side of things well in advance of having saturated the more robot-tolerant markets. [ Agility Robotics ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11008865973525754
68,IEEE,Watch Astrobee's First Autonomous Flight on the International Space Station,Robotics,2019-10-10,-,https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station,"      NASA’s Astrobee robots have come a long, long way since we first met them at NASA Ames back in 2017. In fact, they’ve made it all the way to the International Space Station: Bumble, Honey, and Queen Bee are up there right now. While Honey and Queen Bee are still packed away in a case (and quite unhappy about it, I would imagine), Bumble has been buzzing around, getting used to its new home. To be ready to fly solo, all Bumble needed was some astronaut-assisted mapping of its environment, and last month, the little robotic cube finally embarked on its first fully autonomous ISS adventure.  We cut together the above video from about an hour’s worth of raw footage (without audio) of Astrobee testing, which took place in the Japanese Experiment Module (JEM), also known as Kibo, on the ISS on August 28. Astronaut Christina Koch had been working with roboticists at NASA Ames on earlier Astrobee start-up activities, which hadn’t gone as perfectly as everyone hoped they would, and was (understandably) excited that the robot was able to successfully fly itself though the JEM. Christina and another astronaut, off camera in the Harmony node attached to the JEM, do a little dance to celebrate (with what is now officially the “Astrobee Jig,” we’re told), and apparently Astrobee now has a standing invitation to join in on all future ISS dance parties.  Astrobee’s goal for its first autonomous mission was to undock itself, follow a flight plan consisting of a list of waypoints and objectives that was uploaded to the robot from the ground, and then return to the dock. All of this was done without any direct intervention from the ground controllers or from the astronauts. As you can see in the video, Christina is mostly just following Bumble around as it does its thing, keeping out of the way of the navigation camera but otherwise just making sure the robot didn’t get into any trouble.  So far, the difficult part for Astrobee has been getting its localization to work robustly. While the robot does navigate visually, it’s dependent on preexisting maps rather than doing SLAM. Putting together those initial maps involved hand-carrying Bumble around the JEM to collect images, which were then processed offline (back on Earth) to identify features in the images and correlate them with locations to build up the map that Bumble uses to navigate. With maps like these, you have to find the right mix of features to include for navigation to work optimally. If your maps are too rich in features, there will be too much data for your robot to manage, and if the maps are too sparse, the robot won’t be able to localize accurately. This was a little bit tricky for Astrobee, as deputy group lead Maria Bualat from the Intelligent Systems Division at NASA Ames explained to us: It turned out that our maps needed to be richer. We tried to cull them down to make them fast and efficient, but we weren’t keeping enough features to enable the robot to localize robustly, so it would get lost a lot. During some of our earlier activities when we were trying to fly even basic motions, the robot would tend to drift as it would lose lock. This last activity that we had was great, because it was our first time using the more enriched map, and the localization worked really well. It was kind of nice because [Christina] saw us through those struggles—she saw how tough it was to get the robot to fly. Besides this little bit of software optimization, Bualat says that Astrobee has been working well, without any other software issues or hardware issues of any kind. This is impressive for any robot, and especially so for a robot that was developed entirely on the ground and is now being used in space. And as for the astronauts whose job it is to test Astrobee, it sounds like they’re actually having some fun with it. There was a bit of concern initially that Astrobee’s impellers would be overly loud, but that might be a feature rather than a bug, as Bualat explains: “We’ve asked them if they found it noisy or annoying, and they said no—in fact, they said that you can certainly hear it, but they actually liked it because it means that Astrobee can’t sneak up on them.” Astrobee will be continuing its commissioning activities over the next few months, which includes tuning Bumble so that it can fly as robustly as possible. For example, Astrobee needs to be able to navigate if an astronaut moves in front of its navigation camera, blocking some of the view. Bumble will then get its perching arm installed and tested, after which the goal is to start working with some of the science payloads, like a gecko gripper, a RFID tracker, and a microphone array, which you can read more about here and here. Honey and Queen still need to go through their own start-up tests and calibrations, and Maria Bualat says the goal is to have multiple Astrobees buzzing around the ISS together “not too far in the future.” [ Astrobee ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11007389346273692
69,MIT News,Diagnosing cellular nanomechanics,Electronics and Technology,2019-10-07,-,http://news.mit.edu/2019/smart-mit-diagnosing-cellular-nanomechanics-1007,"  Researchers at Singapore-MIT Alliance for Research and Technology (SMART) and MIT’s Laser Biomedical Research Center (LBRC) have developed a new way to study cells, paving the way for a better understanding of how cancers spread and become killers. The new technology is explained in a paper published recently in Nature Communications. A new confocal reflectance interferometric microscope provides 1.5 microns depth resolution and better than 200 picometers height measurement sensitivity for high-speed characterization of nanometer-scale nucleic envelope and plasma membrane fluctuations in biological cells. It enables researchers to use these fluctuations to understand key biological questions, such as the role of nuclear stiffness in cancer metastasis and genetic diseases. “Current methods for nuclear mechanics are invasive, as they either require mechanical manipulation, such as stretching, or require injecting fluorescent probes that ‘light up’ the nucleus to observe its shape. Both these approaches would undesirably change cells' intrinsic properties, limiting study of cellular mechanisms, disease diagnosis, and cell-based therapies,” say Vijay Raj Singh, SMART research scientist, and Zahid Yaqoob, LBRC principal investigator. “With the confocal reflectance interferometric microscope, we can study nuclear mechanics of biological cells without affecting their native properties.” While the scientists can study about a hundred cells in a few minutes, they believe that the system can be upgraded in the future to improve the throughput to tens of thousands of cells. “Today, many disease mechanisms are not fully understood because we lack a way to look at how cells’ nucleus changes when it undergoes stress,” says Peter So, SMART BioSyM principal investigator, MIT professor, and LBRC director. “For example, people often do not die from the primary cancer, but from the secondary cancers that form after the cancer cells metastasize from the primary site — and doctors do not know why cancer becomes aggressive and when it happens. Nuclear mechanics plays a vital role in cancer metastasis as the cancer cells must ‘squeeze’ through the blood vessel walls into the bloodstream, and again when they enter a new location. This is why the ability to study nuclear mechanics is so important to our understanding of cancer formation, diagnostics, and treatment.” With the new interferometric microscope, scientists at LBRC are studying cancer cells when they undergo mechanical stress, especially during extravasation process, paving the way for new cancer treatments. Further, the scientists are also able to use the same technology to study the effect of “lamin mutations” on nuclear mechanics, which result in rare genetic diseases such as Progeria, which leads to fast aging in young children. The confocal reflectance interferometric microscope also has applications in other sectors. For example, this technology has the potential for studying cellular mechanics within intact living tissues. With the new technology, the scientists could shed new light on biological processes within the body’s major organs such as liver, allowing safer and more accurate cell therapies. Cell therapy is a major focus area for Singapore, with the government recently announcing a S$80 million (US $58 million) boost to the manufacturing of living cells as medicine. About BioSyM BioSystems and Micromechanics (BioSyM) Inter-Disciplinary Research Group brings together a multidisciplinary team of faculties and researchers from MIT and the universities and research institutes of Singapore. BioSyM’s research deals with the development of new technologies to address critical medical and biological questions applicable to a variety of diseases with an aim to provide novel solutions to the health care industry and to the broader research infrastructure in Singapore. The guiding tenet of BioSyM is that accelerated progress in biology and medicine will critically depend upon the development of modern analytical methods and tools that provide a deep understanding of the interactions between mechanics and biology at multiple length scales — from molecules to cells to tissues — that impact maintenance or disruption of human health. About Singapore-MIT Alliance for Research and Technology (SMART) Singapore-MIT Alliance for Research and Technology (SMART) is MIT’s research enterprise in Singapore, established in partnership with the National Research Foundation of Singapore (NRF) since 2007. SMART is the first entity in the Campus for Research Excellence and Technological Enterprise (CREATE) developed by NRF. SMART serves as an intellectual and innovation hub for research interactions between MIT and Singapore. Cutting-edge research projects in areas of interest to both Singapore and MIT are undertaken at SMART. SMART currently comprises an Innovation Centre and six Interdisciplinary Research Groups: Antimicrobial Resistance, BioSystems and Micromechanics, Critical Analytics for Manufacturing Personalized-Medicine, Disruptive & Sustainable Technologies for Agricultural Precision, Future Urban Mobility, and Low Energy Electronic Systems. SMART research is funded by the National Research Foundation Singapore under the CREATE program. About the Laser Biomedical Research Center (LBRC) Established in 1985, the Laser Biomedical Research Center is a National Research Resource Center supported by the National Institute of Biomedical Imaging and Bioengineering, a Biomedical Technology Resource Center of the National Institutes of Health. The LBRC’s mission is to develop the basic scientific understanding and new techniques required for advancing the clinical applications of lasers and spectroscopy. Researchers at the LBRC develop laser-based microscopy and spectroscopy techniques for medical applications, such as the spectral diagnosis of various diseases and investigation of biophysical and biochemical properties of cells and tissues. A unique feature of the LBRC is its ability to form strong clinical collaborations with outside investigators in areas of common interest that further the center’s mandated research objectives. ",Electronics and Technology,0.10738639943508016
70,MIT News,Strong mentorship through great decision-making,Computer Science,2019-10-03,-,http://news.mit.edu/2019/strong-mentorship-through-great-decision-making-c2c-1003,"  Faculty mentors Gabriella Carolini, Paula Hammond, and David Trumper are known for guiding students through the trenches of graduate school — one decision at a time.   Students encounter various obstacles in graduate school, many of which are unexpected. Selecting a research project may become an all-consuming task. Starting a family in graduate school may be both the best and the most-daunting decision. In addition to helping graduate students make choices, caring faculty mentors demonstrate support for the decisions graduate students make on their own and affirm that no obstacle is insurmountable. Through such acts of validation, these professors help to cultivate graduate students as productive and confident researchers. For this reason, among others, Carolini, Hammond, and Trumper have been honored as Committed to Caring (C2C). Gabriella Carolini: making extraordinary the norm One student extols Gabriella Carolini as “the single most-defining influence in my MIT experience to date.” This sentiment is far from an outlier. Associate professor in the Department of Urban Studies and Planning (DUSP), Carolini’s research focuses on the planning, implementation, and administration of infrastructure systems in vulnerable urban and peri-urban communities. Her work, largely based in sub-Saharan Africa and Latin America, examines how financing, project partnering practices, and project evaluations impact distributive justice in urban development, particularly with regard to water and sanitation services as well as community health. Carolini builds community in the DUSP International Development Group by sharing her own experiences with her students, and in doing so “fosters a friendly and inclusive work environment” (a C2C mentoring guidepost). One nominator remarks that Carolini’s “candor in sharing her experiences as a tenure-track female academic” has helped inform the student’s own career decisions. Another nomination describes Carolini as honest about the ups and downs of academia, including the tenure process, the management of research projects and publications, and family-work life balances. When an international graduate student and his wife were expecting a baby, Carolini — pregnant herself at the time — went out of her way to demonstrate her concern and personally provide support for them. Her actions made them feel like they “had a community to rely on at MIT.” This was especially appreciated in light of the current political climate, in which many international students have felt destabilized and socially isolated. In considering general obstacles for her students, Carolini explains that “choice” is perhaps their toughest hurdle. “Making a decision about what specific questions or issues to commit to is a perennially difficult challenge our students face. They are talented and able — so we understand why. But we all still have to choose to move forward.” Faculty members, Carolini says, need to help students find and commit to their research and professional practice aspirations. It helps when faculty members demonstrate excitement about students’ work and help them to develop a plan towards achievement. To Carolini, this means recognizing both the strengths and weaknesses of a project, and addressing the latter “without losing sight of the value of their work.” Paula Hammond: individual and departmental advancement Paula Hammond excels at actively listening to her students, helping her students move successfully through their programs, and improving departmental systems to encourage diversity and inclusion. Hammond is head of the Department of Chemical Engineering and the David H. Koch (1962) Professor in Engineering at MIT. The Hammond Research Group at the MIT Koch Institute for Integrative Cancer Research focuses on the self-assembly of polymeric nanomaterials, including the use of electrostatics and other complementary interactions to generate multifunctional materials with highly controlled architecture. Her work has a number of electro-optical, electro-mechanical, and biological applications. In cancer research, for example, the Hammond lab works on the generation of polymer-based films and nanoparticles for drug delivery. Whether students are struggling with qualifying exams or unproductive data, Hammond assures her students that their obstacles can be overcome. “I let them know that there is a path to a PhD here and we’re going to find it.” This sets students at ease, and alleviates much of the stress. Hammond says that all these challenges are “part of the journey, and everyone experiences them — we just need to get out on the other side of it.” Hammond provides channels for students to express their difficulties (a C2C mentoring guidepost). She says that during a recent departmental retreat, “we learned that there is a gendered experience for women in our department.” As department head, Hammond has set out to learn how this climate can shift into “one in which women feel equally recognized and equally able as soon as they walk in the door.” According to C2C nominators, Hammond makes every effort to empower students from diverse backgrounds. This is perhaps best illustrated by her ongoing commitment to the MIT Summer Research Program (MSRP). By welcoming MSRP interns into her lab, Hammond gives her own graduate students valuable experiences mentoring potential future labmates. “I feel a responsibility as a woman, and as an underrepresented minority to be visible to others,” Hammond says. “I want to say, ‘There are people who look like you and have similar backgrounds to you doing this work.’” David Trumper: champion of balance David Trumper is a reliable guide for his students, who say that he invariably “encourages us to do what we are passionate about and supports us in any way he can.” As professor of mechanical engineering, Trumper’s research investigates the design of precision mechatronic systems, magnetic levitation for nanometer-scale motion control, and novel actuation and sensing devices. As director of the Precision Motion Control Laboratory, Trumper works with his group to conduct research in the design of electromechanical systems for precise positioning applications, such as semiconductor photolithography, high-speed machine tools, and scanned probe microscopy. According to his students, Trumper takes the time to sit down and listen, not just as a professor or advisor, but also as a friend. One nominator wrote, “we talked about the loss of my dad, about the presidential election, about life in general, and about life at MIT. It was the most encouraging and helpful experience that I have ever had with an MIT professor.” In addition to promoting a healthy work/life balance (a C2C mentoring guidepost), Trumper’s students say he constantly stresses balance of every kind, for example “between creative thinking and precise detailing, between analysis and learning from prototyping, and between going fast and slowing down.” Trumper demonstrates to his students that balance is important, and ultimately more effective for everyone. He encourages his students to make time for creative efforts and physical activities. Trumper leads by example, spending time photographing, hiking, and rock climbing. “I also encourage my students to be more broadly educated by reading books that have nothing to do with their technical field,” he relays. “Reading for pleasure should be a lifelong habit.” Along with balance comes perspective, and Trumper is always offering a positive outlook. One student recalls that after making a careless calculation, his experiment failed. Trumper “did not criticize me once about the mistake, and instead, he simply said: ‘Now you will never forget this, which is great’.” More on Committed to Caring  The Committed to Caring (C2C) program is an initiative of the Office of Graduate Education and contributes to its mission of making graduate education at MIT “empowering, exciting, holistic, and transformative.” C2C invites graduate students from across MIT’s campus to nominate professors whom they believe to be outstanding mentors. Selection criteria for the honor include the scope and reach of advisor impact on the experience of graduate students, excellence in scholarship, and demonstrated commitment to diversity and inclusion. By recognizing the human element of graduate education, C2C seeks to encourage excellent advising and mentorship across MIT’s campus.  ",Society,0.10731084047723416
71,Science Daily,How Much Are You Polluting Your Office Air Just by Existing?,Society,2019-10-03,-,https://www.sciencedaily.com/releases/2019/10/191003162541.htm,"   To find out, a team of engineers at Purdue University has been conducting one of the largest studies of its kind in the office spaces of a building rigged with thousands of sensors. The goal is to identify all types of indoor air contaminants and recommend ways to control them through how a building is designed and operated. ""If we want to provide better air quality for office workers to improve their productivity, it is important to first understand what's in the air and what factors influence the emissions and removal of pollutants,"" said Brandon Boor, an assistant professor of civil engineering with a courtesy appointment in environmental and ecological engineering. The data is showing that people and ventilation systems greatly impact the chemistry of indoor air -- possibly more than anything else in an office space. The researchers will present their initial findings at the 2019 American Association for Aerosol Research Conference in Portland, Oregon, Oct. 14-18. ""The chemistry of indoor air is dynamic. It changes throughout the day based on outdoor conditions, how the ventilation system operates and occupancy patterns in the office,"" Boor said. The building, called the Living Labs at Purdue's Ray W. Herrick Laboratories, uses an array of sensors to precisely monitor four open-plan office spaces and to track the flow of indoor and outdoor air through the ventilation system. The team developed a new technique to track occupancy by embedding temperature sensors in each desk chair.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Through use of the Living Labs, Boor's team has begun to identify previously unknown behaviors of chemicals called volatile organic compounds, such as how they are transformed in ventilation systems and removed by filters. ""We wanted to shed light on the behind-the-scenes role ventilation systems have on the air we breathe,"" Boor said. Boor teamed up with researchers at RJ Lee Group to deploy a highly sensitive ""nose"" -- an instrument that scientists call a proton transfer reaction time-of-flight mass spectrometer. The instrument, typically used for measuring outdoor air quality, helped ""sniff"" out compounds in human breath, such as isoprene, in real time. Boor's team found that isoprene and many other volatile compounds linger in the office even after people have left the room. A greater number of people in a room also means more emissions of these compounds. A YouTube video is available at https://youtu.be/mi1xi31QAfQ. ""Our preliminary results suggest that people are the dominant source of volatile organic compounds in a modern office environment,"" Boor said. ""We found levels of many compounds to be 10 to 20 times higher indoors than outdoors. If an office space is not properly ventilated, these volatile compounds may adversely affect worker health and productivity."" The team also revealed that a pollutant entering from outside, ozone, disappears inside. This is because ozone interacts with other indoor compounds and the vast surfaces of a furnished office. The researchers found that ozone and compounds released from peeling an orange, called monoterpenes, mix to form new, super-tiny particles as small as one-billionth of a meter. The newly formed particles could be toxic because they are small enough to get into the deepest regions of a person's lungs. The effects of volatile compounds released in an office might not just be restricted to indoors. The researchers believe that chemicals emitted from self-care products such as deodorant, makeup, and hair spray may elevate levels outdoors as they are vented outside by the ventilation system. This work is funded in part by the National Science Foundation Environmental Engineering Program, the Alfred P. Sloan Foundation Chemistry of Indoor Environments Program and the Purdue Research Foundation. ",Matter & Energy,0.10698817984610413
72,Stanford,Precision physics with ‘tabletop’ experiments,Science,2019-10-15,-,https://news.stanford.edu/2019/09/25/precision-physics-tabletop-experiments/,"   The history of particle accelerators is one of seemingly constant one-upmanship. Ever since the 1920s, the machines – which spur charged particles to near light speeds before crashing them together – have grown ever larger, more complex and more powerful.    Tabletop physics Physicists at Stanford and SLAC are inventing small, sensitive experiments to explore fundamental questions in physics that have eluded large particle accelerators. This series of stories explores these so-called “tabletop” experiments.  Precision physics with ‘tabletop’ experiments A different kind of gravitational wave detector A radio that searches for dark matter    Consider: When the 2-mile-long linear accelerator at SLAC National Accelerator Laboratory opened for business in 1966, it could boost electrons to energies of about 19 gigaelectronvolts. The Large Hadron Collider (LHC) at CERN, which finished construction in 2008, can boost protons to more than 700 times higher energy levels and resides in a massive elliptical tunnel wide enough to encircle a small town. Future supercolliders being planned by CERN, China and Japan promise to be even more immense and energetic (and also more expensive). The strategy has paid off handsomely with discoveries that have helped confirm the soundness of the Standard Model, our current best understanding of how nature’s fundamental forces and subatomic matter interact. As successful as particle accelerators have been, however, Stanford theorists Savas Dimopoulos and Peter Graham are betting that scientific treasures await discovery in the other direction as well. For years, the pair have argued that smaller and less expensive, but more sensitive, instruments could help answer stubborn mysteries in physics that have resisted the efforts of even the largest atom smashers – questions like “What is dark matter?” and “Do extra spatial dimensions exist?” “Peter and I and our group have been thinking about this for 15 years,” said Dimopoulos, who is the Hamamoto Family Professor at Stanford’s School of Humanities and Sciences. “We were sort of lonely but very happy because we were exploring new territory all the time and it was a lot of fun. We felt like eternal graduate students.”  Peter Graham and Savas Dimopoulos are among Stanford physicists working on smaller-scale devices to answer large questions. (Image credit: L.A. Cicero)  Scalpel vs. hammer But their ideas have been slowly gaining traction among physicists, and last fall the Gordon and Betty Moore Foundation awarded Stanford and SLAC researchers three grants totaling roughly $15 million to use quantum technologies to explore new fundamental physics. Key to these efforts are the kinds of small-scale, “tabletop” experiments (so-called because most of them would fit on a lab bench or in a modest-sized room) that Dimopoulos and Graham have long advocated for. “Everything is smaller, except for the ideas,” Dimopoulos quipped. “These types of experiments could help solve some very important problems in physics.” The instruments Dimopoulos and Graham have in mind exploit the weird properties of quantum mechanics – such as wave-particle duality and the seemingly telepathic link between entangled particles – to detect and measure minute signals and effects that particle accelerators are simply not attuned to. Tabletop experiments are considered high-risk, high-reward projects because they are generally cheaper to build and operate than colliders, said Asimina Arvanitaki, a theoretical physicist at the Perimeter Institute. “If you’re pitching a project that costs several billion dollars, you better have a very good reason for its existence and be reasonably sure you’re going to succeed,” added Arvanitaki, a former Stanford postdoc in Dimopoulos’ lab. “But the cost of tabletop experiments is so low, and the timescales for producing results is so short, that it takes some of that pressure off.” Building on existing technologies The Moore Foundation grants will fund three projects: Two are experimental and will focus on developing new technologies for detecting dark matter and measuring gravitational waves. But the third, worth about $2.5 million and awarded to Dimopoulos and Graham, will be used to further develop the theoretical underpinnings that will enable future experiments. “There’s been a history of particle accelerators discovering new physics and finding new particles, but it’s not clear that that can go on forever, so it’s important to think of other complementary ways to get at these underlying questions about nature,” said Ernie Glover, the Moore Foundation’s science program officer.   “Everything is smaller, except for the ideas. These types of experiments could help solve some very important problems in physics.” —Savas Dimopoulos Professor of Physics   Crucially, the experiments Dimopoulos and Graham are proposing rely on relatively mature, high-precision technologies that, for the most part, were developed with other uses in mind and for other fields, such as medicine and applied physics. “That’s what got us really excited,” Dimopoulos said. “We realized there were all these possibilities out there that particle theorists weren’t really thinking about.” A good example is nuclear magnetic resonance, or NMR, imaging, which forms the basis of magnetic resonance imaging, or MRI, a common medical scanning technique. A few years ago, Graham and others theorized that a proposed ultralightweight dark matter candidate called an axion could influence the nuclear spin of normal matter. Dark matter is thought to make up the bulk of the matter in the universe, but it has evaded every attempt so far at characterization. Excited, Graham contacted an atomic physicist at the University of California, Berkeley, named Dmitry Budker to discuss designing a dark matter detector based on this effect – only to discover that the technology already exists. “He said it’s going to work because what we were describing was basically NMR,” said Graham, a theoretical physicist at the Stanford Institute for Theoretical Physics. Graham and Budker teamed up with other physicists to design the Cosmic Axion Spin Precession Experiment, or “CASPEr,” which uses NMR (nuclear magnetic resonance) to detect axion and axion-like particles. These particles are predicted to have such weak interactions and low masses that they would never show up in a collider, which are better equipped to search for massive dark matter candidates such as WIMPs (weakly interacting massive particles).  Former Stanford postdoc Asimina Arvanitaki, now at the Perimeter Institute, has proposed several tabletop experiments to investigate physics beyond the reach of particle colliders. (Image credit: Colin Hunter)  Similarly, another Moore Foundation-funded tabletop experiment called MAGIS-100 relies on atom interferometry technology initially developed in the 1990s as a general-purpose tool for making precise measurements. The project, a collaboration between Stanford’s Mark Kasevich and Jason Hogan and researchers at Fermilab and other universities, could potentially detect ripples in spacetime known as gravitational waves around 1 hertz, a frequency range beyond the sensitivity of most existing or even proposed detectors. Current gravitational wave detectors like LIGO are sensitive to the very final moments of the black hole collisions that generate the spacetime ripples, but MAGIS-100 could provide scientists with a much longer viewing window. “LIGO saw just a fraction of a second of the event, but the black holes were twirling around each other and generating gravitational waves for millions or billions of years before that. Those waves were just in lower frequency bands,” Graham said. “By looking at other frequencies, we could observe the black holes for longer and perhaps discover new gravitational wave sources.” Intuition Dimopoulos and Graham plan to use the Moore Foundation-funding to continue devising new schemes for co-opting technologies like NMR and atom interferometry in the service of fundamental physics research. “It’s that connection that’s hard,” Graham said. “The experimental physicists and engineers who develop the technologies aren’t necessarily thinking about what other deep, fundamental questions could be tested, and the theorists are often unaware that tools for testing their ideas already exist.” But Dimopoulos and Graham are now old hands at making such connections. “In principle, you have to know all possible technologies,” Graham said. “In practice, you just have to know the right ones, but it takes a nontrivial intuition to realize something like ‘Oh, wait a minute, it looks like this technique might actually be able to observe extra dimensions or some other new physics.’” In one sense, what Dimopoulos and Graham are advocating for is a return to the way physics was done before colliders came to play such an important role in physics and the division of physicists into primarily theoretical and experimental camps. “Before World War II, physics was just like what we’re doing right now,” Dimopoulos said. “Felix Bloch was both a theorist and an experimentalist, and so was Enrico Fermi. Even Einstein did experiments. There wasn’t a ready group of experimentalists that you could outsource your ideas to. You had to invent the techniques and look around at emerging technologies.” To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Space & Time,0.10674293210127633
73,Science Daily,Light My Fire: How to Startup Fusion Devices Every Time,Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010095809.htm,"   Physicists at the U.S. Department of Energy's (DOE) Princeton Plasma Physics Laboratory (PPPL), working with researchers at the Culham Centre for Fusion Energy (CCFE) in the United Kingdom, have constructed a simulation framework for developing and testing the plasma startup recipes for the National Spherical Torus Experiment-Upgrade (NSTX-U) at PPPL and the Mega Ampere Spherical Tokamak-Upgrade (MAST-U) at CCFE. ""This is a tool to help an operator design a successful startup recipe before sitting down in the driver seat at NSTX-U or MAST-U,"" said physicist Devon Battaglia, who leads the team of operators on the NSTX-U experiment and is lead author of a paper describing the model in the journal Nuclear Fusion. Fusing plasma particles Fusion fuses plasma particles to release massive amounts of energy. Scientists around the world are seeking to replicate the celestial process to produce a safe, clean, and virtually inexhaustible supply of power for generating electricity. The typical recipe for forming a plasma in magnetic fusion devices called tokamaks begins by applying voltage across a gas injected into a strong magnetic field. The gas becomes plasma within a few milliseconds and quickly heats up to millions of degrees. Creating the best recipe for a successful startup calls for finely tuning the gas pressure with a consistent evolution of the electric and magnetic fields, a delicate task that falls to the operator. The new simulation capability enables operators to quickly achieve that balance, significantly reducing the amount of time spent running experiments to find a recipe that works. Researchers derived and validated the models in the simulation framework against data collected from past experiments on the NSTX-U and its predecessor, and the predecessor of MAST-U. Battaglia worked closely with physicists at CCFE to develop the new model, making the paper a joint effort, and will travel there again for the scheduled startup of MAST-U. ""Plasma breakdown is a key milestone for MAST-U and Devon's work provides valuable insight into the best route to achieve startup,"" said physicist Andrew Thornton, lead operator at MAST-U and coauthor of the paper. ""Having Devon's expertise on site when we restart will be immensely valuable as he has performed similar experiments on NSTX-U that can guide efforts on MAST-U."" Providing new insights Development of the model provides new insights into the startup of spherical tokamaks such as NSTX-U and MAST-U, which are shaped like cored apples rather than the doughnut-like shape of more widely used conventional tokamaks. The process of putting together the simulation framework has also contributed to efforts to develop computational tools for the first operation of ITER, the international tokamak under construction in France to demonstrate the practicality of fusion energy. ",Matter & Energy,0.1055618898269175
74,IEEE,U.S. Semiconductor Industry Veterans Keep Wary Eyes on China,Electronics and Technology,2019-10-11,-,https://spectrum.ieee.org/view-from-the-valley/semiconductors/devices/semiconductor-industry-veterans-keep-wary-eyes-on-china,"      How might the U.S. chip industry solve a problem like China?  A panel of semiconductor industry veterans took up this question at a Churchill Club event this week. The group generally expressed worry about the impact China will have on the future of the U.S. chip industry, and the lack of good ideas about how the U.S. industry can respond to threats posed by China.  “China is the ultimate conundrum,” says Stanford president emeritus and MIPS Computer Systems founder John Hennessy. “It’s a large market that U.S. companies need access to, together with being what will become a major technical competitor. We have never faced that.”  The consolidation of silicon manufacturing into two main foundries raises the threat level, pointed out Diane Bryant, former Intel and Google Cloud executive.  “You really just have TSMC and Samsung left,” she said. “And TSMC is in Taiwan, so you have to be thinking about China and the threat to Taiwan, and what will happen to TSMC.”  China will take over Taiwan “the same time North Korea takes over South Korea,” quipped Hennessy, giving it control over most of the world’s semiconductor manufacturing capabilities.  “What do you do tomorrow if TSMC and Samsung are off limits?” he asked his fellow panel members.  “You can’t go to Global Foundries,” which indeed has some U.S. semiconductor manufacturing capability, said Bryant, “unless you really want Moore’s Law to be dead.” (Global Foundries recently stopped developing the most advanced semiconductor processes.)   Rodrigo Liang, CEO of SambaNova Systems, argued that fixing this problem can only be done at the level of the U.S. government.  Pradeep Sindhu, founder of Juniper Networks and founder and CEO of Fungible, agreed. “The U.S. government needs an industrial policy,” he said, “and it doesn’t have one.”  The foundry issue is a long-term problem. Perhaps a nearer term question is how the growing capability of China’s tech industry will impact U.S.-based companies.       “China is talking about becoming tech independent, becoming net exporters,” said Bryant. “We can talk about how many years [it will take], but it is inevitable.”  Companies in China will catch up for several reasons, panelists indicated. For one, said Sindhu, they are very hungry to learn.   For another, said Navin Chaddha, managing director of the Mayfield Fund, China’s huge market gives Chinese companies a boost. “Usually innovation happens when you are close to a market,” he said. To date, the U.S. companies and Samsung have benefitted from the boom” in the Chinese tech market, but now “we are seeing Chinese companies benefitting from their local market… and China is the biggest market when it comes to broadband users.”  A solution?  “Invest in that market,” says Chaddha.  That strategy is not without pitfalls, Hennessy indicated. “What happens to your technology when you ship it over there?” he asked.  “To the extent that we can protect it, we will,” Sindhu said.  Hennessy remained skeptical. “Just wait until you sign the deal and send it over,” he said.  “This isn’t a redo of semiconductor wars with Japan in the 80s,” he concluded.  “This is a country that has scale, that has entrepreneurial zeal. They will give us a run for the money.”    IEEE Spectrum’s blog featuring the people, places, and passions of the world of technologists in Silicon Valley and its environs. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Matter & Energy,0.10536708000254794
75,MIT News,Learning about China by learning its language,Computer Science,2019-10-11,-,http://news.mit.edu/2019/learning-about-china-learning-language-max-allen-1011,"  Among MIT students who didn’t grow up speaking Chinese, few are able to discuss “machine learning models” in passable Mandarin. But that is just what computer science and engineering senior Max Allen is able to do, and this ability comes as a result of academic work, stints abroad, an internship, and also just having the passion to learn Chinese. With China a growing economic powerhouse and leader in STEM, it is no wonder that more and more students are attracted to studying Chinese. Nationally, enrollments in Chinese classes are up, as they are at MIT. But for Max Allen, his interest was first piqued by a teacher’s visit to his eighth-grade class. Intrigued by the sound of the language and structure of the writing system, Allen started taking Chinese classes in high school. To him, learning the language was akin to a big puzzle whose solution is slowly revealed. And since Allen has always been fond of puzzles, he wanted to pursue this. After only two years of high-school language study, Allen spent his 11th-grade year living with a host family in Beijing and attending school through a program called School Year Abroad. Allen returned to the United States able to converse in Mandarin, and also more adept at fitting in culturally. He found that living with a family gives you a level of familiarity with people that is hard to achieve otherwise. Chinese has gradually occupied a greater and greater area of interest for Allen. Upon entering MIT, he decided to pursue a major in computer science and engineering (Course 6-3). After discovering that he could take Chinese to fulfill his humanities concentration requirements, Allen took Chinese V and VI, building on the work he did in high school. Even among MIT students who are known for high academic achievement, Chinese Lecturer Tong Chen noted that Allen stood out for his effort and seriousness. The more classes he took, and the more time he invested, the more Allen began to consider how Chinese might be part of his future academic and career paths. In spring 2018, Allen took “Business Chinese” as an elective concentration subject. Business Chinese helped Allen understand social dynamics and subtleties of social relations in a business setting in China, including how these express themselves in language. As Panpan Gao, the instructor of Business Chinese, explains, the pedagogical approach of the class emphasizes case studies: “Through case studies of multinational companies and introductions to crucial business issues in China, we try to help students better understand Chinese business culture and trends, and expand their language skills so that they can communicate effectively and professionally with Chinese speakers in the workplace.” The class really got Allen thinking about whether he might want to pursue jobs that would employ his knowledge of Chinese. Allen put his Chinese skills to good use the following summer. He took an engineering internship with Airbnb — on a team with a special focus on mitigating financial fraud coming from China. The team was mostly made up of Chinese nationals, and team members generally discussed work matters in Mandarin. To do business in China, the team would need to understand how to market the product to Chinese customers; how to build a secure platform; and how to build payment applications that are in line with expectations of Chinese consumer. This experience gave Allen a hands-on taste of the complexities of functioning in a Chinese business context. After the internship, Allen realized that to take his Chinese to the next level, he would need to put aside other academic pursuits for a period and spend more time studying the language in an immersive Chinese-speaking setting. He spent academic year 2018-2019 abroad studying Chinese: the fall in Taipei at the International Chinese Language Program of National Taiwan University, and the spring in Beijing at the Inter-University Program for Chinese Language Studies at Tsinghua University. Both programs are top Chinese language centers in the world and are intensive instructional programs with hours of work a day devoted to learning Mandarin. He particularly appreciated the intensive focus on conversation. While abroad, Allen found that when he ventured to out-of-the-way spots, he encountered curiosity from strangers who were less accustomed to seeing tourists. But when he demonstrated he could speak Chinese, people warmed up. “Speaking their native language helps to establish trust and rapport, which is important when they see you as just another outsider. But once a certain level of trust is established, people become more comfortable talking about meaningful things. And that's where the time investment of learning the language really pays off.” Now back at MIT for his senior year, Allen is considering how his multiple interests in computer science, international business, Chinese language, and cross-cultural communication skills might combine into a career path. The answer will take some time to untangle, but Allen is always up for the challenge of a big puzzle, and will remain open to the possibilities as he heads toward graduation. ",Society,0.10489557017678602
76,Science Daily,Analysis of Galileo's Jupiter Entry Probe Reveals Gaps in Heat Shield Modeling,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110659.htm,"   Researchers at the Universidade de Lisboa and the University of Illinois at Urbana-Champaign report their findings from new fluid radiative dynamics models using data transmitted from the of Galileo's 30-second entry. The paper, published in Physics of Fluids, from AIP Publishing, employs new computational techniques developed in the nearly 25 years since the mission. ""Early simulations for the probe design were conducted in the 1980s,"" said Mario Lino da Silva, an author on the paper. ""There are some things we can do in 2019, because we have the computational power, new devices, new theories and new data."" Galileo's probe entered Jupiter's gravity traveling 47.4 kilometers per second, making it one of the fastest human-made objects ever. The fireball caused by the descent warmed the carbon phenolic heat shield to temperatures hotter than the sun's surface. Data from the probe revealed the rim of the heat shield burned significantly more than even today's models would predict, measured by what is called the recession rate. ""The fireball is a kind of soup where a lot of things happen at the same time,"" he said. ""One problem with modeling is that there are many sources of uncertainty and only one observed parameter, the heat shield recession rate."" The group recalculated features of the hydrogen-helium mixture the probe passed through, such as viscosity, thermal conductivity and mass diffusion, and found the oft-cited Wilke/Blottner/Eucken transport model failed to accurately model interactions between hydrogen and helium molecules. They found the radiative heating properties of hydrogen molecules played a significant role in the additional heating the probe's heat shield experienced. ""The built-in heat shield engineering margins actually saved the spacecraft,"" Lino da Silva said. Lino da Silva hopes the work helps improve future spacecraft design, including upcoming projects to explore Neptune that will likely not reach their destinations until after he has retired. ""In a way, it's like building cathedrals or the pyramids,"" he said. ""You don't get to see the work when it's finished."" Lino da Silva next looks to validate some of the simulated findings by reproducing similar conditions in a shock-tube facility tailored for reproducing high-speed flows. ",Space & Time,0.10428942071820449
77,IEEE,Wanted: A Bomb Detector as Sensitive as a Dog's Nose,Electronics and Technology,2019-10-12,-,https://spectrum.ieee.org/tech-talk/semiconductors/devices/using-a-twopronged-approach-to-detect-explosive-substances-from-bombs,"      If a suicide bomber lurks in the public with an explosive device, bomb-sniffing dogs can often detect the explosive chemicals from the tiniest whiff—these canine superheroes can sense the presence of the explosive triacetone triperoxide (TATP) if just a few molecules are present, on the scale of parts per trillion. Researchers at the University of Rhode Island are striving to make a comparable device for detecting TATP in its vapor form. Their new detection system, which pairs a conductance sensor with a traditional thermodynamic sensor, confirms the presence of TATP at the level of parts per billion (ppb). Their work is described in a study published on 2 October in IEEE Sensors Letters. TATP is a common choice of ingredient for terrorists who make explosives. It was used in the 2015 Paris attacks, the 2016 Brussels airport bombings, the 2017 concert bombing in Manchester, and most recently the 2018 bombings in Surabaya, Indonesia. “All of the IEDs used in these attacks relied on TATP as the detonator and often times the energetic material itself,” explains Otto Gregory, a researcher involved in the study. But, he notes, “No electronic trace detection system currently exists that is capable of continuously monitoring TATP or its precursors that can compete with a dog’s nose.” His team, which has received funding for TATP sensors from the U.S. Department of Homeland Security for the last 10 years, hopes to change that. The researchers first developed a thermodynamic sensor that detects TATP at 78 ppb, and can detect 2,4-DNT (a decomposition product of the explosive substance TNT) at 2 ppb. The thermodynamic sensor uses two microheaters: one coated with a metal oxide catalyst, and one without a catalyst coating. When explosive substances like TATP or 2,4-DNT come in contact with the microheaters, the device analyzes the thermodynamic difference between the catalyst and non-catalyst reactions—which reveals the nature of the substance. To make the device more accurate at confirming the presence of these explosives, the researchers added a conductance sensor. The conductance sensor analyzes the same catalyst reactions as the thermodynamic sensor, but instead measures the resistivity changes that occur. “Combining a thermodynamic platform with a conductometric platform provides a built-in redundancy that can mitigate false positives and negatives. Both platforms operate as independent systems looking for a unique ‘fingerprint’ or response to same explosive molecule,” explains Peter Ricci, a co-author and chemical engineer at the University of Rhode Island. The researchers tested mircoheaters made from different catalyst metals, including tin oxide, zinc oxide, and copper oxide. In terms of measuring conductance, the copper oxide was particularly sensitive to detecting 2,4-DNT. Copper oxide was more than three times as sensitive as the other materials in measuring the heat effect and 13 times as sensitive for conductance. Gregory notes that implementing this device in a real-world setting depends on how well they can downsize the system and make it portable. “Our current detection system employs both a thermodynamic and conductometric platform that fits into a small toolbox,” he says. “The final version of our device would have a much smaller footprint, in the form of a handheld or even a wearable [device].” The catalyst coatings are currently layered on ultrathin alumina ceramic substrates, which are responsible for the systems extraordinary sensitivity. But these substrates are still thicker than desired. “Sensors fabricated on much thinner substrates would drastically reduce operating temperature and would also lower the power requirements of the system, thus enabling a smaller portable detection system. We are working towards that goal everyday,” says Ricci.  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Matter & Energy,0.10425747375941584
78,Science Daily,Sharing Data for Improved Forest Protection and Monitoring,Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010125619.htm,"   Forest biomass is an essential indicator for monitoring the Earth's ecosystems and climate. It also provides critical input to greenhouse gas accounting, estimation of carbon losses and forest degradation, assessment of renewable energy potential, and for developing climate change mitigation policies. Although satellite remote sensing technology now allows researchers to produce extensive maps of aboveground biomass, these maps still require reliable, up-to-date, on-site data for calibration and validation. Collecting data in the field by measuring trees and documenting species is, however, a very labor intensive, expensive, and time-consuming exercise and it would therefore make sense to bring together the many extant data sets to provide real added value for a number of applications. In terms of policy applications, doing so can also lead to improved biomass products and better monitoring of forest resources, which could in turn lead to more effective forest protection measures. In a new paper published in the journal Scientific Data, 143 researchers involved in this type of data collection in the field, explored whether it was possible to build a network that openly shares their data on biomass for the benefit of different communities. They particularly wanted to see if they could bring together as much on-site data on biomass as possible to prepare for new satellite missions, such as the European Space Agency's BIOMASS mission, with a view to improving the accuracy of current remote sensing based products, and developing new synergies between remote sensing and ground-based ecosystem research communities. Their efforts have resulted in the establishment of the Forest Observation System (FOS) -- an international, collaborative initiative that aims to establish a global on-site forest aboveground biomass database to support Earth Observation and to encourage investment in relevant field-based measurements and research. ""Keeping in mind that this paper is a data descriptor and not a conventional paper with hypotheses, the whole idea behind this study is a new open database on biomass data. This is important for the following reasons: First, it represents a way to link the ecological/forestry and remote sensing communities. It also overcomes existing data sharing barriers, while promoting data sharing beyond small, siloed communities. Lastly, it provides recognition to the people working in the field, including those who collect the data, which is why there are 143 coauthors on this paper, as they are all contributors to the database,"" explains study lead author Dmitry Shchepashchenko, a researcher in the IIASA Ecosystems Services and Management Program. The researchers collected data from 1,645 permanent forest sample plots from 274 locations distributed around the globe. This data has now been made available for download via the FOS website. The initiative represents the first attempt at bringing this type of data together from different networks in a single location. The researchers point out that their work in this regard is ongoing and there are plans to continue adding more data sets and networks to the FOS. In addition to promoting data sharing, the system also promotes a new leading network on biomass data (through the FOS), which IIASA is leading and will continue to grow into the future. Apart from the obvious benefits that data sharing hold for the scientific community, the data are also essential for training various models at IIASA such as the BioGeoChemistry Management Model (BGC-MAN) and the Global Forest Model (G4M). Several on-going IIASA projects, as well as other ecological-, biophysical-, and economic models and projects outside of IIASA will also benefit, which means that providing access to the data can improve models and understanding of biomass more generally. ""A great deal of effort has gone into collecting forest data in the past, but people working in the field (ecologists and forestry scientists) hardly ever share the collected data, or if they do, they share it only within ecological networks. The data are valuable not only for ecology, but also for remote sensing calibration and validation, in other words, to train algorithms that create biomass maps, and for assessing the accuracy of the products along with inputs to a variety of models. This piece of work represents a real step forward in sharing a very valuable biomass data set,"" concludes IIASA researcher Linda See, who was also a study coauthor. ",Environment,0.10394892980698822
79,Science Daily,How Preprocessing Methods Affect the Conversion Efficiency of Biomass Energy Production,Environment,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011165320.htm,"   Grift is co-author on a new study, published in Bioresource Technology Reports, that takes a look at the bioconversion efficiency of two products often used as biomass for energy production, miscanthus giganteus and sugarcane bagasse. ""Our goal was to determine how much energy it takes to prepare these materials. It's a comprehensive look at various preprocessing methods and their relationship to conversion efficiency,"" he explains. The two materials were chosen because of their importance for energy production. Miscanthus is typically grown as an ornamental crop, but it has a high amount of biomass and grows easily with very little nitrogen use. Sugarcane bagasse is the byproduct left over after sugarcane is crushed to extract the juice for sugar. The study was done in collaboration with chemists from University of California at Berkeley. Grift says the interdisciplinary approach makes the research unique, because it considers the whole energy balance. The U of I researchers studied the energy expenditure of harvesting and preprocessing materials, while the Berkeley chemists focused on converting the biomass to glucose, which is used to make ethanol. The researchers defined the percentage of inherent heating value (PIHV), which measures the amount of energy going into and out of the production process. ""It tells you that you have a certain amount of biomass, which contains a certain amount of energy. How much energy do you spend on processing? You don't want to spend more than 5% of the total energy value,"" Grift says.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The researchers subjected the two materials to nine different preprocessing methods, either separately or as a blend. Preprocessing is done for various reasons, Grift explains. After the crop is harvested, it needs to be transported to a processing plant, and to make transportation efficient, the material first undergoes a process called comminution, in which is it chopped or cut into smaller pieces, and then it is compressed. Grift explains that harvesting and compression do not add much to the energy equation. The main source of energy expenditure is comminution, or size reduction. That brings the energy expenditure to 5%. ""Smaller particle sizes make compression easier,"" he says. ""It's also better for energy production, because it provides a larger surface area for enzymes to attach to in the conversion process. But comminution takes a certain amount of energy, so there is a tradeoff."" The preprocessing methods included chopped and cut stems, pelletization, comminution, and various levels of compression. Of the nine treatment groups, five included miscanthus, three included sugarcane bagasse, and one included a blend of the two products. The processed materials were all subjected to the same chemical processes to release the glucose. The researchers also evaluated the effects of particle size, compression level, and blending on biomass conversion efficiency. The results showed that comminution had a positive effect on the efficiency of miscanthus but not sugarcane bagasse, while the opposite was the case for pelletization. The researchers also found that a 50/50 blend of the two materials had higher conversion efficiency than sugarcane bagasse, but there was no significant difference compared to miscanthus alone. The results can be used to help make biomass energy production more efficient, Grift says. ""The differences are not huge. But if you want to do something on a larger scale it's actually quite important to figure these things out,"" he explains. Grift emphasizes that the results are preliminary and should be examined in further studies. Continued research is needed to substantiate the findings and to broaden the knowledge base to other products and other preprocessing methods. ",Matter & Energy,0.10260685771573877
80,Science Daily,Physicists Have Found a Way to 'Hear' Dark Matter,Space & Time,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009093942.htm,"   Dark matter is a mysterious substance that makes up 85% of the matter in the universe. Originally introduced to explain why the Strong Force (which holds together protons and neutrons) is the same backwards and forwards in time, the so called axion would provide a natural explanation for dark matter. Rather than discrete particles, axion dark matter would form a pervasive wave flowing throughout space. The axion is one of the best explanations for dark matter but has only recently been the focus of large scale experimental effort. Due to this renaissance there has been a rush to come up with new ideas for how to look for the axion in all the areas where it could be hiding. ""Finding the axion is a bit like tuning a radio: you have to tune your antenna until you pick up the right frequency. Rather than music, experimentalists would be rewarded with 'hearing' the dark matter that the Earth is travelling through. Despite being well motivated, axions have been experimentally neglected during the three decades since they were named by coauthor Frank Wilczek,"" says Dr. Alexander Millar, Postdoctor at the Department of Physics, Stockholm University, and author of the study. The key insight of the research team's new study is that inside a magnetic field axions would generate a small electric field that could be used to drive oscillations in the plasma. A plasma is a material where charged particles, such as electrons, can flow freely as a fluid. These oscillations amplify the signal, leading to a better ""axion radio."" Unlike traditional experiments based on resonant cavities, there is almost no limit on how large these plasmas can be, thus giving a larger signal. The difference is somewhat like the difference between a walkie talkie and a radio broadcast tower. ""Without the cold plasma, axions cannot efficiently convert into light. The plasma plays a dual role, both creating an environment which allows for efficient conversion, and providing a resonant plasmon to collect the energy of the converted dark matter,"" says Dr. Matthew Lawson, Postdoctor at the Department of Physics, Stockholm University, also author of the study. ""This is totally a new way to look for dark matter, and will help us search for one of the strongest dark matter candidates in areas that are just completely unexplored. Building a tuneable plasma would allow us to make much larger experiments than traditional techniques, giving much stronger signals at high frequencies,"" says Dr. Alexander Millar. To tune this ""axion radio"" the authors propose using something called a ""wire metamaterial,"" a system of wires thinner than hair that can be moved to change the characteristic frequency of the plasma. Inside a large, powerful magnet, similar to those used in Magnetic Resonance Imaging machines in hospitals, a wire metamaterial turns into a very sensitive axion radio. Searching for dark matter with plasmas will not remain just an interesting idea. In close collaboration with the researchers, an experimental group at Berkeley has been doing research and development on the concept with the intent of building such an experiment in the near future. ""Plasma haloscopes are one of the few ideas that could search for axions in this parameter space. The fact that the experimental community has latched onto this idea so quickly is very exciting and promising for building a full scale experiment,"" says Dr. Alexander Millar. ",Space & Time,0.10241476212432551
81,Science Daily,Scientists Pinpoint Cause of Harmful Dendrites and Whiskers in Lithium Batteries,Matter & Energy,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111723.htm,"   The team, led by Chongmin Wang at the Department of Energy's Pacific Northwest National Laboratory, has shown that the presence of certain compounds in the electrolyte -- the liquid material that makes a battery's critical chemistry possible -- prompts the growth of dendrites and whiskers. The team hopes the discovery will lead to new ways to prevent their growth by manipulating the battery's ingredients. The results were published online Oct. 14 in Nature Nanotechnology. Dendrites are tiny, rigid tree-like structures that can grow inside a lithium battery; their needle-like projections are called whiskers. Both cause tremendous harm; notably, they can pierce a structure known as the separator inside a battery, much like a weed can poke through a concrete patio or a paved road. They also increase unwanted reactions between the electrolyte and the lithium, speeding up battery failure. Dendrites and whiskers are holding back the widespread use of lithium metal batteries, which have higher energy density than their commonly used lithium-ion counterparts. The PNNL team found that the origin of whiskers in a lithium metal battery lies in a structure known as the ""SEI"" or solid-electrolyte interphase, a film where the solid lithium surface of the anode meets the liquid electrolyte. Further, the scientists pinpointed a culprit in the growth process: ethylene carbonate, an indispensable solvent added to electrolyte to enhance battery performance. It turns out that ethylene carbonate leaves the battery vulnerable to damage. Catching fast-moving action inside lithium batteries The team's findings include videos that show the step-by-step growth of a whisker inside a nanosized lithium metal battery specially designed for the study.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     A dendrite begins when lithium ions start to clump, or ""nucleate,"" on the surface of the anode, forming a particle that signifies the birth of a dendrite. The structure grows slowly as more and more lithium atoms glom on, growing the same way that a stalagmite grows from the floor of a cave. The team found that the energy dynamics on the surface of the SEI push more lithium ions into the slowly growing column. Then, suddenly, a whisker shoots forth. It wasn't easy for the team to capture the action. To do so, scientists integrated an atomic force microscope (AFM) and an environmental transmission electron microscope (ETEM), a highly prized instrument that allows scientists to study an operating battery under real conditions. The team used the AFM to measure the tiny force of the whisker as it grew. Much like a physician measures a patient's hand strength by asking the patient to push upward against the doctor's outstretched hands, the PNNL team measured the force of the growing whisker by pushing down on its tip with the cantilever of the AFM and measuring the force the dendrite exerted during its growth. The recipe for electrolyte  The team found that the level of ethylene carbonate directly correlates with dendrite and whisker growth. The more of the material the team put in the electrolyte, the more the whiskers grew. The scientists experimented with the electrolyte mix, changing ingredients in an effort to reduce dendrites. Some changes, such as the addition of cyclohexanone, prevented the growth of dendrites and whiskers. ""We don't want to simply suppress the growth of dendrites; we want to get to the root cause and eliminate them,"" said Wang, a corresponding author of the paper along with Wu Xu. ""We drew upon the expertise of our colleagues who have e xpertise in electrochemistry. My hope is that our findings will spur the community to look at this problem in new ways. Clearly, more research is needed."" Understanding what causes whiskers to start and grow will lead to new ideas for eliminating them or at least controlling them to minimize damage, added first author Yang He. He and the team tracked how whiskers respond to an obstacle, either buckling, yielding, kinking, or stopping. A greater understanding could help clear the path for the broad use of lithium metal batteries in electric cars, laptops, mobile phones, and other areas. ",Matter & Energy,0.10218425661377584
82,Science Daily,Hydrologic Simulation Models That Inform Policy Decisions Are Difficult to Interpret,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011165330.htm,"   Numerical models have become increasingly easy to employ with advances in computer technology and software with graphical user interface (GUI). While these technologies make the models more accessible, problems can arise if they are used by inexperienced modelers, says Juan Sebastian Acero Triana, a doctoral student in the Department of Agricultural and Biological Engineering at the University of Illinois. Acero Triana is lead author on a study that evaluates the accuracy of a commonly used numerical model in hydrology. Findings from the research show that even when the model appears to be properly calibrated, its results can be difficult to interpret correctly. The study, published in the Journal of Hydrology, provides recommendations for how to fine-tune the process and obtain more precise results. Model accuracy is important to ensure that policy decisions are based on realistic scenarios, says Maria Chu, a co-author of the study. Chu is an assistant professor of agricultural and biological engineering in the College of Agricultural, Consumer and Environmental Sciences and The Grainger College of Engineering at U of I. ""For example, you may want to estimate the impacts of future climate on the water availability over the next 100 years. If the model is not representing reality, you are going to draw the wrong conclusions. And wrong conclusions will lead to wrong policies, which can greatly affect communities that rely on the water supply,"" Chu says.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The study focuses on the Soil and Water Assessment model (SWAT), which simulates water circulation by incorporating data on land use, soil, topography, and climate. It is a popular model used to evaluate the impacts of climate and land management practices on water resources and contaminant movement. The researchers conducted a case study at the Fort Cobb Reservoir Experimental Watershed (FCREW) in Oklahoma to assess the model's accuracy. FCREW serves as a test site for the United States Department of Agriculture-Agricultural Research Service (USDA-ARS) and the United States Geological Survey (USGS); thus, detailed data are already available on stream flow, reservoir, groundwater, and topography. The study coupled the SWAT model with another model called MODFLOW, or the Modular Finite-difference Flow Model, which includes more detailed information on groundwater levels and fluxes. ""Our purpose was to determine if the SWAT model by itself can appropriately represent the hydrologic system,"" Acero Triana says. ""We discovered that is not the case. It cannot really represent the entire hydrologic system."" In fact, the SWAT model yielded 12 iterations of water movement that all appeared to be acceptable. However, when combined with MODFLOW it became clear that only some of these results properly accounted for groundwater flow. The researchers compared the 12 results from SWAT with 103 different groundwater iterations from MODFLOW in order to find a realistic representation of the water fluxes in the watershed.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Yielding several different results that all appear equally likely to be correct is called ""equifinality."" Careful calibration of the model can reduce equifinality, Acero Triana explains. Calibration must also be able to account for inherent limitations in the way the model is designed and how parameters are defined. In technical terms, it must account for model and constraint inadequacy. However, inexperienced modelers may not fully understand the intricacies of calibration. And because of the inherent constraints of both SWAT and MODFLOW, using metrics from just one model may not provide accurate results. The researchers recommend using a combination model called SWATmf, which integrates the SWAT and the MODFLOW processes. ""This paper presents a case study that provides general guidelines for how to use hydrological models,"" Acero Triana says. ""We show that to really represent a hydrologic system you need two domain models. You need to represent both the surface and the sub-surface processes that are taking place."" The differences in results may be small, but over time the effect could be significant, he concludes. ",Environment,0.10175111861955634
83,MIT News,Scientists detect tones in the ringing of a newborn black hole for the first time,Space & Time,2019-09-12,-,http://news.mit.edu/2019/ringing-new-black-hole-first-0912,"  If Albert Einstein’s theory of general relativity holds true, then a black hole, born from the cosmically quaking collisions of two massive black holes, should itself “ring” in the aftermath, producing gravitational waves much like a struck bell reverbates sound waves. Einstein predicted that the particular pitch and decay of these gravitational waves should be a direct signature of the newly formed black hole’s mass and spin. Now, physicists from MIT and elsewhere have studied the ringing of an infant black hole, and found that the pattern of this ringing does, in fact, predict the black hole’s mass and spin — more evidence that Einstein was right all along. The findings, published today in Physical Review Letters, also favor the idea that black holes lack any sort of “hair” — a metaphor referring to the idea that black holes, according to Einstein’s theory, should exhibit just three observable properties: mass, spin, and electric charge. All other characteristics, which the physicist John Wheeler termed “hair,” should be swallowed up by the black hole itself, and would therefore be unobservable. The team’s findings today support the idea that black holes are, in fact, hairless. The researchers were able to identify the pattern of a black hole’s ringing, and, using Einstein’s equations, calculated the mass and spin that the black hole should have, given its ringing pattern. These calculations matched measurements of the black hole’s mass and spin made previously by others. If the team’s calculations deviated significantly from the measurements, it would have suggested that the black hole’s ringing encodes properties other than mass, spin, and electric charge — tantalizing evidence of physics beyond what Einstein’s theory can explain. But as it turns out, the black hole’s ringing pattern is a direct signature of its mass and spin, giving support to the notion that black holes are bald-faced giants, lacking any extraneous, hair-like properties. “We all expect general relativity to be correct, but this is the first time we have confirmed it in this way,” says the study’s lead author, Maximiliano Isi, a NASA Einstein Fellow in MIT’s Kavli Institute for Astrophysics and Space Research. “This is the first experimental measurement that succeeds in directly testing the no-hair theorem. It doesn’t mean black holes couldn’t have hair. It means the picture of black holes with no hair lives for one more day.” A chirp, decoded On Sept. 14, 2015, scientists made the first-ever detection of gravitational waves — infinitesimal ripples in space-time, emanating from distant, violent cosmic phenomena. The detection, named GW150914, was made by LIGO, the Laser Interferometer Gravitational-wave Observatory. Once scientists cleared away the noise and zoomed in on the signal, they observed a waveform that quickly crescendoed before fading away. When they translated the signal into sound, they heard something resembling a “chirp.” Scientists determined that the gravitational waves were set off by the rapid inspiraling of two massive black holes. The peak of the signal — the loudest part of the chirp — linked to the very moment when the black holes collided, merging into a single, new black hole. While this infant black hole gave off gravitational waves of its own, its signature ringing, physicists assumed, would be too faint to decipher amid the clamor of the initial collision. Thus, traces of this ringing were only identified some time after the peak, where the signal was too faint to study in detail. Isi and his colleagues, however, found a way to extract the black hole’s reverberation from the moments immediately after the signal’s peak. In previous work led by Isi’s co-author, Matthew Giesler of Caltech, the team showed through simulations that such a signal, and particularly the portion right after the peak, contains “overtones” — a family of loud, short-lived tones. When they reanalyzed the signal, taking overtones into account, the researchers discovered that they could successfully isolate a ringing pattern that was specific to a newly formed black hole. In the team’s new paper, the researchers applied this technique to actual data from the GW150914 detection, concentrating on the last few milliseconds of the signal, immediately following the chirp’s peak. Taking into account the signal’s overtones, they were able to discern a ringing coming from the new, infant black hole. Specifically, they identified two distinct tones, each with a pitch and decay rate that they were able to measure. “We detect an overall gravitational wave signal that’s made up of multiple frequencies, which fade away at different rates, like the different pitches that make up a sound,” Isi says. “Each frequency or tone corresponds to a vibrational frequency of the new black hole.” Listening beyond Einstein Einstein’s theory of general relativity predicts that the pitch and decay of a black hole’s gravitational waves should be a direct product of its mass and spin. That is, a black hole of a given mass and spin can only produce tones of a certain pitch and decay. As a test of Einstein’s theory, the team used the equations of general relativity to calculate the newly formed black hole’s mass and spin, given the pitch and decay of the two tones they detected. They found their calculations matched with measurements of the black hole’s mass and spin previously made by others. Isi says the results demonstrate that researchers can, in fact, use the very loudest, most detectable parts of a gravitational wave signal to discern a new black hole’s ringing, where before, scientists assumed that this ringing could only be detected within the much fainter end of the gravitational wave signal, and identifying many tones would require much more sensitive instruments than what currently exist. “This is exciting for the community because it shows these kinds of studies are possible now, not in 20 years,” Isi says. As LIGO improves its resolution, and more sensitive instruments come online in the future, researchers will be able to use the group’s methods to “hear” the ringing of other newly born black holes. And if they happen to pick up tones that don’t quite match up with Einstein’s predictions, that could be an even more exciting prospect. “In the future, we’ll have better detectors on Earth and in space, and will be able to see not just two, but tens of modes, and pin down their properties precisely,” Isi says. “If these are not black holes as Einstein predicts, if they are more exotic objects like wormholes or boson stars, they may not ring in the same way, and we’ll have a chance of seeing them.” This research was supported, in part, by NASA, the Sherman Fairchild Foundation, the Simons Foundation, and the National Science Foundation. ",Space & Time,0.10073880608743825
84,MIT News,This is how a “fuzzy” universe may have looked ,Research,2019-10-03,-,http://news.mit.edu/2019/early-galaxy-fuzzy-universe-simulation-1003,"  Dark matter was likely the starting ingredient for brewing up the very first galaxies in the universe. Shortly after the Big Bang, particles of dark matter would have clumped together in gravitational “halos,” pulling surrounding gas into their cores, which over time cooled and condensed into the first galaxies. Although dark matter is considered the backbone to the structure of the universe, scientists know very little about its nature, as the particles have so far evaded detection. Now scientists at MIT, Princeton University, and Cambridge University have found that the early universe, and the very first galaxies, would have looked very different depending on the nature of dark matter. For the first time, the team has simulated what early galaxy formation would have looked like if dark matter were “fuzzy,” rather than cold or warm. In the most widely accepted scenario, dark matter is cold, made up of slow-moving particles that, aside from gravitational effects, have no interaction with ordinary matter. Warm dark matter is thought to be a slightly lighter and faster version of cold dark matter. And fuzzy dark matter, a relatively new concept, is something entirely different, consisting of ultralight particles, each about 1 octillionth (10-27) the mass of an electron (a cold dark matter particle is far heavier — about 105 times more massive than an electron). In their simulations, the researchers found that if dark matter is cold, then galaxies in the early universe would have formed in nearly spherical halos. But if the nature of dark matter is fuzzy or warm, the early universe would have looked very different, with galaxies forming first in extended, tail-like filaments. In a fuzzy universe, these filaments would have appeared striated, like star-lit strings on a harp.   As new telescopes come online, with the ability to see further back into the early universe, scientists may be able to deduce, from the pattern of galaxy formation, whether the nature of dark matter, which today makes up nearly 85 percent of the matter in the universe, is fuzzy as opposed to cold or warm. “The first galaxies in the early universe may illuminate what type of dark matter we have today,” says Mark Vogelsberger, associate professor of physics in MIT’s Kavli Institute for Astrophysics and Space Research. “Either we see this filament pattern, and fuzzy dark matter is plausible, or we don’t, and we can rule that model out. We now have a blueprint for how to do this.” Vogelsberger is a co-author of a paper appearing today in Physical Review Letters, along with the paper’s lead author, Philip Mocz of Princeton University, and Anastasia Fialkov of Cambridge University and previously the University of Sussex. Fuzzy waves While dark matter has yet to be directly detected, the hypothesis that describes dark matter as cold has proven successful at describing the large-scale structure of the observable universe. As a result, models of galaxy formation are based on the assumption that dark matter is cold. “The problem is, there are some discrepancies between observations and predictions of cold dark matter,” Vogelsberger points out. “For example, if you look at very small galaxies, the inferred distribution of dark matter within these galaxies doesn’t perfectly agree with what theoretical models predict. So there is tension there.” Enter, then, alternative theories for dark matter, including warm, and fuzzy, which researchers have proposed in recent years. “The nature of dark matter is still a mystery,” Fialkov says. “Fuzzy dark matter is motivated by fundamental physics, for instance, string theory, and thus is an interesting dark matter candidate. Cosmic structures hold the key to validating or ruling out such dark matter modles.” Fuzzy dark matter is made up of particles that are so light that they act in a quantum, wave-like fashion, rather than as individual particles. This quantum, fuzzy nature, Mocz says, could have produced early galaxies that look entirely different from what standard models predict for cold dark matter. “Even though in the late universe these different dark matter scenarios may predict similar shapes for galaxies, the first galaxies would be strikingly different, which will give us a clue about what dark matter is,” Mocz says. To see how different a cold and a fuzzy early universe could be, the researchers simulated a small, cubic space of the early universe, measuring about 3 million light years across, and ran it forward in time to see how galaxies would form given one of the three dark matter scenarios: cold, warm, and fuzzy. The team began each simulation by assuming a certain distribution of dark matter, which scientists have some idea of, based on measurements of the cosmic microwave background — “relic radiation” that was emitted by, and was detected just 400,000 years after, the Big Bang. “Dark matter doesn’t have a constant density, even at these early times,” Vogelsberger says. “There are tiny perturbations on top of a constant density field.” The researchers were able to use existing algorithms to simulate galaxy formation under scenarios of cold and warm dark matter. But to simulate fuzzy dark matter, with its quantum nature, they needed a new approach. A map of harp strings The researchers modified their simulation of cold dark matter, enabling it to solve two extra equations in order to simulate galaxy formation in a fuzzy dark matter universe. The first, Schrödinger’s equation, describes how a quantum particle acts as a wave, while the second, Poisson’s equation, describes how that wave generates a density field, or distribution of dark matter, and how that distribution leads to gravity — the force that eventually pulls in matter to form galaxies. They then coupled this simulation to a model that describes the behavior of gas in the universe, and the way it condenses into galaxies in response to gravitational effects. In all three scenarios, galaxies formed wherever there were over-densities, or large concentrations of gravitationally collapsed dark matter. The pattern of this dark matter, however, was different, depending on whether it was cold, warm, or fuzzy.  In a scenario of cold dark matter, galaxies formed in spherical halos, as well as smaller subhalos. Warm dark matter produced  first galaxies in tail-like filaments, and no subhalos. This may be due to warm dark matter’s lighter, faster nature, making particles less likely to stick around in smaller, subhalo clumps. Similar to warm dark matter, fuzzy dark matter formed stars along filaments. But then quantum wave effects took over in shaping the galaxies, which formed more striated filaments, like strings on an invisible harp. Vogelsberger says this striated pattern is due to interference, an effect that occurs when two waves overlap. When this occurs, for instance in waves of light, the points where the crests and troughs of each wave align form darker spots, creating an alternating pattern of bright and dark regions. In the case of fuzzy dark matter, instead of bright and dark points, it generates an alternating pattern of over-dense and under-dense concentrations of dark matter. “You would get a lot of gravitational pull at these over-densities, and the gas would follow, and at some point would form galaxies along those over-densities, and not the under-densities,” Vogelsberger explains. “This picture would be replicated throughout the early universe.” The team is developing more detailed predictions of what early galaxies may have looked like in a universe dominated by fuzzy dark matter. Their goal is to provide a map for upcoming telescopes, such as the James Webb Space Telescope, that may be able to look far enough back in time to spot the earliest galaxies. If they see filamentary galaxies such as those simulated by Mocz, Fialkov, Vogelsberger, and their colleagues, it could be the first signs that dark matter’s nature is fuzzy. “It’s this observational test we can provide for the nature of dark matter, based on observations of the early universe, which will become feasible in the next couple of years,” Vogelsberger says. This research was supported, in part, by NASA. ",Space & Time,0.10040409725690473
85,Science Daily,Astronomers Use Giant Galaxy Cluster as X-Ray Magnifying Lens,Space & Time,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111718.htm,"   While galaxy clusters have been used to magnify objects at optical wavelengths, this is the first time scientists have leveraged these massive gravitational giants to zoom in on extreme, distant, X-ray-emitting phenomena. What they detected appears to be a blue speck of an infant galaxy, about 1/10,000 the size of our Milky Way, in the midst of churning out its first stars -- supermassive, cosmically short-lived objects that emit high-energy X-rays, which the researchers detected in the form of a bright blue arc. ""It's this little blue smudge, meaning it's a very small galaxy that contains a lot of super-hot, very massive young stars that formed recently,"" says Matthew Bayliss, a research scientist in MIT's Kavli Institute for Astrophysics and Space Research. ""This galaxy is similar to the very first galaxies that formed in the universe ... the kind of which no one has ever seen in X-ray in the distant universe before."" Bayliss says the detection of this single, distant galaxy is proof that scientists can use galaxy clusters as natural X-ray magnifiers, to pick out extreme, highly energetic phenomena in the universe's early history. ""With this technique, we could, in the future, zoom in on a distant galaxy and age-date different parts of it -- to say, this part has stars that formed 200 million years ago, versus another part that formed 50 million years ago, and pick them apart in a way you cannot otherwise do,"" says Bayliss, who will be moving on to the University of Cincinnati as an assistant professor of physics.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     He and his co-authors, including Michael McDonald, assistant professor of physics at MIT, have published their results in the journal Nature Astronomy. A candle in the light Galaxy clusters are the most massive objects in the universe, composed of thousands of galaxies, all bound together by gravity as one enormous, powerful force. Galaxy clusters are so massive, and their gravitational pull is so strong, that they can distort the fabric of space-time, bending the universe and any surrounding light, much like an elephant would stretch and warp a trapeze net. Scientists have used galaxy clusters as cosmic magnifying glasses, with a technique known as gravitational lensing. The idea is that if scientists can approximate the mass of a galaxy cluster, they can estimate its gravitational effects on any surrounding light, as well as the angle at which a cluster may deflect that light. For instance, imagine if an observer, facing a galaxy cluster, were trying to detect an object, such as a single galaxy, behind that cluster. The light emitted by that object would travel straight toward the cluster, then bend around the cluster. It would continue traveling toward the observer, though at slightly different angles, appearing to the observer as mirrored images of the same object, which in the end can be combined as a single, ""magnified"" image.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Scientists have used galaxy clusters to magnify objects at optical wavelengths, but never in the X-ray band of the electromagnetic spectrum, mainly because galaxy clusters themselves emit an enormous amount of X-rays. Scientists have thought that any X-rays coming from a background source would be impossible to discern from the cluster's own glare. ""If you're trying to see an X-ray source behind a cluster, it's like trying to see a candle next to a really bright light,"" Bayliss says. ""So we knew this was a challenging measurement to make."" X-ray subtraction The researchers wondered: Could they subtract that bright light and see the candle behind it? In other words, could they remove the X-ray emissions coming from the galaxy cluster, to view the much fainter X-rays coming from an object, behind and magnified by the cluster? The team tested this idea with observations taken by NASA's Chandra X-ray Observatory, one of the world's most powerful X-ray space telescopes. They looked in particular at Chandra's measurements of the Phoenix cluster, a distant galaxy cluster located 5.7 billion light-years from Earth, which has been estimated to be about a quadrillion times as massive as the sun, with gravitational effects that should make it a powerful, natural magnifying lens. ""The idea is to take whatever your best X-ray telescope is -- in this case, Chandra -- and use a natural lens to magnify and effectively make Chandra bigger, so you can see more distant things,"" Bayliss says. He and his colleagues analyzed observations of the Phoenix cluster, taken continuously by Chandra for over a month. They also looked at images of the cluster taken by two optical and infrared telescopes -- the Hubble Space Telescope and the Magellan telescope in Chile. With all these various views, the team developed a model to characterize the cluster's optical effects, which allowed the researchers to precisely measure the X-ray emissions from the cluster itself, and subtract it from the data. They were left with two similar patterns of X-ray emissions around the cluster, which they determined were ""lensed,"" or gravitationally bent, by the cluster. When they traced the emissions backward in time, they found that they all originated from a single, distant source: a tiny dwarf galaxy from 9.4 billion years ago, when the universe itself was roughly 4.4 billion years old -- about a third of its current age. ""Previously, Chandra had seen only a handful of things at this distance,"" Bayliss says. ""In less than 10 percent of the time, we discovered this object, similarly far away. And gravitational lensing is what let us do it."" The combination of Chandra and the Phoenix cluster's natural lensing power enabled the team to see the tiny galaxy hiding behind the cluster, magnified about 60 times. At this resolution, they were able to zoom in to discern two distinct clumps within the galaxy, one producing many more X-rays than the other. As X-rays are typically produced during extreme, short-lived phenomena, the researchers believe that the first X-ray-rich clump signals a part of the dwarf galaxy that has very recently formed supermassive stars, while the quieter region is an older region that contains more mature stars. ""We're catching this galaxy at a very useful stage, where it's got these really young stars,"" Bayliss says. ""Every galaxy had to start out in this phase, but we don't see a lot of these kinds of galaxies in our own neighborhood. Now we can go back in time, look in the distant universe, find galaxies in this early phase of their life, and start to study how star formation is different there."" This research was funded, in part, by NASA, and by the Space Telescope Science Institute. ",Space & Time,0.10015859720391143
86,MIT News,Astronomers use giant galaxy cluster as X-ray magnifying lens,Space & Time,2019-10-14,-,http://news.mit.edu/2019/astronomers-galaxy-x-ray-magnifying-1014,"  Astronomers at MIT and elsewhere have used a massive cluster of galaxies as an X-ray magnifying glass to peer back in time, to nearly 9.4 billion years ago. In the process, they spotted a tiny dwarf galaxy in its very first, high-energy stages of star formation. While galaxy clusters have been used to magnify objects at optical wavelengths, this is the first time scientists have leveraged these massive gravitational giants to zoom in on extreme, distant, X-ray-emitting phenomena. What they detected appears to be a blue speck of an infant galaxy, about 1/10,000 the size of our Milky Way, in the midst of churning out its first stars — supermassive, cosmically short-lived objects that emit high-energy X-rays, which the researchers detected in the form of a bright blue arc. “It’s this little blue smudge, meaning it’s a very small galaxy that contains a lot of super-hot, very massive young stars that formed recently,” says Matthew Bayliss, a research scientist in MIT’s Kavli Institute for Astrophysics and Space Research. “This galaxy is similar to the very first galaxies that formed in the universe … the kind of which no one has ever seen in X-ray in the distant universe before.” Bayliss says the detection of this single, distant galaxy is proof that scientists can use galaxy clusters as natural X-ray magnifiers, to pick out extreme, highly energetic phenomena in the universe’s early history. “With this technique, we could, in the future, zoom in on a distant galaxy and age-date different parts of it — to say, this part has stars that formed 200 million years ago, versus another part that formed 50 million years ago, and pick them apart in a way you cannot otherwise do,” says Bayliss, who will be moving on to the University of Cincinnati as an assistant professor of physics. He and his co-authors, including Michael McDonald, assistant professor of physics at MIT, have published their results today in the journal Nature Astronomy.  A candle in the light Galaxy clusters are the most massive objects in the universe, composed of thousands of galaxies, all bound together by gravity as one enormous, powerful force. Galaxy clusters are so massive, and their gravitational pull is so strong, that they can distort the fabric of space-time, bending the universe and any surrounding light, much like an elephant would stretch and warp a trapeze net. Scientists have used galaxy clusters as cosmic magnifying glasses, with a technique known as gravitational lensing. The idea is that if scientists can approximate the mass of a galaxy cluster, they can estimate its gravitational effects on any surrounding light, as well as the angle at which a cluster may deflect that light. For instance, imagine if an observer, facing a galaxy cluster, were trying to detect an object, such as a single galaxy, behind that cluster. The light emitted by that object would travel straight toward the cluster, then bend around the cluster. It would continue traveling toward the observer, though at slightly different angles, appearing to the observer as mirrored images of the same object, which in the end can be combined as a single, “magnified” image. Scientists have used galaxy clusters to magnify objects at optical wavelengths, but never in the X-ray band of the electromagnetic spectrum, mainly because galaxy clusters themselves emit an enormous amount of X-rays. Scientists have thought that any X-rays coming from a background source would be impossible to discern from the cluster’s own glare. “If you’re trying to see an X-ray source behind a cluster, it’s like trying to see a candle next to a really bright light,” Bayliss says. “So we knew this was a challenging measurement to make.” X-ray subtraction The researchers wondered: Could they subtract that bright light and see the candle behind it? In other words, could they remove the X-ray emissions coming from the galaxy cluster, to view the much fainter X-rays coming from an object, behind and magnified by the cluster? The team tested this idea with observations taken by NASA’s Chandra X-ray Observatory, one of the world’s most powerful X-ray space telescopes. They looked in particular at Chandra’s measurements of the Phoenix cluster, a distant galaxy cluster located 5.7 billion light-years from Earth, which has been estimated to be about a quadrillion times as massive as the sun, with gravitational effects that should make it a powerful, natural magnifying lens. “The idea is to take whatever your best X-ray telescope is — in this case, Chandra — and use a natural lens to magnify and effectively make Chandra bigger, so you can see more distant things,” Bayliss says. He and his colleagues analyzed observations of the Phoenix cluster, taken continuously by Chandra for over a month. They also looked at images of the cluster taken by two optical and infrared telescopes — the Hubble Space Telescope and the Magellan telescope in Chile. With all these various views, the team developed a model to characterize the cluster’s optical effects, which allowed the researchers to precisely measure the X-ray emissions from the cluster itself, and subtract it from the data. They were left with two similar patterns of X-ray emissions around the cluster, which they determined were “lensed,” or gravitationally bent, by the cluster. When they traced the emissions backward in time, they found that they all originated from a single, distant source: a tiny dwarf galaxy from 9.4 billion years ago, when the universe itself was roughly 4.4 billion years old — about a third of its current age. “Previously, Chandra had seen only a handful of things at this distance,” Bayliss says. “In less than 10 percent of the time, we discovered this object, similarly far away. And gravitational lensing is what let us do it.” The combination of Chandra and the Phoenix cluster’s natural lensing power enabled the team to see the tiny galaxy hiding behind the cluster, magnified about 60 times. At this resolution, they were able to zoom in to discern two distinct clumps within the galaxy, one producing many more X-rays than the other. As X-rays are typically produced during extreme, short-lived phenomena, the researchers believe that the first X-ray-rich clump signals a part of the dwarf galaxy that has very recently formed supermassive stars, while the quieter region is an older region that contains more mature stars. “We’re catching this galaxy at a very useful stage, where it’s got these really young stars,” Bayliss says. “Every galaxy had to start out in this phase, but we don’t see a lot of these kinds of galaxies in our own neighborhood. Now we can go back in time, look in the distant universe, find galaxies in this early phase of their life, and start to study how star formation is different there.” This research was funded, in part, by NASA, and by the Space Telescope Science Institute. ",Space & Time,0.1001145434087368
87,IEEE,Nuclear Weapons Inspection: Encryption System Could Thwart Spies and Expose Hoaxes,Space & Time,2019-10-11,-,https://spectrum.ieee.org/tech-talk/aerospace/military/encrypting-nuclear-weapons-inspection-could-a-system-that-defeats-both-spies-and-hoaxes-have-been-invented,"      A new nuclear weapons inspection technology could enhance inspectors’ ability to verify that a nuclear warhead has been dismantled without compromising state secrets behind the weapon’s design. This new non-proliferation tool, its inventors argue, would greatly assist the often delicate dance of nuclear weapons inspectors—who want to know they haven’t been hoaxed but are also sensitive to a military’s fear that spies may have infiltrated their ranks. While nuclear non-proliferation treaties have historically verified the dismantlement of weapons delivery systems like ICBMs and cruise missiles, there have in fact never been any verified dismantlements of nuclear warheads themselves (in part for the reasons described above). Yet there are 13,000 nuclear warheads in the world, meaning the entire globe is still just a hair trigger away from apocalypse—even as we approach the thirtieth anniversary of the Berlin Wall’s collapse. As UN Secretary-General Antonio Guterres told world leaders last month, “I worry that we are slipping back into bad habits that will once again hold the entire world hostage to the threat of nuclear annihilation.” How, then, to verifiably dismantle a nuclear bomb? The challenge, says Areg Danagoulian, assistant professor of nuclear science and engineering at MIT, is finding a way for both sides of a nuclear disarmament treaty to arrive at a point of confidence. Any party to such a treaty, he says, must verifiably destroy the warheads they say they will destroy. But their military should also be confident that none of their weapons design secrets have leaked out during the inspection and verification process. Previous research has put forward systems that have strong cryptography but could conceivably be hoaxed with some sleight-of-hand—or which are sensitive to hoaxes but cannot guarantee the security of information about the design and composition of warheads. Danagoulian says he and his coauthor Ezra Engel (a former student who is now in the U.S. Army) relied on two innovations to create their new system. The first is the neutron beam that the system emits to study an individual warhead. These neutrons, at energies in the range of 5 to 50 electron volts (eV), are “slower than fast neutrons,” Danagoulian says. They sit within a range of neutron energies that probe nuclear resonances for most of the heavy elements found in nuclear warheads—including, of course, the relevant isotopes of plutonium and uranium. This means the inspection technology can potentially spot the difference between weapons-grade and reactor-grade uranium in a sample, as well as detect other elements like tungsten and molybdenum. This capability could reduce the chance of being hoaxed: A deceitful party could claim they’re dismantling a tranche of warheads—but those supposed warheads could in fact be lookalikes that are loaded instead with lower purity uranium or other heavy elements. Or the warheads could “look” to a neutron beam like a real warhead but only when viewed in one particular direction. Which is why Danagoulian and Engel’s method, described in a recent issue of the journal Nature Communications, also exposes candidate warheads to their neutron beam from multiple viewing angles. The team’s experiment—performed on fake warheads composed of non-weaponized heavy elements via a neutron beam from a linear accelerator at Rensselaer Polytechnic Institute—were able to verify bonafide “warheads” and pick out the hoaxes in both scenarios described above. The other innovation behind Danagoulian’s technology involved adding what the team calls an “encrypting filter” to the neutron beam’s path. The neutron beam passes through the warhead, then through the “filter,” and proceeds on to the neutron detectors. The filter in this case is just a slab of various heavy elements whose composition is unknown to the inspectors. The country whose weapons are being inspected, Danagoulian says, could even create the filter themselves. So long as the inspectors cannot perform any separate experiments on the encrypting filter, the ultimate composition and geometry of the warhead will be obscured to the neutron beam. “Inspectors cannot learn anything useful about the warhead’s composition,” Danagoulian says. However, if inspectors are able to take a snapshot of a “gold-standard” warhead whose genuineness has independently been established, they now have an encrypted picture of what a legitimate warhead looks like to their system. Then they can place other warheads of the same design into the encrypted neutron beam and compare the snapshots they take of candidate warheads: If the candidate has the same nuclear resonance spectrum as their “gold-standard” warhead, then the candidate warhead is real. If the candidate warhead’s spectrum is different from the gold standard, they may have just detected a hoax. Between the encrypting filter and the sensitivity of their setup, a hoaxer or spy on either side of the exchange would have to work a lot harder to deceive inspectors. Danagoulian says that his group hopes to adapt its method to a commercial neutron source and a portable detection system. The total pricetag, he says, should be around US $100,000. “You wouldn’t need this at every ICBM site,” he says. “We think we can build instruments that are 5 meters [in size]. You could put it in a truck or a van. The inspectors could bring it themselves—set up at some particular base or some particular facility. They could do these measurements and leave.”  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Space & Time,0.09999814706147679
88,Science Daily,The Milky Way Kidnapped Several Tiny Galaxies from Its Neighbor,Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010113203.htm,"   For example, more than 50 discovered satellite galaxies orbit our own galaxy, the Milky Way. The largest of these is the Large Magellanic Cloud, or LMC, a large dwarf galaxy that resembles a faint cloud in the Southern Hemisphere night sky. A team of astronomers led by scientists at the University of California, Riverside, has discovered that several of the small -- or ""dwarf"" -- galaxies orbiting the Milky Way were likely stolen from the LMC, including several ultrafaint dwarfs, but also relatively bright and well-known satellite galaxies, such as Carina and Fornax. The researchers made the discovery by using new data gathered by the Gaia space telescope on the motions of several nearby galaxies and contrasting this with state-of-the-art cosmological hydrodynamical simulations. The UC Riverside team used the positions in the sky and the predicted velocities of material, such as dark matter, accompanying the LMC, finding that at least four ultrafaint dwarfs and two classical dwarfs, Carina and Fornax, used to be satellites of the LMC. Through the ongoing merger process, however, the more massive Milky Way used its powerful gravitational field to tear apart the LMC and steal these satellites, the researchers report. ""These results are an important confirmation of our cosmological models, which predict that small dwarf galaxies in the universe should also be surrounded by a population of smaller fainter galaxy companions,"" said Laura Sales, an assistant professor of physics and astronomy, who led the research team. ""This is the first time that we are able to map the hierarchy of structure formation to such faint and ultrafaint dwarfs."" The findings have important implications for the total mass of the LMC and also on the formation of the Milky Way.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""If so many dwarfs came along with the LMC only recently, that means the properties of the Milky Way satellite population just 1 billion years ago were radically different, impacting our understanding of how the faintest galaxies form and evolve,"" Sales said. Study results appear in the November 2019 issue of the Monthly Notices of the Royal Astronomical Society. Dwarf galaxies are small galaxies that contain anywhere from a few thousand to a few billion stars. The researchers used computer simulations from the Feedback In Realistic Environments project to show the LMC and galaxies similar to it host numerous tiny dwarf galaxies, many of which contain no stars at all -- only dark matter, a type of matter scientists think constitutes the bulk of the universe's mass. ""The high number of tiny dwarf galaxies seems to suggest the dark matter content of the LMC is quite large, meaning the Milky Way is undergoing the most massive merger in its history, with the LMC, its partner, bringing in as much as one third of the mass in the Milky Way's dark matter halo -- the halo of invisible material that surrounds our galaxy,"" said Ethan Jahn, the first author of the paper and a graduate student in Sales' research group. Jahn explained that the number of tiny dwarf galaxies the LMC hosts may be higher than astronomers previously estimated, and that many of these tiny satellites have no stars.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""Small galaxies are hard to measure, and it's possible that some already-known ultrafaint dwarf galaxies are in fact associated with the LMC,"" he said. ""It's also possible that we will discover new ultrafaints that are associated with the LMC."" Dwarf galaxies can either be satellites of larger galaxies, or they can be ""isolated,"" existing on their own and independent of any larger object. The LMC used to be isolated, Jahn explained, but it was captured by the gravity of the Milky Way and is now its satellite. ""The LMC hosted at least seven satellite galaxies of its own, including the Small Magellanic Cloud in the Southern Sky, prior to them being captured by the Milky Way,"" he said. Next, the team will study how the satellites of LMC-sized galaxies form their stars and how that relates to how much dark matter mass they have. ""It will be interesting to see if they form differently than satellites of Milky Way-like galaxies,"" Jahn said. ",Space & Time,0.09998981122238873
89,Science Daily,Milky Way Raids Intergalactic 'Bank Accounts',Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010125615.htm,"   ""We expected to find the Milky Way's books balanced, with an equilibrium of gas inflow and outflow, but 10 years of Hubble ultraviolet data has shown there is more coming in than going out,"" said astronomer Andrew Fox of the Space Telescope Science Institute, Baltimore, Maryland, lead author of the study to be published in the Astrophysical Journal. Fox said that, for now, the source of the excess inflowing gas remains a mystery. One possible explanation is that new gas could be coming from the intergalactic medium. But Fox suspects the Milky Way is also raiding the gas ""bank accounts"" of its small satellite galaxies, using its considerably greater gravitational pull to siphon away their resources. Additionally, this survey, while galaxy-wide, looked only at cool gas, and hotter gas could play a role, too. The new study reports the best measurements yet for how fast gas flows in and out of the Milky Way. Prior to this study, astronomers knew that the galactic gas reserves are replenished by inflow and depleted by outflow, but they did not know the relative amounts of gas coming in compared to going out. The balance between these two processes is important because it regulates the formation of new generations of stars and planets. Astronomers accomplished this survey by collecting archival observations from Hubble's Cosmic Origins Spectrograph (COS), which was installed on the telescope by astronauts in 2009 during its last servicing mission. Researchers combed through the Hubble archives, analyzing 200 past ultraviolet observations of the diffuse halo that surrounds the disk of our galaxy. The decade's worth of detailed ultraviolet data provided an unprecedented look at gas flow across the galaxy and allowed for the first galaxy-wide inventory. The gas clouds of the galactic halo are only detectable in ultraviolet light, and Hubble is specialized to collect detailed data about the ultraviolet universe.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""The original Hubble COS observations were taken to study the universe far beyond our galaxy, but we went back to them and analyzed the Milky Way gas in the foreground. It's a credit to the Hubble archive that we can use the same observations to study both the near and the more distant universe. Hubble's resolution allows us to simultaneously study local and remote celestial objects,"" noted Rongmon Bordoloi of North Carolina State University in Raleigh, North Carolina, a co-author on the paper. Because the galaxy's gas clouds are invisible, Fox's team used light from background quasars to detect these clouds and their motion. Quasars, the cores of active galaxies powered by well-fed black holes, shine like brilliant beacons across billions of light-years. When the quasar's light reaches the Milky Way, it passes through the invisible clouds. The gas in the clouds absorbs certain frequencies of light, leaving telltale fingerprints in the quasar light. Fox singled out the fingerprint of silicon and used it to trace the gas around the Milky Way. Outflowing and inflowing gas clouds were distinguished by the Doppler shift of the light passing through them -- approaching clouds are bluer, and receding clouds are redder. Currently, the Milky Way is the only galaxy for which we have enough data to provide such a full accounting of gas inflow and outflow. ""Studying our own galaxy in detail provides the basis for understanding galaxies across the universe, and we have realized that our galaxy is more complicated than we imagined,"" said Philipp Richter of the University of Potsdam in Germany, another co-author on the study. Future studies will explore the source of the inflowing gas surplus, as well as whether other large galaxies behave similarly. Fox noted that there are now enough COS observations to conduct an audit of the Andromeda galaxy (M31), the closest large galaxy to the Milky Way. The Hubble Space Telescope is a project of international cooperation between ESA (the European Space Agency) and NASA. NASA's Goddard Space Flight Center in Greenbelt, Maryland, manages the telescope. The Space Telescope Science Institute (STScI) in Baltimore, Maryland, conducts Hubble science operations. STScI is operated for NASA by the Association of Universities for Research in Astronomy in Washington, D.C. ",Space & Time,0.09994954573054166
90,Science Daily,Pressure Runs High at Edge of Solar System,Space & Time,2019-10-08,-,https://www.sciencedaily.com/releases/2019/10/191008115915.htm,"   Using observations of galactic cosmic rays -- a type of highly energetic particle -- from NASA's Voyager spacecraft scientists calculated the total pressure from particles in the outer region of the solar system, known as the heliosheath. At nearly 9 billion miles away, this region is hard to study. But the unique positioning of the Voyager spacecraft and the opportune timing of a solar event made measurements of the heliosheath possible. And the results are helping scientists understand how the Sun interacts with its surroundings. ""In adding up the pieces known from previous studies, we found our new value is still larger than what's been measured so far,"" said Jamie Rankin, lead author on the new study and astronomer at Princeton University in New Jersey. ""It says that there are some other parts to the pressure that aren't being considered right now that could contribute."" On Earth we have air pressure, created by air molecules drawn down by gravity. In space there's also a pressure created by particles like ions and electrons. These particles, heated and accelerated by the Sun create a giant balloon known as the heliosphere extending millions of miles out past Pluto. The edge of this region, where the Sun's influence is overcome by the pressures of particles from other stars and interstellar space, is where the Sun's magnetic influence ends. (Its gravitational influence extends much farther, so the solar system itself extends farther, as well.) In order to measure the pressure in the heliosheath, the scientists used the Voyager spacecraft, which have been travelling steadily out of the solar system since 1977. At the time of the observations, Voyager 1 was already outside of the heliosphere in interstellar space, while Voyager 2 still remained in the heliosheath. ""There was really unique timing for this event because we saw it right after Voyager 1 crossed into the local interstellar space,"" Rankin said. ""And while this is the first event that Voyager saw, there are more in the data that we can continue to look at to see how things in the heliosheath and interstellar space are changing over time."" The scientists used an event known as a global merged interaction region, which is caused by activity on the Sun. The Sun periodically flares up and releases enormous bursts of particles, like in coronal mass ejections. As a series of these events travel out into space, they can merge into a giant front, creating a wave of plasma pushed by magnetic fields. When one such wave reached the heliosheath in 2012, it was spotted by Voyager 2. The wave caused the number of galactic cosmic rays to temporarily decrease. Four months later, the scientists saw a similar decrease in observations from Voyager 1, just across the solar system's boundary in interstellar space. Knowing the distance between the spacecraft allowed them to calculate the pressure in the heliosheath as well as the speed of sound. In the heliosheath sound travels at around 300 kilometers per second -- a thousand times faster than it moves through air. The scientists noted that the change in galactic cosmic rays wasn't exactly identical at both spacecraft. At Voyager 2 inside the heliosheath, the number of cosmic rays decreased in all directions around the spacecraft. But at Voyager 1, outside the solar system, only the galactic cosmic rays that were traveling perpendicular to the magnetic field in the region decreased. This asymmetry suggests that something happens as the wave transmits across the solar system's boundary. ""Trying to understand why the change in the cosmic rays is different inside and outside of the heliosheath remains an open question,"" Rankin said. Studying the pressure and sound speeds in this region at the boundary of the solar system can help scientists understand how the Sun influences interstellar space. This not only informs us about our own solar system, but also about the dynamics around other stars and planetary systems. ",Space & Time,0.09986407473902295
91,Science Daily,Liquifying a Rocky Exoplanet,Space & Time,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009095830.htm,"   Rocky planets are built from the leftovers of the leftovers. ""Everything that doesn't make its way into the central star or a giant planet has the potential to end up forming a much smaller terrestrial planet,"" says Bower: ""We have reason to believe that processes occurring during the baby years of a planet's life are fundamental in determining its life path."" Therefore, Bower and a team of post-docs -- dominantly from within the PlanetS network -- were intrigued to uncover the observable nature of such a planet. Their study is now published in the journal Astronomy & Astrophysics. It shows that a molten Earth would actually be around 5% larger in radius than a solid Earth, and this is due to the difference in the behavior of molten versus solid materials at the extreme conditions of a planetary interior. ""In essence, a molten silicate occupies more volume than its equivalent solid, and this increases the size of the planet,"" Bower explains. A difference that CHEOPS can detect In the characterization of exoplanets outside our solar system and the search for potentially habitable worlds, researchers at the University of Bern are among the world leaders. Although detection of a rocky planet around a bright Sun-like star will remain beyond reach at least until the launch of the PLATO space mission in 2026, Earth-size planets around cooler and smaller stars such as the red dwarfs Trappist-1 or Proxima b are now set to take center stage. Interestingly, 5% difference in planetary radii can be measured with current and future observational facilities, notably the space telescope CHEOPS which was developed and assembled in Bern and will launch later this year. Indeed, the latest exoplanet data already provides an inkling that low mass molten planets, sustained by intense star-light, are present in the exoplanet catalogue. Some exoplanets could therefore be Earth-like in terms of similar building blocks, yet have different amounts of solid and molten rock to explain observed variations in planet size. ""They do not necessarily need to be made of exotic light materials to explain the data,"" says Bower. However, even a totally molten planet may not be able to explain the observation of the most extreme low density planets. But on this the research team also has a proposition: Molten planets early in their history can outgas large atmospheres of volatile species that were originally trapped inside the magma in the interior of the planet. This could explain an additional decrease in the observed planetary density. The James Webb Space Telescope (JWST) should be able to distinguish such an outgassed atmosphere on a planet around a cool red dwarf if it is dominated by either water or carbon dioxide. In addition to the consequences for observations, Bower, with his roots as an Earth Scientist, sees his study in a broader context: ""Clearly, we can never observe our own Earth in its history when it was also hot and molten. But interestingly, exoplanetary science is opening the door for observations of early Earth and early Venus analogues that could greatly impact our understanding of Earth and the Solar System planets. Thinking about Earth in the context of exoplanets, and vice-versa, offers new opportunities for understanding planets both within and beyond the Solar System."" ",Space & Time,0.09971156204022753
92,Science Daily,How Do the Strongest Magnets in the Universe Form?,Space & Time,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009131744.htm,"   Our Universe is threaded by magnetic fields. The Sun, for example, has an envelope in which convection continuously generates magnetic fields. ""Even though massive stars have no such envelopes, we still observe a strong, large-scale magnetic field at the surface of about ten percent of them,"" explains Dr Fabian Schneider from the Centre for Astronomy of Heidelberg University, who is the first author of the study in ""Nature."" Although such fields were already discovered in 1947, their origin has remained elusive so far. Over a decade ago, scientists suggested that strong magnetic fields are produced when two stars collide. ""But until now, we weren't able to test this hypothesis because we didn't have the necessary computational tools,"" says Dr Sebastian Ohlmann from the computing centre of the Max Planck Society in Garching near Munich. This time, the researchers used the AREPO code, a highly dynamic simulation code running on compute clusters of the Heidelberg Institute for Theoretical Studies (HITS), to explain the properties of Tau Scorpii (τ Sco), a magnetic star located 500 light years from Earth. Already in 2016, Fabian Schneider and Philipp Podsiadlowski from the University of Oxford realised that τ Sco is a so-called blue straggler. Blue stragglers are the product of merged stars. ""We assume that Tau Scorpii obtained its strong magnetic field during the merger process,"" explains Prof. Dr Philipp Podsiadlowski. Through its computer simulations of τ Sco, the German-British research team has now demonstrated that strong turbulence during the merger of two stars can create such a field. Stellar mergers are relatively frequent: Scientists assume that about ten percent of all massive stars in the Milky Way are the products of such processes. This is in good agreement with the occurrence rate of magnetic massive stars, according to Dr Schneider. Astronomers think that these very stars could form magnetars when they explode in supernovae. This may also happen to τ Sco when it explodes at the end of its life. The computer simulations suggest that the magnetic field generated would be sufficient to explain the exceptionally strong magnetic fields in magnetars. ""Magnetars are thought to have the strongest magnetic fields in the Universe -- up to one hundred million times stronger than the strongest magnetic field ever produced by humans,"" says Prof. Dr Friedrich Röpke from HITS. The research was funded by the Oxford Hintze Centre for Astrophysical Surveys and the Klaus Tschira Foundation (Heidelberg). ",Space & Time,0.09908683893488632
93,Science Daily,Fire Blankets Can Protect Buildings from Wildfires,Environment,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015075319.htm,"   By rigorously testing different fabric materials in the laboratory and using them to shield structures that were exposed to fires of increasing magnitude, this research, published in Frontiers in Mechanical Engineering, confirms that existing blanket technology can protect structures from a short wildfire attack. For successful deployment against severe fires and in areas of high housing density, technological advancement of blanket materials and deployment methods, as well as multi-structure protection strategies, are needed. ""The whole-house fire blanket is a viable method of protection against fires at the wildland-urban interface,"" says lead study author Fumiaki Takahashi, a Professor at Case Western Reserve University, Cleveland, Ohio, USA, who teamed up with the NASA Glenn Research Center, U.S. Forest Service, New Jersey Forest Fire Service, and Cuyahoga Community College for this study. He continues, ""Current technology can protect an isolated structure against a relatively short wildfire attack and further technological developments are likely to enable this method to be applied to severe situations."" A burning need Wildfires in urban and suburban settings can have a devastating effect on communities and pose one of the greatest fire challenges of our time.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     People living and working in fire-risk areas contacted Professor Takahashi to find out if commercial products are available to help reduce the likelihood of structure ignition, which would reduce fire damage and improve public and firefighter safety. These pleas motivated the research and an initial investigation revealed that the concept of whole-structure fire blankets has been around for quite some time. ""I thought about a means to reduce wildland fire damage and found a U.S. patent 'conflagration-retardative curtain' i.e., a fire blanket, issued during World War Two. In addition, the U.S. Forest Service firefighters managed to save a historic forest cabin by wrapping it with their fire shelter materials,"" Takahashi reports. An old flame-retardant While there are anecdotal reports on the ability of fire blankets to protect buildings from fires, Takahashi's research highlighted a severe lack of scientific evidence to back up these claims. To rectify this, funded by a research grant from the U.S. Department of Homeland Security, the team conducted several experiments to test the ability of different blanket materials to shield structures against fires of increasing magnitude. ""The fire exposure tests determined how well the fire blankets protected various wooden structures, from a birdhouse in a burning room to a full-size shed in a real forest fire. We tested four types of fabric materials: aramid, fiberglass, amorphous silica, and pre-oxidized carbon, each with and without an aluminum surface. In addition, we conducted laboratory experiments under controlled heat exposure and measured the heat-insulation capabilities of these materials against direct flame contact or radiation heat."" A hot new industry    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The laboratory and real-fire assessments demonstrate that fire blankets could protect structures from a short exposure to a wildfire, but also highlight the technical limitations of their existing form. Further technological advancements are needed in the areas of material composition, deployment methods and multi-structure protection strategies. Takahashi explains, ""The fiberglass or amorphous silica fabrics laminated with aluminum foil performed best, due to high reflection/emission of radiation and good thermal insulation by the fabric. New technology is needed to enhance the fire blankets' heat-blocking capability for an extended period to prevent structure-to-structure ignition. In addition, it will be more effective If dozens or hundreds of homes are protected by such advanced fire blankets at the same time, particularly in high housing-density Wildland-Urban Interface communities."" He concludes by suggesting communities potentially affected by wildfires work together to turn the concept of whole-building fire blankets into a reality. ""Fire blanket protection will be significant to those living and fighting fires at the Wildland-Urban Interface and presents entrepreneurs and investors with business opportunities. The implication of the present findings is that the technical community, the general public, and the fire service must work together to take a step-by-step approach toward the successful application of this technology."" ",Environment,0.09495337545398859
94,Science Daily,Neuroimaging Reveals Hidden Communication Between Brain Layers During Reading,Society,2019-10-01,-,https://www.sciencedaily.com/releases/2019/10/191001102222.htm,"   A research team led by Daniel Sharoh from the Donders Center for Cognitive Neuroimaging at Radboud University Nijmegen, Kirsten Weber (Radboud University, MPI), David Norris (Radboud University, MPI), and Peter Hagoort (Radboud University, MPI) wanted to investigate the brain's reading network at a more fine-grained level. They used the 7 Tesla MRI at the Erwin L Hahn Institute in Essen for laminar functional Magnetic Resonance Imaging (lfMRI), to measure brain activation at different depths or 'layers' of the brain-typically right next to each other and smaller than a millimetre. Measuring at this level is important, as the layers can be related to the direction of the signals. Deep layers are associated with top-down information, whereas middle layers are associated with bottom-up information. Only laminar fMRI is sensitive enough to detect the deeper layers of the brain. With this new technique, would the investigators be able to find a top-down flow of information to the deeper layers of the brain for word reading? To answer this question, the researchers created pseudowords such as ""rorf"" and ""bofgieneer,"" to be compared with real words such as ""zalm"" (salmon) and ""batterij"" (battery). Pseudowords are 'possible' words that happen not to exist; they are pronounceable and therefore 'readable'. Twenty-two native Dutch speakers were asked to silently read the words and pseudowords as their brains were being scanned. The participants also viewed 'unreadable' sequences of invented 'false font' characters that resembled existing letters. The task was to only press a button when the items were real words. By comparing the brain activation for 'readable' items (words and pseudowords) and 'unreadable' items (false font), the investigators could isolate the 'reading area' of the brain. This area is also known as the 'visual word form area' (VWFA) and is situated in the temporal lobe (the left occipitotemporal cortex). As a next step, the researchers compared words directly to pseudowords, to further explore the VWFA. Bottom-up sensory information is needed for both types of items, to recognise the strings as letters. But would top-down information from language areas be visible as well, needed to distinguish words from pseudowords? The researchers found stronger activation for words than pseudo-words in the deep layers of the VWFA. This activation was caused by top-down projections from higher language areas of the brain (the left Middle Temporal Gyrus (lMTG) and left posterior middle temporal gyrus (lpMTG)). These are well-known language areas involved in retrieving words and their meaning. In contrast, the researchers found decreased activation in the middle layer of the reading area, indicating that the deep layer 'suppresses' activation of the middle layer during word reading. A conventional fMRI would have missed this nuance, as only laminar fMRI is sensitive to layer-specific activation. ""This is a breakthrough finding for all imaging research,"" says Peter Hagoort, director at the Max Planck Institute for Psycholinguistics in Nijmegen and co-author of the study. ""For the first time we have established different activation profiles at different layers of cortex, and figured out how this pattern is related to top-down influence in cortical processing. This has far-reaching consequences for cognitive neuroimaging, expanding our knowledge of brain networks."" ",Health,0.09322279888060456
95,Science Daily,"Beyond the 'Replication Crisis,' Does Research Face an 'Inference Crisis'?",Science,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010161540.htm,"   Using an example from their own field of memory research, they designed a test for the accuracy of theoretical conclusions made by researchers. The study was spearheaded by associate professor Jeffrey Starns, professor Caren Rotello, and doctoral student Andrea Cataldo, who has now completed her Ph.D. They shared authorship with 27 teams or individual cognitive psychology researchers who volunteered to submit their expert research conclusions for data sets sent to them by the UMass researchers. ""Our results reveal substantial variability in experts' judgments on the very same data,"" the authors state, suggesting a serious inference problem. Details are newly released in the journal Advancing Methods and Practices in Psychological Science. Starns says that objectively testing whether scientists can make valid theoretical inferences by analyzing data is just as important as making sure they are working with replicable data patterns. ""We want to ensure that we are doing good science. If we want people to be able to trust our conclusions, then we have an obligation to earn that trust by showing that we can make the right conclusions in a public test."" For this work, the researchers first conducted an online study testing recognition memory for words, ""a very standard task"" in which people decide whether or not they saw a word on a previous list. The researchers manipulated memory strength by presenting items once, twice, or three times and they manipulated bias -- the overall willingness to say things are remembered -- by instructing participants to be extra careful to avoid certain types of errors, such as failing to identify a previously studied item. Starns and colleagues were interested in one tricky interpretation problem that arises in many recognition studies, that is, the need to correct for differences in bias when comparing memory performance across populations or conditions. Unfortunately, this situation can arise if memory for the population of interest if equal to, better than, or worse than controls. Recognition researchers use a number of analysis tools to distinguish these possibilities, some of which have been around since the 1950's.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     To determine if researchers can use these tools to accurately distinguish memory and bias, the UMass researchers created seven two-condition data sets and sent them to contributors without labels, asking them to indicate whether or not the conditions were from the same or different levels of the memory strength or response bias manipulations. Rotello explains, ""These are the same sort of data they'd be confronted with in an experiment in their own labs, but in this case we knew the answers. We asked, 'did we vary memory strength, response bias, both or neither?'"" The volunteer cognitive psychology researchers could use any analyses they thought were appropriate, Starns adds, and ""some applied multiple techniques, or very complex, cutting-edge techniques. We wanted to see if they could make accurate inferences and whether they could accurately gauge uncertainty. Could they say, 'I think there's a 20 percent chance that you only manipulated memory in this experiment,' for example."" Starns, Rotello and Cataldo were mainly interested in the reported probability that memory strength was manipulated between the two conditions. What they found was ""enormous variability between researchers in what they inferred from the same sets of data,"" Starns says. ""For most data sets, the answers ranged from 0 to 100 percent across the 27 responders,"" he adds, ""that was the most shocking."" Rotello reports that about one-third of responders ""seemed to be doing OK,"" one-third did a bit better than pure guessing, and one-third ""made misleading conclusions."" She adds, ""Our jaws dropped when we saw that. How is it that researchers who have used these tools for years could come to completely different conclusions about what's going on?"" Starns notes, ""Some people made a lot more incorrect calls than they should have. Some incorrect conclusions are unavoidable with noisy data, but they made those incorrect inferences with way too much confidence. But some groups did as well as can be expected. That was somewhat encouraging."" In the end, the UMass Amherst researchers ""had a big reveal party"" and gave participants the option of removing their responses or removing their names from the paper, but none did. Rotello comments, ""I am so impressed that they were willing to put everything on the line, even though the results were not that good in some cases."" She and colleagues note that this shows a strong commitment to improving research quality among their peers. Rotello adds, ""The message here is not that memory researchers are bad, but that this general tool can assess the quality of our inferences in any field. It requires teamwork and openness. It's tremendously brave what these scientists did, to be publicly wrong. I'm sure it was humbling for many, but if we're not willing to be wrong we're not good scientists."" Further, ""We'd be stunned if the inference problems that we observed are unique. We assume that other disciplines and research areas are at risk for this problem."" ",Society,0.09179996508206471
96,Stanford,Global analysis of submarine canyons may shed light on Martian landscapes,Science,2019-10-15,-,https://news.stanford.edu/2019/10/08/global-analysis-submarine-canyons-may-shed-light-martian-landscapes/,"   Submarine canyons are a final frontier on planet Earth. There are thousands of these breathtaking geological features hidden within the depths of the ocean – yet scientists have more high-resolution imagery of the surface of Mars than of Earth’s ocean floor.  The Monterey Canyon is one of thousands of submarine canyons hidden on the ocean floor. New research reveals distinct differences from land canyons for the first time. (Image credit: Courtesy Monterey Bay Aquarium Research Institute)  In an effort to shed light on these mysterious underwater features, Stanford researchers analyzed a collection of global images from an online repository of data from the ocean floor. They found that submarine canyons, which had been believed to form in ways similar to canyons on land, are instead fundamentally different from the land-based ravines that cut through vast stretches of our mountain ranges. The research was published online in Geology Sept. 25. “People would say, ‘Oh well, there is no real difference between the two systems because at the end of the day, a river flowing versus a sediment gravity flow flowing – they’re just going to do the same thing,’” said lead author Stephen Dobbs, a PhD candidate in geological sciences. “And it turns out that’s not necessarily the case.” The researchers analyzed multi-beam sonar data, which is collected by ships or small underwater vehicles just above the sea floor that send a sonar wave that is used to make maps of the seafloor. They acquired data for the study from the Global Multi-Resolution Topography synthesis, an open-source online repository. Dobbs said it was surprising to discover differences in the underwater and above-ground canyons, since on a map, formations 9,000 feet underwater can’t be distinguished from canyons that are 9,000 feet above ground. “When you look from a purely qualitative sense – when you’re just looking at a map – they look shockingly similar,” said Dobbs. “We needed to use a quantitative method to actually test if these are different systems.” The scientists found distinctions in the shapes and profiles of submarine canyons. On land, significant changes in canyon shape are often triggered by large flood events or landslides. Under water, researches hypothesize processes that form submarine canyons are periodic landslides from extreme steepness, seismic activity or large winter storms that funnel a lot of sediment from the shallow continental shelf. “This is all frontier – we don’t actually know the answers to these things,” Dobbs said. “Now we have all these measurements and we can more aptly look at what causes these formations.” From classroom to peer review The student-led research grew out of a graduate seminar exploring submarine canyons held in spring of 2018. The colloquium brought together graduate students from two different laboratories – those of geological sciences professors Don Lowe and Stephan Graham. The research was guided by co-authors George Hilley, a professor of geological sciences, and adjunct professor of geological sciences Tim McHargue, who are both part of Stanford Earth’s sedimentary research group along with Lowe and Graham. Hilley said most people don’t realize that sediment-laden water can erode the seafloor, never mind the fact that these flows have carved features deeper than the Grand Canyon right off the coast of Monterey. Because a lot of high-resolution imagery has been collected in recent years, the faculty members knew it should be possible to analyze a large sampling of the underwater features. “We used the seminar as a vehicle for answering whether or not the forms produced by these density flows share essential characteristics with those produced by rivers,” Hilley said. “By asking these questions using real data, everyone learned how to formulate hypotheses and falsify them using sophisticated data analysis.” While the project was a deviation from the graduate students’ main research projects, Dobbs said he was pleased with how much the group accomplished in one year of pursuing this new topic. Application to Mars? Dobbs said he is excited about the prospect of using these methods to understand not only geology on Earth but also on other planets. For example, it could help researchers understand Martian landscapes, which are pocked with features that may share similarities with Earth’s canyons. The research is also relevant to ocean-dependent industries, including communications companies – whose data cables can be severed by events in submarine canyons – as well as offshore energy and oil and gas operations. “These things have huge impacts on Earth systems and they are fundamentally not understood,” Dobbs said. “We’re just now able to actually measure them in a rigorous geomorphic sense and from that, we’re able to make inferences about both how they form and how they influence our systems and our cycles.” Dobbs plans to continue working with this data set in order to learn more about submarine canyon formation and behavior. “The exciting thing for me is that – while I love fieldwork – we can literally discover new things using very simple tools that are available to the public and open access,” he said.  Stanford co-authors from the Department of Geological Sciences include Matthew Malkowski, acting assistant professor, and PhD students Jared Gooley, Chayawan Jaikla and Colin White. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest.  ",Environment,0.08846715010030132
97,MIT News,Using algorithms to build a map of the placenta,Research,2019-10-02,-,http://news.mit.edu/2019/using-algorithms-to-build-placenta-map-1002,"  The placenta is one of the most vital organs when a woman is pregnant. If it’s not working correctly, the consequences can be dire: Children may experience stunted growth and neurological disorders, and their mothers are at increased risk of blood conditions like preeclampsia, which can impair kidney and liver function.  Unfortunately, assessing placental health is difficult because of the limited information that can be gleaned from imaging. Traditional ultrasounds are cheap, portable, and easy to perform, but they can’t always capture enough detail. This has spurred researchers to explore the potential of magnetic resonance imaging (MRI). Even with MRIs, though, the curved surface of the uterus makes images difficult to interpret. This problem got the attention of a team of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), who wondered whether the placenta’s scrunched shape could be flattened out using some fancy geometry. Next month they’re publishing a paper showing that it can. Their new algorithm unfolds images from MRI scans to better visualize the organ. For example, their images more clearly show the “cotyledons,” circular structures that allow for the exchange of nutrients between the mother and her developing child or children. Being able to visualize such structures could allow doctors to diagnose and treat placental issues much earlier in the pregnancy.  “The idea is to unfold the image of the placenta while it’s in the body, so that it looks similar to how doctors are used to seeing it after delivery,” says PhD student Mazdak Abulnaga, lead author of the new paper with MIT professors Justin Solomon and Polina Golland. “While this is just a first step, we think an approach like this has the potential to become a standard imaging method for radiologists.”  Golland says that the algorithm could also be used in clinical research to find specific  biomarkers associated with poor placental health. Such research could help radiologists save time and more accurately locate problem areas without having to examine many different slices of the placenta. Chris Kroenke, an associate professor at Oregon Health and Science University, says that the project opens up many new possibilities for monitoring placental health.  “The biological processes that underlie cotyledon patterning are not completely understood, nor is it known whether a standard pattern should be expected for a given population,” says Kroenke, who was not involved in the paper. “The tools provided by this work will certainly aid researchers to address these questions in the future.”        Abulnaga, Solomon, and Golland co-wrote the paper with former CSAIL postdoc Mikhail Bessmeltsev and their collaborators, Esra Abaci Turk and P. Ellen Grant of Boston Children’s Hospital (BCH). Grant is the director of BCH’s Fetal-Neonatal Neuroimaging and Development Science Center, and a professor of radiology and pediatrics at Harvard Medical School. The team also worked closely with collaborators at Massachusetts General Hospital (MGH) and MIT Professor Elfar Adalsteinsson. The paper will be presented Oct. 14 in Shenzhen, China, at the International Conference on Medical Image Computing and Computer-Assisted Intervention.  The team’s algorithm first models the placenta’s shape by subdividing it into thousands of tiny pyramids, or tetrahedra. This serves an efficient representation for computers to perform operations to manipulate the shape. The algorithm then arranges those pyramids into a template that resembles the flattened shape that a placenta holds once it’s out of the body. (The algorithm does this by essentially moving the corners of the pyramids on the surface of the placenta to match the two parallel planes of the template and letting the rest fill the new shape.) The model has to make a tradeoff between the pyramids matching the shape of the template and minimizing the amount of distortion. The team showed the system can ultimately achieve accuracy at the scale of less than one voxel (a 3-D pixel).  The project is far from the first aimed at improving medical imaging by actually manipulating said images. There have been recent efforts to unfold scans of ribs, and researchers have also spent many years developing ways to flatten images of the brain’s cerebral cortex to better visualize areas between the folds. Meanwhile, work involving the womb is much newer. Previous approaches to this problem focused on flattening different layers of the placenta separately. The team says that they feel that the new volumetric method results in more consistency and less distortion because it maps the whole 3-D placenta at once, enabling it to more closely model the physical unfolding process. “The team’s work provides a very elegant tool to address the issue of the placenta’s irregular shape being difficult to image,” says Kroenke.  As a next step, the team hopes to work with MGH and BCH to directly compare in-utero images with ones of the same placentas post-birth. Because the placenta loses fluid and changes shape during the birth process, this will require using a special chamber designed by MGH and BCH where researchers can put the placenta directly after the birth. The source code for the project is available on github. The work was supported in part by the National Institute of Child Health and Human Development, the National Institute of Biomedical Imaging and Bioengineering, the National Science Foundation, the U.S. Air Force, and the Natural Sciences and Engineering Research Council of Canada. ",Health,0.08668558427990401
98,MIT News,A look at Japan’s evolving intelligence efforts,Research,2019-10-08,-,http://news.mit.edu/2019/special-duty-japan-intelligence-1008,"  Once upon a time — from the 1600s through the 1800s — Japan had a spy corps so famous we know their name today: the ninjas, intelligence agents serving the ruling Tokugawa family. Over the last 75 years, however, as international spying and espionage has proliferated, Japan has mostly been on the sidelines of this global game. Defeat in World War II, and demilitarization afterward, meant that Japanese intelligence services were virtually nonexistent for decades. Japan’s interest in spycraft has returned, however. In addition to a notable military expansion — as of last year, the country has aircraft carriers again — Japan is also ramping up its formal intelligence apparatus, as a response to what the country’s chief cabinet secretary has called “the drastically changing security environment” around it. “Intelligence is a critical element of any national security strategy,” says MIT political scientist Richard Samuels, a leading expert on Japanese politics and foreign policy. “It’s just a question of how robust, and openly robust, any country is willing to make it.” Examining the status of Japan’s intelligence efforts, then, helps us understand Japan’s larger strategic outlook and goals. And now Samuels has written a wide-ranging new history of Japan’s intelligence efforts, right up to the present. The book, “Special Duty: A History of the Japanese Intelligence Community,” is being published this week by Cornell University Press. “Japan didn’t have a comprehensive intelligence capability, but they’re heading in that direction,” says Samuels, who is the director of the Center for International Studies and the Ford International Professor of Political Science at MIT. As firm as Japan’s taboo on military and intelligence activity once was, he adds, “that constraint is coming undone.” Ruffians and freelance agents Aside from the ninjas, who focused on domestic affairs, Japan’s international intelligence efforts have seen a few distinct phases: a patchy early period, a big buildup before World War II, the dismantling of the system under the postwar U.S. occupation, and — especially during the current decade — a restoration of intelligence capabilities. Famously, Japan was closed off to much of the rest of the world until the late 19th century. It did not formally pursue international intelligence activities until the late 1860s. By the early 1900s, Japanese agents had found some success: They decoded Russian cables in the Russo-Japanese War of 1904-05 and cut off Russian raids during the conflict. But as Samuels details in the book, during this period Japan heavily relied on a colorful array of spies and agents working on an unofficial basis, an arrangement that gave the country “plausible deniability” in case these operatives were caught. “There was an interesting reliance upon scoundrels, ruffians, and freelance agents,” Samuels says. Some of these figures were quite successful. One agent, Uchida Ryohei, founded an espionage group, the Amur River Society (also sometimes called the Black Dragon Society), which opened its own training school, created Japan’s best battlefield maps and conducted all manner of operations meant to limit Russian expansion. In the 1930s, another undercover agent, Doihara Kenji, became so successful at creating pro-Japanese local governments in northern China, that he became known as “Lawrence of Manchuria.” Meanwhile, Japan’s official intelligence units had a chronic lack of coordination; they divided along military branches and between military and diplomatic bureaucracies. Still, in the decades before World War II, Japan leveraged some existing strengths in the study of foreign cultures — “The Japanese invented area studies before we did,” says Samuels — and used technological advances to make huge strides in information-gathering. “They had strengths, they had weaknesses, they had official intelligence, they had nonofficial intelligence, but overall that was a period of great growth in their intelligence capability,” Samuels says. “That of course comes to a crashing halt at the end of the war, when the entire military apparatus was taken down. So there was this period immediately after the war where there was no formal intelligence.” Japan’s subsequent postwar political reorientation toward the U.S. created many advantages for the country but was simultaneously a source of frustration to some. The country became an economic powerhouse while lacking the same covert capabilities as other countries.   “The Cold War was a period in which many Japanese in the intelligence world resented having to accommodate to American power in the intelligence world, and resented it,” Samuels says. “They had economic intelligence capability. They were very good at doing foreign economic analysis and were all over the world, but they were underperforming on the diplomatic and military fronts.” The Asian pivot In “Special Duty,” Samuels suggests three main reasons why any country reforms its intelligence services: Shifts in the strategic environment, technological innovations, and intelligence failures. The first of these seems principally responsible for the current revival of Japan’s intelligence operations. As Samuels notes, some Japanese officials wanted to change the country’s intelligence structure during the 1980s — to little avail. The end of the Cold War, and the more complicated geopolitcal map that resulted, provided a more compelling rationale for doing so, without producing many tangible results. Instead, more recent events in Asia have had a much bigger impact in Japan: namely, North Korean missile testing and China’s massive surge in economic and military power. In 2005, Samuels notes, Japan’s GDP was still twice that of China. A decade later, China’s economy was two and a half times as large as Japan’s, and its military budget was twice as big. U.S. power relative to China has also declined. Those developments have altered Japanese security priorities. “There’s been a Japanese pivot in Asia,” Samuels notes. “That’s really very important.” Moreover, he adds, from the Japanese perspective, “The question about China is obvious. Is its rise going to be disjunctive, or is it going to be stabilizing?” These regional changes have led Japan to chart a course of greater confidence in foreign policy — reflected in its growing intelligence function. Since 2013 in particular, after Prime Minister Shinzo Abe took office for a second time, Japan has built up its own intelligence function as never before, making operations more unified and better-supported. Japan still coordinates extensively with the U.S. in some areas of intelligence but is also taking intelligence matters into its own hands, in a way not seen for several decades. As Samuels notes, Japan’s increasing foreign-policy independence is also supported by voters. “Japanese public opinion has changed,” Samuels says. “They see the issues now, they talk about it now. Used to be, you couldn’t talk about intelligence in polite company. But people talk about it now, and they’re much more willing to go forward.” “Special Duty” has been praised by other scholars in the field of Japanese security studies and foreign policy. Sheila Smith of the Council on Foreign Relations in Washington calls it a “truly wonderful book” that “offers much needed insight to academics and policymakers alike as they seek to understand the changes in Japan's security choices. ” By looking at intelligence issues in this way, Samuels has also traced larger contours in Japanese history: first, an opening up to the world, then the alignment with the U.S. in the postwar world, and now a move toward greater capabilities. On the intelligence front, those capabilities include enhanced analysis and streamlined relations across units, heading toward the full spectrum of functions seen in the other major states.   “It’s been the assumption that the Japanese just don’t do [intelligence activities], except economics,” Samuels reflects. “Well, I hope after people see this book they will understand that’s no longer the case, and hasn’t been for some time.” ",Others,0.08591566015945681
99,Science Daily,Fast-Acting German Insecticide Lost in the Aftermath of WWII,Environment,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011131907.htm,"   ""We set out to study the growth of crystals in a little-known insecticide and uncovered its surprising history, including the impact of World War II on the choice of DDT -- and not DFDT -- as a primary insecticide in the 20th century,"" said Bart Kahr, professor of chemistry at New York University and one of the study's senior authors. Discovering solid forms of DFDT Kahr and fellow NYU chemistry professor Michael Ward study the growth of crystals, which two years ago led them to discover a new crystal form of the notorious insecticide DDT. DDT is known for its detrimental effect on the environment and wildlife. But the new form developed by Kahr and Ward was found to be more effective against insects -- and in smaller amounts, potentially minimizing its environmental impact. In continuing to explore the crystal structure of insecticides, the research team began studying fluorinated forms of DDT, swapping out chlorine atoms for fluorine. They prepared two solid forms of the compound -- a monofluoro and a difluoro analog -- and tested them on fruit flies and mosquitoes, including mosquito species that carry malaria, yellow fever, Dengue, and Zika. The solid forms of fluorinated DDT killed insects more quickly than did DDT; the difluoro analog, known as DFDT, killed mosquitoes two to four times faster. ""Speed thwarts the development of resistance,"" said Ward, a senior author on the study. ""Insecticide crystals kill mosquitoes when they are absorbed through the pads of their feet. Effective compounds kill insects quickly, possibly before they are able to reproduce."" The researchers also made a detailed analysis of the relative activities of the solid-state forms of fluorinated DDT, noting that less thermodynamically stable forms -- in which the crystals liberate molecules more easily -- were more effective at quickly killing insects.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The forgotten history of DFDT In addition to their chemical analyses, the researchers sought to determine if their creation had a precedent. In doing so, they uncovered a rich and unsettling backstory for DFDT. Through historical documents, they learned that DFDT was created as an insecticide by German scientists during World War II and was used by the German military for insect control in the Soviet Union and North Africa, in parallel with the use of DDT by American armed forces in Europe and the South Pacific. In the post-war chaos, however, DFDT manufacturing came to an abrupt end. Allied military officials who interviewed Third Reich scientists dismissed the Germans' claims that DFDT was faster and less toxic to mammals than DDT, calling their studies ""meager"" and ""inadequate"" in military intelligence reports. In his 1948 Nobel Prize address for the discovery of the insect-killing capability of DDT, Paul Müller noted that DFDT should be the insecticide of the future, given that it works more quickly than does DDT. Despite this, DFDT has largely been forgotten and was unknown to contemporary entomologists with whom the NYU researchers consulted. ""We were surprised to discover that at the outset DDT had a competitor which lost the race because of geopolitical and economic circumstances, not to mention its connection to the German military, and not necessarily because of scientific considerations. A faster, less persistent insecticide, as is DFDT, might have changed the course of the 20th century; it forces us to imagine counterfactual science histories,"" said Kahr.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The need for new insecticides Mosquito-borne diseases such as malaria -- which kills a child every two minutes -- are major public health concerns, resulting in 200 million illnesses annually. Newer diseases like Zika may pose growing threats to health in the face of a changing climate. Mosquitoes are increasingly resistant and are failing to respond to the pyrethroid insecticides built into bed nets. Public health officials are concerned and have reconsidered the use of DDT -- which has been banned for decades in much of the world with the exception of selective use for malaria control -- but its controversial history and environmental impact encourage the need for new insecticides. ""While more research is needed to better understand the safety and environmental impact of DFDT, we, along with the World Health Organization, recognize the urgent need for new, fast insecticides. Not only are fast-acting insecticides critical for fighting the development of resistance, but less insecticide can be used, potentially reducing its environmental impact,"" said Ward. In addition to Ward and Kahr, the study authors are Xiaolong Zhu, Chunhua T. Hu, Jingxiang Yang, and Mengdi Qi of NYU's Department of Chemistry, as well as Leo A. Joyce of Arrowhead Pharma. This work was supported by the NYU Materials Research Science and Engineering Center (MRSEC) program of the National Science Foundation (award number DMR-1420073). The NYU X-ray facility is supported partially by the NSF (award number CRIF/CHE-0840277). ",Environment,0.08461039384691005
100,Science Daily,Scientists 'Must Be Allowed to Cry' About Destruction of Nature,Science,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010142124.htm,"   In a letter published in the journal Science, three leading researchers say it is ""dangerously misguided"" to assume scientists are dispassionate observers. They say many scientists experience ""strong grief responses"" to the current ecological crisis, and there are profound risks to ignoring this emotional trauma. Tim Gordon, lead author of the letter and a marine biologist from the University of Exeter, said ""We're documenting the destruction of the world's most beautiful and valuable ecosystems, and it's impossible to remain emotionally detached. ""When you spend your life studying places like the Great Barrier Reef or the Arctic ice caps, and then watch them bleach into rubble fields or melt into the sea, it hits you really hard."" Co-writer Professor Andy Radford, of the University of Bristol, added: ""The emotional burden of this kind of research should not be underestimated.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Grief, when unaddressed, can cloud judgment, inhibit creativity and engender a sense that there is no way forward."" The letter calls on academic institutions to support environmental scientists, allowing them to address their ecological grief professionally and emerge stronger from traumatic experiences to discover new insights about the natural world. The authors fear that environmental scientists tend to respond to degradation of the natural world by ignoring, suppressing or denying the resulting painful emotions while at work. But they propose that much can be learned from professions where distressing events are common, such as healthcare, emergency services and the military. In these fields, well-defined strategies exist for employees to anticipate and manage their emotional distress, including training, debriefing, support and counselling after disturbing events.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Dr Steve Simpson of the University of Exeter, also a co-writer of the letter, said: ""Instead of ignoring or suppressing our grief, environmental scientists should be acknowledging, accepting and working through it. ""In doing so, we can use grief to strengthen our resolve and find ways to understand and protect ecosystems that still have a chance of survival in our rapidly changing world."" The letter ends by suggesting that better psychological support for environmental scientists might improve their ability to think creatively about the future. Gordon said: ""If we're serious about finding any sort of future for our natural ecosystems, we need to avoid getting trapped in cycles of grief. ""We need to allow ourselves to cry -- and then see beyond our tears."" ",Environment,0.08408047434935714
101,ACM,Researchers Use Game Theory to Identify Bacterial Antibiotic Resistance,ACM,2019-10-09,-,https://news.wsu.edu/2019/10/09/researchers-use-game-theory-successfully-identify-bacterial-antibiotic-resistance/,"       Researchers Use Game Theory to Identify Bacterial Antibiotic Resistance     WSU InsiderTina HildingOctober 9, 2019   Washington State University (WSU) researchers have developed a new technique that taps machine learning and game theory to identify previously unknown antibiotic-resistance genes in bacteria. The team used a machine learning algorithm and game theory to examine interactions of several features of genetic material, including structure and physiochemical, evolutionary, and compositional characteristics of protein sequences. The application was able to detect antibiotic-resistant genes in three different types of bacteria with 93% to 99% accuracy. Said WSU Professor Shira Broschat, “This novel game theory approach is especially powerful because features are chosen on the basis of how well they work together as a whole to identify likely antimicrobial-resistance genes — taking into account both the relevance and interdependency of features.""                 ",Health,0.08188268007842926
102,Science Daily,Lakes Worldwide Are Experiencing More Severe Algal Blooms,Environment,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111746.htm,"   Reports of harmful algal blooms -- like the ones that shut down Toledo's water supply in 2014 or led to states of emergency being declared in Florida in 2016 and 2018 -- are growing. These aquatic phenomena are harmful either because of the intensity of their growth, or because they include populations of toxin-producing phytoplankton. But before this research effort, it was unclear whether the problem was truly getting worse on a global scale. Likewise, the degree to which human activity -- including agriculture, urban development, and climate change -- was contributing to this problem was uncertain. ""Toxic algal blooms affect drinking water supplies, agriculture, fishing, recreation, and tourism,"" explained lead author Ho. ""Studies indicate that just in the United States, freshwater blooms result in the loss of $4 billion each year."" Despite this, studies on freshwater algal blooms have either focused on individual lakes or specific regions, or the period examined was comparatively short. No long-term global studies of freshwater blooms had been undertaken until now. Ho, Michalak, and Pahlevan used 30 years of data from NASA and the U.S. Geological Survey's Landsat 5 near-Earth satellite, which monitored the planet's surface between 1984 and 2013 at 30 meter resolution, to reveal long-term trends in summer algal blooms in 71 large lakes in 33 countries on six continents. To do so, they created a partnership with Google Earth Engine to process and analyze more than 72 billion data points. ""We found that the peak intensity of summertime algal blooms increased in more than two-thirds of lakes but decreased in a statistically significant way in only six of the lakes,"" Michalak explained. ""This means that algal blooms really are getting more widespread and more intense, and it's not just that we are paying more attention to them now than we were decades ago."" Although the trend towards more-intense blooms was clear, the reasons for this increase seemed to vary from lake to lake, with no consistent patterns among the lakes where blooms have gotten worse when considering factors such as fertilizer use, rainfall, or temperature. One clear finding, however, is that among the lakes that improved at any point over the 30-year period, only those that experienced the least warming were able to sustain improvements in bloom conditions. This suggests that climate change is likely already hampering lake recovery in some areas. ""This finding illustrates how important it is to identify the factors that make some lakes more susceptible to climate change,"" Michalak said. ""We need to develop water management strategies that better reflect the ways that local hydrological conditions are affected by a changing climate."" This research was supported by the U.S. National Science Foundation, the Natural Sciences and Engineering Research Council of Canada, a Google Earth Engine Research Award, a NASA ROSES grant, and by a USGS Landsat Science Team Award. ",Environment,0.08152636201347756
103,Stanford,How uncertainty in scientific predictions can help and harm credibility,Science,2019-10-15,-,https://news.stanford.edu/2019/10/14/uncertainty-scientific-predictions-can-help-harm-credibility/,"   The more specific climate scientists are about the uncertainties of global warming, the more the American public trusts their predictions, according to new research by Stanford scholars.  A new study examines how Americans respond to scientists’ predictions about climate change. (Image credit: Getty Images)  But scientists may want to tread carefully when talking about their predictions, the researchers say, because that trust falters when scientists acknowledge that other unknown factors could come into play. In a new study publishing Oct. 14 in Nature Climate Change, researchers examined how Americans respond to climate scientists’ predictions about sea level rise. They found that when climate scientists include best-case and worst-case case scenarios in their statements, the American public is more trusting and accepting of their statements. But those messages may backfire when scientists also acknowledge they do not know exactly how climate change will unfold. “Scientists who acknowledge that their predictions of the future cannot be exactly precise and instead acknowledge a likely range of possible futures may bolster their credibility and increase acceptance of their findings by non-experts,” said Jon Krosnick, a Stanford professor of communication and of political science and a co-author on the paper. “But these gains may be nullified when scientists acknowledge that no matter how confidently they can make predictions about some specific change in the future, the full extent of the consequences of those predictions cannot be quantified.” Effects of communicating uncertainty Predicting the future always comes with uncertainty, and climate scientists routinely recognize limitations in their predictions, note the researchers. “In the context of global warming specifically, scientific uncertainty has been of great interest, in part because of concerted efforts by so-called ‘merchants of doubt’ to minimize public concern about the issue by explicitly labeling the science as ‘uncertain,’” said Lauren Howe, who was a postdoctoral scholar at Stanford when she conducted the research with Krosnick and is first author on the paper. “We thought that, especially in this critical context, it was important to understand whether expressing uncertainty would undermine persuasion, or whether the general public might instead recognize that the study of the future has to involve uncertainty and trust predictions where that uncertainty is openly acknowledged more than those where it is minimized,” Howe said. To better understand how the public reacts to scientists’ messages about the uncertainties of climate change, the researchers presented a nationally representative sample of 1,174 American adults with a scientific statement about anticipated sea level rise. Respondents were randomly assigned to read either a prediction of the most likely amount of future sea level rise; a prediction plus a worst-case scenario; or a robust prediction with worst-case and best-case scenarios, for example: “Scientists believe that, during the next 100 years, global warming will cause the surface of the oceans around the world to rise about 4 feet. However, sea level could rise as little as 1 foot, or it could rise by as much as 7 feet.” The researchers found that when predictions included a best-case and worst-case scenario, it increased the number of participants who reported high trust in scientists by 7.9 percentage points compared with participants who only read a most likely estimate of sea level rise. Changes in environmental policies, human activities, new technologies and natural disasters make it difficult for climate scientists to quantify the long-term impact of a specific change – which scientists often acknowledge in their predictions, the researchers said. They wanted to know if providing such well-intended, additional context and acknowledging complete uncertainty would help or hurt public confidence in scientific findings. To find out, the researchers asked half of their respondents to read a second statement acknowledging that the full extent of likely future damage of sea level rise cannot be measured because of other forces, such as storm surge: “Storm surge could make the impacts of sea level rise worse in unpredictable ways.” The researchers found that this statement eliminated the persuasive power of the scientists’ messages. When scientists acknowledged that storm surge makes the impact of sea level rise unpredictable, it decreased the number of participants who reported high trust in scientists by 4.9 percentage points compared with the participants who only read a most likely estimate of sea level rise. The findings held true regardless of education levels and political party affiliation. Not all expressions of uncertainty are equal, Howe said: “Scientists may want to carefully weigh which forms of uncertainty they discuss with the public. For example, scientists could highlight uncertainty that has predictable bounds without overwhelming the public with the discussion of factors involving uncertainty that can’t be quantified.”  Howe earned her doctorate in social psychology in 2017 from Stanford University, where she was the Shaper Family Stanford Interdisciplinary Graduate Fellow. Howe is now a postdoctoral scholar at the University of Zurich. Other co-authors on the paper include Bo MacInnis, an adjunct lecturer in the Department of Communication at Stanford. Ezra Markowitz of the University of Massachusetts Amherst and Robert Socolow of Princeton University are also co-authors. This study was funded by the Woods Institute for the Environment and the Center for Ocean Solutions at Stanford University. The work was also supported by a National Science Foundation Graduate Research Fellowship grant and the Princeton University Institute for International and Regional Studies. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest.  ",Environment,0.08044206703441892
104,Stanford,Archaea hold clues to ancient ocean temperatures,Science,2019-10-15,-,https://news.stanford.edu/2019/10/07/archaea-hold-clues-ancient-ocean-temperatures/,"   Solving a decades-old mystery, Stanford researchers have discovered proteins that enable hardy microbes called archaea to toughen up their membranes when waters are overly warm. Finding these proteins could help scientists piece together the state of Earth’s climate going back millions of years to when those archaea were cruising the ancient oceans.  A view of the Pacific Ocean from Pescadero, California. Researchers used archaea with a genetic dataset from the north Pacific Ocean to identify proteins that could help gauge ancient ocean temperatures. (Image credit: Patrick Tomasso / Unsplash)  “People have been looking for these proteins for 40 years,” said Paula Welander, an associate professor of Earth system science in Stanford University’s School of Earth, Energy & Environmental Sciences (Stanford Earth), and lead author of a study describing the finding published Oct. 7 in Proceedings of the National Academy of Sciences. With this finding scientists can more accurately use the lipids – or fats – found in archaeal membranes and preserved in the ocean’s sediments to estimate historic ocean temperatures, Welander said. Battening down the hatches When under stress, archaea fuse their usually double-layered cell membranes into a single layer. Battening down the hatches in this manner firms up the membranes, which, being mostly made of fat, can get too floppy when the temperature spikes – like butter left on a kitchen counter. Some archaea further modify the structures that fuse their membrane layers by adding on ring-like pieces that make the membranes even more compact and sturdy. These adaptations are helpful from a climatology perspective, since the membrane-linking structures – along with those sets of rings – readily preserve in marine sediments. By examining the numbers and kinds of rings, climate scientists can gauge surface water temperatures where and when those archaea lived. This technique has been used as evidence of the warmer seas of the Jurassic era, dating back more than 150 million years to the heyday of the dinosaurs. Finding the proteins involved in making those structures resolves some uncertainties scientists have had about inferring ancient temperatures from archaeal lipids – what they call the paleotemperature proxies. Climatologists have presumed that a single group of archaea, the Thaumarchaeota, are responsible for making lipids with rings found in open oceans and that they add those rings in response to water temperature changes. But if other environmental factors such as salinity and acidity trigger ring production in other marine archaeal groups, that could scramble how they read the temperature signals. According to the new study, climatologists can breathe a sigh of relief. By finally nailing down the proteins in play, the Stanford researchers show that Thaumarchaeota are indeed the dominant source of the ring-bearing membrane structures in ocean waters, supporting previous ideas of ancient sea surface temperatures. “With that critical information now in hand, we can start to constrain some of the uncertainty about this particular archaea-based paleotemperature proxy,” Welander said. Pursuing the proteins Although not identified until the late 1970s, archaea have since been recognized as constituting a whole new third domain of life, alongside the more-familiar bacteria and eukaryotes – multicellular organisms, including humans. Although archaea superficially resemble bacteria, biochemical and reproductive differences testify to their uniqueness. Many archaea are also extremophiles, which thrive in austere environments like hot springs where other life cannot survive. To find the ring-making proteins, the Stanford team experimented with Sulfolobus acidocaldarius, among the least difficult archaea to grow and manipulate in a lab. “This organism is one of the very few archaea that has a genetic system where we can do the kind of work we like to do,” Welander said. Her team set out to find which proteins enabled S. acidocaldarius to attach rings to its membrane-spanning structures. The researchers first found three possible genes by looking across the genomes of archaea that do and don’t construct rings. They then created mutants in the lab lacking one, two or all three genes and, ultimately, two of these genes proved integral to the ring structures. Those genes failed to turn up in another group of archaea that share marine environments with Thaumarchaeota and were considered as a possible, additional source of ringed structures in sediment samples. With that contribution ruled out, the sea temperature estimates derived from the paleotemperature proxy in question look more robust. Taking it global Welander said that scientists can now look into extending the Stanford team’s findings into well-sampled marine regions worldwide. Her team picked through a genetic dataset from the north Pacific Ocean, and it therefore only directly speaks to that particular biome. Other datasets from the Atlantic Ocean and the Mediterranean Sea, for example, should reveal if Thaumarchaeota are also responsible for laying down the molecular fossils of interest in those areas. These paleotemperature proxies could even be extended into lakes and other environments, Welander said, opening up still more pages in Earth’s climate chronicles. Going beyond the climatological aspects of the findings, Welander noted that figuring out how the archaeal proteins handle the arcane work of membrane fusing could reveal compelling new biochemistry for potential real-world applications, such as drug discovery and materials science. “Microbes invent all kinds of weird biochemistry to do all kinds of weird reactions,” Welander said. “Anytime you can expand that chemistry of what is possible, it’s really exciting from just a basic science perspective.”  Welander is also a member of Stanford Bio-X. Other Stanford co-authors include postdoctoral researcher Zhirui Zeng and research scientist Jeremy Hu-Chen Wei. Other co-authors are from the University of Oklahoma, the University of Illinois Urbana-Champaign and the Massachusetts Institute of Technology. The study was funded by the Simons Foundation, the National Science Foundation and the U.S. Department of Energy. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest.  ",Environment,0.08037618707673891
105,Science Daily,New Tool Enables Nova Scotia Lobster Fishery to Address Impacts of Climate Change,Environment,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011155319.htm,"   ""Climate change has socio-economic impacts on coastal communities and the seafood market, but integrating that information into planning and decision-making has been a challenge,"" said Vincent Saba, a fishery biologist at the Northeast Fisheries Science Center and a co-author of the study. ""Ocean warming is leading to an accelerated redistribution of marine species. Knowing how animals will shift distribution, and what to do about shifts across management borders both regional and international, will be critical to planning on how to adapt to those changes."" American lobster is Canada's most valuable fishery, contributing 44 percent of the total commercial value of all fisheries in Atlantic Canada in 2016. Lobster landings have been trending upward in recent decades, and many small rural communities in Atlantic Canada rely heavily on lobster for their economic well-being. Changing climate could have a significant impact on the fishery and on those communities. Researchers from Fisheries and Oceans Canada at the Bedford Institute of Oceanography in Halifax, Nova Scotia and at NOAA's Northeast Fisheries Science Center collaborated on the study. Their findings, published in Frontiers in Marine Science, indicate that overall projected changes in offshore lobster habitat for the region as a whole are positive, but that changes in resource management need to be considered to promote the long-term sustainability of the fishery in Nova Scotia. Ocean temperatures have been warming in the Gulf of Maine and along the Northeast Continental Shelf during the past few decades, causing many species to shift their distribution to the northeast. When ocean temperatures are above the preferred range for lobsters, it can reduce their survival, growth, and reproduction. The potential effects of marine heat waves, like that observed in the Gulf of Maine lobster population in 2012, can also be significant on the Scotian Shelf, a region with a relatively high proportion of species at the edge of their thermal range. Coastal Infrastructure and Lobster Vulnerability Researchers generated two climate change vulnerability indices, one for coastal communities and one for lobster in Nova Scotia. Two ocean models, a regional ocean model with high resolution in the Scotian Shelf and Gulf of Maine region and a global climate model, provided projections of ocean bottom temperatures over multiple decades. The coastal infrastructure vulnerability index puts a numerical value on each lobster management area to indicate relative vulnerability to the effects of climate change. Factors included economic dependence on the fishery, community population size, diversity of the fishery revenue, status of harbor infrastructure, total replacement cost of each harbor, increased relative sea level and flooding, impacts of wind and wave climate, and sea ice. The vulnerability index also puts a numerical value on the vulnerability of offshore lobster habitat to ocean warming and changes in zooplankton, a primary prey, as well as anticipated changes in fishery productivity across management borders. Study authors suggest the new assessment tool could prepare a region for changes in potential catch through adjustments in licensing and quotas, or adapting to a decrease in productivity by encouraging and assisting fishermen to diversify their targeted species, where they fish, or to seek non-fisheries-related income. It could also support planning for projected increases in catch through investing in upgrades to coastal community infrastructure. ""Our analysis is a first step in considering this information in local fishery management decisions and longer-term economic development strategies,' said Saba, who is located at NOAA's Geophysical Fluid Dynamics Laboratory at Princeton University. ""This tool anticipates change and could be incorporated into assessment models and help fishermen and resource managers with long-term planning."" ",Environment,0.08020133150963075
106,Science Daily,Evolutionary History of Oaks,Environment,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111726.htm,"   Fundamental questions about relationships between organisms and the genes that drive ecological diversification underlie the secrets of biodiversity. Understanding the past of this ecologically, economically and culturally important group provides a baseline of knowledge that will allow scientists to address additional questions about oaks and other trees, as well as help with conservation efforts. ""This paper demonstrates that oaks have repeatedly and globally diversified in response to ecological opportunity"" says Hipp. ""The changes in the global landscape have given us the gift of the oak diversity we observe today."" Patchwork of Histories  The new paper, to be published in New Phytologist, is available free through an Early View online for one month beginning October 14. The study provides the most detailed account to date of the evolutionary history of the world's oaks. Investigating which parts of the oak genome distinguish species from one another, researchers at The Morton Arboretum, in collaboration with 17 institutions around the world, discovered that each gene or stretch of DNA in the genome has the potential to record multiple histories; each section bears the history of speciation of one oak lineage, but it may record the history of hybridization for a different lineage. In other words, there is no one region of the genome that defines oaks: it is the patchwork of histories embedded in the genome that characterize the history of oak evolution. In addition, this research shows that different oak lineages have repeatedly diversified in the same area. Red oaks, white oaks, ring-cupped oaks, turkey and cork oaks, and three of the other oak sections arose rapidly and segregated to either the Americas or Eurasia. All of these lineages can be found in part of their range with at least one other lineage. As oaks migrated, species interbred, hybridized and diversified opportunistically in response to changes in the landscape. The highest rates of species diversification have been in response to migrations into new territory. Over and over, oaks have taken advantage of ecological opportunity to produce the diversity we see today, providing humans with ships, homes, wine barrels, furniture and acorns to eat, and providing food and homes for countless insects, mammals, birds and fungi. ""For the first time, this paper demonstrates that the history of different [oak] lineages is driven by different sets of genes,"" said co-author Dr. Antoine Kremer from the French National Institute for Agricultural Research. ""The story of oak evolution is especially fascinating due to the ecological and morphological convergence in different oak lineages that cohabit on the same continent."" The importance of oaks  Oaks support the planet's ecosystem like very few other tree species do. As both stately trees and dry-land shrubs, oaks are fundamental to the health of forests, providing critical food, habitat and shelter for animals, birds and insects, and have the highest amount of biomass compared to any other tree species in the forest, working harder to clean the air than many other tree species. Today, oaks need the help of people. Around the world, oaks are under threat, due to pests, diseases and loss of habitat. If oaks are lost, it will upset the delicate balance of forest ecosystems and leave humans without their benefits. Researchers and conservationists at The Morton Arboretum are committed to ensuring oaks thrive. Learn more about what The Morton Arboretum is doing to conserve oaks globally. ",Environment,0.079872533966192
107,Science Daily,No Evidence That Power Posing Works,Society,2019-10-01,-,https://www.sciencedaily.com/releases/2019/10/191001110824.htm,"   The concept of power posing -- think of a Wonder Woman stance -- gained popularity after a 2010 study reported that people who adopted an expansive physical pose decreased cortisol levels (an indicator of stress), increased testosterone levels and felt more powerful and willing to take risks. However, Marcus Credé, an associate professor of psychology at Iowa State, says there is not a single study to support the claims that power posing works. Not long after the original study was published it drew criticism because the results could not be replicated. In 2018, the researchers responded to critics by presenting an updated analysis of their own research and other studies on power posing to support their claims. In a new commentary, published by the open-access journal Meta-Psychology, Credé reviewed every study on power posing as well as the analysis the researchers provided and found a significant flaw. Nearly all of the studies he reviewed were poorly designed and failed to compare power poses to normal poses. Instead, they only compared power poses to contractive ones, such as slouching. Credé says not having a neutral pose for comparison can skew the results. That's because any difference between a power pose and a contractive pose could occur because a contractive pose makes you feel worse, rather than an expansive pose making you feel better. The lack of oversight is troubling, Credé said, knowing that dozens of researchers have worked on this issue and never identified the problem. What he finds even more concerning is the number of people who have bought into the concept. A TED Talk on power posing has been viewed more than 70 million times and a book on power posing was a New York Times bestseller. ""There has literally never been a study that compared a power pose to a normal pose and found any positive effect for a power pose,"" Credé said. ""I find this pretty stunning because of the multimillion-dollar industry that has been built up around power posing. It is not dissimilar to a drug being sold to the public without a single study ever having been able to show that the drug works better than placebo or doing nothing."" Feelings of power diminished when compared to neutral pose Only four of the nearly 40 studies that exist on power posing were designed in such a way as to shed light on the benefits, Credé said. One of those studies compared the effect of slouched, neutral and power poses on feelings of dominance. According to the findings, feelings of dominance were highest in the neutral position and the power pose was associated with diminished feelings of power. Similarly, three other studies examined the three poses to determine the effect on mood. All reported significant differences in mood for different poses, but Credé says the results are driven by the negative effect of the slouching posture. ""The only conclusion researchers should draw from the existing literature on postural feedback is that contractive poses such as slouching should be avoided, which is hardly novel,"" he said. ""I recall my elementary school teachers yelling at us about slouching and not what has been sold here."" ",Society,0.07835727450896146
108,MIT News,New method visualizes groups of neurons as they compute,Research,2019-10-09,-,http://news.mit.edu/2019/flourescent-visualize-neuron-activity-1009,"  Using a fluorescent probe that lights up when brain cells are electrically active, MIT and Boston University researchers have shown that they can image the activity of many neurons at once, in the brains of mice. This technique, which can be performed using a simple light microscope, could allow neuroscientists to visualize the activity of circuits within the brain and link them to specific behaviors, says Edward Boyden, the Y. Eva Tan Professor in Neurotechnology and a professor of biological engineering and of brain and cognitive sciences at MIT. “If you want to study a behavior, or a disease, you need to image the activity of populations of neurons because they work together in a network,” says Boyden, who is also a member of MIT’s McGovern Institute for Brain Research, Media Lab, and Koch Institute for Integrative Cancer Research. Using this voltage-sensing molecule, the researchers showed that they could record electrical activity from many more neurons than has been possible with any existing, fully genetically encoded, fluorescent voltage probe. Boyden and Xue Han, an associate professor of biomedical engineering at Boston University, are the senior authors of the study, which appears in the Oct. 9 online edition of Nature. The lead authors of the paper are MIT postdoc Kiryl Piatkevich, BU graduate student Seth Bensussen, and BU research scientist Hua-an Tseng. Seeing connections Neurons compute using rapid electrical impulses, which underlie our thoughts, behavior, and perception of the world. Traditional methods for measuring this electrical activity require inserting an electrode into the brain, a process that is labor-intensive and usually allows researchers to record from only one neuron at a time. Multielectrode arrays allow the monitoring of electrical activity from many neurons at once, but they don’t sample densely enough to get all the neurons within a given volume.  Calcium imaging does allow such dense sampling, but it measures calcium, an indirect and slow measure of neural electrical activity. In 2018, Boyden’s team developed an alternative way to monitor electrical activity by labeling neurons with a fluorescent probe. Using a technique known as directed protein evolution, his group engineered a molecule called Archon1 that can be genetically inserted into neurons, where it becomes embedded in the cell membrane. When a neuron’s electrical activity increases, the molecule becomes brighter, and this fluorescence can be seen with a standard light microscope. In the 2018 paper, Boyden and his colleagues showed that they could use the molecule to image electrical activity in the brains of transparent worms and zebrafish embryos, and also in mouse brain slices. In the new study, they wanted to try to use it in living, awake mice as they engaged in a specific behavior. To do that, the researchers had to modify the probe so that it would go to a subregion of the neuron membrane. They found that when the molecule inserts itself throughout the entire cell membrane, the resulting images are blurry because the axons and dendrites that extend from neurons also fluoresce. To overcome that, the researchers attached a small peptide that guides the probe specifically to membranes of the cell bodies of neurons. They called this modified protein SomArchon. “With SomArchon, you can see each cell as a distinct sphere,” Boyden says. “Rather than having one cell’s light blurring all its neighbors, each cell can speak by itself loudly and clearly, uncontaminated by its neighbors.” The researchers used this probe to image activity in a part of the brain called the striatum, which is involved in planning movement, as mice ran on a ball. They were able to monitor activity in several neurons simultaneously and correlate each one’s activity with the mice’s movement. Some neurons’ activity went up when the mice were running, some went down, and others showed no significant change. “Over the years, my lab has tried many different versions of voltage sensors, and none of them have worked in living mammalian brains until this one,” Han says. Using this fluorescent probe, the researchers were able to obtain measurements similar to those recorded by an electrical probe, which can pick up activity on a very rapid timescale. This makes the measurements more informative than existing techniques such as imaging calcium, which neuroscientists often use as a proxy for electrical activity. “We want to record electrical activity on a millisecond timescale,” Han says. “The timescale and activity patterns that we get from calcium imaging are very different. We really don’t know exactly how these calcium changes are related to electrical dynamics.” With the new voltage sensor, it is also possible to measure very small fluctuations in activity that occur even when a neuron is not firing a spike. This could help neuroscientists study how small fluctuations impact a neuron’s overall behavior, which has previously been very difficult in living brains, Han says. The study “introduces a new and powerful genetic tool” for imaging voltage in the brains of awake mice, says Adam Cohen, a professor of chemistry, chemical biology, and physics at Harvard University. “Previously, researchers had to impale neurons with fine glass capillaries to make electrical recordings, and it was only possible to record from one or two cells at a time. The Boyden team recorded from about 10 cells at a time. That’s a lot of cells,” says Cohen, who was not involved in the research. “These tools open new possibilities to study the statistical structure of neural activity. But a mouse brain contains about 75 million neurons, so we still have a long way to go.” Mapping circuits The researchers also showed that this imaging technique can be combined with optogenetics — a technique developed by the Boyden lab and collaborators that allows researchers to turn neurons on and off with light by engineering them to express light-sensitive proteins. In this case, the researchers activated certain neurons with light and then measured the resulting electrical activity in these neurons. This imaging technology could also be combined with expansion microscopy, a technique that Boyden’s lab developed to expand brain tissue before imaging it, make it easier to see the anatomical connections between neurons in high resolution. “One of my dream experiments is to image all the activity in a brain, and then use expansion microscopy to find the wiring between those neurons,” Boyden says. “Then can we predict how neural computations emerge from the wiring.” Such wiring diagrams could allow researchers to pinpoint circuit abnormalities that underlie brain disorders, and may also help researchers to design artificial intelligence that more closely mimics the human brain, Boyden says. The MIT portion of the research was funded by Edward and Kay Poitras, the National Institutes of Health, including a Director’s Pioneer Award, Charles Hieken, John Doerr, the National Science Foundation, the HHMI-Simons Faculty Scholars Program, the Human Frontier Science Program, and the U.S. Army Research Office. ",Health,0.07804545432484906
109,Science Daily,"Happy, Angry or Neutral Expressions? Eyes React Just as Fast",Health,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110655.htm,"   In everyday life, people are exposed to numerous stimuli, just as when walking through a town, there are people and faces, clothes and shop windows. From this onslaught of information, relevant content must be filtered and reacted to. ""This shift in attention is often accompanied by a movement of the eyes,"" says Kulke, a researcher in the Department of Affective Neuroscience and Psychophysiology at the University of Göttingen. In the current study, she combined two methods to investigate what happens in the brain during this attention shift: eye-tracking and EEG. With the Eye-Tracker, research volunteers sat in front of a device that records eye movements. Kulke then showed them standardised faces with different emotional expressions. At the same time, EEG measured brain waves via electrodes placed on their head. ""We investigated how quickly our test subjects look at the faces that appear on the screen in different places,"" says Kulke. The result shows that quick and immediate eye movements were occurring independently of the facial expression. ""We quickly look at people in our environment with the same speed, regardless of whether they look cheerful, angry or indifferent. The study also showed that the first reactions of the brain are independent of facial expression. Only later -- after the eye movement is completed -- do the reactions of the brain show strong responses to the emotional expression of the face."" According to Kulke, the result is also interesting for follow-up studies. The question is whether the processing of emotions also takes place later, when the eyes do not react reflexively, but are consciously controlled using a tangible task. Kulke explains: ""For example, do we process facial expressions that we see out of the corner of our eye before we move our eyes? When given the explicit goal of only looking at certain faces, such as our friendly fellow student, and ignoring others, such as our annoying neighbour, how does that affect our reactions?"" ",Health,0.07750009296402627
110,Science Daily,CO2 Emissions Cause Lost Labor Productivity,Science,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011131856.htm,"   Climate change may also be making outdoor labour more dangerous, according to a new study published in Scientific Reports. It was led by Yann Chavaillaz, a former postdoctoral researcher at Concordia and the Ouranos Institute, and Damon Matthews, professor and Concordia Research Chair in Climate Science and Sustainability in the Department of Geography, Planning and Environment. The researchers examine how extreme high temperatures caused by CO2 emissions could lead to losses in labour productivity. Using calculations based on widely used guidelines regarding rest time recommendations per hour of labour and heat exposure, the authors found that every trillion tonnes of CO2 emitted could cause global GDP losses of about half a percent. They add that we may already be seeing economic losses of as much as two per cent of global GDP as a result of what we have already emitted. They identify agriculture, mining and quarrying, manufacturing and construction as the economic sectors most vulnerable to heat exposure. These sectors account for 73 per cent of low-income countries' output, according to the authors. Developing countries are hardest hit ""The thresholds of heat exposure leading to labour productivity loss are likely to be exceeded sooner and more extensively in developing countries in warmer parts of the world,"" says Matthews.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""These countries are also more vulnerable because a higher fraction of their work force is employed in these sectors and because they have less ability to implement infrastructural changes that deal with a changing climate."" The research suggests that lower-income countries will experience much stronger economic impacts than higher-income countries. Worst hit are tropical areas of the globe such as Southeast Asia, north-central Africa and northern South America. ""The labour productivity loss computed for low- and lower-middle-income countries is approximately nine times higher than the one of high-income countries,"" reads the report. (The authors are also careful to point out that health recommendations are not obligatory and are often not seriously or consistently applied at real-world work sites. Their estimates of productivity loss is based on the strict adherence to health guidelines regarding labour in extreme heat.) From emissions to impacts Matthews and his co-authors based their calculations of historical and future increases of heat exposure using simulations from eight separate Earth Systems Models. While many academic studies have estimated socioeconomic impacts of climate change, he says this paper is novel because it predicts future impacts as a direct function of CO2 emissions.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""The relationship between emissions and impact is pretty linear, so we are able to say that this additional quantity of CO2 emissions will lead to this additional increase in impact,"" he explains. ""The impact scales pretty well with the total amount of emissions we produce."" Cost of business The authors write that their research linking CO2 emissions to loss of labour productivity from heat exposure can help countries adopt mitigating measures. But Matthews says it may also help people change their thinking about the overall consequences of a relentlessly warming planet. ""We can see that every additional ton of CO2 emission that we produce will have this additional impact, and we can quantify that increase,"" he says. ""So this study can help us point to specific countries that are experiencing a quantifiable share of the economic damages that result from the emissions we produce."" ",Environment,0.07747827830967287
111,Science Daily,Achieving a Safe and Just Future for the Ocean Economy,Science,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092238.htm,"   In a climate of environmental change and financial uncertainty, much attention has been given to the growth of the ""Blue Economy"" -- a term which refers to the sustainable use of ocean and marine resources for economic growth, jobs, and improved livelihoods. Ocean resources are viewed as lucrative areas for increased investment, including in fisheries, aquaculture, bio-prospecting, renewable energy, oil and gas, and other businesses. Ensuring that socially equitable and sustainable development occurs should be the mandate of governments and industry, maintain an international group of researchers, led by UBC's Nathan Bennett and Rashid Sumaila. ""Coastal countries and small island developing states have the most at stake when it comes to increased economic activities in local waters,"" said Nathan Bennett, research faculty member in UBC's Institute for the Oceans and Fisheries and lead author on the paper. ""It is important that this not be like a Gold Rush scenario, where unbridled ocean development produces substantial harms for both the marine environment and the wellbeing of the populations who dependent on it. In this paper, we provide solutions to proactively address the potential harms produced by ocean development."" The five recommendations in the paper focus on managing for sustainability, benefit sharing, and creating inclusive decision-making processes at local, national and international levels:1. Establish a global coordinating body and develop international guidelines;    2. Ensure national policies and institutions safeguard sustainability; 3. Promote equitable sharing of benefits and minimization of harms; 4. Employ inclusive governance and decision-making processes; and 5. Engage with insights from interdisciplinary ocean science.""There are currently no set of guidelines, or even an obvious international coordinating body, which focuses on the Blue Economy,"" said Dr. Rashid Sumaila, senior author, professor at UBC's Institute for the Oceans and Director of the OceanCanada Partnership. ""Nothing exists in many nations either. This lack of coordination can lead to situations like we are already seeing in the global fishing industry, where harmful subsidies are leading to overfishing, human rights abuses are occurring, and local access to fish stocks and food security are being undermined."" ""The blue economy is already growing. But, we have an opportunity and responsibility to shape future growth so that it is sustainable and equitable,"" said Bennett. ""Including civil society, such as small-scale fishers, women and Indigenous people, in the decision-making and management processes will help to ensure that benefits are shared."" ",Environment,0.0769912909158029
112,Science Daily,Reading the Past Like an Open Book: Researchers Use Text to Measure 200 Years of Happiness,Health,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111713.htm,"   Using innovative new methods researchers at the University of Warwick, University of Glasgow Adam Smith Business School and The Alan Turing Institute in London have built a new index that uses data from books and newspaper to track levels of national happiness from 1820. Their research could help governments to make better decisions about policy priorities. Governments the world over are making increasing use of ""national happiness"" data derived from surveys to help them consider the impact of policy on national wellbeing. Unfortunately, data for most countries is only available from 2011 onwards, and for a select few from the mid 1970s. This makes it hard to establish long-run trends, or to say anything about the main historical causes of happiness. In order to tackle this problem, a team of researchers including Professor Thomas Hills (Warwick and The Alan Turing Institute), Professor Eugenio Proto (Glasgow), Professor Daniel Sgroi (Warwick), and Dr Chanuki Seresinhe (The Alan Turing Institute) took a key insight from psychology -- that more often than not what people say or write reveals much about their underlying happiness level -- and developed a method to apply it to online texts from millions of books and newspapers published over the past 200 years. The main source of language used for the analysis was the Google Books corpus, a collection of word frequency data for over 8 million books -- that's more than 6 per cent of all books ever published. The method uses psychological valence norms -- values of happiness that can be derived from text -- for thousands of words in di?erent languages to compute the relative proportion of positive and negative language for four di?erent nations (the USA, UK, Germany and Italy). The research team also controlled for the evolution of language, to take into account the fact that some words change their meaning over time. The new index was validated against existing survey-based measures and proven to be an accurate guide to the national mood. One theory as to why books and newspaper articles are such a good source of data is that editors prefer to publish pieces which match the mood of their readers. Studying the index, the researchers found that: Increases in national income do generate increases in national happiness but it takes a huge rise to have a noticeable effect at the national level An increase in longevity of one year had the same effect on happiness as a 4.3 per cent increase in GDP One less year of war had the equivalent effect on happiness of a 30 per cent rise in GDP In post-war UK the worst period for national happiness occurred around the appropriately named ""Winter of Discontent."" In post-war USA the lowest point of the index coincides with the Vietnam War and the evacuation of Saigon.Commenting on the findings, Professor Thomas Hills said: ""What's remarkable is that national subjective well-being is incredibly resilient to wars. Even temporary economic booms and busts have little long-term effect. We can see the American Civil War in our data, the revolutions of 48' across Europe, the roaring 20's and the Great Depression. But people quickly returned to their previous levels of subjective well-being after these events were over. Our national happiness is like an adjustable spanner that we open and close to calibrate our experiences against our recent past, with little lasting memory for the triumphs and tragedies of our age."" Professor Eugenio Proto added: ""Our index is an important first step in understanding people's satisfaction in the past. Looking at the Italian data, it is interesting to note a slow but constant decline in the years of fascism and a dramatic decline in the years after the last crisis."" Professor Daniel Sgroi said: 'Aspirations seem to matter a lot: after the end of rationing in the 1950s national happiness was very high as were expectations for the future, but unfortunately things did not pan out as people might have hoped and national happiness fell for many years until the low-point of the Winter of Discontent.' Dr Chanuki Seresinhe said: ""It was really important to ensure that the changing meaning of words over time was taken into account. For example, the word ""gay"" had a completely different meaning in the 1800s than it does today. We processed terabytes of word co-occurrence data from Google Books to understand how the meaning of words has changed over time, and we validate our findings using only words with the most stable historical meanings."" ",Society,0.07675407072354413
113,Science Daily,The Makeup of Mariculture: Global Trends in Seafood Farming,Environment,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015113320.htm,"   The process of farming seafood in the ocean, known as mariculture, is a growing trend yet little is known about the trajectories of its development. That's why a team of Florida State University researchers set out to shed some light on the industry. FSU postdoctoral researcher Rebecca Gentry, doctoral student Elizabeth Bess Ruff and Assistant Professor of Geography Sarah Lester examined more than 50 years of data from 1950 to 2016 from more than 100 countries around the world. Their study, published in Nature Sustainability, outlined several consistent patterns of mariculture taking place globally. ""Aquaculture is an increasingly important component of global food production,"" Gentry said. ""Therefore, understanding patterns of development has important implications for managing our changing global food systems and ensuring economic development, food security and environmental sustainability."" Gentry and her team examined different development trajectories of mariculture production overall and that of specific groups of species, such as fish and crustaceans. They found that countries with relatively stable production farmed a greater diversity of species than countries with other development trajectories. For example, stable countries produced 15.2 species on average, compared to 6.5 for countries who have experienced a crash in production. Lester pointed out that this result suggests that increasing the diversity of mariculture crops could support more robust and resilient seafood production. Additionally, researchers found the type of species grown had a positive connection with a country's development trajectory. Specifically, countries that initially farmed molluscs, such as oysters or mussels, were more likely to have stable production than countries that started with farming fish. Researchers also found that governance and economic indicators were related to trajectories of mariculture production. For instance, low production countries tended to have lower annual gross domestic product (GDP) scores, lower governmental regulatory quality and lower levels of internet connectivity. Further, the team demonstrated that many countries had stabilized their mariculture production at a level far below their potential productivity. ""This indicates that governance, regulatory or economic changes could unlock further opportunities for growth,"" Gentry said. ""Environmental regulations are important for preventing significant environmental decline, local over-development and unsustainable farm practices. However, for those countries currently failing to meet their mariculture potential, policies to encourage thoughtful growth may be worth considering."" The study is just one part of a larger National Science Foundation-funded project led by Lester that is examining the socioeconomic and ecological drivers of mariculture development. ""This type of multidisciplinary research is essential for better understanding the current effects and future potential of marine aquaculture -- which will be all the more important as the global human population continues to increase and we reach the sustainable limits of other types of food production,"" Lester said. ",Environment,0.0765864735702867
114,Science Daily,New Study Analyzes FEMA-Funded Home Buyout Program,Science,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010113205.htm,"   A research team led by scientists at the University of Miami (UM) Rosenstiel School of Marine and Atmospheric Science found that FEMA-funded voluntary buyouts of flood-prone properties have been more likely to take place in counties with higher population and income. However, the buyouts themselves were concentrated in neighborhoods with lower income and greater social vulnerability. The researchers hope their analysis can provide insights to help revise this program in the future. ""In recent decades, communities throughout the United States have been building experiences with retreat from flood-prone areas, largely through voluntary buyouts of properties after disasters,"" said the study's lead author Katharine Mach, an associate professor of marine ecosystems and society at the UM Rosenstiel School. ""While a proven method of reducing risk, buyouts to date have also illustrated the challenges with locally driven managed retreat and the potential benefits of experimentation with different retreat policies in the future."" In the U.S., over 40,000 homes in flood-prone areas have been purchased and returned to open space in 49 states and three U.S. territories under the FEMA program since its inception in the late 1990s. The federally funded program is typically administered through local government agencies. Mach and colleagues analyzed data on the more than 40,000 buyouts alongside data on flood risk, socioeconomics and demographics to reveal that local governments in counties with higher population and income were more likely to administer buyouts of flood-prone properties. However, at the zip-code level, the bought-out properties themselves were concentrated in areas of lower population and income. As a first national-level analysis of all FEMA-funded buyouts implemented to date, the authors' findings suggest insights on what is working and what is not. The results can support revision of buyout practices, including their potential deployment at larger scales in the future in combination with different types of retreat and climate adaptation policies. ""Retreat in some places will become unavoidable under intensifying climate change, but there are open questions about where, when, and how retreat under climate change will occur,"" said Caroline Kraan, a graduate student at the UM Abess Center for Ecosystem Science and Policy and a coauthor of the study. ""Our analysis offers lessons from buyouts to date that can inform and support strategies of managed retreat in response to climate change."" The analysis also showed that the top three states for most flood damage -- Florida, Louisiana and Mississippi -- ranked 23, 18, and 21 respectively in deployment of buyouts, pointing to the fact that not all flood-damaged areas are currently utilizing the FEMA program for flood risk management. ""These findings suggest that buyouts are resource-intensive to administer, which might make them less feasible in areas that need them the most,"" said study coauthor A.R. Siders, an assistant professor in the Disaster Research Center at the University of Delaware. ""They also suggest that something in the FEMA program favors using buyouts in lower-income neighborhoods, which may have social justice implications."" The authors believe that the analysis can help inform a larger dialogue to better understand how managed retreat is being used now and into the future as governments increasingly look for ways to protect people and property from rising sea levels and more frequent flooding and other natural hazards as a result of climate change. ""The buyout program represents billions of dollars in government spending, yet until now, we didn't have a broad understanding of who benefitted from those investments,"" said Miyuki Hino, a postdoctoral researcher at Stanford University and coauthor of the study. ""Now, we can see not only who has benefitted, but also what types of places and communities may be falling through the cracks."" This study is part of a broader project on managed retreat by the research team. ",Environment,0.0764342951370389
115,Science Daily,Unlocking the Biochemical Treasure Chest Within Microbes,Matter & Energy,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014122620.htm,"   Now, a team of microbiologists and genomicists led by the Department of Energy Joint Genome Institute (JGI) has invented a genetic engineering tool, called CRAGE, that could not only make studying these compounds much easier, but also fill significant gaps in our understanding of how microbes interact with their surroundings and evolve. Their work, a collaboration with Goethe University Frankfurt and DOE Environmental Molecular Sciences Laboratory (EMSL), is published in Nature Microbiology. Diving into microbiomes Secondary metabolites are thusly named because their activities and functions aren't essential for a microbe's survival, yet they may give the organism an advantage in the face of environmental pressures. Encoded by groups of genes called biosynthetic gene clusters (BGCs), the ability to produce these metabolites is easily passed back and forth among both closely and distantly related microbes through horizontal gene transfer. This rapid and widespread sharing allows microbes to adapt to changing conditions by quickly gaining or losing traits, and because the frequent swapping introduces mutations, horizontal gene transfer of BGCs drives the development of diverse compounds. Unfortunately, the fascinating world of secondary metabolism has traditionally been very hard to study because when microbes are brought into the lab, an artificial environment that presents little hardship or competition, they typically don't bother making these compounds. CRAGE -- short for chassis-independent recombinase-assisted genome engineering -- helps scientists get around this roadblock. ""These metabolites are like a language that microbes use to interact with their biomes, and when isolated, they go silent,"" said co-lead author Yasuo Yoshikuni, a scientist at JGI. ""We currently lack the technology to stimulate microbes into activating their BGCs and synthesizing the complete product -- a cellular process that involves many steps."" CRAGE is a highly efficient means of transplanting BGCs originating from one organism into many different potential production hosts simultaneously in order to identify microbial strains that are naturally capable of producing the secondary metabolite under laboratory conditions.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""CRAGE therefore allows us to access these compounds much more readily than before,"" said Helge Bode, co-lead author from Goethe University Frankfurt, Germany. ""In several cases, it has already enabled us to produce and characterize for the first time a compound of interest."" More broadly, by providing a technique to transfer microbial machinery from one species to another, CRAGE will enable scientists to go beyond theories and predictions and finally observe how compounds relegated to the category of ""biological dark matter"" actually work. ""This is a landmark development, because with CRAGE we can examine how different organisms can express one gene network differently, and thus how horizontally transferred capabilities can evolve. The previous tools to do this are much more limited,"" said co-author David Hoyt, a chemist at EMSL, which located at the Pacific Northwest National Laboratory. Hoyt and his colleagues Kerem Bingol and Nancy Washton helped characterize one of the previously unknown secondary metabolites produced when Yoshikuni's group tested CRAGE. Co-first author Jing Ke, a scientific engineering associate at JGI, added, ""Looking beyond secondary metabolites, CRAGE can be used to engineer microbes for the production of proteins, RNAs, and other molecules with a huge range of applications."" Next steps So far, the team has successfully transferred BGCs into 30 diverse bacterial strains, and expect that it should work in many others, though the technique will likely need to be adapted for some species. Further research and product development are currently underway, but the technique is now available to research teams who utilize JGI (a DOE Office of Science User Facility) through pilot programs.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Meanwhile, Yoshikuni -- who developed the precursor gene recombinant tool, RAGE, in 2013 -- and his JGI colleagues have begun applying CRAGE to their own projects, such as exploring unconventional bacterial hosts for biomanufacturing. ""Aside from a few very well-studied microbes, the so-called model organisms like E. coli, we don't know whether a strain will have the skills needed to perform all the steps of BGC activation,"" said Yoshikuni. ""Hopefully with CRAGE, we can start to shift that paradigm -- we can look into more wild species and find their properties that are more suitable for a production of products and medicines."" This work was supported by the DOE Office of Science, the DFG (German Research Foundation), and the LOEWE Center for Translational Biodiversity Genomics. ",Environment,0.07621732898686247
116,Science Daily,Another Reason to Get Cataract Surgery: It Can Make You 48% Safer on the Road,Health,2019-10-12,-,https://www.sciencedaily.com/releases/2019/10/191012141221.htm,"   Cataracts are a normal consequence of aging. They happen gradually over years, as the clear lens inside the eye becomes cloudy. The effects of a developing cataract are sometimes hard to distinguish from other age-related vision changes. You may become more nearsighted; colors appear duller and glare from lights make it harder to see at night. By age 80, about half of us will have developed cataracts. Cataract surgery replaces the cloudy lens with an artificial lens. The surgery is low-risk, fast and effective. But not everyone has surgery right away. The decision is usually based on how much the cataract is interfering with daily life activities. Ophthalmologists typically operate on one eye at a time, starting with the eye with the denser cataract. If surgery is successful and vision improves substantially, sometimes surgery in the second eye is forgone or delayed. However, most people get significant benefit from having surgery on the second eye. Depth perception is improved, vision is crisper, making reading and driving easier. To better understand the true benefit of cataract surgery to patients' quality of life, Jonathon Ng, MD, and his colleagues at the University of Western Australia, tested the driving performance of 44 patients before they had cataract surgery. The driving simulator assessed a variety of variables: adjusted speed limits, traffic densities, uncontrolled intersections and pedestrian crossings. Patients were put through the driving simulator again after their first surgery and then again after their second eye surgery. After the first, near misses and crashes decreased by 35 percent; after the second surgery, the number fell to 48 percent. While visual acuity -- how well one sees the eye chart -- is an important method to assess a person's fitness to drive, it's an incomplete assessment, Dr. Ng said. Quality of vision is also an important indicator. Improved contrast sensitivity and better night vision improves drivers' safety on the road. ""In Australia and other countries, people may often wait months to receive government funded surgery after a cataract is diagnosed,"" said Dr. Ng. ""These results highlight the importance of timely cataract surgery in maintaining safety and continued mobility and independence in older adult drivers."" Some things to consider, when considering cataract surgery: Can you see to safety do your job and to drive? Do you have problems reading or watching TV? Is it difficult to cook, shop, climb stairs or take medications? Do vision problems affect your independence? Do bright lights make is harder to see? ",Health,0.07504503242857277
117,MIT News,Using machine learning to estimate risk of cardiovascular death ,Computer Science,2019-09-12,-,http://news.mit.edu/2019/using-machine-learning-estimate-risk-cardiovascular-death-0912,"  Humans are inherently risk-averse: We spend our days calculating routes and routines, taking precautionary measures to avoid disease, danger, and despair.  Still, our measures for controlling the inner workings of our biology can be a little more unruly.  With that in mind, a team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with a new system for better predicting health outcomes: a machine learning model that can estimate, from the electrical activity of their heart, a patient’s risk of cardiovascular death.  The system, called “RiskCardio,” focuses on patients who have survived an acute coronary syndrome (ACS), which refers to a range of conditions where there’s a reduction or blockage of blood to the heart. Using just the first 15 minutes of a patient's raw electrocardiogram (ECG) signal, the tool produces a score that places patients into different risk categories.  RiskCardio’s high-risk patients — patients in the top quartile — were nearly seven times more likely to die of cardiovascular death when compared to the low-risk group in the bottom quartile. By comparison, patients identified as high risk by the most common existing risk metrics were only three times more likely to suffer an adverse event compared to their low-risk counterparts.  ""We're looking at the data problem of how we can incorporate very long time series into risk scores, and the clinical problem of how we can help doctors identify patients at high risk after an acute coronary event,” says Divya Shanmugam, lead author on a new paper about RiskCardio. “The intersection of machine learning and healthcare is replete with combinations like this — a compelling computer science problem with potential real-world impact.”  Risky business  Previous machine learning models have attempted to get a handle on risk by either making use of external patient information like age or weight, or using knowledge and expertise specific to the system — more broadly known as domain-specific knowledge — to help their model select different features.  RiskCardio, however, uses just the patients’ raw ECG signal, with no additional information. Say a patient checks into the hospital following an ACS. After intake, a physician would first estimate the risk of cardiovascular death or heart attack using medical data and lengthy tests, and then choose a course of treatment.  RiskCardio aims to improve that first step of estimating risk. To do this, the system separates a patient’s signal into sets of consecutive beats, with the idea that variability between adjacent beats is telling of downstream risk. The system was trained using data from a study of past patients. To get the model up and running, the team first separated each patient's signal into a collection of adjacent heart beats. They then assigned a label — i.e., whether or not the patient died of cardiovascular death — to each set of adjacent heartbeats. The researchers trained the model to classify each pair of adjacent heartbeats to its patient outcome: Heartbeats from patients who died were labeled “risky,” while heartbeats from patients who survived were labeled “normal.”  Given a new patient, the team created a risk score by averaging the patient prediction from each set of adjacent heartbeats. Within the first 15 minutes of a patient experiencing an ACS, there was enough information to estimate whether or not they would suffer from cardiovascular death within 30, 60, 90, or 365 days.  Still, calculating a risk score from just the ECG signal is no simple task. The signals are very long, and as the number of inputs to a model increase, it becomes harder to learn the relationship between those inputs.  The team tested the model by producing risk scores for a set of patients. Then, they measured how much more likely a patient would suffer from cardiovascular death as a high-risk patient when compared to a set of low-risk patients. They found that in roughly 1,250 post-ACS patients, 28 would die of cardiovascular death within a year. Using the proposed risk score, 19 of those 28 patients were classified as high-risk.  In the future, the team hopes to make the dataset more inclusive to account for different ages, ethnicities, and genders. They also plan to examine medical scenarios where there’s a lot of poorly labeled or unlabeled data, and evaluate how their system processes and handles that information to account for more ambiguous cases.  “Machine learning is particularly good at identifying patterns, which is deeply relevant to assessing patient risk,'' says Shanmugam. “Risk scores are useful for communicating patient state, which is valuable in making efficient care decisions.”  Shanmugam presented the paper at the Machine Learning for Healthcare Conference alongside PhD student Davis Blalock and MIT Professor John Guttag. ",Health,0.07415069272538495
118,Science Daily,Analysis of US Labor Data Suggests 'Reskilling' Workers for a 'Feeling Economy',Society,2019-10-07,-,https://www.sciencedaily.com/releases/2019/10/191007153437.htm,"   The first wave of AI already has replaced humans for physical repetitive tasks like inspecting equipment, manufacturing goods, repairing things and crunching numbers. That shift started way back with the Industrial Revolution and gave rise to our current Thinking Economy, where employment and wages are more tied to workers' abilities to process, analyze and interpret information to make decisions and solve problems. But be prepared, because AI is already starting to take over those thinking tasks, Rust says. ""It means that if humans want jobs, they better get good at feeling,"" Rust says. ""Things like interpersonal relationships and emotional intelligence will be much more important."" Even though people skills have always been important, what the researchers conclude is that the value of these skills will soon be of unprecedented importance. Rust and Maryland Smith finance professor Vojislav Maksimovic, along with Professor Ming-Hui Huang of National Taiwan University, have been studying this shift. They sifted through U.S. Department of Labor data about work tasks associated with jobs and the people who perform those jobs, covering millions of workers throughout the U.S. economy. They coded the things people report doing in their day-to-day jobs as physical tasks, thinking tasks or feeling tasks and compared the breakdown for each job in 2006 and 2016. Their results reveal a profound shift across the board toward feeling tasks, a big indication that the move to a Feeling Economy is already under way. Their paper, ""The Feeling Economy: Managing in the Next Generation of Artificial Intelligence,"" appears in the most recent issue of the peer-reviewed California Management Review that examines how AI will change business. ""This is something that is going to hit people before they know it,"" says Rust. ""It's already happening. We're already seeing the shift in feeling as being more important, not only in terms of employment growth, but in terms of compensation growth. There is greater compensation growth in feeling than there is in thinking. This is really across the board -- you name a job and we can show a shift from thinking to feeling."" Take the job of a financial analyst, for example. Think that sounds pretty quantitative and thinking-oriented? No so, says Rust. The research reveals it has become much more feeling oriented in the last 10 years. ""People are using more AI-powered tools that can do a lot more of their analytical work for them, and what's left in their job is to hold people's hands and to reassure them about things like stock market dips,"" he says. Going forward, those ""feeling"" skills become even more critical.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""What we're expecting is 'people-people' will be the ones who will be the big successes,"" says Rust. ""This is different from how it is right now and how people assume it's going to be in the future."" Since AI can do more of the thinking tasks, firms need to recruit people who can perform well in feeling tasks and jobs, say the researchers. People management, working with others, emotional intelligence and negotiation skills are already in strong demand and will continue to be top skills for the future. As the Feeling Economy emerges, the nature of all jobs will change, so companies and individuals should prepare, says Rust. Organizations need to manage differently, with more emphasis on feeling, empathy and emotional intelligence. The companies that take advantage of this trend will be the most successful, he says. There will be new opportunities for feeling-oriented companies and products. This also creates opportunities to pull ahead in the global market, says Rust. Individual workers can safeguard their jobs by enhancing their feeling and empathetic skills and gravitating toward jobs that emphasize those tasks. The most successful workers will be those who can manage relationships in an empathetic and emotionally intelligent way. Managerial jobs need to be more people-oriented and feeling-conscious. This may give the edge to women for their emotional intelligence, say the researchers. The ""people"" person becomes much more valuable than the anti-social tech geek. Rust says there are also big implications for education at all levels, where more emphasis is needed on developing emotional intelligence. ""You certainly don't need to worry about things like multiplication tables,"" he says. ""You can do that on a machine, and everybody's cell phone will do that for them. That kind of skill is just useless."" Rust says we better get used to the idea of AI doing more. He thinks AI will eventually even take over most of the emotional tasks of relating to people. And as AI gets more sophisticated, there's no going back, he says. ""The genie is out of the bottle."" ",Society,0.07267481830099351
119,Science Daily,Virtual Reality Training Could Improve Employee Safety,Society,2019-09-16,-,https://www.sciencedaily.com/releases/2019/09/190916212516.htm,"   The Human Factors Research Group at the University of Nottingham, developed an immersive VR system to stimulate participants' perception of temperature, and senses of smell, sight and hearing to explore how they behaved during two health and safety training scenarios: an emergency evacuation in the event of a fire and a fuel leak. In one scenario, participants had to evacuate from a virtual fire in an office, seeing and hearing using a VR headset but could also feel heat from three 2kW heaters, and could smell smoke from a scent diffuser, creating a multisensory virtual environment. This group was compared against another group who were observed in this scenario using only audio-visual elements of VR. Observing real life behaviours Previous research on human behaviour during real-world fire incidents has shown that a lack of understanding of the spread and movement of fire often means that occupants are unprepared and misjudge appropriate actions. Immersive health and safety training enables employers to train people about hazards and hazardous environments without putting anyone at risk. The Nottingham research, funded by the Institution of Occupational Safety and Health (IOSH), found contrasts between the groups in the way participants reacted to the scenario. Those in the multi-sensory group had a greater sense of urgency, reflecting a real-life scenario, and were more likely to avoid the virtual fires. Evidence from the audio-visual participants suggested that they were treating the experience more like a game and behaviours were less consistent with those expected in a real world situation.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Dr Glyn Lawson, Associate Professor in the Faculty of Engineering, University of Nottingham, said: ""Health and safety training can fail to motivate and engage employees and can lack relevance to real-life contexts. Our research, which has been funded by the Institution of Occupational Safety and Health, suggests that virtual environments can help address these issues, by increasing trainees' engagement and willingness to participate in further training. There are also business benefits associated with the use of virtual environment training, such as the ability to deliver training at or near the workplace and at a time that is convenient to the employee."" Virtual Reality vs. PowerPoint A further test was done, as part of the study, to measure the effectiveness of VR training versus traditional PowerPoint training. Participants took questionnaires, testing their knowledge on either fire safety or safe vehicle disassembly procedure, before and after training as well as one week later. While those trained via PowerPoint appeared to have gained more knowledge when tested directly after training, there was a significantly larger decrease in knowledge scores when participants were retested one week later. In comparison, the VR group's long term retention was better and reported higher levels of engagement; attitude to occupational safety and health; and willingness to undertake training in the future. The research suggests that the increased cognitive engagement of learning in the virtual environment creates more established and comprehensive mental models which can improve recall, and implies that testing an employee's knowledge immediately following health and safety training may not be an effective means of gaging long-term knowledge of health and safety.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Applications to the work place Mary Ogungbeje, Research Manager at IOSH, said: ""The wheels are turning so that virtual and smart learning is increasingly engrained in the workplace and everyday life. ""Technology is continuously advancing and in many cases becoming more affordable, so this study gives us a taste of what's to come. By improving training strategies with the use of technology and stimulated sensory experiences, we are heading in a direction where the workforce will not just enjoy a more immersive and interesting training course but participate in an effective learning experience, so they are better prepared and equipped to stay safe, healthy and well at work."" The researchers conducted meetings, discussions, and visits with partners including Rolls-Royce, for expert advice around fire safety and safe handling of hazardous chemicals. The University of Nottingham's Health and Safety advisors also contributed to help the researchers better understand how the training may be implemented in industry. The study aims to produce evidence-based guidance for the development and use of virtual environments in engaging and effective training using cost-effective and accessible solutions. The full study features in a report, titled 'Immersive virtual worlds: Multisensory virtual environments for health and safety training', to be released at the IOSH's annual conference on Tuesday 17 September. Further information: https://www.iosh.com/multisensoryVE ",Society,0.07187049620649516
120,MIT News,Robots help patients manage chronic illness at home,Robotics,2019-10-10,-,http://news.mit.edu/2019/catalia-health-mabu-robot-1011,"  The Mabu robot, with its small yellow body and friendly expression, serves, literally, as the face of the care management startup Catalia Health. The most innovative part of the company’s solution, however, lies behind Mabu’s large blue eyes. Catalia Health’s software incorporates expertise in psychology, artificial intelligence, and medical treatment plans to help patients manage their chronic conditions. The result is a sophisticated robot companion that uses daily conversations to give patients tips, medication reminders, and information on their condition while relaying relevant data to care providers. The information exchange can also take place on patients’ mobile phones. “Ultimately, what we’re building are care management programs to help patients in particular disease states,” says Catalia Health founder and CEO Cory Kidd SM ’03, PhD ’08. “A lot of that is getting information back to the people providing care. We’re helping them scale up their efforts to interact with every patient more frequently.” Heart failure patients first brought Mabu into their homes about a year and a half ago as part of a partnership with the health care provider Kaiser Permanente, who pays for the service. Since then, Catalia Health has also partnered with health care systems and pharmaceutical companies to help patients dealing with conditions including rheumatoid arthritis and kidney cancer. Treatment plans for chronic diseases can be challenging for patients to manage consistently, and many people don’t follow them as prescribed. Kidd says Mabu’s daily conversations help not only patients, but also human care givers as they make treatment decisions using data collected by their robot counterpart. Robotics for change Kidd was a student and faculty member at Georgia Tech before coming to MIT for his master’s degree in 2001. His work focused on addressing problems in health care caused by an aging population and an increase in the number of people managing chronic diseases. “The way we deliver health care doesn’t scale to the needs we have, so I was looking for technologies that might help with that,” Kidd says. Many studies have found that communicating with someone in person, as opposed to over the phone or online, makes that person appear more trustworthy, engaging, and likeable. At MIT, Kidd conducted studies aimed at understanding if those findings translated to robots. “What I found was when we used an interactive robot that you could look in the eye and share the same physical space with, you got the same psychological effects as face-to-face interaction,” Kidd says. As part of his PhD in the Media Lab’s Media Arts and Sciences program, Kidd tested that finding in a randomized, controlled trial with patients in a diabetes and weight management program at the Boston University Medical Center. A portion of the patients were given a robotic weight-loss coach to take home, while another group used a computer running the same software. The tabletop robot conducted regular check ups and offered tips on maintaining a healthy diet and lifestyle. Patients who received the robot were much more likely to stick with the weight loss program. Upon finishing his PhD in 2007, Kidd immediately sought to apply his research by starting the company Intuitive Automata to help people manage their diabetes using robot coaches. Even as he pursued the idea, though, Kidd says he knew it was too early to be introducing such sophisticated technology to a health care industry that, at the time, was still adjusting to electronic health records. Intuitive Automata ultimately wasn’t a major commercial success, but it did help Kidd understand the health care sector at a much deeper level as he worked to sell the diabetes and weight management programs to providers, pharmaceutical companies, insurers, and patients. “I was able to build a big network across the industry and understand how these people think about challenges in health care,” Kidd says. “It let me see how different entities think about how they fit in the health care ecosystem.” Since then, Kidd has watched the costs associated with robotics and computing plummet. Many people have also enthusiastically adopted computer assistance like Amazon’s Alexa and Apple’s Siri. Finally, Kidd says members of the health care industry have developed an appreciation for technology’s potential to complement traditional methods of care. “The common ways [care is delivered] on the provider side is by bringing patients to the doctor’s office or hospital,” Kidd explains. “Then on the pharma side, it’s call center-based. In the middle of these is the home visitation model. They’re all very human powered. If you want to help twice as many patients, you hire twice as many people. There’s no way around that.” In the summer of 2014, he founded Catalia Health to help patients with chronic conditions at scale. “It’s very exciting because I’ve seen how well this can work with patients,” Kidd says of the company’s potential. “The biggest challenge with the early studies was that, in the end, the patients didn’t want to give the robots back. From my perspective, that’s one of the things that shows this really does work.” Mabu makes friends Catalia Health uses artificial intelligence to help Mabu learn about each patient through daily conversations, which vary in length depending on the patient’s answers. “A lot of conversations start off with ‘How are you feeling?’ similar to what a doctor or nurse might ask,” Kidd explains. “From there, it might go off in many directions. There are a few things doctors or nurses would ask if they could talk to these patients every day.” For example, Mabu would ask heart failure patients how they are feeling, if they have shortness of breath, and about their weight. “Based on patients’ answers, Mabu might say ‘You might want to call your doctor,’ or ‘I’ll send them this information,’ or ‘Let’s check in tomorrow,’” Kidd says. Last year, Catalia Health announced a collaboration with the American Heart Association that has allowed Mabu to deliver the association’s guidelines for patients living with heart failure. “A patient might say ‘I’m feeling terrible today’ and Mabu might ask ‘Is it one of these symptoms a lot of people with your condition deal with?’ We’re trying to get down to whether it’s the disease or the drug. When that happens, we do two things: Mabu has a lot of information about problems a patient might be dealing with, so she’s able to give quick feedback. Simultaneously, she’s sending that information to a clinician — a doctor, nurse, or pharmacists — whoever’s providing care.” In addition to health care providers, Catalia also partners with pharmaceutical companies. In each case, patients pay nothing out of pocket for their robot companions. Although the data Catalia Health sends pharmaceutical companies is completely anonymized, it can help them follow their treatment’s effects on patients in real time and better understand the patient experience. Details about many of Catalia Health’s partnerships have not been disclosed, but the company did announce a collaboration with Pfizer last month to test the impact of Mabu on patient treatment plans. Over the next year, Kidd hopes to add to the company’s list of partnerships and help patients dealing a wider swath of diseases. Regardless of how fast Catalia Health scales, he says the service it provides will not diminish as Mabu brings its trademark attentiveness and growing knowledge base to every conversation. “In a clinical setting, if we talk about a doctor with good bedside manner, we don’t mean that he or she has more clinical knowledge than the next person, we simply mean they’re better at connecting with patients,” Kidd says. “I’ve looked at the psychology behind that — what does it mean to be able to do that? — and turned that into the algorithms we use to help create conversations with patients.” ",Health,0.07160438484362888
121,MIT News,Artificial gut aims to expose the elusive microbiome,Research,2019-10-03,-,http://news.mit.edu/2019/artificial-gut-aims-to-expose-human-microbiome-1003,"  The microbiome is a collection of trillions of bacteria that reside in and on our bodies. Each person’s microbiome is unique — just like a fingerprint — and researchers are finding more and more ways in which it impacts our health and daily lives. One example involves an apparent link between the brain and the bacteria in the gut. This brain-gut “axis” is believed to influence conditions such as Parkinson’s disease, depression, and irritable bowel syndrome. However, many studies into the brain-gut axis have stalled because of one central problem: the lack of an adequate testable model of the gut. Current testing platforms cannot emulate the human gut accurately and cheaply enough for large-scale studies. The research community needs something new, which is what a team at MIT Lincoln Laboratory is tackling in a project funded through the Technology Office. Researchers there aim to create the perfect artificial gut. “The question from the mechanical side is, how do you emulate the colon?” says Todd Thorsen, the project’s principal investigator from the Biological and Chemical Technologies Group. “Bacteria in the colon occupy lots of ecological niches.” Thorsen is referring to the complexity of the human gut, which includes a community of 100 trillion microbes that all have specific, and sometimes clashing, needs. For example, certain types of bacteria in the gut will die in the presence of oxygen, while others need it to survive. The gut also contains both hard and soft mucus that allows different types of bacteria to grow. All of these conditions need to be mimicked in a single platform in order to properly maintain and test microbiome samples — and that’s not an easy task. “Until now, no one has been able to culture a microbiome sample and maintain it,” says David Walsh from the Biological and Chemical Technologies Group, who led the device’s development and fabrication. “If we can maintain a culture, we can do things like add toxins and therapeutics to see how they change the culture over time.” To address the problem, the laboratory team developed a multimaterial platform made of permeable silicon rubber and other plastics, such as polystyrene, all of which are cheap and can be rapidly prototyped. The two components of the platform emulate the essential oxygen and mucosal gradients. The above photo (left) shows the component that controls the oxygen gradient. Air diffuses through the plastic while the blue ports allow researchers to change the local oxygen concentrations at different positions within the adjacent microculture chambers. The right photo shows the component that controls mucus, which is welled up into the device from below. Both components implement careful geometry to yield the precise conditions found in the gut. “The final system will allow us to tackle real-world problems,” Walsh says. Those problems, in addition to unraveling the brain-gut axis, include developing resilience to current and emerging pathogens, combating biological warfare, and more. This year, the research team is partnering with the University of Alabama at Birmingham, Northeastern University, and the University of California at San Francisco to implement their first tests of microbiome samples to study links to Parkinson’s disease. The laboratory’s role is to use the artificial gut to culture microbiome samples taken from people with and without Parkinson’s disease and test what happens when different suspected adverse influencers are added. The goal is to correlate how changes in the microbiome caused by exposure to certain toxins may induce Parkinson’s-like nerve damage. The laboratory will also continue advancing other aspects of the project. Some examples include building a tubular core-shell origami-like gut that rolls up during assembly to emulate the colon and the surrounding vascularized tissue, and developing modeling software to predict how microbial communities might change over time. ",Health,0.07139804645155134
122,Stanford,Science lessons through a different lens,Science,2019-10-15,-,https://news.stanford.edu/2019/10/14/science-lessons-different-lens/,"   For years, one of the highest-rated comedy series on television was The Big Bang Theory, a show whose central characters portray “old, tired images of the science community, sending a resounding message about who belongs in science,” says Bryan A. Brown, an associate professor of science education at Stanford Graduate School of Education. “These stereotypes have been reinforced for generations. We can’t ignore the barriers these expectations impose.”  A new book examines obstacles young people of color face in the science classroom, and how teachers can better connect their lessons to students’ cultural identities. (Image credit: Getty Images)  Brown’s research examines why cultural stereotypes and language about science matter, especially for students in multilingual and multicultural communities who might not fit the images they see or relate to the words they hear in popular culture and in the classroom. Brown, a former high school science teacher, has studied science education in urban communities for more than two decades, exploring the relationship between student identity, classroom culture and academic achievement. For the past eight summers, Brown has brought fifth- and sixth-graders from inner-city schools to the Stanford campus for a weeklong science camp, where the students get excited about biology, physics, chemistry and engineering – and learn from teachers of color, who provide critical role models in the field. In his new book, Science in the City, Brown looks at the role that language and culture play in teaching science and technology. We spoke with him about some of the obstacles young people of color face in the science classroom, and how science teachers can better connect their lessons to students’ cultural identities.   You write that students of color have to pay a “black tax” when they enter a science classroom. What do you mean by that? It’s the idea that people of color are judged harshly and have to pay an additional price to belong. When students arrive in science classrooms, they bear the weight of the history, cultural expectations and stereotypes about who can become a scientist.  Bryan A. Brown (Image credit: Courtesy Stanford Graduate School of Education)  Black people have often been told by their elders that in order to succeed, they have to be better than their white counterparts. School culture imposes even more of a tax: Students have to prove their academic competency, negotiate awkward discussions and know what language practices are deemed acceptable. They know that if they don’t speak the way the teacher speaks, they might not be seen as intelligent. There’s an iconic photo from 1962 of James Meredith, the first African American student admitted to the University of Mississippi. He’s walking into the school for the first time, surrounded by U.S. marshals and angry community members who clearly don’t believe he deserves to be there. When I think about the challenges he had to face in that learning environment, there’s no way they were equal to the challenges his classmates faced.   How are students of color at a disadvantage in the science classroom in particular? For one thing, we know that scientific concepts are learned and solidified when students have an opportunity to explain them. The more you talk about something, the more you understand it. But in many schools, especially urban ones, teachers are the ones doing the explaining. If teachers want students to understand a scientific idea thoroughly, they need to give them opportunities to explain it. But academic language is not culturally neutral. Teachers need to broaden their idea of the language of science as something that’s rooted in the realities of the lives of people of color.   Can you give an example? Well, for instance, if you’re teaching the process of osmosis, you might have a kid who knows a lot about cooking – maybe he learned how to marinate carne asada from his grandmother. Marinating food is an example of osmosis. If you soak meat in a mixture of seasonings and water – a solution – then how does the meat end up with seasoning on the inside? But teachers don’t necessarily view that as a scientific act. Talking about something students already know through their culture can give them a vision of the science we experience every day. It’s the same science they’re learning about in the classroom. There is no cultural distance between students of color and a successful science education.   How can teachers make science lessons more culturally meaningful for students while still satisfying national guidelines? My research team has actually been working for some time to help teachers with this. The Next Generation Science Standards introduced an ambitious and innovative new curriculum, but the curriculum doesn’t really emphasize culture – it doesn’t push the idea that science can be culturally relevant for kids in the context of their community. We just launched a resource at scienceinthecity.stanford.edu especially for teachers in urban schools that offers lesson plans that are culturally relevant but also match NGSS expectations. It’s also a place where teachers can connect to share their lessons, get input from other teachers, find a mentor or even become a mentor for someone else. We hope teachers will get involved to make the community even stronger. ",Society,0.07136014324719253
123,Science Daily,Anticipating Performance Can Hinder Memory,Society,2019-10-03,-,https://www.sciencedaily.com/releases/2019/10/191003105427.htm,"   The study's findings also suggest that the presence of an audience may be an important factor that contributes to this pre-performance memory deficit. ""Performance anticipation could weaken memory because people tend to focus on the details of their upcoming presentation instead of paying attention to information that occurs before their performance,"" says lead author Noah Forrin, a postdoctoral fellow in Psychology at Waterloo. ""People who experience performance anxiety may be particularly likely to experience this phenomenon."" Building on what previous research called the next-in-line effect, Forrin and his co-authors explored how different ways of preparing for a presentation impact the pre-performance memory deficit. They experimented with a variety of techniques that enhance memory, including the production effect, which is the simple yet powerful idea that we can remember something best if we say it aloud. One of the study's co-authors, Psychology professor Colin MacLeod, coined the term production effect from previous research which identified that reading aloud involves at least three distinct processes that help to encode memory: articulation, audition, and self-reference. Research by Forrin and MacLeod has demonstrated that reading aloud is better for memory than reading silently, writing, or hearing another person speak aloud. In the new study, however, the findings suggest that the production effect has a downside: When people anticipate reading out loud, they may have worse memory for information that they encounter before reading aloud. The researchers conducted four experiments with 400 undergraduate students and found that students have worse memory for words that they read silently when they anticipate having to read upcoming words aloud (compared to when they anticipate having to read upcoming words silently). ""Our results show that performance anticipation may be detrimental to effective memory encoding strategies,"" said Forrin. ""Students frequently have upcoming performances -- whether for class presentations or the expectation of class participation."" ""We are currently examining whether the anticipation of those future performances reduces students' learning and memory in the classroom."" One strategy to avoid pre-performance memory deficits, says Forrin, is ""try to get your performance over with by being the first student in class (or employee in a meeting) to present. After that, you can focus on others' presentations without anticipating your own."" ",Society,0.06688625009028937
124,Science Daily,"E-Cigarettes, Tobacco and Cannabis Products Are Littering High Schools",Society,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010135706.htm,"   The study of a dozen Bay Area high schools uncovered hundreds of waste items littering the parking lots and sidewalks in and around the schools. In addition to reflecting the widespread use of these products among teens, the researchers say these items are an environmental hazard due to the heavy metals, plastics, nicotine, lithium-ion batteries and other toxins these products can contain. The paper publishes Oct. 10, 2019 in Morbidity and Mortality Weekly Report (MMWR), a scientific journal of the federal Centers for Disease Control and Prevention. ""We're in the midst of an epidemic of teen e-cigarette use that is causing substantial toxic waste contamination at some high schools from teens discarding these products on the ground,"" said first author Jeremiah Mock, PhD, a health anthropologist and associate professor in UCSF's School of Nursing's Institute for Health and Aging. He also is a member of UCSF's Center for Tobacco Control Research and Education. ""But e-cigarettes are not the only problem,"" Mock said. ""Our research also shows that little cigars, cigarillos and menthol cigarettes are popular at schools with large proportions of lower-income Latinx and African American families. These toxic products are contaminating school environments and surrounding areas, going down storm drains and contaminating the bay."" The United States is experiencing an epidemic of vaping, especially by youth using flavored products. Often, the vaping takes place in the bathrooms, classrooms and parking lots at their schools.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     From 2017 to 2018, e-cigarette use among U.S. high schools increased by 78 percent -- from 12 percent of high school students to 21 percent -- and by 48 percent among middle school students, according to the CDC. Communities in the Bay Area have been at the forefront of this epidemic: for example, in Marin County between 2016 and 2018, e-cigarette use among 11th graders rose 155 percent, from 11 percent to 28 percent, reports the MMWR study. JUUL, the dominant e-cigarette maker, is headquartered in San Francisco. The Trump administration in September announced plans to ban flavored e-cigarette products, which account for a significant portion of the market. A recent analysis of the National Youth Tobacco Survey showed that among high school students who currently used e-cigarettes, the percentage who used flavored e-cigarettes increased from 65.1% in 2014 to 67.8% in 2018. San Francisco and other cities and towns throughout the Bay Area are banning retail sales of flavored e-cigarettes and other flavored tobacco products including, menthol cigarettes; various commercial retailers are also ending e-cigarette sales. San Francisco has banned retail and internet sales of all e-cigarettes. The new garbology study of environmental contamination was conducted at 12 public high schools in San Francisco, Contra Costa, Alameda and Marin counties. The schools, which are not identified by name, were stratified by the percentages of students from low-income families. Researchers and student interns collected waste items from the student parking lots and exterior perimeter areas on a single day at each school between July 2018 and April 2019.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""Our study novelly detected the under-addressed problem of flavored little cigars and cigarillos in low-income youth populations,"" said co-author Yogi Hale Hendlin, PhD, a public health scientist and environmental philosopher with appointments at Erasmus University Rotterdam and UCSF's Environmental Health Initiative. ""Youth e-cigarette use -- as epidemic as it is -- seems to be lopsided towards higher-income student populations, with combustible tobacco product waste found in higher concentrations in lower-income schools,"" he said. Altogether, nearly 900 waste items were collected. The researchers found: Approximately 19 percent of the product waste was from e-cigarettes -- nearly all of these were JUUL or JUUL-compatible pods and pod caps. Far more JUUL products were found at middle- and upper-income area schools. 73 out of 74 pod caps were from flavored pods other than tobacco flavors, including ""Cool Mint,"" ""Cool Cucumber,"" and ""Classic Menthol."" At four high schools where the student populations are predominantly lower-income African American and Latinx, 71 little cigar or cigarillo plastic packages and mouthpieces were found; no such items were found at upper-income area schools. Across all the schools, 620 cigarette butts were collected, with 65 percent of identifiable butts being from recently smoked cigarettes. ""Action is needed to reduce youth tobacco smoking and cannabis access and use, and to eliminate environmental contamination from these products,"" said Mock, who has started a crowdfunding site to engage parents and communities in addressing the problem. ""Schools can engage students in garbology projects to clean up their schools and to raise awareness about the health and environmental hazards of these products."" ",Society,0.06641026952832371
125,Science Daily,A Little Kindness Goes a Long Way for Worker Performance and Health,Society,2019-09-10,-,https://www.sciencedaily.com/releases/2019/09/190910154708.htm,"   ""An ultimate solution to improve worker performance and health could be big pay raises or reduced workloads, but when those solutions aren't feasible, we found that even small offerings can make a big difference,"" said Bu Zhong, associate professor of journalism at Penn State. According to Zhong, bus drivers are vulnerable to specific health problems due in large part to their stressful working environment, which often includes irregular shift schedules, unpredictable traffic conditions and random meal times. In addition, the sedentary nature of driving and continuous whole-body vibration contributes to fatigue, musculoskeletal problems such as lower-back pain, cardiovascular diseases and gastrointestinal issues. Zhong and his colleagues conducted an experiment with 86 Shenzen bus drivers. During the experiment, on-duty bus drivers were given, in addition to their typical box lunch which includes no fruit, a serving of fresh fruit -- either an apple or a banana -- for three weeks. The cost of the fruit was 73 cents per meal. The team distributed surveys to the bus drivers at three time intervals -- one week before the experiment began, once in the middle of the three-week-long experiment and one week following the end of the experiment. The findings appear today in the International Journal of Occupational Safety and Ergonomics. The researchers assessed depression with a personal health questionnaire that is recommended by the U.S. Centers for Disease Control and Prevention. The scale consisted of eight items, asking the participants to rate, for example, how often during the past two weeks they felt down, depressed or hopeless, and had trouble falling or staying asleep. ""Bus drivers reported significantly decreased depression levels one week after the experiments ended compared to one week before it began,"" said Zhong. The team measured self-efficacy -- perceived confidence and ability to implement the necessary actions and tasks so as to achieve specific goals -- using the 10-item General Self-Efficacy Scale. Items on this scale included, ""I can always manage to solve difficult problems if I try hard enough"" and ""I can usually handle whatever comes my way."" ""We found that self-efficacy was significantly higher in the middle of the experiment week than in the week after the experiment ended,"" said Zhong. Zhong concluded that while eating an extra apple at lunchtime may seem trivial, its impact can be large. ""This research suggests that employees can be sensitive to any improvement at the workplace,"" he said. ""Before an ultimate solution is possible, some small steps can make a difference -- one apple at a time."" ",Society,0.06411982953569634
126,MIT News,Engineered viruses could fight drug resistance,Research,2019-10-03,-,http://news.mit.edu/2019/engineered-phage-viruses-drug-resistance-1003,"  In the battle against antibiotic resistance, many scientists have been trying to deploy naturally occurring viruses called bacteriophages that can infect and kill bacteria. Bacteriophages kill bacteria through different mechanisms than antibiotics, and they can target specific strains, making them an appealing option for potentially overcoming multidrug resistance. However, quickly finding and optimizing well-defined bacteriophages to use against a bacterial target is challenging. In a new study, MIT biological engineers showed that they could rapidly program bacteriophages to kill different strains of E. coli by making mutations in a viral protein that binds to host cells. These engineered bacteriophages are also less likely to provoke resistance in bacteria, the researchers found. “As we’re seeing in the news more and more now, bacterial resistance is continuing to evolve and is increasingly problematic for public health,” says Timothy Lu, an MIT associate professor of electrical engineering and computer science and of biological engineering. “Phages represent a very different way of killing bacteria than antibiotics, which is complementary to antibiotics, rather than trying to replace them.” The researchers created several engineered phages that could kill E. coli grown in the lab. One of the newly created phages was also able to eliminate two E. coli strains that are resistant to naturally occurring phages from a skin infection in mice. Lu is the senior author of the study, which appears in the Oct. 3 issue of Cell. MIT postdoc Kevin Yehl and former postdoc Sebastien Lemire are the lead authors of the paper. Engineered viruses The Food and Drug Administration has approved a handful of bacteriophages for killing harmful bacteria in food, but they have not been widely used to treat infections because finding naturally occurring phages that target the right kind of bacteria can be a difficult and time-consuming process. To make such treatments easier to develop, Lu’s lab has been working on engineered viral “scaffolds” that can be easily repurposed to target different bacterial strains or different resistance mechanisms. “We think phages are a good toolkit for killing and knocking down bacteria levels inside a complex ecosystem, but in a targeted way,” Lu says. In 2015, the researchers used a phage from the T7 family, which naturally kills E.coli, and showed that they could program it to target other bacteria by swapping in different genes that code for tail fibers, the protein that bacteriophages use to latch onto receptors on the surfaces of host cells. While that approach did work, the researchers wanted to find a way to speed up the process of tailoring phages to a particular type of bacteria. In their new study, they came up with a strategy that allows them to rapidly create and test a much greater number of tail fiber variants. From previous studies of tail fiber structure, the researchers knew that the protein consists of segments called beta sheets that are connected by loops. They decided to try systematically mutating only the amino acids that form the loops, while preserving the beta sheet structure. “We identified regions that we thought would have minimal effect on the protein structure, but would be able to change its binding interaction with the bacteria,” Yehl says. They created phages with about 10,000,000 different tail fibers and tested them against several strains of E. coli that had evolved to be resistant to the nonengineered bacteriophage. One way that E. coli can become resistant to bacteriophages is by mutating “LPS” receptors so that they are shortened or missing, but the MIT team found that some of their engineered phages could kill even strains of E. coli with mutated or missing LPS receptors. This helps to overcome one of the limiting factors in using phages as antimicrobials, which is that bacteria can generate resistance by mutating receptors that the phages use to enter bacteria, says Rotem Sorek, a professor of molecular genetics at the Weizmann Institute of Science. “Through deep understanding of the biology entailing the phage-bacteria recognition, together with smart bioengineering approaches, Lu and his team managed to design a large library of phage variants, each of which has the potential to recognize a slightly different receptor. They show that treating bacteria with this library rather than with a single phage limits the emergence of resistance,” says Sorek, who was not involved in the study. Other targets Lu and Yehl now plan to apply this approach to targeting other resistance mechanisms used by E. coli, and they also hope to develop phages that can kill other types of harmful bacteria. “This is just the beginning, as there are many other viral scaffolds and bacteria to target,” Yehl says. The researchers are also interested in using bacteriophages as a tool to target specific strains of bacteria that live in the human gut and cause health problems. “Being able to selectively hit those nonbeneficial strains could give us a lot of benefits in terms of human clinical outcomes,” Lu says. The research was funded by the Defense Threat Reduction Agency, the National Institutes of Health, the U.S. Army Research Laboratory/Army Research Office through the MIT Institute for Soldier Nanotechnologies, and the Koch Institute Support (core) Grant from the National Cancer Institute. ",Health,0.06389301519650617
127,Stanford,Young children have intuitions of great teachers,Science,2019-10-15,-,https://news.stanford.edu/2019/10/14/young-children-intuitions-great-teachers/,"   Human are incredible learners, in part because they are also accomplished teachers. Even at a very early age, people are adept at instructing others. But while there has been a lot of research into how people teach, there has been much less research on how they decide what to teach in the first place – a critical piece of the educational puzzle.  From an early age, children can make decisions about what kinds of information to teach. (Image credit: Getty Images)  Now, new research from Stanford scientists reveals that even young children consider what their students will find most useful or rewarding when deciding what to teach. A team led by Hyowon Gweon, assistant professor of psychology, showed that 5- to 7-year-olds decide to teach things that will not only be rewarding but also challenging for their students to learn on their own, maximizing what the student gets out of the interaction. “People have to be choosy about what they teach, because it is impossible to teach everything; our results suggest that even young children are able to reason about the expected reward and the cost of learning from the learner’s perspective to determine what is best to teach,” Gweon said. The study, published Oct. 14 in Nature Human Behavior, shows that even young children know what is useful to the learner. To find out how children think about what to teach, the researchers had children explore two toys on their own before deciding which toy to teach someone else to use. The toys differed in how interesting they were to play, how hard they were to learn, or both. Prior to the experiment, Gweon’s team had worked out that toys consisting of an orb that emitted different light colors were generally more interesting to kids than toys that played music. They also knew that toys became harder to learn depending on the number of buttons and the combination involved in making the toy work. Using this information, the team developed a computational model that predicts what children might choose if they understood how to maximize the learner’s benefit. After having children explore the pair of toys, the experimenter told children that a friend would need help learning to play with the toys later. The experimenter then asked children which toy they wanted to teach someone to use. Across six different conditions, the researchers found that children’s decisions about which toy to teach minimized the difficulty of learning while maximizing the fun of the toy, consistent with the computational model. “Children prioritized to teach both the harder toy and the cooler toy,” said doctoral student Sophie Bridgers, lead author of the study. “This shows that children not only think about what is fun for others to learn, but also what is challenging.” Julian Jara-Ettinger, assistant professor of psychology at Yale University, was also a co-author on the study. Two of the older participants actually chose the opposite of what the researchers found more generally; they wanted the learner to explore the harder toy, rather than teaching the learner how to use it. When the experimenters asked them why they made this decision, the children said they wanted to give the learner the chance to figure out a challenging problem. In other words, they knew that discovering something costly could be rewarding, “which is an intuition that great teachers have, but exactly when we perceive the cost of learning as a negative or a positive is something we cannot fully explain yet,” Gweon said. The development of such intuitions very early on might explain why humans have always been incredible learners, able to adapt to their environment. “The content of what is helpful to teach others has changed over time, but the key factors that determine what is helpful are the same,” Gweon said. “If I can only teach you one thing, I want it to be something useful; that is, something that brings you reward and saves you from trouble.”  Gweon is also a member of the Wu Tsai Neurosciences Institute. The work was funded by the John Templeton Foundation, a James S. McDonnell Scholar Award and the National Science Foundation. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest.  ",Society,0.0606014494140495
128,Science Daily,Good at Math? It Means Little If You're Not Confident,Society,2019-09-09,-,https://www.sciencedaily.com/releases/2019/09/190909154211.htm,"   In two studies, researchers found that the key to success in personal finances and dealing with a complex disease was a match between a person's math abilities and how comfortable and assured he or she felt using those skills. A lack of numeric confidence can essentially wipe out most of the advantage a person with good math skills may have, said Ellen Peters, who completed the work as a professor of psychology at The Ohio State University. And the effects were not small. Take, for example, people who scored 100 percent on a math test used in this research, published today (Sept. 9, 2019) in the journal PNAS. Having high confidence in their math ability compared to low confidence was the equivalent of having $94,000 more in annual income. ""If you have low numeric confidence, all the math skills in the world appear not to help much,"" said Peters, who is now at the University of Oregon.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""We think that those who lack confidence don't persist with the numbers when the going gets tough or tedious. As a result, their skills aren't used."" In both studies, participants took a test that measured their objective math skills. They also completed a questionnaire that measured how confident and self-assured they felt using numbers. Those who scored high in numeric confidence reported feeling comfortable in their abilities with numbers and believed they were good with fractions and probabilities. These people are likely to enjoy doing number-related tasks more, Peters said. Most importantly, they are more likely to persist when a math-related task is tedious or difficult, she said. In the first study, the researchers investigated self-reported financial outcomes among 4,572 Americans who participated in the Understanding America Study, run by the University of Southern California.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Participants reported various financial outcomes, such as credit card debt, investments and whether they had payday loans. Results showed that the interaction between participants' objective math scores and their numeric confidence predicted how well they were doing financially -- as the above example from the study revealed. The second study involved 91 patients at Ohio State's Wexner Medical Center who were being treated for lupus. Lupus has no cure, but medical interventions and lifestyle changes can help control it. However, it takes good math skills to navigate the disease, such as understanding the risks and benefits of drugs, using correct drug doses and making good health insurance and provider choices, Peters said. But just as importantly, because it is a chronic disease, patients must persist using these math skills over a lifetime in order to adhere to multiple timed medications, navigate frequent treatment changes and adopt healthy behaviors. Results showed that the interaction between patients' objective math skills and their numeric confidence was related to how their physicians rated disease activity, such as new rashes or seizures. Those who scored high in skill and confidence showed less disease activity than those who had the skills but not the confidence. But the worst outcomes came to those who thought they were great at math, but actually were not. Among those who had high confidence, patients who were good at math had only a 7 percent chance of having bad disease activity -- compared to 44 percent who had low math skills. ""If you have low ability and high confidence, you may end up making mistakes that you don't recognize. You're not asking for help because you think you don't need it, so you end up in worse shape,"" Peters said. So how many people are mismatched between their abilities and skills? In these two studies, 18 to 20 percent had good math skills and low confidence. Another 12 to 13 percent had bad math skills combined with high confidence. ""Almost a third of our population is mismatched in ways that could harm them,"" Peters said. The results suggest that efforts to improve people's math skills or their numeric confidence alone may not be helpful. ""More of one is not necessarily better if you don't increase both,"" she said. The best advice for everyone should be to understand their skills, Peters said. For some, that may mean to ""be open to the possibility that you're good at math,"" she said. For others, it may mean asking for and accepting help as needed. ",Society,0.060295795249411595
129,MIT News,Detecting patients’ pain levels via their brain signals,Computer Science,2019-09-12,-,http://news.mit.edu/2019/detecting-pain-levels-brain-signals-0912,"  Researchers from MIT and elsewhere have developed a system that measures a patient’s pain level by analyzing brain activity from a portable neuroimaging device. The system could help doctors diagnose and treat pain in unconscious and noncommunicative patients, which could reduce the risk of chronic pain that can occur after surgery. Pain management is a surprisingly challenging, complex balancing act. Overtreating pain, for example, runs the risk of addicting patients to pain medication. Undertreating pain, on the other hand, may lead to long-term chronic pain and other complications. Today, doctors generally gauge pain levels according to their patients’ own reports of how they’re feeling. But what about patients who can’t communicate how they’re feeling effectively — or at all — such as children, elderly patients with dementia, or those undergoing surgery? In a paper presented at the International Conference on Affective Computing and Intelligent Interaction, the researchers describe a method to quantify pain in patients. To do so, they leverage an emerging neuroimaging technique called functional near infrared spectroscopy (fNIRS), in which sensors placed around the head measure oxygenated hemoglobin concentrations that indicate neuron activity. For their work, the researchers use only a few fNIRS sensors on a patient’s forehead to measure activity in the prefrontal cortex, which plays a major role in pain processing. Using the measured brain signals, the researchers developed personalized machine-learning models to detect patterns of oxygenated hemoglobin levels associated with pain responses. When the sensors are in place, the models can detect whether a patient is experiencing pain with around 87 percent accuracy. “The way we measure pain hasn’t changed over the years,” says Daniel Lopez-Martinez, a PhD student in the Harvard-MIT Program in Health Sciences and Technology and a researcher at the MIT Media Lab. “If we don’t have metrics for how much pain someone experiences, treating pain and running clinical trials becomes challenging. The motivation is to quantify pain in an objective manner that doesn’t require the cooperation of the patient, such as when a patient is unconscious during surgery.” Traditionally, surgery patients receive anesthesia and medication based on their age, weight, previous diseases, and other factors. If they don’t move and their heart rate remains stable, they’re considered fine. But the brain may still be processing pain signals while they’re unconscious, which can lead to increased postoperative pain and long-term chronic pain. The researchers’ system could provide surgeons with real-time information about an unconscious patient’s pain levels, so they can adjust anesthesia and medication dosages accordingly to stop those pain signals. Joining Lopez-Martinez on the paper are: Ke Peng of Harvard Medical School, Boston Children’s Hospital, and the CHUM Research Centre in Montreal; Arielle Lee and David Borsook, both of Harvard Medical School, Boston Children’s Hospital, and Massachusetts General Hospital; and Rosalind Picard, a professor of media arts and sciences and director of affective computing research in the Media Lab. Focusing on the forehead In their work, the researchers adapted the fNIRS system and developed new machine-learning techniques to make the system more accurate and practical for clinical use. To use fNIRS, sensors are traditionally placed all around a patient’s head. Different wavelengths of near-infrared light shine through the skull and into the brain. Oxygenated and deoxygenated hemoglobin absorb the wavelengths differently, altering their signals slightly. When the infrared signals reflect back to the sensors, signal-processing techniques use the altered signals to calculate how much of each hemoglobin type is present in different regions of the brain. When a patient is hurt, regions of the brain associated with pain will see a sharp rise in oxygenated hemoglobin and decreases in deoxygenated hemoglobin, and these changes can be detected through fNIRS monitoring. But traditional fNIRS systems place sensors all around the patient’s head. This can take a long time to set up, and it can be difficult for patients who must lie down. It also isn’t really feasible for patients undergoing surgery. Therefore, the researchers adapted the fNIRS system to specifically measure signals only from the prefrontal cortex. While pain processing involves outputs of information from multiple regions of the brain, studies have shown the prefrontal cortex integrates all that information. This means they need to place sensors only over the forehead.  Another problem with traditional fNIRS systems is they capture some signals from the skull and skin that contribute to noise. To fix that, the researchers installed additional sensors  to capture and filter out those signals. Personalized pain modeling On the machine-learning side, the researchers trained and tested a model on a labeled pain-processing dataset they collected from 43 male participants. (Next they plan to collect a lot more data from diverse patient populations, including female patients — both during surgery and while conscious, and at a range of pain intensities — in order to better evaluate the accuracy of the system.) Each participant wore the researchers’ fNIRS device and was randomly exposed to an innocuous sensation and then about a dozen shocks to their thumb at two different pain intensities, measured on a scale of 1-10: a low level (about a 3/10) or high level (about 7/10). Those two intensities were determined with pretests: The participants self-reported the low level as being only strongly aware of the shock without pain, and the high level as the maximum pain they could tolerate.  In training, the model extracted dozens of features from the signals related to how much oxygenated and deoxygenated hemoglobin was present, as well as how quickly the oxygenated hemoglobin levels rose. Those two metrics — quantity and speed — give a clearer picture of a patient’s experience of pain at the different intensities. Importantly, the model also automatically generates “personalized” submodels that extract high-resolution features from individual patient subpopulations. Traditionally, in machine learning, one model learns classifications — “pain” or “no pain” — based on average responses of the entire patient population. But that generalized approach can reduce accuracy, especially with diverse patient populations. The researchers’ model instead trains on the entire population but simultaneously identifies shared characteristics among subpopulations within the larger dataset. For example, pain responses to the two intensities may differ between young and old patients, or depending on gender. This generates learned submodels that break off and learn, in parallel, patterns of their patient subpopulations. At the same time, however, they’re all still sharing information and learning patterns shared across the entire population. In short, they’re simultaneously leveraging fine-grained personalized information and population-level information to train better. The personalized models and a traditional model were evaluated in classifying pain or no-pain in a random hold-out set of participant brain signals from the dataset, where the self-reported pain scores were known for each participant. The personalized models outperformed the traditional model by about 20 percent, reaching about 87 percent accuracy. “Because we are able to detect pain with this high accuracy, using only a few sensors on the forehead, we have a solid basis for bringing this technology to a real-world clinical setting,” Lopez-Martinez says. ",Health,0.05912532440566481
130,Science Daily,New Approach to Slowing Nearsightedness in Children Shows Promise,Health,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092236.htm,"   There is a world-wide epidemic of myopia, also known as nearsightedness. Since 1971, the incidence of nearsightedness in the US nearly doubled, to 42 percent. In Asia, up to 90 percent of teenagers and adults are nearsighted. Nearsightedness may not seem like a serious eye condition. Glasses and contact lenses can provide effective treatment. But high myopia, defined as -6 D or more, can lead to potentially blinding complications, such as glaucoma, retinal detachment and retinal degeneration. Myopia can't be stopped, but it can be slowed. There are two methods for slowing progression. One method uses 0.01% atropine eye drops, instilled in the eye every day. Atropine is a medication commonly used to dilate or widen the eye before an exam. How it slows progression is unclear (some evidence suggests atropine blocks muscarinic receptors in the retina). But research shows it is effective and safe. Another method is orthokeratology, which involves using rigid gas permeable contact lenses every night to reshape the cornea, the clear, front part of the eye. It's also unclear how contact lenses slow progression, but it is thought that reshaping the cornea changes the peripheral focus of the eye to reduce myopia progression. There are risks with overnight contact lens wear, such as corneal abrasions, ulcers or infections, and scaring that can lead to vision loss. Myopia progression can rebound with both methods, though less so with 0.01% atropine. Two treatments, each effective, each appear to work in a different way. What if they were combined? Would the combination have an additive or synergistic effect? To learn more, Nozomi Kinoshita, M.D., Ph.D., and colleagues at Jichi Medical University in Japan, randomized 80 children into two groups: one received both orthokeratology and atropine, while the second group received only orthokeratology. The children, aged 8 to 12 years old, exhibited a range of myopia, from low to high (from -1D to -6 D). They were treated for three months and then followed for two years. In children with higher myopia, combination treatment was 28 percent more effective compared with contact lenses alone. In children with lower myopia, combination treatment was 38 percent more effective. ""At present, using atropine together with orthokeratology can become a better treatment option to slowing myopia progression,"" Dr. Kinoshita said. ""We believe this combination will be an optimal treatment option because together, both therapies complement the weakness of each other."" ",Health,0.05902167991876857
131,Science Daily,"Bad Behavior Between Moms Driven by Stereotypes, Judgment",Science,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009132326.htm,"   The results, published in the Journal of Family Communication, are concerning to Kelly Odenweller, lead author and assistant teaching professor of communication studies at Iowa State University. She says support networks are critical and negative experiences with other mothers may be detrimental to a mother's overall well-being. ""It's not unusual for moms to have low self-esteem or feel they're not living up to the standards of what it means to be a mom,"" Odenweller said. ""If other moms treat them poorly, even when they're trying to do a good job, they may feel they can't turn to other people in their community for support. It can be very isolating and all that self-doubt can lead to anxiety and depression, which can negatively affect the entire family."" The study builds on previous research, in which Odenweller identified seven different stereotypes of stay-at-home and working mothers (see below). She and co-authors at West Virginia and Chapman universities surveyed more than 500 mothers to learn more about their attitudes, emotions and even harmful behaviors toward mothers who fit one of the seven stereotypes. According to the results, ideal and lazy mothers drew the most contempt from both working and stay-at-home mothers. The overworked stay-at-home mom also was near the top of the list. Odenweller says survey participants expressed negative feelings and admitted they would treat a lazy or ideal mother poorly, by excluding her, arguing with or verbally attacking her. Not all of the responses were negative. All mothers felt pity for overworked working mothers and were more willing to offer them help. Working mothers did express admiration for ideal moms who appear to have it all together. Odenweller says this response only came from working mothers and she suspects they see these ideal moms as a champion for their cause.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Working moms juggle a lot and want more support for all mothers with careers. For them, it may be more of a social statement that women can be great at their careers and being moms,"" Odenweller said. Stereotypes may not be reality The positive and negative responses varied depending on how mothers categorized themselves and the stereotypes they applied to other mothers. Odenweller says this was one of the more interesting findings because the way a mother treated another was based on her own perception of the other mother. For example, a working mom may feel envy or contempt toward an ideal, stay-at-home mom, but that mom may see herself differently. ""In some cases,"" she said, ""these are mothers who embody what our culture believes is a good mom and yet among mothers, they are treating each other very negatively."" Odenweller says many of the stereotypes have developed from societal ideals applied to mothers. TV, movies and other types of media perpetuate these standards of what makes a good mom. This all adds to the pressures on mothers.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     What's a mom to do? While mothers cannot control how they are judged, they can control the impression they make on other mothers. Odenweller says one way to do that is to establish common ground and shared interests. When you first meet another mother, it may be tempting to boast about the things you do for your kids or share pictures, but Odenweller recommends avoiding that temptation until you've built a relationship. ""Mothers should think of other mothers as an ally, not someone to compare themselves to,"" she said. ""Try to avoid coming across like the best mom. Talk about things you have in common, things you both enjoy as mothers and do not feel like it's necessary to be better than her."" Christine Rittenour, Megan Dillow, Aaron Metzger and Scott Myers, West Virginia University; and Keith Weber, Chapman University; all contributed to the research. Stereotypes of mothers The following apply to both stay-at-home and working mothers, with the exception of lazy Overworked: Wants to do it all, but is overextended and it shows  Home, family-oriented: Prioritizes children, partner's needs and responsibilities at home  Ideal: Juggling several responsibilities, but gets it done and doesn't appear stressed  Hardworking, balanced: Not an ideal mom, but ambitious, dedicated  Non-traditional: Modern, liberal progressive -- makes choices that are good for herself and family, whether at home or work  Traditional: Embodies the roles expected of a woman, believes her main purpose is to raise children and maintain the household  Lazy: Not nurturing, attentive or hardworking -- applies only to stay-at-home moms ",Society,0.05783734780566063
132,Science Daily,Impostor Syndrome Is More Common Than You Think; Study Finds Best Way to Cope With It,Society,2019-09-24,-,https://www.sciencedaily.com/releases/2019/09/190924080016.htm,"   Findings of the study, co-authored by Brigham Young University professors Jeff Bednar, Bryan Stewart, and James Oldroyd, revealed that 20 percent of the college students in their sample suffered from very strong feelings of impostorism. The researchers conducted interviews with students in an elite academic program to understand the various coping mechanisms students used to escape these feelings, but one particular method stood out above the rest: seeking social support from those outside their academic program. The findings of their interview study suggest that if students ""reached in"" to other students within their major, they felt worse more often than they felt better. However, if the student ""reached out"" to family, friends outside their major, or even professors, perceptions of impostorism were reduced. ""Those outside the social group seem to be able to help students see the big picture and recalibrate their reference groups,"" said Bednar, a BYU management professor and co-author on the study. ""After reaching outside their social group for support, students are able to understand themselves more holistically rather than being so focused on what they felt they lacked in just one area."" Along with seeking social support, the study also uncovered negative ways students coped with impostorism. Some students tried to get their mind off schoolwork through escapes such as video games but ended up spending more time gaming than studying. Other students tried to hide how they really felt around their classmates, pretending they were confident and excited about their performance when deep down they questioned if they actually belonged. In a second study, the researchers surveyed 213 students to confirm what was revealed in their interview study about seeking social support: reaching out to individuals outside the major proved to be more effective than reaching in to individuals within the major. Surprisingly, the study also reveals that perceptions of impostorism lack a significant relationship with performance. This means that individuals who suffer with the impostor syndrome are still capable of doing their jobs well, they just don't believe in themselves. Researchers also explain that social-related factors impact impostorism more than an individual's actual ability or competence. ""The root of impostorism is thinking that people don't see you as you really are,"" said Stewart, an accounting professor at BYU and co-author on the study. ""We think people like us for something that isn't real and that they won't like us if they find out who we really are."" Outside the classroom, researchers believe that implications from this study can and should be applied in the workplace as well. ""It's important to create cultures where people talk about failure and mistakes,"" Bednar said. ""When we create those cultures, someone who is feeling strong feelings of impostorism will be more likely to get the help they need within the organization."" ",Society,0.05489152959429008
133,Science Daily,Teenagers Less Likely to Respond to Mothers With Controlling Tone of Voice,Society,2019-09-26,-,https://www.sciencedaily.com/releases/2019/09/190926202307.htm,"   Speaking to a son or daughter in a pressurising tone is also accompanied by a range of negative emotions and less feelings of closeness, a new study has discovered. The experimental study involving over 1000 adolescents aged 14-15 is the first to examine how subjects respond to the tone of voice when receiving instructions from their mothers, even when the specific words that are used are exactly the same. Lead author of the study Dr Netta Weinstein, from Cardiff University, said: ""If parents want conversations with their teens to have the most benefit, it's important to remember to use supportive tones of voice. It's easy for parents to forget, especially if they are feeling stressed, tired, or pressured themselves."" The study showed that subjects were much more likely to engage with instructions that conveyed a sense of encouragement and support for self-expression and choice. The results, whilst of obvious interest to parents, could also be of relevance to schoolteachers whose use of more motivational language could impact the learning and well-being of students in their classrooms.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Adolescents likely feel more cared about and happier, and as a result they try harder at school, when parents and teachers speak in supportive rather than pressuring tones of voice,"" Dr Weinstein continued. The new study, published today in the journal Developmental Psychology, involved 486 males and 514 females, aged 14-15. In the experiment each of the subjects was randomly assigned to groups that would hear identical messages delivered by mothers of adolescents in either a controlling, autonomy-supportive, or neutral tone of voice. Expressions of control impose pressure and attempt to coerce or push listeners to action. In contrast, those that express 'autonomy support' convey a sense of encouragement and support for listeners' sense of choice and opportunity for self-expression. Each of the mothers delivered 30 sentences that centred around school work, and included instructions such as: ""It's time now to go to school,"" ""you will read this book tonight,"" and ""you will do well on this assignment."" After the delivery of the messages, each student undertook a survey and answered questions about how they would feel if their own mother had spoken to them in that particular way.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The findings showed that the tone of voice used by mothers can impact significantly on teenagers' emotional, relational, and behavioural intention responses. Across most outcomes, adolescents who listened to mothers making motivational statements in a controlling tone of voice responded in undesirable ways. In contrast, autonomy-supportive tones elicited positive reactions from listeners as compared to listening to mothers who used a neutral tone of voice to deliver their motivational sentences. Co-author of the study Professor Silke Paulmann, of the University of Essex, added: ""These results nicely illustrate how powerful our voice is and that choosing the right tone to communicate is crucial in all of our conversations."" The researchers now intend to take their work a step further by investigating how tone of voice can impact physiological responses, such as heart rates or skin conductance responses, and how long lasting these effects may be. The study was funded by the Leverhulme Trust and involved researchers from Ghent University and the University of Essex. ",Society,0.05426357376830597
134,MIT News,Study reveals how mucus tames microbes,Research,2019-10-14,-,http://news.mit.edu/2019/how-mucus-tames-microbes-1014,"  More than 200 square meters of our bodies — including the digestive tract, lungs, and urinary tract — are lined with mucus. In recent years, scientists have found some evidence that mucus is not just a physical barrier that traps bacteria and viruses, but it can also disarm pathogens and prevent them from causing infections.  A new study from MIT reveals that glycans — branched sugar molecules found in mucus — are responsible for most of this microbe-taming. There are hundreds of different glycans in mucus, and the MIT team discovered that these molecules can prevent bacteria from communicating with each other and forming infectious biofilms, effectively rendering them harmless. “What we have in mucus is a therapeutic gold mine,” says Katharina Ribbeck, the Mark Hyman, Jr. Career Development Professor of Biological Engineering at MIT. “These glycans have biological functions that are very broad and sophisticated. They have the ability to regulate how microbes behave and really tune their identity.” In this study, which appears today in Nature Microbiology, the researchers focused on glycans’ interactions with Pseudomonas aeruginosa, an opportunistic pathogen that can cause infections in cystic fibrosis patients and people with compromised immune systems. Work now underway in Ribbeck’s lab has shown that glycans can regulate the behavior of other microbes as well. The lead author of the Nature Microbiology paper is MIT graduate student Kelsey Wheeler. Powerful defenders The average person produces several liters of mucus every day, and until recently this mucus was thought to function primarily as a lubricant and a physical barrier. However, Ribbeck and others have shown that mucus can actually interfere with bacterial behavior, preventing microbes from attaching to surfaces and communicating with one another. In the new study, Ribbeck wanted to test whether glycans were involved in mucus’ ability to control the behavior of microbes. These sugar molecules, a type of oligosaccharide, attach to proteins called mucins, the gel-forming building blocks of mucus, to form a bottlebrush-like structure. Mucus-associated glycans have been little studied, but Ribbeck thought they might play a major role in the microbe-disarming activity she had previously seen from mucus. To explore that possibility, she isolated glycans and exposed them to Pseudomonas aeruginosa. Upon exposure to mucin glycans, the bacteria underwent broad shifts in behavior that rendered them less harmful to the host. For example, they no longer produced toxins, attached to or killed host cells, or expressed genes essential for bacterial communication. This microbe-disarming activity had powerful consequences on the ability of this bacterium to establish infections. Ribbeck has shown that treatment of Pseudomonas-infected burn wounds with mucins and mucin glycans reduces bacterial proliferation, indicating the therapeutic potential of these virulence-neutralizing agents. “We’ve seen that intact mucins have regulatory effects and can cause behavioral switches in a whole range of pathogens, but now we can pinpoint the molecular mechanism and the entities that are responsible for this, which are the glycans,” Ribbeck says. In these experiments, the researchers used collections of hundreds of glycans, but they now plan to study the effects of individual glycans, which may interact specifically with different pathways or different microbes. “This is an important paper, as it shows that bacterial biofilm formation is inhibited by normal mucus, and especially its glycans. [Ribbeck] has now once more shown that normal mucus has beneficial effects on bacteria and that mucus is more complex than mostly appreciated,” says Gunnar Hansson, a professor of medical biochemistry at the University of Gothenburg, who was not involved in the study. Bacterial interactions Pseudomonas aeruginosa is just one of many opportunistic pathogens that healthy mucus keeps in check. Ribbeck is now studying the role of glycans in regulating other pathogens, including Streptococcus and the fungus Candida albicans, and she is also working on identifying receptors on microbe cell surfaces that interact with glycans. Her work on Streptococcus has shown that glycans can block horizontal gene transfer, a process that microbes often use to spread genes for drug resistance. Ribbeck and other researchers are now interested in using what they have learned about mucins and glycans to develop artificial mucus, which could offer a new way to treat diseases stemming from lost or defective mucus. Harnessing the powers of mucus could also lead to new ways to treat antibiotic-resistant infections, because it offers a complementary strategy to traditional antibiotics, Ribbeck says. “What we find here is that nature has evolved the ability to disarm difficult microbes, instead of killing them. This would not only help limit selective pressure for developing resistance, because they are not under pressure to find ways to survive, but it should also help create and maintain a diverse microbiome,” she says. Ribbeck suspects that glycans in mucus also play a key role in determining the composition of the microbiome — the trillions of bacterial cells that live inside the human body. Many of these microbes are beneficial to their human hosts, and glycans may be providing them with nutrients they need, or otherwise helping them to flourish, she says. In this way, mucus-associated glycans are similar to the many oligosaccharides found in human milk, which also contains a wide array of sugars that can regulate microbe behavior. “This is a theme that is likely at play in many systems where the goal is to shape and manipulate communities inside the body, not just in humans but throughout the animal kingdom,” Ribbeck says. The research was funded by the National Institute of Biomedical Imaging and Bioengineering, the National Institutes of Health, the National Science Foundation, the National Institute of Environmental Health Sciences, and the MIT Deshpande Center for Technological Innovation. ",Health,0.052595736732354674
135,Science Daily,Finding Upends Theory About the Cerebellum's Role in Reading and Dyslexia,Society,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009131759.htm,"   The cerebellum, a brain structure traditionally considered to be involved in motor function, has been implicated in the reading disability, developmental dyslexia, however, this ""cerebellar deficit hypothesis"" has always been controversial. The new research shows that the cerebellum is not engaged during reading in typical readers and does not differ in children who have dyslexia. That is the finding of a new study involving children with and without dyslexia published October 9, 2019, in the journal Human Brain Mapping. It is well established that dyslexia, a common learning disability, involves a weakness in understanding the mapping of sounds in spoken words to their written counterparts, a process that requires phonological awareness. It is also well known that this kind of processing relies on brain regions in the left cortex. However, it has been argued by some that the difficulties in phonological processing that lead to impaired reading originate in the cerebellum, a structure outside (and below the back) of the cortex. ""Prior imaging research on reading in dyslexia had not found much support for this theory called the cerebellar deficit hypothesis of dyslexia, but these studies tended to focus on the cortex,"" says the study's first author, Sikoya Ashburn, a Georgetown PhD candidate in neuroscience. ""Therefore, we tackled the question by specifically examining the cerebellum in more detail. We found no signs of cerebellar involvement during reading in skilled readers nor differences in children with reading disability."" The researchers used functional magnetic resonance imaging to look for brain activation during reading. They also tested for functional connections between the cerebellum and the cortex during reading.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Functional connectivity occurs when two brain regions behave similarly over time; they operate in sync,"" says Ashburn. ""However, brain regions in the cortex known to partake in the reading process were not communicating with the cerebellum in children with or without dyslexia while the brain was processing words."" The results revealed that when reading was not considered in the analysis -- that is when just examining the communications between brain regions at rest -- the cerebellum was communicating with the cortex more strongly in the children with dyslexia. ""These differences are consistent with the widely distributed neurobiological alterations that are associated with dyslexia, but not all of them are likely to be causal to the reading difficulties,"" Ashburn explains. ""The evidence for the cerebellar deficit theory was never particularly strong, yet people have jumped on the idea and even developed treatment approaches targeting the cerebellum,"" says senior author and neuroscientist Guinevere Eden, D. Phil, professor in the Department of Pediatrics at Georgetown University Medical Center and director for its Center for the Study of Learning. ""Standing on a wobble board -- one exercise promoted for improving dyslexia that isn't supported by the evidence -- is not going to improve a child's reading skills. Such treatments are a waste money and take away from other treatment approaches that entail structured intervention for reading difficulties, involving the learning of phonologic and orthographic processing."" In the long run, these researchers believe the findings can be used to refine models of dyslexia and to assist parents of struggling readers to make informed decisions about which treatment programs to pursue. More information about dyslexia can be found at the International Dyslexia Association or at Understood.org. This work was supported in part by grants from the Eunice Kennedy Shriver National Institute of Child Health and Human Development (P50 HD040095, R01 HD081078), and the National Center for Advancing Translational Sciences of the National Institutes of Health (TL1 TR001431). ",Health,0.052564282022295086
136,Science Daily,Children's Language Skills May Be Harmed by Social Hardship,Society,2019-10-08,-,https://www.sciencedaily.com/releases/2019/10/191008094254.htm,"   Researchers say the findings highlight the need for policies to address the social factors that can hamper speech, language and communication (SLC) development. Failing to do so means children might not fully develop the language skills that are critical for emotional development, wellbeing and educational and employment opportunities. A team from the University of Edinburgh and NHS Lothian looked at more than 26,000 records of children who had a routine health review between 27 and 30 months between April 2013 and April 2016. It showed that pre-school children living in the most economically deprived neighbourhoods were three times more likely to have SLC concern than those brought up in better-off areas. It is believed growing up in neighbourhoods with low income and unemployment -- which experience problems with education, health, access to services, crime and housing -- can increase the risk of setbacks. Researchers also discovered that each week a child spent in the womb from 23 to 36 weeks was associated with an 8.8 per cent decrease in the likelihood of them having an SLC concern reported at 27 months. The study used birth data from children born in the Lothians but experts say similar results might be expected across the UK. Professor of Neonatal Medicine at the University of Edinburgh's MRC Centre for Reproductive Health, James Boardman, said: ""Growing up in a disadvantaged neighbourhood where there is poverty and reduced access to services is closely associated with problems with pre-school language development. These results suggest that policies designed to lessen deprivation could reduce language and communication difficulties among pre-school children."" ",Society,0.05242854254457045
137,Science Daily,Screening Kindergarten Readiness,Society,2019-10-08,-,https://www.sciencedaily.com/releases/2019/10/191008155708.htm,"   Now, University of Missouri College of Education researchers have found that a readiness test can predict kindergarteners' success in school after 18 months. Melissa Stormont, a professor of special education, says identifying students early in the academic year who may need additional support can allow teachers and parents more time to build essential academic and social behavioral skills. ""Kindergarteners come to school from varying backgrounds and have different abilities,"" Stormont said. ""This is a critical time to assess student academic and social readiness, so that teachers can provide support as early as possible before issues worsen and become harder to change. This screening tool is a simple first step that can help children in the long run."" The researchers distributed the screening tool to 19 teachers in six elementary schools. Early in the school year, those teachers used the screener to rate 350 students. The MU researchers then compared the students' scores from the screener to their performances on a math and reading achievement test and to teacher ratings of their social and emotional skills 18 months later. Children who rated poor in academic readiness were nine to 10 times more likely to have low reading scores at the end of first grade. In addition, children who rated poor in behavior readiness were six times more likely to be rated as having displayed disruptive behavior and poor social skills by their first-grade teachers. ""Using this tool could help teachers in developing lessons and interventions to help their students who are having difficulties,"" Stormont said. ""This study highlights the need to support children more when they transition to kindergarten and these positive results definitely merit further study."" Stormont recommends that parents support children entering kindergarten by talking with their child about social behavior expectations in kindergarten and have them practice doing things like taking turns and following directions. In addition, parents and their children can meet with teachers to discuss what those expectations are. Parents also can explore summer programs before school starts that can help acclimate children to the classroom and learn routines. The study results also support efforts to help children with reading and math, as initial poor academic readiness predicted problems 18 months later. ",Society,0.051214675879282336
138,Science Daily,"Urban, Home Gardens Could Help Curb Food Insecurity, Health Problems",Society,2019-10-07,-,https://www.sciencedaily.com/releases/2019/10/191007180035.htm,"   Researchers from the University of California at San Francisco partnered with Valley Verde, a community-based urban garden organization in Santa Clara County, California, to better understand participants' perceptions of the health benefits and acceptability of urban home gardening programs. Interest in such programs has been on the rise, and this is a critical next step before beginning large-scale trials of how effective they are. ""This home-based model can play a vital role in urban agriculture and has the potential to directly impact health by tying the garden to the household,"" said lead author Kartika Palar, PhD, Department of Medicine, University of California San Francisco, San Francisco, CA, USA. She added that home and community gardens are complementary approaches to urban agriculture, together promoting a more resilient local food system. Researchers followed 32 participants -- mostly Hispanic/Latinos and women -- in Valley Verde's gardening program for one year. The program serves a predominantly low-income and immigrant population, providing them with the knowledge, skills and tools needed to grow their own organic vegetable gardens. Valley Verde staff provided 10 monthly workshops for each participant focused on organic gardening skills building as well as nutritional education, like strategies to increase vegetable, fruit and whole-grain intake; healthy shopping strategies; and culturally preferred healthy recipes. Participants were interviewed before, during and after the program to track what they learned and how they were implementing it. Nearly every participant indicated they ate more vegetables and fruits because of the program, citing increased affordability, accessibility, freshness, flavor and convenience of the garden produce. ""We had some delicious meals with lots of peas because the winter peas were doing really well, and then we could just draw on that when you're out of options,"" a 47-year-old female participant said in the study, describing how the garden helped during times of the month when money ran low. ""[Fruits and vegetables] are a more steady supply. Yeah, it isn't like, 'Oh guess what? In this pay period we can actually afford some salad.' Now we just go and just harvest it and just have it all the time."" ""I value more the things that I cook, and the things that I get from my garden, over the things I buy,"" a 34-year-old male participant said in the study. ""There's a big difference....I feel good that I grew it and I am eating something that I grew. So for me, it's priceless."" Participants also frequently described having less stress, as well as a rise in exercise and drop in sedentary behavior both for adults and children. Tending the garden led to more physical activity because of the need to water, weed, harvest and plant at regular intervals. The study suggests an urban gardening model that integrates home gardening with culturally appropriate nutrition and gardening education has the potential to improve a range of health behaviors that are critical to preventing and managing chronic disease, especially among low-income, urban Hispanic/Latino households. ""Urban agriculture is an important community resource that may contribute not only to nutrition and health, but also to urban development and social connection,"" said Dr. Sheri Weiser, MD, the senior author of the study. She added that combining urban home gardening with nutrition education is an innovative strategy to help to reduce the burden of preventable diseases, such as diabetes, in low-income populations with limited access to healthy food. ",Health,0.04898162889872594
139,Science Daily,Study Finds Topsoil Is Key Harbinger of Lead Exposure Risks for Children,Health,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014181654.htm,"   The study, which focuses on New Orleans but could serve as a model for cities around the world, is the first to show how long-term changes in soil lead levels have a corresponding impact in lead blood levels in children. ""Lead dust is invisible and it's tragic that lead-contaminated outdoor areas are unwittingly provided for children as places to play,"" says lead study author Howard Mielke, a pharmacology research professor at Tulane University School of Medicine. ""Young children are extremely vulnerable to lead poisoning because of their normal crawling, hand-to-mouth, exploratory behavior."" Exposure to lead is often irreversible, particularly for children, and includes behavioral or learning problems, decreased IQ, hyperactivity, delayed growth, hearing problems, anemia, kidney disease and cancer. In rare cases, exposure can lead to seizures, coma, or death. In metropolitan New Orleans, children living in communities with more lead in the soil and higher blood lead levels have the lowest school performance scores. Lead was recently cited as a top risk factor for premature death in the United States, particularly from cardiovascular disease, and is responsible for 412,000 premature deaths each year. The research team began tracking the amount of lead in New Orleans soil in 2001, collecting about 5,500 samples in neighborhoods, along busy streets, close to houses and in open spaces including parks. The team from Mielke's Lead Lab collected another round of soil sampling 16 years later. Those samples showed a 44% decrease in the amount of soil lead in communities flooded during Hurricane Katrina in 2005 as well as soils in communities not affected by the levee failures and storm surge. Researchers then compared the soil lead with children's blood lead data maintained by the Louisiana Healthy Homes and Childhood Lead Poisoning Prevention Program from 2000-2005 and 2011-2016. Researchers found that lead in blood samples decreased by 64% from 2000-2005 to the 2011-2016 time period and that decreasing lead in topsoil played a key factor in the declining children's blood lead levels. Lead exposure is a critical environmental justice issue, according to researchers. The team found black children were three times more likely than white children to have higher blood lead levels, which could be explained by socioeconomic status and education, the type and age of housing and proximity to major roads and industry. ""While the metabolism of the city could theoretically affect all residents equally, in reality social formations produce inequitable outcomes in which vulnerable populations tend to bear greater burdens of contaminant exposure,"" Mielke says. Mielke says further study is needed to determine if demographic changes in New Orleans since 2001 contributed to the decline in children's blood lead levels, and if decreases are occurring equitably for all populations. This new study is co-authored by researchers from Australia, Colorado State University, and City University of New York. ",Health,0.045309109112514595
140,Science Daily,Dementia Spreads Via Connected Brain Networks,Health,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111730.htm,"   ""Knowing how dementia spreads opens a window onto the biological mechanisms of the disease -- what parts of our cells or neural circuits are most vulnerable,"" said study lead author Jesse Brown, PhD, an assistant professor of neurology at the UCSF Memory and Aging Center and UCSF Weill Institute for Neurosciences. ""You can't really design a treatment until you know what you're treating."" FTD, the most common form of dementia in people under the age of 60, comprises a group of neurodegenerative conditions with diverse linguistic and behavioral symptoms. As in Alzheimer's disease, the diversity of FTD symptoms reflects significant differences in how the neurodegenerative disease spreads through patients' brains. This variability makes it difficult for scientists searching for cures to pin down the biological drivers of brain atrophy and for clinical trials to evaluate whether a novel treatment is making a difference in the progression of a patient's disease. Previous research by the study's senior author, William Seeley, MD, a professor of neurology and pathology at the Memory and Aging Center and Weill Institute, set off a sea change in dementia research by showing that patterns of brain atrophy in many forms of dementia map closely onto well-known brain networks -- groups of functionally related brain regions that work cooperatively via their synaptic connections, sometimes over long distances. In other words, Seeley's work proposed that neurodegenerative diseases don't spread evenly in all directions like a tumor, but can jump from one part of the brain to another along the anatomical circuits that wire these networks together. In their new study -- published October 14 in Neuron -- Brown, Seeley and colleagues provided further evidence supporting this idea by examining how well neural network maps based on brain scans in healthy individuals could predict the spread of brain atrophy in FTD patients over the course of a year. The researchers recruited 42 patients at the UCSF Memory and Aging Center with behavioral variant fronto-temporal dementia (bvFTD), a form of FTD that causes patients to exhibit inappropriate social behaviors, and 30 patients with semantic variant primary progressive aphasia (svPPA), a form of FTD that mainly impacts patients' language abilities. In their first visits to UCSF, each of these patients underwent a ""baseline"" MRI scan to assess the extent of existing brain degeneration and then had a follow-up scan about a year later to measure how their disease had progressed.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The researchers first estimated where the brain atrophy seen in each patient's baseline scans had begun, based on the hypothesis that brain degeneration begins in some particularly vulnerable location, then spreads out to anatomically connected brain regions. To do this, the researchers built standardized maps of the main functional partners of 175 different brain regions based on functional MRI (fMRI) scans of 75 healthy adults. They then identified which of these networks best matched the pattern of brain atrophy seen in a given FTD patient's baseline brain scans, and defined that network's central hub as the likely epicenter of the patient's degeneration. They then used the same standardized connectivity maps to predict where the patient's brain atrophy was most likely to have spread in the follow-up scans done one year later, and compared the accuracy of these predictions to others that didn't take functional network connectivity into account. They found that two particular connectivity measures significantly improved their predictions of a given brain region's chances of developing brain atrophy between the baseline and follow-up brain scans. One, called ""shortest path to the epicenter,"" captured the number of synaptic ""steps"" that region was from the estimated disease epicenter -- essentially the number of links in the neural chain connecting the two areas -- while the other, called ""nodal hazard,"" represented how many regions connected to a given region were already experiencing significant atrophy. ""It's like with an infectious disease, where your chances of becoming infected can be predicted by how many degrees of separation you have from 'Patient Zero' but also by how many people in your immediate social network are already sick,"" Brown said. The researchers showed that on average these two measures of network connectivity did better at predicting the spread of disease to a new brain region than its simple straight-line distance from a patient's existing atrophy. In many cases the disease completely bypassed brain areas that were adjacent but not anatomically connected to already-atrophied regions, instead jumping to more functionally linked regions.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Although this method shows great promise, the researchers emphasize that it is not yet ready for clinical use. They hope to improve the accuracy of their predictions by -- among other approaches -- using individualized network maps for each patient rather than using average connectivity maps, and by developing more specialized prediction models for particular subtypes of FTD. In addition to the biological insights the discovery provides about the mechanisms of spreading brain atrophy in FTD, which will inform ongoing efforts to develop treatments, the researchers also hope the findings will lead to improved metrics for evaluating therapies already entering clinical trials -- for instance by giving trial scientists early insights into whether the treatment is altering a predicted course of disease progression. Researchers could also use better predictions of how atrophy will spread through the brain to help prepare patients and their families for the symptoms they are likely to experience as their disease progresses. ""We are excited about this result because it represents an important first step toward a more precision medicine type of approach to predicting progression and measuring treatment effects in neurodegenerative disease,"" Seeley said. In the future, Brown said, scientists might be able to develop therapies that specifically target the likely next site of disease and perhaps prevent atrophy from spreading from one region to another. ""Just like epidemiologists rely on models of how infectious diseases spread to develop interventions targeted to key hubs or choke points,"" Brown said. ""Neurologists need to understand the underlying biological mechanisms of neurodegeneration to develop ways of slowing or halting the spread of the disease."" ",Health,0.04343933490915236
141,Science Daily,Slower Walkers Have Older Brains and Bodies at 45,Health,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011112250.htm,"   Slower walkers were shown to have ""accelerated aging"" on a 19-measure scale devised by researchers, and their lungs, teeth and immune systems tended to be in worse shape than the people who walked faster. ""The thing that's really striking is that this is in 45-year-old people, not the geriatric patients who are usually assessed with such measures,"" said lead researcher Line J.H. Rasmussen, a post-doctoral researcher in the Duke University department of psychology & neuroscience. Equally striking, neurocognitive testing that these individuals took as children could predict who would become the slower walkers. At age 3, their scores on IQ, understanding language, frustration tolerance, motor skills and emotional control predicted their walking speed at age 45. ""Doctors know that slow walkers in their seventies and eighties tend to die sooner than fast walkers their same age,"" said senior author Terrie E. Moffitt, the Nannerl O. Keohane University Professor of Psychology at Duke University, and Professor of Social Development at King's College London. ""But this study covered the period from the preschool years to midlife, and found that a slow walk is a problem sign decades before old age."" The data come from a long-term study of nearly 1,000 people who were born during a single year in Dunedin, New Zealand. The 904 research participants in the current study have been tested, quizzed and measured their entire lives, mostly recently from April 2017 to April 2019 at age 45.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The study appears Oct. 11 in JAMA Network Open. MRI exams during their last assessment showed the slower walkers tended to have lower total brain volume, lower mean cortical thickness, less brain surface area and higher incidence of white matter ""hyperintensities,"" small lesions associated with small vessel disease of the brain. In short, their brains appeared somewhat older. Adding insult to injury perhaps, the slower walkers also looked older to a panel of eight screeners who assessed each participant's 'facial age' from a photograph. Gait speed has long been used as a measure of health and aging in geriatric patients, but what's new in this study is the relative youth of these study subjects and the ability to see how walking speed matches up with health measures the study has collected during their lives. ""It's a shame we don't have gait speed and brain imaging for them as children,"" Rasmussen said. (The MRI was invented when they were five, but was not given to children for many years after.) Some of the differences in health and cognition may be tied to lifestyle choices these individuals have made. But the study also suggests that there are already signs in early life of who would become the slowest walkers, Rasmussen said. ""We may have a chance here to see who's going to do better health-wise in later life."" This research was supported by grants the US National Institute on Aging (AG032282, AG049789, AG028716), the UK Medical Research Council (MR/P005918/1), the Jacobs Foundation, the New Zealand Health Research Council (16-604), the New Zealand Ministry of Business, Innovation and Employment, the Lundbeck Foundation (R288-2018-380), the US National Science Foundation (NSF DGE-1644868), the US National Institute of Child Health and Human Development (T32-HD007376). ",Health,0.0430268722051582
142,Science Daily,Repeated Febrile Convulsions Linked to Epilepsy and Psychiatric Disorders,Health,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103408.htm,"   The register-based study is based on two million Danish children born between 1977 and 2011. The researchers have identified health data from approx. 17,000 children with more than a single febrile convulsion. This makes the register-based study the most comprehensive so far to study the long-term consequences of repeated febrile convulsions. The study has just been published in the scientific journal JAMA Pediatrics. ""Though previous research has documented an increased occurrence of epilepsy among children with febrile convulsions, this is still one of the first studies to demonstrate such a convincing correlation between febrile convulsions and psychiatric disorders. Not least due to the size of the study, the long period of time that the study covers and the valid Danish data,"" says the study's lead author, Postdoc Julie Werenberg Dreyer from the National Centre for Register-based Research. The researcher emphasises that although the study demonstrates a clear correlation, this is not the same as concluding that febrile convulsions in themselves cause epilepsy or psychiatric disorders. ""A statistical correlation does not necessarily mean that one thing causes the other and that it is the febrile convulsions themselves which have a damaging effect on the brain. But the study's results are so significant that looking into this more closely is more than relevant when it comes to possibly being able to provide the best possible prevention and treatment,"" says Julie Werenberg Dreier. According to Julie Werenberg Dreier, a future study could look into the significance of genetics for the child's risk of suffering febrile convulsions and subsequent epilepsy or psychiatric disorders. ""There are still many unknown factors that we don't know enough about. As we learn more about the importance of genes for health and disease, it may be that it is here we will find an explanation for why some children suffer repeated febrile convulsions and then later in life also develop epilepsy and psychiatric disorders,"" she says. The study shows that among children who have three or more attacks of febrile convulsions, the risk of developing epilepsy within thirty years is approximately fifteen per cent. The risk of a psychiatric disorder that requires treatment is approx. thirty per cent. In comparison, children without prior febrile convulsions have a risk of developing epilepsy of approx. two per cent and children without prior febrile convulsions have a seventeen per cent risk of developing a psychiatric disorder. The study points towards new correlations that can in the long-term improve the possibilities of prevention and treatment of patients with epilepsy and psychiatric disorders. This is according to another of the project contributors, Jakob Christensen. He is a clinical associate professor at Aarhus University and consultant at the Department of Neurology at Aarhus University Hospital. He has conducted intensive research into epilepsy over many years. ""Both epilepsy and psychiatric disorder can be extremely serious and associated with high morbidity and mortality -- so in this way the diseases have major consequences for both the individual patient, their family and society,"" says Jakob Christensen. Both researchers hope that the study will help to provide impetus for an intensified effort to clarify the cause of the correlation between the febrile convulsions and the long-term consequences. ""Our results may be frightening reading for parents who have a child that suffers from repeated attacks of febrile convulsions. But these are families who are already deeply concerned about their children. The new knowledge can help them and healthcare professionals to be extra aware of these children's health and development,"" says Julie Werenberg Dreier. ",Health,0.04295134532779052
143,MIT News,Alzheimer’s plaque emerges early and deep in the brain,Research,2019-10-08,-,http://news.mit.edu/2019/study-pinpoints-early-alzheimers-plaque-emergence-1008,"  Long before symptoms like memory loss even emerge, the underlying pathology of Alzheimer’s disease, such as an accumulation of amyloid protein plaques, is well underway in the brain. A longtime goal of the field has been to understand where it starts so that future interventions could begin there. A new study by MIT neuroscientists at The Picower Institute for Learning and Memory could help those efforts by pinpointing the regions with the earliest emergence of amyloid in the brain of a prominent mouse model of the disease. Notably, the study also shows that the degree of amyloid accumulation in one of those same regions of the human brain correlates strongly with the progression of the disease.        “Alzheimer’s is a neurodegenerative disease, so in the end you can see a lot of neuron loss,” says Wen-Chin “Brian” Huang, co-lead author of the study and a postdoc in the lab of co-senior author Li-Huei Tsai, Picower Professor of Neuroscience and director of the Picower Institute. “At that point, it would be hard to cure the symptoms. It’s really critical to understand what circuits and regions show neuronal dysfunction early in the disease. This will, in turn, facilitate the development of effective therapeutics.” In addition to Huang, the study’s co-lead authors are Rebecca Canter, a former member of the Tsai lab, and Heejin Choi, a former member of the lab of co-senior author Kwanghun Chung, associate professor of chemical engineering and a member of the Picower Institute and the MIT Institute for Medical Engineering and Science. Tracking plaques Many research groups have made progress in recent years by tracing amyloid’s path in the brain using technologies such as positron emission tomography, and by looking at brains post-mortem, but the new study in Communications Biology adds substantial new evidence from the 5XFAD mouse model because it presents an unbiased look at the entire brain as early as one month of age. The study reveals that amyloid begins its terrible march in deep brain regions such as the mammillary body, the lateral septum, and the subiculum before making its way along specific brain circuits that ultimately lead it to the hippocampus, a key region for memory, and the cortex, a key region for cognition. The team used SWITCH, a technology developed by Chung, to label amyloid plaques and to clarify the whole brains of 5XFAD mice so that they could be imaged in fine detail at different ages. The team was consistently able to see that plaques first emerged in the deep brain structures and then tracked along circuits, such as the Papez memory circuit, to spread throughout the brain by six-12 months (a mouse’s lifespan is up to three years). The findings help to cement an understanding that has been harder to obtain from human brains, Huang says, because post-mortem dissection cannot easily account for how the disease developed over time and PET scans don’t offer the kind of resolution the new study provides from the mice. Key validations Importantly, the team directly validated a key prediction of their mouse findings in human tissue: If the mammillary body is indeed a very early place that amyloid plaques emerge, then the density of those plaques should increase in proportion with how far advanced the disease is. Sure enough, when the team used SWITCH to examine the mammillary bodies of post-mortem human brains at different stages of the disease, they saw exactly that relationship: The later the stage, the more densely plaque-packed the mammillary body was. “This suggests that human brain alterations in Alzheimer’s disease look similar to what we observe in mouse,” the authors wrote. “Thus we propose that amyloid-beta deposits start in susceptible subcortical structures and spread to increasingly complex memory and cognitive networks with age.” The team also performed experiments to determine whether the accumulation of plaques they observed were of real disease-related consequence for neurons in affected regions. One of the hallmarks of Alzheimer’s disease is a vicious cycle in which amyloid makes neurons too easily excited, and overexcitement causes neurons to produce more amyloid. The team measured the excitability of neurons in the mammillary body of 5XFAD mice and found they were more excitable than otherwise similar mice that did not harbor the 5XFAD set of genetic alterations. In a preview of a potential future therapeutic strategy, when the researchers used a genetic approach to silence the neurons in the mammillary body of some 5XFAD mice but left neurons in others unaffected, the mice with silenced neurons produced less amyloid. While the study findings help explain much about how amyloid spreads in the brain over space and time, they also raise new questions, Huang said. How might the mammillary body affect memory, and what types of cells are most affected there? “This study sets a stage for further investigation of how dysfunction in these brain regions and circuits contributes to the symptoms of Alzheimer’s disease,” he says. In addition to Huang, Canter, Choi, Tsai, and Chung, the paper’s other authors are Jun Wang, Lauren Ashley Watson, Christine Yao, Fatema Abdurrob, Stephanie Bousleiman, Jennie Young, David Bennett and Ivana Dellalle. The National Institutes of Health, the JPB Foundation, Norman B. Leventhal and Barbara Weedon fellowships, The Burroughs Wellcome Fund, the Searle Scholars Program, a Packard Award, a NARSAD Young Investigator Award, and the NCSOFT Cultural Foundation funded the research. ",Health,0.042102879360735444
144,Science Daily,Potential Therapy to Treat Detrimental Effects of Marijuana,Health,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092247.htm,"   A University of Maryland School of Medicine study using a preclinical animal model suggests that prenatal exposure to THC, the psychoactive component of cannabis, makes the brain's dopamine neurons (an integral component of the reward system) hyperactive and increases sensitivity to the behavioral effects of THC during pre-adolescence. This may contribute to the increased risk of psychiatric disorders like schizophrenia and other forms of psychosis later in adolescence that previous research has linked to prenatal cannabis use, according to the study published today in journal Nature Neuroscience. The team of researchers, from UMSOM, the University of Cagliari (Italy) and the Hungarian Academy of Sciences (Hungary), found that exposure to THC in the womb increased susceptibility to THC in offspring on several behavioral tasks that mirrors the effects observed in many psychiatric diseases. These behavioral effects were caused, at least in part, by hyperactivity of dopamine neurons in a brain region called the ventral tegmental area (VTA), which regulates motivated behaviors. More importantly, the researchers were able to correct these behavioral problems and brain abnormalities by treating experimental animals with pregnenolone, an FDA-approved drug currently under investigation in clinical trials for cannabis use disorder, schizophrenia, autism, and bipolar disorder. ""This is an exciting finding that suggests a therapeutic approach for children born to mothers who used cannabis during pregnancy,"" said Joseph Cheer, PhD, a Professor of Anatomy & Neurobiology and Psychiatry at the University of Maryland School of Medicine. ""It also raises important questions that need to be addressed such as how does pregnenolone exert its effects and how can we improve its efficacy? Do these detrimental effects persist into adulthood, and if so, could they also be treated in a similar way?"" The researchers concluded that as physicians caution pregnant women against alcohol and cocaine intake because of their detrimental effects to the fetus, they should also, based on these new findings, advise them on the potential negative consequences of using cannabis specifically during pregnancy. ",Health,0.041584314587998925
145,Science Daily,Widely Available Drug Reduces Head Injury Deaths,Health,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015113316.htm,"   Led by the London School of Hygiene & Tropical Medicine, the global randomised trial included more than 12,000 head injury patients who were given either intravenous tranexamic acid or a placebo. [2] It found that administration of TXA within three hours of injury reduced the number of deaths. This effect was greatest in patients with mild and moderate traumatic brain injury (20% reduction in deaths), while no clear benefit was seen in the most severely injured patients. The trial found no evidence of adverse effects and there was no increase in disability in survivors when the drug was used. [3,4,5] Traumatic brain injury (TBI) is a leading cause of death and disability worldwide with an estimated 69 million new cases each year. [6] The CRASH-3 (Clinical Randomisation of an Antifbrinolytic in Significant Head Injury) trial is one of the largest clinical trials ever conducted into head injury. Patients were recruited from 175 hospitals across 29 countries. Bleeding in or around the brain due to tearing of blood vessels is a common complication of TBI and can lead to brain compression and death. Although patients with very severe head injuries are unlikely to benefit from tranexamic acid treatment because they often have extensive brain bleeding prior to hospital admission and treatment, the study found a substantial benefit in patients with less severe injuries who comprise the majority (over 90%) of TBI cases. [7] Ian Roberts, Professor of Clinical Trials at the London School of Hygiene & Tropical Medicine, who co-led the study, said: ""We already know that rapid administration of tranexamic acid can save lives in patients with life threatening bleeding in the chest or abdomen such as we often see in victims of traffic crashes, shootings or stabbings. This hugely exciting new result shows that early treatment with TXA also cuts deaths from head injury. It's an important breakthrough and the first neuroprotective drug for patients with head injury. ""Traumatic brain injury can happen to anyone at any time, whether it's through an incident like a car crash or simply falling down the stairs. We believe that if our findings are widely implemented they will boost the chances of people surviving head injuries in both high income and low income countries around the world."" Because TXA prevents bleeds from getting worse, but cannot undo damage already done, early treatment is critical. The trial data showed a 10% reduction in treatment effectiveness for every 20-minute delay, suggesting that patients should be treated with TXA as soon as possible after head injury. [8,9]    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Antoni Belli, Neurosurgeon and Professor of Trauma Neurosurgery at the University of Birmingham and co-investigator for trial, said: ""This is a landmark study. After decades of research and many unsuccessful attempts, this is the first ever clinical trial to show that a drug can reduce mortality after traumatic brain injury. Not only do we think this could save hundreds of thousands of lives worldwide, but it will no doubt renew the enthusiasm for drug discovery research for this devastating condition."" Dr Ben Bloom, Consultant in Emergency Medicine at Barts Health NHS Trust, the UK's largest recruiter into the trial with more than 500 patients enrolled, said: ""Treating traumatic brain injury is extremely challenging with very few treatment options available for patients. Thanks to these latest results, which are applicable to patients with head injuries of any cause and of all demographics, clinicians now have a potentially powerful new treatment available to them."" The most common causes of TBI worldwide are road traffic crashes (which predominantly affect young adults) or falls (which are a major problem in older adults), and the incidence is increasing. In both cases, patients can experience permanent disability or death. Representatives from the charity that supports roach crash victims in the UK, Roadpeace, were involved in the design of the trial. Amy Aeron-Thomas, Justice and Advocacy Manager from Roadpeace and co-author on the paper said: ""It's always better to prevent road crashes in the first place, but these results show that if a crash can't be prevented, death can still be avoided. Given the time to treatment implications, it's more important than ever that the post-crash response is as efficient as possible."" CRASH-3 follows successful previous research involving 20,000 trauma patients, which showed that TXA reduced deaths due to bleeding outside of the skull by almost a third if given within three hours. Based on those trial results, tranexamic acid was included in guidelines for the pre-hospital care of trauma patients. However, patients with isolated traumatic brain injury were specifically excluded. [10] The authors noted some limitations of the trial, including wide confidence intervals despite the large trial size, and the fact that more patients with un-survivable head injuries were included in the trial than anticipated, which diluted the treatment effect.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The trial was jointly funded by the Department for International Development (DFID), the Medical Research Council (MRC), the National Institute for Health Research (NIHR), (through the Department of Health and Social Care), and Wellcome. The early phase of the trial was funded was funded by The JP Moulton Charitable Foundation. [11] Notes to Editors1. Tranexamic acid is a low cost and widely available drug as many different companies sell it. Costs vary slightly per country. In the UK, 500mg is roughly £1.55, so the total dose used in CRASH-3 is about £6.20 (https://bnf.nice.org.uk/medicinal-forms/tranexamic-acid.html). In Malaysia, 500mg is 3.30 Malaysian Ringitt (64p) so around £2.50 for the CRASH-3 dose https://www.pharmacy.gov.my/v2/en/apps/drug-price 2. Patients were randomly allocated to receive a loading dose of 1 g of tranexamic acid infused over 10 minutes, started immediately after randomisation, followed by an intravenous infusion of 1 g over 8 hours, or matching placebo. 3. Among patients treated within 3 hours of injury, there was a reduction in the risk of head injury death with tranexamic acid in mild to moderate head injury (RR=0·78 95%CI 0·64-0·95), numbers of deaths can be seen in figure 3 of the paper (TXA group = 166 / 2846 (5.8%), placebo group = 207 / 2769 (7.5%). In severe head injury (RR=0·99, 95%CI 0·91-1·07) there was no clear evidence of a reduction (p-value for heterogeneity 0·030). The impact of baseline GCS in a regression analysis showed evidence (p=0·007) that tranexamic acid is more effective in less severely injured patients. 4. The most common classification system for TBI severity is based on the Glasgow Coma Scale (GCS) score determined at the time of injury. A total score of 3-8 indicates severe TBI, a score of 9-12 indicates moderate TBI, and a score of 13-15 indicates mild TBI. 5. The risk of deep vein thrombosis, pulmonary embolism, stroke and myocardial infarction was similar in the tranexamic acid and placebo groups. There was no evidence that tranexamic acid increased fatal or non-fatal stroke (RR=1.08). The risk of seizures was similar between groups (RR=1.09). 6. Sixty-nine million (95% CI 64-74 million) individuals are estimated to suffer TBI from all causes each year (https://www.ncbi.nlm.nih.gov/pubmed/29701556). 7. Mild TBI occurs with far greater frequency than moderate or severe TBI -- nearly 10-fold the burden of both moderate and severe injury. Of the estimated 69 million TBIs that occur each year, 81% will be mild, 11% will be moderate, and 8% will be severe (https://www.ncbi.nlm.nih.gov/pubmed/29701556). 8. Early treatment was more effective in patients with mild and moderate head injury (p=0·005) but there was no obvious impact of time to treatment in severe head injury (p=0·73). This is consistent with the hypothesis that tranexamic acid improves outcome by reducing intracranial bleeding. 9. The left hand graph of Figure 4 marked ""Mild and Moderate GCS score"" shows how treatment benefit is related to time on the risk ratio scale. When the treatment effect for mild and moderate patients is modelled using logistic regression with a time treatment interaction term adjusting for GCS, age and systolic blood pressure, the odds ratio is reduced by approximately 10% for every 20 minute delay. 10. A previous trial (CRASH-2) of 20,211 bleeding trauma patients from hospitals in 40 countries showed that TXA reduces bleeding deaths by a third if given soon after injury (https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(10)60835-5/fulltext) 11. Funds to support the drug and placebo costs in the run-in phase of the trial were provided by Pfizer. ",Health,0.04156926196221568
