,Source,Heading,Category,Date,Time,URL,Text,Category_predicted,weights
0,MIT News,IBM gives artificial intelligence computing at MIT a lift,Computer Science,2019-08-26,-,http://news.mit.edu/2019/ibm-gives-lift-artificial-intelligence-computing-mit-0826,"  IBM designed Summit, the fastest supercomputer on Earth, to run the calculation-intensive models that power modern artificial intelligence (AI). Now MIT is about to get a slice.  IBM pledged earlier this year to donate an $11.6 million computer cluster to MIT modeled after the architecture of Summit, the supercomputer it built at Oak Ridge National Laboratory for the U.S. Department of Energy. The donated cluster is expected to come online this fall when the MIT Stephen A. Schwarzman College of Computing opens its doors, allowing researchers to run more elaborate AI models to tackle a range of problems, from developing a better hearing aid to designing a longer-lived lithium-ion battery.  “We’re excited to see a range of AI projects at MIT get a computing boost, and we can’t wait to see what magic awaits,” says John E. Kelly III, executive vice president of IBM, who announced the gift in February at MIT’s launch celebration of the MIT Schwarzman College of Computing.   IBM has named the cluster Satori, a Zen Buddhism term for “sudden enlightenment.” Physically the size of a shipping container, Satori is intellectually closer to a Ferrari, capable of zipping through 2 quadrillion calculations per second. That’s the equivalent of each person on Earth performing more than 10 million multiplication problems each second for an entire year, making Satori nimble enough to join the middle ranks of the world’s 500 fastest computers. Rapid progress in AI has fueled a relentless demand for computing power to train more elaborate models on ever-larger datasets. At the same time, federal funding for academic computing facilities has been on a three-decade decline. Christopher Hill, director of MIT’s Research Computing Project, puts the current demand at MIT at five times what the Institute can offer.   “IBM’s gift couldn’t come at a better time,” says Maria Zuber, a geophysics professor and MIT’s vice president of research. “The opening of the new college will only increase demand for computing power. Satori will go a long way in helping to ease the crunch.” The computing gap was immediately apparent to John Cohn, chief scientist at the MIT-IBM Watson AI Lab, when the lab opened last year. “The cloud alone wasn’t giving us all that we needed for challenging AI training tasks,” he says. “The expense and long run times made us ask, could we bring more compute power here, to MIT?” It’s a mission Satori was built to fill, with IBM Power9 processors, a fast internal network, a large memory, and 256 graphics processing units (GPUs). Designed to rapidly process video-game images, graphics processors have become the workhorse for modern AI applications. Satori, like Summit, has been configured to wring as much power from each GPU as possible. IBM’s gift follows a history of collaborations with MIT that have paved the way for computing breakthroughs. In 1956, IBM helped launch the MIT Computation Center with the donation of an IBM 704, the first mass-produced computer to handle complex math. Nearly three decades later, IBM helped fund Project Athena, an initiative that brought networked computing to campus. Together, these initiatives spawned time-share operating systems, foundational programming languages, instant messaging, and the network-security protocol, Kerberos, among other technologies.  More recently, IBM agreed to invest $240 million over 10 years to establish the MIT-IBM Watson AI Lab, a founding sponsor of MIT’s Quest for Intelligence. In addition to filling the computing gap at MIT, Satori will be configured to allow researchers to exchange data with all major commercial cloud providers, as well as prepare their code to run on IBM’s Summit supercomputer. Josh McDermott, an associate professor at MIT’s Department of Brain and Cognitive Sciences, is currently using Summit to develop a better hearing aid, but before he and his students could run their models, they spent countless hours getting the code ready. In the future, Satori will expedite the process, he says, and in the longer term, make more ambitious projects possible. “We’re currently building computer systems to model one sensory system but we’d like to be able to build models that can see, hear and touch,” he says. “That requires a much bigger scale.” Richard Braatz, the Edwin R. Gilliland Professor at MIT’s Department of Chemical Engineering, is using AI to improve lithium-ion battery technologies. He and his colleagues recently developed a machine learning algorithm to predict a battery’s lifespan from past charging cycles, and now, they’re developing multiscale simulations to test new materials and designs for extending battery life. With a boost from a computer like Satori, the simulations could capture key physical and chemical processes that accelerate discovery. “With better predictions, we can bring new ideas to market faster,” he says.  Satori will be housed at a silk mill-turned data center, the Massachusetts Green High Performance Computing Center (MGHPCC) in Holyoke, Massachusetts, and connect to MIT via dedicated, high-speed fiber optic cables. At 150 kilowatts, Satori will consume as much energy as a mid-sized building at MIT, but its carbon footprint will be nearly fully offset by the use of hydro and nuclear power at the Holyoke facility. Equipped with energy-efficient cooling, lighting, and power distribution, the MGHPCC was the first academic data center to receive LEED-platinum status, the highest green-building award, in 2011. “Siting Satori at Holyoke minimizes its carbon emissions and environmental impact without compromising its scientific impact,” says John Goodhue, executive director of the MGHPCC. Visit the Satori website for more information. ",Computer Science,0.19981804197634234
1,ACM,Tools to Minimize Risks in Shared AR Environments,ACM,2019-08-20,-,https://www.washington.edu/news/2019/08/20/shared-augmented-reality-environments/,"       Tools to Minimize Risks in Shared AR Environments     UW NewsSarah McQuateAugust 20, 2019   University of Washington security researchers have developed a prototype toolkit that lets developers incorporate collaborative and interactive features into augmented reality (AR) apps while maintaining users' privacy and security. The ShareAR toolkit for the Microsoft HoloLens helps apps produce, share, and monitor objects shared by users. Testing with case study apps revealed that the most computationally rigorous actions were creating objects, and changing permission settings within apps. However, ShareAR took no longer than 5 milliseconds to complete a task—and less than 1 millisecond in most cases—even when researchers intentionally strained the system, with large numbers of users and shared objects.                 ",Computer Science,0.19430471165553878
2,ACM,Google AI's New Algorithm Will Let Smartphones Read Sign Language,ACM,2019-08-20,-,https://www.ibtimes.co.uk/google-ais-new-algorithm-will-let-smartphones-read-sign-language-1668078,"    Google AI's New Algorithm Will Let Smartphones Read Sign Language     International Business TimesRishabh JainAugust 20, 2019   Google researchers have developed an algorithm that will allow smartphones to perceive hand movements and shapes across a variety of platforms, including reading sign language using augmented reality. The algorithm uses machine learning to learn patterns, remember them, and create data modalities. The system infers 21 three-dimensional keypoints on hands in every frame. The algorithm is made up of three models that work together: a palm detector; a hand detector, and gesture recognition. Rather than tracking the whole hand, the system merely tracks the palm to read overall hand movements. In addition, the position of every finger is analyzed separately to create proper interpretations. Google says it has opened the source code so other researchers can build on its achievement.                 ",Computer Science,0.18752342748823098
3,ACM,Using Wall Street Secrets to Reduce the Cost of Cloud Infrastructure,ACM,2019-08-18,-,https://news.mit.edu/2019/reduce-cost-cloud-infrastructure-0819,"       Using Wall Street Secrets to Reduce the Cost of Cloud Infrastructure     MIT NewsRob MathesonAugust 18, 2019   Researchers at the Massachusetts Institute of Technology (MIT) and Microsoft have developed a ""risk-aware"" mathematical model that could improve the performance of cloud computing networks around the world. The TeaVar model (whose code is freely available on GitHub) accounts for failure probabilities of links between datacenters worldwide, similar to predicting the volatility of stocks. Then, TeaVar runs an optimization engine to allocate traffic through optimal paths to minimize loss while maximizing overall usage of the network. While conventional methods keep links idle to handle unexpected traffic shifts resulting from link failures, TeaVar guarantees that for a targeted percentage of time the network can handle all data traffic, so there is no need to keep any links idle. During testing based on real-world data, the model supported three times the traffic throughput of traditional traffic-engineering models, while maintaining the same high level of network availability.                 ",Computer Science,0.18225713429581536
4,ACM,"Router Guest Networks Lack Adequate Security, According to BGU Researchers",ACM,2019-08-15,-,https://in.bgu.ac.il/en/pages/news/guest_networks.aspx,"    Router Guest Networks Lack Adequate Security, According to BGU Researchers     Ben-Gurion University of the Negev (Israel)August 15, 2019   A study by researchers at Israel’s Ben-Gurion University of the Negev (BGU) found routers made by leading manufacturers are vulnerable to cross-router data leakage via an attack on one of the separate host and guest networks. The researchers noted the presence of different levels of cross-router covert channels, which can be integrated and taken advantage of to either direct a malicious implant or to exfiltrate data. These flaws in certain cases can be patched as a software bug, but more pervasive and concealed cross-channel communication is impervious to prevention unless data streams are partitioned on different hardware. Said BGU's Adar Ovadya, ""A hardware-based solution seems to be the safest approach to guaranteeing isolation between secure and non-secure network devices.""                 ",Computer Science,0.14735049028996142
5,ACM,"Here's the No. 1 Highest-Paid, Most In-Demand Job in Every State",ACM,2019-08-22,-,https://www.marketwatch.com/story/heres-the-no-1-highest-paid-and-fastest-growing-job-in-every-us-state-2019-08-20,"    Here's the No. 1 Highest-Paid, Most In-Demand Job in Every State     MarketWatchQuentin FottrellAugust 22, 2019   A study by online employment website CareerBuilder found software developers to be among the highest-compensated and fastest-growing jobs in every U.S. state, according to data from the U.S. Bureau of Labor Statistics. Developers usually need a bachelor's degree, and programming skills like Java, JavaScript, SQL, C Sharp, Cascading Style Sheets, and NET Framework. Their median annual salary last year was $105,590, or $50.77 an hour, and the profession's job-growth outlook is higher than average. The study predicts the number of jobs for software developers will rise 24% nationwide from 2016 to 2026. An independent study by U.S. News & World Report found software developer was the best job in terms of work-life balance and career development.                 ",Computer Science,0.14610649604496612
6,IEEE,Eben Upton on the Raspberry Pi’s Industrial Crossover and Why There Will Never Be a Pi 9,Electronics and Technology,2019-08-28,-,https://spectrum.ieee.org/semiconductors/processors/eben-upton-on-the-raspberry-pis-industrial-crossover-and-why-there-will-never-be-a-pi-9,"      Seven years ago, Eben Upton created the first Raspberry Pi. As Upton told IEEE Spectrum in our March 2015 cover story, the Pi was inspired in part by his childhood experiments with a BBC Micro home computer: He wanted modern kids to have a simple machine that allowed for similar experimentation. Since then, the Pi has exploded in popularity, and the fourth major revision of the Pi was released in June. Upton talked with Spectrum senior editor Stephen Cass about the Pi 4’s design, its growing commercial use, and what might be next. Stephen Cass: How has the Pi’s user base evolved? Eben Upton: Our first year, our volume was almost entirely bought by hobbyists. But you have a lot of hobbyists who are also professional design engineers, and when their boss asked them to do something, often they used a Pi. So now you have people who are building industrial products around the Pi to resell. And then you have what we call, for want of a better word, DIY industrial, which is “I own a factory and I need control computers.” And where I might have historically gone and bought an embedded PC, I’ll buy a Pi. Last year we sold 6 million units and [we think as much as] half of those went to some kind of commercial use. S.C.: How did that evolution shape the design of the Pi 4? E.U.: We’re quite lucky in that all our markets have similar requirements. The things we do to make it a better toy make it a better industrial computer. And the things you do to make it a better industrial computer make it a better hobbyist platform. For example, we had a number of people building thin-client solutions, and the feedback was that most of the people doing that wanted the option of being able to deploy two monitors. So we added that feature. Another obvious example would be the serial interfaces. Prior generations are a little underprovisioned for things like UARTs, serial ports, SPIs, I2C interfaces, but because we were redesigning the silicon from scratch, putting UARTs in was pretty straightforward. S.C.: What other changes did you make in redesigning the Pi’s system processor? E.U.: The last three Pis have all been made using the same [40-nanometer] process, so modifications have been limited, mostly putting in a larger ARM [CPU]. If you look at the floor plan of the chip, each new ARM gets stuck on the left-hand side. The chip stretches horizontally and becomes bigger and bigger. But nothing on the right-hand side changes, and the right-hand side is where the memory controller is, where the UARTs are.... But putting in a larger ARM core means your power goes up and eventually you run out of your thermal budget.... We realized we needed to go to a 28-nm process node. And once you’re going to a new process node, you might as well fix all of the wrinkles. And that’s why you see two displays, many UARTs, PCI Express, Gigabit Ethernet. S.C.: How long before the slowdown in Moore’s Law affects the Pi? E.U.: I think it’s relevant to think in terms of how much is left. How much for a given thermal footprint? We’ve come a factor of 40 [in the Pi’s computing power from the first Pi to the Pi 4]. There’s not another factor of 40 left, which means you’ve come through more than half this process.... On some level that’s intimidating. But I’m a software engineer. It’s actually really nice to feel that we’re going into an era where software engineering makes a contribution again! My Ph.D. is in optimizing compilers, and while I was doing it I felt really depressed, because [improving software by a factor of two was small compared to the exponential hardware improvements of Moore’s Law]. But the thing about that factor of two is that it’s there at the end. There will come a time when they’ll be very grateful for it! This article appears in the September 2019 print issue as “How the Raspberry Pi Infiltrated Industry.” Seven years ago, Eben Upton created the first Raspberry Pi. As Upton told IEEE Spectrum in our March 2015 cover story, the Pi was inspired in part by his childhood experiments with a BBC Micro home computer: He wanted modern kids to have a simple machine that allowed for similar experimentation. Since then, the Pi has exploded in popularity, and the fourth major revision of the Pi was released in June. Upton talked with Spectrum senior editor Stephen Cass about the Pi 4’s design, its growing commercial use, and what might be next. Stephen Cass: How has the Pi’s user base evolved? Eben Upton: Our first year, our volume was almost entirely bought by hobbyists. But you have a lot of hobbyists who are also professional design engineers, and when their boss asked them to do something, often they used a Pi. So now you have people who are building industrial products around the Pi to resell. And then you have what we call, for want of a better word, DIY industrial, which is “I own a factory and I need control computers.” And where I might have historically gone and bought an embedded PC, I’ll buy a Pi. Last year we sold 6 million units and [we think as much as] half of those went to some kind of commercial use. S.C.: How did that evolution shape the design of the Pi 4? E.U.: We’re quite lucky in that all our markets have similar requirements. The things we do to make it a better toy make it a better industrial computer. And the things you do to make it a better industrial computer make it a better hobbyist platform. For example, we had a number of people building thin-client solutions, and the feedback was that most of the people doing that wanted the option of being able to deploy two monitors. So we added that feature. Another obvious example would be the serial interfaces. Prior generations are a little underprovisioned for things like UARTs, serial ports, SPIs, I2C interfaces, but because we were redesigning the silicon from scratch, putting UARTs in was pretty straightforward. S.C.: What other changes did you make in redesigning the Pi’s system processor? E.U.: The last three Pis have all been made using the same [40-nanometer] process, so modifications have been limited, mostly putting in a larger ARM [CPU]. If you look at the floor plan of the chip, each new ARM gets stuck on the left-hand side. The chip stretches horizontally and becomes bigger and bigger. But nothing on the right-hand side changes, and the right-hand side is where the memory controller is, where the UARTs are.... But putting in a larger ARM core means your power goes up and eventually you run out of your thermal budget.... We realized we needed to go to a 28-nm process node. And once you’re going to a new process node, you might as well fix all of the wrinkles. And that’s why you see two displays, many UARTs, PCI Express, Gigabit Ethernet. S.C.: How long before the slowdown in Moore’s Law affects the Pi? E.U.: I think it’s relevant to think in terms of how much is left. How much for a given thermal footprint? We’ve come a factor of 40 [in the Pi’s computing power from the first Pi to the Pi 4]. There’s not another factor of 40 left, which means you’ve come through more than half this process.... On some level that’s intimidating. But I’m a software engineer. It’s actually really nice to feel that we’re going into an era where software engineering makes a contribution again! My Ph.D. is in optimizing compilers, and while I was doing it I felt really depressed, because [improving software by a factor of two was small compared to the exponential hardware improvements of Moore’s Law]. But the thing about that factor of two is that it’s there at the end. There will come a time when they’ll be very grateful for it! This article appears in the September 2019 print issue as “How the Raspberry Pi Infiltrated Industry.” Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Computer Science,0.14106616671941055
7,ACM,"Faced with Data Deluge, Astronomers Turn to Automation",ACM,2019-08-21,-,https://www.scientificamerican.com/article/faced-with-data-deluge-astronomers-turn-to-automation/,"       Faced with Data Deluge, Astronomers Turn to Automation     Scientific AmericanAnil AnanthaswamyAugust 21, 2019   Researchers hope to use algorithms to improve multimessenger astronomy, and more precisely simulate evolutionary cosmic phenomena by automating certain discovery phases, and filtering massive datasets with optimal speed and efficiency. For example, signals of gravitational waves collected by the Laser Interferometer Gravitational-Wave Observatory must be matched by supercomputers against hundreds of thousands of templates of potential wave signatures. Scientists at the University of Illinois at Urbana-Champaign's National Center for Supercomputing Applications used convolutional neural networks (CNNs) for real-time detection/decryption of gravitational-wave signals. The team then scaled up the initiative with supercomputer-trained deep learning algorithms, which search through a larger series of parameters to identify overlooked signals. Meanwhile, Harvard University researchers developed a CNN to analyze x-ray images of galaxy clusters, and applied a technique allowing the network's observations to be visualized to give users a better idea of how the CNN was operating.                 ",Space & Time,0.13254010012532516
8,ACM,Ions Clear Another Hurdle Toward Scaled-Up Quantum Computing,ACM,2019-08-16,-,https://jqi.umd.edu/news/ions-clear-another-hurdle-toward-scaled-quantum-computing,"       Ions Clear Another Hurdle Toward Scaled-Up Quantum Computing     Joint Quantum InstituteE. EdwardsAugust 16, 2019   Researchers at the University of Maryland's Joint Quantum Institute (JQI) used laser pulses to simultaneously create quantum connections between different pairs of qubits, marking the first time these kinds of parallel operations have been executed in an ion trap. This breakthrough illustrates that advancing ion trap quantum processors is not limited by the physics of qubits, but rather tied to the engineering of their controllers. The new ion trap is made from gold-coated electrodes, which carry the electric fields that confine Ytterbium ions. The ions are caught in the middle of the trap where they form a line, each separated from its neighbor by just a few microns. This configuration allows researchers to control individual ions and set them up as qubits.                 ",Matter & Energy,0.12146471737881012
9,IEEE,New Double 3 Robot Makes Telepresence Easier than Ever,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever,"      Today, Double Robotics is announcing Double 3, the latest major upgrade to its line of consumer(ish) telepresence robots. We had a (mostly) fantastic time testing out Double 2 back in 2016. One of the things that we found out back then was that it takes a lot of practice to remotely drive the robot around. Double 3 solves this problem by leveraging the substantial advances in 3D sensing and computing that have taken place over the past few years, giving their new robot a level of intelligence that promises to make telepresence more accessible for everyone. Double 2’s iPad has been replaced by “a fully integrated solution”—which is a fancy way of saying a dedicated 9.7-inch touchscreen and a whole bunch of other stuff. That other stuff includes an NVIDIA Jetson TX2 AI computing module, a beamforming six-microphone array, an 8-watt speaker, a pair of 13-megapixel cameras (wide angle and zoom) on a tilting mount, five ultrasonic rangefinders, and most excitingly, a pair of Intel RealSense D430 depth sensors.  It’s those new depth sensors that really make Double 3 special. The D430 modules each uses a pair of stereo cameras with a pattern projector to generate 1280 x 720 depth data with a range of between 0.2 and 10 meters away. The Double 3 robot uses all of this high quality depth data to locate obstacles, but at this point, it still doesn’t drive completely autonomously. Instead, it presents the remote operator with a slick, augmented reality view of drivable areas in the form of a grid of dots. You just click where you want the robot to go, and it will skillfully take itself there while avoiding obstacles (including dynamic obstacles) and related mishaps along the way.  This effectively offloads the most stressful part of telepresence—not running into stuff—from the remote user to the robot itself, which is the way it should be. That makes it that much easier to encourage people to utilize telepresence for the first time. The way the system is implemented through augmented reality is particularly impressive, I think. It looks like it’s intuitive enough for an inexperienced user without being restrictive, and is a clever way of mitigating even significant amounts of lag.  Otherwise, Double 3’s mobility system is exactly the same as the one featured on Double 2. In fact, that you can stick a Double 3 head on a Double 2 body and it instantly becomes a Double 3. Double Robotics is thoughtfully offering this to current Double 2 owners as a significantly more affordable upgrade option than buying a whole new robot. For more details on all of Double 3  new features, we spoke with the co-founders of Double Robotics, Marc DeVidts and David Cann. IEEE Spectrum: Why use this augmented reality system instead of just letting the user click on a regular camera image? Why make things more visually complicated, especially for new users? Marc DeVidts and David Cann: One of the things that we realized about nine months ago when we got this whole thing working was that without the mixed reality for driving, it was really too magical of an experience for the customer. Even us—we had a hard time understanding whether the robot could really see obstacles and understand where the floor is and that kind of thing. So, we said “What would be the best way of communicating this information to the user?” And the right way to do it ended up drawing the graphics directly onto the scene. It’s really awesome—we have a full, real time 3D scene with the depth information drawn on top of it. We’re starting with some relatively simple graphics, and we’ll be adding more graphics in the future to help the user understand what the robot is seeing. How robust is the vision system when it comes to obstacle detection and avoidance? Does it work with featureless surfaces, IR absorbent surfaces, in low light, in direct sunlight, etc? We’ve looked at all of those cases, and one of the reasons that we’re going with the RealSense is the projector that helps us to see blank walls. We also found that having two sensors—one facing the floor and one facing forward—gives us a great coverage area. Having ultrasonic sensors in there as well helps us to detect anything that we can't see with the cameras. They're sort of a last safety measure, especially useful for detecting glass.  It seems like there’s a lot more that you could do with this sensing and mapping capability. What else are you working on? We're starting with this semi-autonomous driving variant, and we're doing a private beta of full mapping. So, we’re going to do full SLAM of your environment that will be mapped by multiple robots at the same time while you're driving, and then you'll be able to zoom out to a map and click anywhere and it will drive there. That  where we're going with it, but we want to take baby steps to get there. It  the obvious next step, I think, and there are a lot more possibilities there. Do you expect developers to be excited for this new mapping capability? We're using a very powerful computer in the robot, a NVIDIA Jetson TX2 running Ubuntu. There  room to grow. It’s actually really exciting to be able to see, in real time, the 3D pose of the robot along with all of the depth data that gets transformed in real time into one view that gives you a full map. Having all of that data and just putting those pieces together and getting everything to work has been a huge feat in of itself.  We have an extensive API for developers to do custom implementations, either for telepresence or other kinds of robotics research. Our system isn't running ROS, but we're going to be adding ROS adapters for all of our hardware components. Telepresence robots depend heavily on wireless connectivity, which is usually not something that telepresence robotics companies like Double have direct control over. Have you found that connectivity has been getting significantly better since you first introduced Double? When we started in 2013, we had a lot of customers that didn’t have WiFi in their hallways, just in the conference rooms. We very rarely hear about customers having WiFi connectivity issues these days. The bigger issue we see is when people are calling into the robot from home, where they don't have proper traffic management on their home network. The robot doesn't need a ton of bandwidth, but it does need consistent, low latency bandwidth. And so, if someone else in the house is watching Netflix or something like that, it’s going to saturate your connection. But for the most part, it’s gotten a lot better over the last few years, and it’s no longer a big problem for us. Do you think 5G will make a significant difference to telepresence robots? We’ll see. We like the low latency possibilities and the better bandwidth, but it  all going to be a matter of what kind of reception you get. LTE can be great, if you have good reception; it’s all about where the tower is. I’m pretty sure that WiFi is going to be the primary thing for at least the next few years. DeVidts also mentioned that an unfortunate side effect of the new depth sensors is that hanging a t-shirt on your Double to give it some personality will likely render it partially blind, so that  just something to keep in mind. To make up for this, you can switch around the colorful trim surrounding the screen, which is nowhere near as fun. When the Double 3 is ready for shipping in late September, US $2,000 will get you the new head with all the sensors and stuff, which seamlessly integrates with your Double 2 base. Buying Double 3 straight up (with the included charging dock) will run you $4,ooo. This is by no means an inexpensive robot, and my impression is that it’s not really designed for individual consumers. But for commercial, corporate, healthcare, or education applications, $4k for a robot as capable as the Double 3 is really quite a good deal—especially considering the kinds of use cases for which it’s ideal. [ Double Robotics ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.1138742701220733
10,IEEE,Blue Ocean Robotics Acquires Beam Telepresence Robot From Suitable Technologies,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot,"      Today, Blue Ocean Robotics, a Danish robotics company, is announcing the acquisition of Suitable Technologies’ Beam telepresence robot business. Blue Ocean has been a Beam partner for five years, but now they’re taking things over completely. The Beam robot began its life as an internal project within Willow Garage. It was spun out in 2012 as Suitable Technologies, which produced a couple different versions of the Beam. As telepresence platforms go, Beam is on the powerful and expensive side, designed primarily for commercial and enterprise customers.  The most recent news from Suitable was the introduction of the BeamPro 2, which was announced over a year ago at CES 2018. The Suitable Tech website still lists it as “coming soon,” and our guess is that it’s now up to Blue Ocean to decide whether to go forward with this new version. Blue Ocean calls itself a “robot venture factory.” I’m not entirely sure what a “robot venture factory” is but Blue Ocean describes itself thusly: The company is known for developing professional service robots from the problem, idea and design phase to the development, commercialization and scaling phase. Every robot is placed in its own subsidiary which is responsible for scaling sales, customer service, support and everything else oriented towards global markets and customers. The parent company handles all development and production of robots across the organization.  Ah, that explains it! Blue Ocean does already have a couple portfolio companies making very specific robots, including a UV disinfection robot for hospitals and a sort of mobile patient lift also for hospitals. They’re working on some kind of agriculture robot, too. I’d love to be able to tell you more, but the press release doesn’t offer much: With the acquisition, Blue Ocean Robotics sees an opportunity to generate additional synergy: “Our development of robots is based on our own in-house created toolbox with reusable technology components. This means that we can build all of our robots fast, inexpensively, and better than others,” says Blue Ocean Robotics’ CTO John Erland Østergaard. “Some of our robots, for example the UVD disinfection robot, are already equipped with remote controls. With the Beam technology being a big seller in the healthcare sector, we can continue to grow our business within this industry by having our distributors present both UVD and Beam when they visit customers.” The press release is very specific that Blue Ocean isn’t acquiring Suitable Technologies itself—they’re acquiring the “assets and rights associated with the robot Beam” from Suitable, which I guess means that Suitable is still around somehow. But it’s really not clear what Suitable is without Beam, which (as far as we can make out) is the entirety of what the company does. Anyway, we’re glad that there’s enough interest in high-end telepresence robots to support this acquisition, and we hope that Blue Ocean will be investing in BeamPro 2 and further generations of the robot. It’s come a long way from the original Texai robot from Willow Garage, and still has a lot of potential. For more information, visit the new Beam website that Blue Ocean has just launched. [ Beam ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11152792061413662
11,IEEE,All of the Winners in the DARPA Subterranean Challenge Tunnel Circuit,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit,"      The first competitive event in the DARPA Subterranean Challenge concluded last week—hopefully you were able to follow along on the livestream, on Twitter, or with some of the articles that we’ve posted about the event. We’ll have plenty more to say about how things went for the SubT teams, but while they take a bit of a (well earned) rest, we can take a look at the winning teams as well as who won DARPA’s special superlative awards for the competition.  With their rugged, reliable robots featuring giant wheels and the ability to drop communications nodes, Team Explorer was in the lead from day 1, scoring in double digits on every single run.  Team CoSTAR had one of the more diverse lineups of robots, and they switched up which robots they decided to send into the mine as they learned more about the course.   While many teams came to SubT with DARPA funding, Team CTU-CRAS was self-funded, making them eligible for a special $200,000 Tunnel Circuit prize.   DARPA also awarded a bunch of “superlative awards” after SubT: To score a point, teams had to submit the location of an artifact that was correct to within 5 meters of the artifact itself. However, DARPA was tracking the artifact locations with much higher precision—for example, the “zero” point on the backpack artifact was the center of the label on the front, which DARPA tracked to the millimeter. Team Explorer managed to return the location of a backpack with an error of just 0.18 meter, which is kind of amazing. With just an hour to find as many artifacts as possible, teams had to find the right balance between sending robots off to explore and bringing them back into communication range to download artifact locations. Team CSIRO Data61 cut their last point pretty close, sliding their final point in with a mere 22 seconds to spare.  Team Robotika had some of the quirkiest and most recognizable robots, which DARPA recognized with the “Most Distinctive” award. Robotika told us that part of the reason for that distinctiveness was practical—having a robot that was effectively in two parts meant that they could disassemble it so that it would fit in the baggage compartment of an airplane, very important for a team based in the Czech Republic. Kevin Knoedler, who won NASA’s Space Robotics Challenge entirely by himself, brought his own personal swarm of drones to SubT. With a ratio of seven robots to one human, Kevin was almost certainly the hardest working single human at the challenge. The Fan Favorite award went to the team that was most popular on Twitter (with the #SubTChallenge hashtag), and it may or may not be the case that I personally tweeted enough about Team NCTU’s blimp to win them this award. It’s also true that whenever we asked anyone on other teams what their favorite robot was (besides their own, of course), the blimp was overwhelmingly popular. So either way, the award is well deserved.  DARPA shared this little behind-the-scenes clip of the blimp in action (sort of), showing what happened to the poor thing when the mine ventilation system was turned on between runs and DARPA staff had to chase it down and rescue it: The thing to keep in mind about the results of the Tunnel Circuit is that unlike past DARPA robotics challenges (like the DRC), they don’t necessarily indicate how things are going to go for the Urban or Cave circuits because of how different things are going to be. Explorer did a great job with a team of rugged wheeled vehicles, which turned out to be ideal for navigating through mines, but they’re likely going to need to change things up substantially for the rest of the challenges, where the terrain will be much more complex. DARPA hasn’t provided any details on the location of the Urban Circuit yet; all we know is that it’ll be sometime in February 2020. This gives teams just six months to take all the lessons that they learned from the Tunnel Circuit and update their hardware, software, and strategies. What were those lessons, and what do teams plan to do differently next year? Check back next week, and we’ll tell you. [ DARPA SubT ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity.",Robotics,0.11048195193931849
12,Stanford,Water droplets spontaneously produce hydrogen peroxide,Science,2019-08-28,-,https://news.stanford.edu/2019/08/26/water-droplets-spontaneously-produce-hydrogen-peroxide/,"   Water is everywhere on Earth, but maybe that just gives it more space to hide its secrets. Its latest surprise, Stanford researchers report Aug. 26 in Proceedings of the National Academy of Sciences, is that microscopic droplets of water spontaneously produce hydrogen peroxide.  Chemistry Professor Richard Zare and his lab have shown that water microdroplets spontaneously – and unexpectedly – produce hydrogen peroxide. (Image credit: L.A. Cicero)  The discovery could pave the way for greener ways to produce the molecule, a common bleaching agent and disinfectant, said Richard Zare, the Marguerite Blake Wilbur Professor in Natural Science and a professor of chemistry in the Stanford School of Humanities and Sciences. “Water is one of the most commonly found materials, and it’s been studied for years and years and you would think that there was nothing more to learn about this molecule. But here’s yet another surprise,” said Zare, who is also a member of Stanford Bio-X. The discovery was made serendipitously while Zare and his lab were studying a new, more efficient way to create gold nanostructures in tiny water droplets known as microdroplets. To make those structures, the team added an additional molecule called a reducing agent. As a control test, Zare suggested seeing if they could create gold nanostructures without the reducing agent. Theoretically that should have been impossible, but it worked anyway – hinting at an as yet undiscovered feature of microdroplet chemistry. The team eventually traced those results to the presence of a molecule called hydroxyl – a single hydrogen atom paired with an oxygen atom – that can also act as a reducing agent. That equally unexpected result led Katherine Walker, at the time a graduate student in Zare’s lab, to wonder whether hydrogen peroxide – a molecule with two hydrogen and two oxygen atoms – was also present. To find out, Zare, Walker, staff scientist Jae Kyoo Lee and colleagues conducted a series of tests, the simplest of which involved spraying ostensibly pure water microdroplets onto a surface treated so that it would turn blue in the presence of hydrogen peroxide – and turn blue it did. (See video here.) Additional tests confirmed that water microdroplets spontaneously form hydrogen peroxide, that smaller microdroplets produced higher concentrations of the molecule, and that hydrogen peroxide was not lost when the microdroplets recombined into bulk water. The researchers ruled out a number of possible explanations before arriving at what they argue is the most likely explanation for hydrogen peroxide’s presence. They suggest that a strong electric field near the surface of water microdroplets in air triggers hydroxyl molecules to bind into hydrogen peroxide. Although the results are something of a basic science curiosity, Zare said, they could have important practical consequences. Hydrogen peroxide is an important commercial and industrial chemical, most often manufactured through an ecologically unfriendly process. The new discovery could help make those methods greener, Zare said, and it could lead to simpler ways to disinfect surfaces – simply spraying water microdroplets on a table or floor might be enough to clean it. “I think it could be one of the most important things I’ve ever done,” Zare said. Additional authors include Robert Waymouth, the Robert Eckles Swain Professor in Chemistry; Friedrich Prinz, the Finmeccanica Professor and a professor of mechanical engineering and of materials science and engineering; postdoctoral fellow Hyun Soo Han; and researchers from the Institute for Basic Science and the Daegu Gyeongbuk Institute of Science and Technology. Zare is also a member of the Cardiovascular Institute, the Stanford Cancer Institute, Stanford ChEM-H, the Stanford Woods Institute for the Environment and the Wu Tsai Neurosciences Institute. The research was funded in part by a grant from the U.S. Air Force Office of Scientific Research and the Institute for Basic Science, South Korea. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Electronics and Technology,0.10984760314147865
13,ACM,Computer Model Could Help Test Sickle Cell Drugs,ACM,2019-08-22,-,https://www.brown.edu/news/2019-08-22/sicklecell,"    Computer Model Could Help Test Sickle Cell Drugs     News from BrownKevin StaceyAugust 22, 2019   Brown University researchers have created a computer model to simulate the process by which sickle cell disease distorts red blood cells, which could be used in preclinical drug screening. The researchers created biophysical models of each stage of the sickling (deformation) of cells, including a simulated red blood cell function called OpenRBC, and a supercomputer model of sickle cell fiber formation. The end-product was a kinetic model of the sickling process, with supercomputer-derived data fed into a streamlined iteration enveloping key sickling dynamics, which can run on a laptop. The team demonstrated that the model could replicate the results of earlier experiments, both in the laboratory and in humans. The model allows users to enter the mode of action by which a drug is presumed to function, to view its potential effects on the cells.                 ",Health,0.10724662953609007
14,Stanford,New coating brings lithium metal battery closer to reality,Science,2019-08-28,-,https://news.stanford.edu/2019/08/26/new-coating-brings-lithium-metal-battery-closer-reality/,"   Hope has been restored for the rechargeable lithium metal battery – a potential battery powerhouse relegated for decades to the laboratory by its short life expectancy and occasional fiery demise while its rechargeable sibling, the lithium-ion battery, now rakes in more than $30 billion a year.  A new coating could make lightweight lithium metal batteries safe and long lasting, a boon for development of next-generation electric vehicles. (Image credit: Shutterstock)  A team of researchers at Stanford University and SLAC National Accelerator Laboratory has invented a coating that overcomes some of the battery’s defects, described in a paper published Aug. 26 in Joule. In laboratory tests, the coating significantly extended the battery’s life. It also dealt with the combustion issue by greatly limiting the tiny needlelike structures – or dendrites – that pierce the separator between the battery’s positive and negative sides. In addition to ruining the battery, dendrites can create a short circuit within the battery’s flammable liquid. Lithium-ion batteries occasionally have the same problem, but dendrites have been a non-starter for lithium metal rechargeable batteries to date. “We’re addressing the holy grail of lithium metal batteries,” said Zhenan Bao, a professor of chemical engineering, who is senior author of the paper along with Yi Cui, professor of materials science and engineering and of photon science at SLAC. Bao added that dendrites had prevented lithium metal batteries from being used in what may be the next generation of electric vehicles. The promise Lithium metal batteries can hold at least a third more power per pound as lithium-ion batteries do and are significantly lighter because they use lightweight lithium for the positively charged end rather than heavier graphite. If they were more reliable, these batteries could benefit portable electronics from notebook computers to cell phones, but the real pay dirt, Cui said, would be for cars. The biggest drag on electric vehicles is that their batteries spend about a fourth of their energy carrying themselves around. That gets to the heart of EV range and cost.  Lead authors and PhD students David Mackanic, left, and Zhiao Yu in front of their battery tester. Yu is holding a dish of already tested cells that they call the “battery graveyard.” (Image credit: Mark Golden)  “The capacity of conventional lithium-ion batteries has been developed almost as far as it can go,” said Stanford PhD student David Mackanic, co-lead author of the study. “So, it’s crucial to develop new kinds of batteries to fulfill the aggressive energy density requirements of modern electronic devices.” The team from Stanford and SLAC tested their coating on the positively charged end – called the anode – of a standard lithium metal battery, which is where dendrites typically form. Ultimately, they combined their specially coated anodes with other commercially available components to create a fully operational battery. After 160 cycles, their lithium metal cells still delivered 85 percent of the power that they did in their first cycle. Regular lithium metal cells deliver about 30 percent after that many cycles, rendering them nearly useless even if they don’t explode. The new coating prevents dendrites from forming by creating a network of molecules that deliver charged lithium ions to the electrode uniformly. It prevents unwanted chemical reactions typical for these batteries and also reduces a chemical buildup on the anode, which quickly devastates the battery’s ability to deliver power. “Our new coating design makes lithium metal batteries stable and promising for further development,” said the other co-lead author, Stanford PhD student Zhiao Yu. The group is now refining their coating design to increase capacity retention and testing cells over more cycles. “While use in electric vehicles may be the ultimate goal,” said Cui, “commercialization would likely start with consumer electronics to demonstrate the battery’s safety first.” Zhenan Bao and Yi Cui are also senior fellows at Stanford’s Precourt Institute for Energy. Other Stanford researchers include Jian Qin, assistant professor of chemical engineering; postdoctoral scholars Dawei Feng, Jiheong James Kang, Minah Lee, Chibueze Amanchukwu, Xuzhou Yan, Hansen Wang and Kai Liu; students Wesley Michaels, Allen Pei, Shucheng Chen and Yuchi Tsao; and visiting scholar Qiuhong Zhang from Nanjing University. This work was supported by the U.S. Department of Energy Office of Energy Efficiency & Renewable Energy. The facility used at Stanford is supported by the National Science Foundation. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. ",Matter & Energy,0.10041093629870013
15,ACM,Girls Now More Than 20% of Computing GCSE Entries,ACM,2019-08-22,-,https://www.computerweekly.com/news/252469008/Girls-now-account-for-more-than-20-computing-GCSE-entries,"    Girls Now More Than 20% of Computing GCSE Entries     ComputerWeekly.comClare McDonaldAugust 22, 2019   The number of girls in the U.K. choosing to take computing at the General Certificate of Secondary Education (GCSE) level increased from 15,046 in 2018 to 17,158 this year, with girls now making up 21.4% of U.K. GCSE computing entries. In higher-level grades in computing, girls outperformed boys, with 24.9% of girls receiving a 7/A grade in the subject compared to 20.8% of boys. Said Russ Shaw, founder of Tech London Advocates and Global Tech Advocates, ""What is particularly significant is the increase in girls studying STEM subjects, which paints an optimistic future for the tech industry, which has struggled to increase its gender diversity.""                 ",Society,0.09010895250535277
16,Stanford,Traditional fire management could help revitalize American Indian cultures,Science,2019-08-28,-,https://news.stanford.edu/2019/08/27/traditional-fire-management-help-revitalize-american-indian-cultures/,"   It costs more than a new iPhone XS, and it’s made out of hazelnut shrub stems. Traditional baby baskets of Northern California’s Yurok and Karuk tribes come at a premium not only because they are handcrafted by skilled weavers, but because the stems required to make them are found only in forest understory areas experiencing a type of controlled burn once practiced by the tribes but suppressed for more than a century.  Traditional tribal fire treatments can increase production of high-quality raw materials for baskets while reducing the danger of uncontrolled wildfires. (Image credit: Tony Marks-Block)  A new Stanford-led study with the U.S. Forest Service in collaboration with the Yurok and Karuk tribes found that incorporating traditional techniques into current fire suppression practices could help revitalize American Indian cultures, economies and livelihoods, while continuing to reduce wildfire risks. The findings could inform plans to incorporate the cultural burning practices into forest management across an area one and a half times the size of Rhode Island. “Burning connects many tribal members to an ancestral practice that they know has immense ecological and social benefit especially in the aftermath of industrial timber activity and ongoing economic austerity,” said study lead author Tony Marks-Block, a doctoral candidate in anthropology who worked with Lisa Curran, the Roger and Cynthia Lang Professor in Environmental Anthropology. “We must have fire in order to continue the traditions of our people,” said Margo Robbins, a Yurok basket weaver and director of the Yurok Cultural Fire Management Council who advised the researchers. “There is such a thing as good fire.” The study, published in Forest Ecology and Management, replicates Yurok and Karuk fire treatments that involve cutting and burning hazelnut shrub stems. The approach increased the production of high-quality stems (straight, unbranched and free of insect marks or bark blemishes) needed to make culturally significant items such as baby baskets and fish traps up to 10-fold compared with untreated shrubs.  Sisters Lillian Rentz and Janet Morehead of the Karuk Tribe examine recently harvested California hazel stems from a prescribed burn area.Credit: Frank K. Lake US Forest Service/Karuk Tribe (Image credit: Frank K. Lake US Forest Service/Karuk Tribe)  Previous studies have shown that repeated prescribed burning reduces fuel for wildfires, thus reducing their intensity and size in seasonally dry forests such as the one the researchers studied in the Klamath Basin area near the border with Oregon. This study was part of a larger exploration of prescribed burns being carried out by Stanford and U.S. Forest Service researchers who collaborated with the Yurok and Karuk tribes to evaluate traditional fire management treatments. Together, they worked with a consortium of federal and state agencies and nongovernmental organizations across 5,570 acres in the Klamath Basin. The consortium has proposed expanding these “cultural burns” – which have been greatly constrained throughout the tribes’ ancestral lands – across more than 1 million acres of federal and tribal lands that are currently managed with techniques including less targeted controlled burns or brush removal. Tribes traditionally burned specific plants or landscapes as a way of generating materials or spurring food production, as opposed to modern prescribed burns that are less likely to take these considerations into account. The authors argue that increasing the number of cultural burns could ease food insecurity among American Indian communities in the region. Traditional food sources have declined precipitously due in part to the suppression of prescribed burns that kill acorn-eating pests and promote deer populations by creating beneficial habitat and increasing plants’ nutritional content.  Nicholas Nix sleeps in a traditional baby basket woven out of hazelnut stems by his grandmother Margo Robbins of the Yurok Tribe. (Image credit: Margo Robbins)  “This study was founded upon tribal knowledge and cultural practices,” said co-author Frank Lake, a research ecologist with the U.S. Forest Service and a Karuk descendant with Yurok family. “Because of that, it can help us in formulating the best available science to guide fuels and fire management that demonstrate the benefit to tribal communities and society for reducing the risk of wildfires.” The researchers write that it would be easy and efficient to include traditional American Indian prescribed burning practices in existing forest management strategies. For example, federal fire managers could incorporate hazelnut shrub propane torching and pile burning into their fuel reduction plans to meet cultural needs. Managers would need to consult and collaborate with local tribes to plan these activities so that the basketry stems could be gathered post-treatment. Larger-scale pile burning treatments typically occur over a few days and require routine monitoring by forestry technicians to ensure they do not escape or harm nearby trees. As these burn, it would be easy for a technician to simultaneously use a propane torch to top-kill nearby hazelnut shrubs. This would not require a significant increase in personnel hours. “These are fires with a purpose, said Curran, who is also a senior fellow at the Stanford Woods Institute for the Environment. “Now that science has quantified and documented the effectiveness of these practices, fire managers and scientists have the information they need to collaborate with tribes to implement them on a large scale.” Marks-Block will teach a course at Stanford this fall on the socio-ecology of fire. It will include field trips to a prescribed fire site in the Santa Cruz mountains and tribal prescribed fire training exchanges in the Klamath Basin area of Northern California. To read all stories about Stanford science, subscribe to the biweekly Stanford Science Digest. The research was funded by the National Science Foundation, the U.S. Joint Fire Science Program, Stanford’s Department of Anthropology, the Stanford Office of the Vice Provost for Graduate Education’s Diversity Dissertation Research Opportunity and the Stanford School of Humanities and Sciences Community Engagement grant. ",Environment,0.07730696671632344
