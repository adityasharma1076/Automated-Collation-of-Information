Source,Heading,Category,Date,Time,URL,Text
IEEE,Eben Upton on the Raspberry Pi’s Industrial Crossover and Why There Will Never Be a Pi 9,Semiconductors,2019-08-28,-,https://spectrum.ieee.org/semiconductors/processors/eben-upton-on-the-raspberry-pis-industrial-crossover-and-why-there-will-never-be-a-pi-9,"      Seven years ago, Eben Upton created the first Raspberry Pi. As Upton told IEEE Spectrum in our March 2015 cover story, the Pi was inspired in part by his childhood experiments with a BBC Micro home computer: He wanted modern kids to have a simple machine that allowed for similar experimentation. Since then, the Pi has exploded in popularity, and the fourth major revision of the Pi was released in June. Upton talked with Spectrum senior editor Stephen Cass about the Pi 4’s design, its growing commercial use, and what might be next. Stephen Cass: How has the Pi’s user base evolved? Eben Upton: Our first year, our volume was almost entirely bought by hobbyists. But you have a lot of hobbyists who are also professional design engineers, and when their boss asked them to do something, often they used a Pi. So now you have people who are building industrial products around the Pi to resell. And then you have what we call, for want of a better word, DIY industrial, which is “I own a factory and I need control computers.” And where I might have historically gone and bought an embedded PC, I’ll buy a Pi. Last year we sold 6 million units and [we think as much as] half of those went to some kind of commercial use. S.C.: How did that evolution shape the design of the Pi 4? E.U.: We’re quite lucky in that all our markets have similar requirements. The things we do to make it a better toy make it a better industrial computer. And the things you do to make it a better industrial computer make it a better hobbyist platform. For example, we had a number of people building thin-client solutions, and the feedback was that most of the people doing that wanted the option of being able to deploy two monitors. So we added that feature. Another obvious example would be the serial interfaces. Prior generations are a little underprovisioned for things like UARTs, serial ports, SPIs, I2C interfaces, but because we were redesigning the silicon from scratch, putting UARTs in was pretty straightforward. S.C.: What other changes did you make in redesigning the Pi’s system processor? E.U.: The last three Pis have all been made using the same [40-nanometer] process, so modifications have been limited, mostly putting in a larger ARM [CPU]. If you look at the floor plan of the chip, each new ARM gets stuck on the left-hand side. The chip stretches horizontally and becomes bigger and bigger. But nothing on the right-hand side changes, and the right-hand side is where the memory controller is, where the UARTs are.... But putting in a larger ARM core means your power goes up and eventually you run out of your thermal budget.... We realized we needed to go to a 28-nm process node. And once you’re going to a new process node, you might as well fix all of the wrinkles. And that’s why you see two displays, many UARTs, PCI Express, Gigabit Ethernet. S.C.: How long before the slowdown in Moore’s Law affects the Pi? E.U.: I think it’s relevant to think in terms of how much is left. How much for a given thermal footprint? We’ve come a factor of 40 [in the Pi’s computing power from the first Pi to the Pi 4]. There’s not another factor of 40 left, which means you’ve come through more than half this process.... On some level that’s intimidating. But I’m a software engineer. It’s actually really nice to feel that we’re going into an era where software engineering makes a contribution again! My Ph.D. is in optimizing compilers, and while I was doing it I felt really depressed, because [improving software by a factor of two was small compared to the exponential hardware improvements of Moore’s Law]. But the thing about that factor of two is that it’s there at the end. There will come a time when they’ll be very grateful for it! This article appears in the September 2019 print issue as “How the Raspberry Pi Infiltrated Industry.” Seven years ago, Eben Upton created the first Raspberry Pi. As Upton told IEEE Spectrum in our March 2015 cover story, the Pi was inspired in part by his childhood experiments with a BBC Micro home computer: He wanted modern kids to have a simple machine that allowed for similar experimentation. Since then, the Pi has exploded in popularity, and the fourth major revision of the Pi was released in June. Upton talked with Spectrum senior editor Stephen Cass about the Pi 4’s design, its growing commercial use, and what might be next. Stephen Cass: How has the Pi’s user base evolved? Eben Upton: Our first year, our volume was almost entirely bought by hobbyists. But you have a lot of hobbyists who are also professional design engineers, and when their boss asked them to do something, often they used a Pi. So now you have people who are building industrial products around the Pi to resell. And then you have what we call, for want of a better word, DIY industrial, which is “I own a factory and I need control computers.” And where I might have historically gone and bought an embedded PC, I’ll buy a Pi. Last year we sold 6 million units and [we think as much as] half of those went to some kind of commercial use. S.C.: How did that evolution shape the design of the Pi 4? E.U.: We’re quite lucky in that all our markets have similar requirements. The things we do to make it a better toy make it a better industrial computer. And the things you do to make it a better industrial computer make it a better hobbyist platform. For example, we had a number of people building thin-client solutions, and the feedback was that most of the people doing that wanted the option of being able to deploy two monitors. So we added that feature. Another obvious example would be the serial interfaces. Prior generations are a little underprovisioned for things like UARTs, serial ports, SPIs, I2C interfaces, but because we were redesigning the silicon from scratch, putting UARTs in was pretty straightforward. S.C.: What other changes did you make in redesigning the Pi’s system processor? E.U.: The last three Pis have all been made using the same [40-nanometer] process, so modifications have been limited, mostly putting in a larger ARM [CPU]. If you look at the floor plan of the chip, each new ARM gets stuck on the left-hand side. The chip stretches horizontally and becomes bigger and bigger. But nothing on the right-hand side changes, and the right-hand side is where the memory controller is, where the UARTs are.... But putting in a larger ARM core means your power goes up and eventually you run out of your thermal budget.... We realized we needed to go to a 28-nm process node. And once you’re going to a new process node, you might as well fix all of the wrinkles. And that’s why you see two displays, many UARTs, PCI Express, Gigabit Ethernet. S.C.: How long before the slowdown in Moore’s Law affects the Pi? E.U.: I think it’s relevant to think in terms of how much is left. How much for a given thermal footprint? We’ve come a factor of 40 [in the Pi’s computing power from the first Pi to the Pi 4]. There’s not another factor of 40 left, which means you’ve come through more than half this process.... On some level that’s intimidating. But I’m a software engineer. It’s actually really nice to feel that we’re going into an era where software engineering makes a contribution again! My Ph.D. is in optimizing compilers, and while I was doing it I felt really depressed, because [improving software by a factor of two was small compared to the exponential hardware improvements of Moore’s Law]. But the thing about that factor of two is that it’s there at the end. There will come a time when they’ll be very grateful for it! This article appears in the September 2019 print issue as “How the Raspberry Pi Infiltrated Industry.” Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,New Double 3 Robot Makes Telepresence Easier than Ever,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever,"      Today, Double Robotics is announcing Double 3, the latest major upgrade to its line of consumer(ish) telepresence robots. We had a (mostly) fantastic time testing out Double 2 back in 2016. One of the things that we found out back then was that it takes a lot of practice to remotely drive the robot around. Double 3 solves this problem by leveraging the substantial advances in 3D sensing and computing that have taken place over the past few years, giving their new robot a level of intelligence that promises to make telepresence more accessible for everyone. Double 2’s iPad has been replaced by “a fully integrated solution”—which is a fancy way of saying a dedicated 9.7-inch touchscreen and a whole bunch of other stuff. That other stuff includes an NVIDIA Jetson TX2 AI computing module, a beamforming six-microphone array, an 8-watt speaker, a pair of 13-megapixel cameras (wide angle and zoom) on a tilting mount, five ultrasonic rangefinders, and most excitingly, a pair of Intel RealSense D430 depth sensors.  It’s those new depth sensors that really make Double 3 special. The D430 modules each uses a pair of stereo cameras with a pattern projector to generate 1280 x 720 depth data with a range of between 0.2 and 10 meters away. The Double 3 robot uses all of this high quality depth data to locate obstacles, but at this point, it still doesn’t drive completely autonomously. Instead, it presents the remote operator with a slick, augmented reality view of drivable areas in the form of a grid of dots. You just click where you want the robot to go, and it will skillfully take itself there while avoiding obstacles (including dynamic obstacles) and related mishaps along the way.  This effectively offloads the most stressful part of telepresence—not running into stuff—from the remote user to the robot itself, which is the way it should be. That makes it that much easier to encourage people to utilize telepresence for the first time. The way the system is implemented through augmented reality is particularly impressive, I think. It looks like it’s intuitive enough for an inexperienced user without being restrictive, and is a clever way of mitigating even significant amounts of lag.  Otherwise, Double 3’s mobility system is exactly the same as the one featured on Double 2. In fact, that you can stick a Double 3 head on a Double 2 body and it instantly becomes a Double 3. Double Robotics is thoughtfully offering this to current Double 2 owners as a significantly more affordable upgrade option than buying a whole new robot. For more details on all of Double 3  new features, we spoke with the co-founders of Double Robotics, Marc DeVidts and David Cann. IEEE Spectrum: Why use this augmented reality system instead of just letting the user click on a regular camera image? Why make things more visually complicated, especially for new users? Marc DeVidts and David Cann: One of the things that we realized about nine months ago when we got this whole thing working was that without the mixed reality for driving, it was really too magical of an experience for the customer. Even us—we had a hard time understanding whether the robot could really see obstacles and understand where the floor is and that kind of thing. So, we said “What would be the best way of communicating this information to the user?” And the right way to do it ended up drawing the graphics directly onto the scene. It’s really awesome—we have a full, real time 3D scene with the depth information drawn on top of it. We’re starting with some relatively simple graphics, and we’ll be adding more graphics in the future to help the user understand what the robot is seeing. How robust is the vision system when it comes to obstacle detection and avoidance? Does it work with featureless surfaces, IR absorbent surfaces, in low light, in direct sunlight, etc? We’ve looked at all of those cases, and one of the reasons that we’re going with the RealSense is the projector that helps us to see blank walls. We also found that having two sensors—one facing the floor and one facing forward—gives us a great coverage area. Having ultrasonic sensors in there as well helps us to detect anything that we can't see with the cameras. They're sort of a last safety measure, especially useful for detecting glass.  It seems like there’s a lot more that you could do with this sensing and mapping capability. What else are you working on? We're starting with this semi-autonomous driving variant, and we're doing a private beta of full mapping. So, we’re going to do full SLAM of your environment that will be mapped by multiple robots at the same time while you're driving, and then you'll be able to zoom out to a map and click anywhere and it will drive there. That  where we're going with it, but we want to take baby steps to get there. It  the obvious next step, I think, and there are a lot more possibilities there. Do you expect developers to be excited for this new mapping capability? We're using a very powerful computer in the robot, a NVIDIA Jetson TX2 running Ubuntu. There  room to grow. It’s actually really exciting to be able to see, in real time, the 3D pose of the robot along with all of the depth data that gets transformed in real time into one view that gives you a full map. Having all of that data and just putting those pieces together and getting everything to work has been a huge feat in of itself.  We have an extensive API for developers to do custom implementations, either for telepresence or other kinds of robotics research. Our system isn't running ROS, but we're going to be adding ROS adapters for all of our hardware components. Telepresence robots depend heavily on wireless connectivity, which is usually not something that telepresence robotics companies like Double have direct control over. Have you found that connectivity has been getting significantly better since you first introduced Double? When we started in 2013, we had a lot of customers that didn’t have WiFi in their hallways, just in the conference rooms. We very rarely hear about customers having WiFi connectivity issues these days. The bigger issue we see is when people are calling into the robot from home, where they don't have proper traffic management on their home network. The robot doesn't need a ton of bandwidth, but it does need consistent, low latency bandwidth. And so, if someone else in the house is watching Netflix or something like that, it’s going to saturate your connection. But for the most part, it’s gotten a lot better over the last few years, and it’s no longer a big problem for us. Do you think 5G will make a significant difference to telepresence robots? We’ll see. We like the low latency possibilities and the better bandwidth, but it  all going to be a matter of what kind of reception you get. LTE can be great, if you have good reception; it’s all about where the tower is. I’m pretty sure that WiFi is going to be the primary thing for at least the next few years. DeVidts also mentioned that an unfortunate side effect of the new depth sensors is that hanging a t-shirt on your Double to give it some personality will likely render it partially blind, so that  just something to keep in mind. To make up for this, you can switch around the colorful trim surrounding the screen, which is nowhere near as fun. When the Double 3 is ready for shipping in late September, US $2,000 will get you the new head with all the sensors and stuff, which seamlessly integrates with your Double 2 base. Buying Double 3 straight up (with the included charging dock) will run you $4,ooo. This is by no means an inexpensive robot, and my impression is that it’s not really designed for individual consumers. But for commercial, corporate, healthcare, or education applications, $4k for a robot as capable as the Double 3 is really quite a good deal—especially considering the kinds of use cases for which it’s ideal. [ Double Robotics ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Blue Ocean Robotics Acquires Beam Telepresence Robot From Suitable Technologies,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot,"      Today, Blue Ocean Robotics, a Danish robotics company, is announcing the acquisition of Suitable Technologies’ Beam telepresence robot business. Blue Ocean has been a Beam partner for five years, but now they’re taking things over completely. The Beam robot began its life as an internal project within Willow Garage. It was spun out in 2012 as Suitable Technologies, which produced a couple different versions of the Beam. As telepresence platforms go, Beam is on the powerful and expensive side, designed primarily for commercial and enterprise customers.  The most recent news from Suitable was the introduction of the BeamPro 2, which was announced over a year ago at CES 2018. The Suitable Tech website still lists it as “coming soon,” and our guess is that it’s now up to Blue Ocean to decide whether to go forward with this new version. Blue Ocean calls itself a “robot venture factory.” I’m not entirely sure what a “robot venture factory” is but Blue Ocean describes itself thusly: The company is known for developing professional service robots from the problem, idea and design phase to the development, commercialization and scaling phase. Every robot is placed in its own subsidiary which is responsible for scaling sales, customer service, support and everything else oriented towards global markets and customers. The parent company handles all development and production of robots across the organization.  Ah, that explains it! Blue Ocean does already have a couple portfolio companies making very specific robots, including a UV disinfection robot for hospitals and a sort of mobile patient lift also for hospitals. They’re working on some kind of agriculture robot, too. I’d love to be able to tell you more, but the press release doesn’t offer much: With the acquisition, Blue Ocean Robotics sees an opportunity to generate additional synergy: “Our development of robots is based on our own in-house created toolbox with reusable technology components. This means that we can build all of our robots fast, inexpensively, and better than others,” says Blue Ocean Robotics’ CTO John Erland Østergaard. “Some of our robots, for example the UVD disinfection robot, are already equipped with remote controls. With the Beam technology being a big seller in the healthcare sector, we can continue to grow our business within this industry by having our distributors present both UVD and Beam when they visit customers.” The press release is very specific that Blue Ocean isn’t acquiring Suitable Technologies itself—they’re acquiring the “assets and rights associated with the robot Beam” from Suitable, which I guess means that Suitable is still around somehow. But it’s really not clear what Suitable is without Beam, which (as far as we can make out) is the entirety of what the company does. Anyway, we’re glad that there’s enough interest in high-end telepresence robots to support this acquisition, and we hope that Blue Ocean will be investing in BeamPro 2 and further generations of the robot. It’s come a long way from the original Texai robot from Willow Garage, and still has a lot of potential. For more information, visit the new Beam website that Blue Ocean has just launched. [ Beam ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,All of the Winners in the DARPA Subterranean Challenge Tunnel Circuit,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit,"      The first competitive event in the DARPA Subterranean Challenge concluded last week—hopefully you were able to follow along on the livestream, on Twitter, or with some of the articles that we’ve posted about the event. We’ll have plenty more to say about how things went for the SubT teams, but while they take a bit of a (well earned) rest, we can take a look at the winning teams as well as who won DARPA’s special superlative awards for the competition.  With their rugged, reliable robots featuring giant wheels and the ability to drop communications nodes, Team Explorer was in the lead from day 1, scoring in double digits on every single run.  Team CoSTAR had one of the more diverse lineups of robots, and they switched up which robots they decided to send into the mine as they learned more about the course.   While many teams came to SubT with DARPA funding, Team CTU-CRAS was self-funded, making them eligible for a special $200,000 Tunnel Circuit prize.   DARPA also awarded a bunch of “superlative awards” after SubT: To score a point, teams had to submit the location of an artifact that was correct to within 5 meters of the artifact itself. However, DARPA was tracking the artifact locations with much higher precision—for example, the “zero” point on the backpack artifact was the center of the label on the front, which DARPA tracked to the millimeter. Team Explorer managed to return the location of a backpack with an error of just 0.18 meter, which is kind of amazing. With just an hour to find as many artifacts as possible, teams had to find the right balance between sending robots off to explore and bringing them back into communication range to download artifact locations. Team CSIRO Data61 cut their last point pretty close, sliding their final point in with a mere 22 seconds to spare.  Team Robotika had some of the quirkiest and most recognizable robots, which DARPA recognized with the “Most Distinctive” award. Robotika told us that part of the reason for that distinctiveness was practical—having a robot that was effectively in two parts meant that they could disassemble it so that it would fit in the baggage compartment of an airplane, very important for a team based in the Czech Republic. Kevin Knoedler, who won NASA’s Space Robotics Challenge entirely by himself, brought his own personal swarm of drones to SubT. With a ratio of seven robots to one human, Kevin was almost certainly the hardest working single human at the challenge. The Fan Favorite award went to the team that was most popular on Twitter (with the #SubTChallenge hashtag), and it may or may not be the case that I personally tweeted enough about Team NCTU’s blimp to win them this award. It’s also true that whenever we asked anyone on other teams what their favorite robot was (besides their own, of course), the blimp was overwhelmingly popular. So either way, the award is well deserved.  DARPA shared this little behind-the-scenes clip of the blimp in action (sort of), showing what happened to the poor thing when the mine ventilation system was turned on between runs and DARPA staff had to chase it down and rescue it: The thing to keep in mind about the results of the Tunnel Circuit is that unlike past DARPA robotics challenges (like the DRC), they don’t necessarily indicate how things are going to go for the Urban or Cave circuits because of how different things are going to be. Explorer did a great job with a team of rugged wheeled vehicles, which turned out to be ideal for navigating through mines, but they’re likely going to need to change things up substantially for the rest of the challenges, where the terrain will be much more complex. DARPA hasn’t provided any details on the location of the Urban Circuit yet; all we know is that it’ll be sometime in February 2020. This gives teams just six months to take all the lessons that they learned from the Tunnel Circuit and update their hardware, software, and strategies. What were those lessons, and what do teams plan to do differently next year? Check back next week, and we’ll tell you. [ DARPA SubT ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
