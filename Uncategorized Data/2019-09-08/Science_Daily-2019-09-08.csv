Source,Heading,Category,Date,Time,URL,Text
Science Daily,Teens Who Don't Date Are Less Depressed and Have Better Social Skills,Mind & Brain,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906134007.htm,"   Yet new research from the University of Georgia has found that not dating can be an equally beneficial choice for teens. And in some ways, these teens fared even better. The study, published online in The Journal of School Health, found that adolescents who were not in romantic relationships during middle and high school had good social skills and low depression, and fared better or equal to peers who dated. ""The majority of teens have had some type of romantic experience by 15 to 17 years of age, or middle adolescence,"" said Brooke Douglas, a doctoral student in health promotion at UGA's College of Public Health and the study's lead author. ""This high frequency has led some researchers to suggest that dating during teenage years is a normative behavior. That is, adolescents who have a romantic relationship are therefore considered 'on time' in their psychological development."" If dating was considered normal and essential for a teen's individual development and well-being, Douglas began to wonder what this suggested about adolescents who chose not to date. ""Does this mean that teens that don't date are maladjusted in some way? That they are social misfits? Few studies had examined the characteristics of youth who do not date during the teenage years, and we decided we wanted to learn more,"" she said. To do this, Douglas and study co-author Pamela Orpinas examined whether 10th grade students who reported no or very infrequent dating over a seven-year period differed on emotional and social skills from their more frequently dating peers. They analyzed data collected during a 2013 study led by Orpinas, which followed a cohort of adolescents from Northeast Georgia from sixth through 12th grade. Each spring, students indicated whether they had dated, and reported on a number of social and emotional factors, including positive relationships with friends, at home, and at school, symptoms of depression, and suicidal thoughts. Their teachers completed questionnaires rating each student's behavior in areas that included social skills, leadership skills and levels of depression. Non-dating students had similar or better interpersonal skills than their more frequently dating peers. While the scores of self-reported positive relationships with friends, at home, and at school did not differ between dating and non-dating peers, teachers rated the non-dating students significantly higher for social skills and leadership skills than their dating peers. Students who didn't date were also less likely to be depressed. Teachers' scores on the depression scale were significantly lower for the group that reported no dating. Additionally, the proportion of students who self-reported being sad or hopeless was significantly lower within this group as well. ""In summary, we found that non-dating students are doing well and are simply following a different and healthy developmental trajectory than their dating peers,"" said Orpinas, a professor of health promotion and behavior. ""While the study refutes the notion of non-daters as social misfits, it also calls for health promotion interventions at schools and elsewhere to include non-dating as an option for normal, healthy development,"" said Douglas. ""As public health professionals, we can do a better job of affirming that adolescents do have the individual freedom to choose whether they want to date or not, and that either option is acceptable and healthy,"" she said. "
Science Daily,How Do We Get So Many Different Types of Neurons in Our Brain?,Mind & Brain,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906104058.htm,"   Neurons are cells inside the brain and nervous system that are responsible for everything we do, think or feel. They use electrical impulses and chemical signals to send information between different areas of the brain, and between the brain and the rest of the nervous system, to tell our body what to do. Humans have approximately 86 billion neurons in the brain that direct us to do things like lift an arm or remember a name. Yet only a few thousand genes are responsible for creating those neurons. All cells in the human nervous system have the same genetic information. But ultimately, genes are turned ""on"" or ""off"" like a light switch to give neurons specific features and roles. Understanding the mechanism of how a gene is or is not turned on -- the process known as gene expression -- could help explain how so many neurons are developed in humans and other mammals. ""Studies like this are showing how by unique combinations of specific genes, you can make different specific neurons,"" said Adam D. Norris, co-author of the new study and Floyd B. James Assistant Professor in the Department of Biological Sciences at SMU. ""So down the road, this could help us explain: No. 1, how did our brain get this complex? And No. 2, how can we imitate nature and make whatever type of neurons we might be interested in following these rules?"" Scientists already have part of the gene expression puzzle figured out, as previous studies have shown that proteins called transcription factors play a key role in helping to turn specific genes on or off by binding to nearby DNA. It is also known that a process called RNA splicing, which is controlled by RNA binding proteins, can add an additional layer of regulation to that neuron. Once a gene is turned on, different versions of the RNA molecule can be created by RNA splicing. But before the SMU study was done, which was published in the journal eLife, it was not exactly clear what the logistics of creating that diversity was. ""Before this, scientists had mostly been focused on transcription factors, which is layer No. 1 of gene expression. That's the layer that usually gets focused on as generating specific neuron types,"" Norris said. ""We're adding that second layer and showing that [transcription factors and RNA binding proteins] have to be coordinated properly. And Norris noted, ""this was the first time where coordination of gene expression has been identified in a single neuron."" Using a combination of old school and cutting-edge genetics techniques, researchers looked at how the RNA of a gene called sad-1, also found in humans, was spliced in individual neurons of the worm Caenorhabditis elegans. They found that sad-1 was turned on in all neurons, but sad-1 underwent different splicing patterns in different neuron types. And while transcription factors were not shown to be directly participating in the RNA splicing for the sad-1 gene, they were activating genes that code for RNA binding proteins differently between different types of neurons. It is these RNA binding proteins that control RNA splicing. ""Once that gene was turned on, these factors came in and subtly changed the content of that gene,"" Norris said. As a result, sad-1 was spliced according to neuron-specific patterns. They also found that the coordinated regulation had different details in different neurons. ""Picture two different neurons wanting to reach the same goal. You can imagine they either go through the exact same path to get there or they take divergent paths. In this study, we're showing that the answer so far is divergent paths,"" said Norris. ""Even in a single neuron, there are multiple different layers of gene expression that together make that neuron the unique neuron that it is."" Norris used worm neurons because ""unlike in humans, we know where every worm neuron is and what it should be up to. Therefore, we can very confidently know which genes are responsible for which neural process. ""The very specific details from this study will not apply to humans. But hopefully the principles involved will,"" Norris explained. ""From the last few decades of work in the worm nervous system, specific genes found to have a specific effect on the worm's behavior were later shown to be responsible for the same types of things in human nerves."" "
Science Daily,How Our Brain Filters Sounds,Mind & Brain,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906092755.htm,"   Our sound environment is extremely dense, which is why the brain has to adapt and implement filtering mechanisms that allow it to hold its attention on the most important elements and save energy. When two identical sounds are repeated quickly, one of these filters -- called auditory sensory gating -- drastically reduces the attention that the brain directs to the second sound it hears. In people with schizophrenia, this ability to reduce the brain's response to identical sounds does not function properly. The brain, it seems, is constantly assailed by a multitude of auditory stimuli, which disrupt its attentional capacity. But the question is: Why? Neuroscientists from the University of Geneva (UNIGE), Switzerland, have been investigating the mechanism that lies behind this auditory sensory gating, which was previously unknown. Their results, published in the journal eNeuro, show that the filtering begins at the very beginning of the auditory stimuli processing, i. e. in the brainstem. This finding runs counter to earlier hypotheses, which held that it was a function of the frontal cortex control, which is heavily impacted in schizophrenics. One of the main characteristics of schizophrenia, which affects 0.5% of the population, is a difficulty in prioritising and ranking surrounding sounds, which then assail the individual. This is why schizophrenia is diagnosed using a simple test: the P50. ""The aim is to have the patient hear two identical sounds spaced 500 milliseconds apart. We then measure the brain activity in response to these two sounds using an external encephalogram,"" explains Charles Quairiaux, a researcher in the Department of Basic Neurosciences in UNIGE's Faculty of Medicine. ""If brain activity decreases drastically when listening to the second sound, everything is okay. But if it's almost identical, then that's one of the best-known symptoms of schizophrenia."" Although widely used to perform such diagnostics, the functioning of this filtering mechanism -- called auditory sensory gating -- is still a mystery. Most hypotheses held that this brain property is provided by a frontal cortex control, located at the front of the brain. ""This area of control is badly affected in people suffering from schizophrenia, and it's situated at the end of the brain's sound processing pathway,"" explains Dr Quairiaux. The failure is situated at the base of sound processing In order to test this hypothesis, the Geneva-based neuroscientists placed external electroencephalographic electrodes on mice, which were then subjected to the P50 test, varying the intervals between the two sounds from 125 milliseconds to 2 seconds. The results proved to be exactly the same as those observed in humans: there was a clear decrease in brain activity when listening to the second sound. The scientists then placed internal electrodes in the cortical and subcortical auditory regions of the brain, from the brainstem to the frontal cortex -- the pathway for processing sounds. The mice were given the P50 test a second time and, contrary to the initial hypothesis formulated by the scientists, the researchers observed that the drop in attention given to the second sound had already occurred at the brainstem and not only at the cortical level, with a 60% decrease in brain activity. ""This discovery means we're going to have to reconsider our understanding of the mechanism, because it demonstrates that the filter effect begins at the very moment when the brain perceives the sound!"" points out Dr Quairiaux. And where does this leave people suffering from schizophrenia? ""We're currently carrying out the same study on mice with 22q11 deletion syndrome, a mutation that often leads to schizophrenia in humans, so we can see if the lack of a filter is situated in the brainstem, taking account of the new results we obtained,"" continues the researcher. And that does indeed seem to be the case! The first tests on ""schizophrenic"" mice revealed the total absence of a filter for the second sound at the brain stem. The source of one of the most common symptoms of schizophrenia is about to be discovered. "
Science Daily,Bad to the Bone or Just Bad Behavior?,Mind & Brain,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906092749.htm,"   A new study out of Columbia University suggests that the way we perceive others' bad behavior -- as either biological and innate or potentially changeable -- impacts our willingness to cut them some slack. The study, published in the Journal of Experimental Psychology: General, found that adults are less willing to be charitable toward ""bad"" individuals whose moral characteristics are attributed to an innate biological source. Conversely, adults are more apt to be generous toward individuals when led to focus on other explanations for moral ""badness"" that suggest potential for change. Unlike adults, children did not appear to distinguish between characters whose moral characteristics were described in different ways. The findings may have implications for how we perceive individuals in society, such as those imprisoned for crimes. ""If people want to take something away from this study and apply it to their own lives, it is to be mindful of how they talk about others and their transgressions,"" said Larisa Heiphetz, an assistant professor of psychology and the study's principal investigator. ""People often encounter moral transgressions, whether in others' behaviors or their own. This study reveals that the way we treat those individuals can be strongly influenced by the way others describe their transgressions."" Heiphetz's research also revealed that a person's ""goodness"" was seen by both age groups as more of a biological, innate trait than ""badness."" Both children and adults were more likely to say that goodness, rather than badness, was something with which people are born and a fundamental, unchanging part of who they are. The study, funded by Columbia University, the Indiana University Lilly School of Philanthropy and the John Templeton Foundation, is one in a growing area of research focused on psychological essentialism -- how we think about people's characteristics in essentialist terms (e.g., innate, immutable and due to biological factors) or non-essentialist terms (socially learned, changeable). Prior work has shown that people readily attribute many human characteristics to innate, unchanging factors. To learn how people perceive moral goodness and moral badness, Heiphetz and a group of Columbia students asked children and adults what they thought about a variety of morally good and morally bad characteristics. They found that both groups perceived ""goodness"" as a more central, unchanging feature of who someone is than badness, which was more likely to be perceived as something that can improve over time. That led Heiphetz to wonder if there were any consequences associated with this perception, so she gave children and adults material resources, including stickers and entries to a lottery, and told them about pairs of fictional people that had the same ""bad"" moral characteristics, but for different reasons: One was described in an essentialist way -- born bad -- and the other in a non-essentialist way -- bad as a result of behavior they learned from other people in their lives. When study participants were asked to share their possessions with the characters, the children shared equally but adults shared more resources with the character described as bad due to learned behavior, with the potential to change. When study participants were then told that neither of the fictional characters -- whether born bad or having learned the behavior -- would ever change for the better, adults still shared more resources with the character who had been described in the non-essentialist way, as having learned the behavior. Words, as this study shows, have impact. "
Science Daily,"More Learning in 'Active Learning' Classrooms, but Students Don't Know It",Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905171810.htm,"   And a new Harvard study suggests it may be important to let students know it. The study, published September 4 in the Proceedings of the National Academy of Sciences, shows that, though students felt like they learned more through traditional lectures, they actually learned more when taking part in active learning classrooms. The lead author Louis Deslauriers, Director of Science Teaching and Learning and senior Physics preceptor, knew that students would learn more from active learning. He published a key study in Science in 2011 that showed just that. But many students and faculty remained hesitant to switch to active learning. ""Often, students seemed genuinely to prefer smooth-as-silk traditional lectures,"" Deslauriers said. ""We wanted to take them at their word. Perhaps they actually felt like they learned more from lectures than they did from active learning."" In addition to Deslauriers, the study is authored by Director of Science Education and Lecturer on Physics Logan McCarty, senior preceptor in Applied Physics Kelly Miller, preceptor in Physics Greg Kestin, and Kristina Callaghan, now a lecturer in Physics at the University of California, Merced. The question of whether students' perceptions of their learning matches with their actual learning is particularly important, Deslauriers said, because though students eventually see the value of active learning, it can initially feel frustrating. ""Deep learning is hard work. The effort involved in active learning can be misinterpreted as a sign of poor learning,"" he said. ""On the other hand, a superstar lecturer can explain things in such a way as to make students feel like they are learning more than they actually are."" To understand that dichotomy, Deslauriers and his co-authors designed an experiment that would expose students in an introductory physics class to both traditional lectures and active learning. For the first 11 weeks of the 15-week class, students were taught using standard methods by an experienced instructor. In the 12th week, though, things changed -- half the class was randomly assigned to a classroom that used active learning, while the other half attended highly polished lectures. In a subsequent class, the two groups were reversed. Notably, both groups used identical class content and only active engagement with the material was toggled on and off. Following each class, students were surveyed on how much they agreed or disagreed with statements like ""I feel like I learned a lot from this lecture"" and ""I wish all my physics courses were taught this way."" Students were also tested on how much they learned in the class with 12 multiple choice questions. When the results were tallied, the authors found that students felt like they learned more from the lectures, but in fact scored higher on tests following the active learning sessions. ""Actual learning and feeling of learning were strongly anticorrelated,"" Deslauriers said, ""as shown through the robust statistical analysis by co-author Kelly Miller, who is an expert in educational statistics and active learning."" But those results, the study authors warned, shouldn't be interpreted as suggesting students dislike active learning. In fact many studies have shown students quickly warm to the idea, once they begin to see the results. ""In all the courses at Harvard that we've transformed to active learning,"" Deslauriers said, ""the overall course evaluations went up."" ""It can be tempting to engage the class simply by folding lectures into a compelling 'story,' especially when that's what students seem to like,"" said Kestin, a co-author of the study, who is a physicist and a video producer with NOVA | PBS. ""I show my students the data from this study on the first day of class to help them appreciate the importance of their own involvement in active learning."" McCarty, who oversees curricular efforts across the sciences, hopes this study will encourage more faculty colleagues to use active learning in their courses. ""We want to make sure that other instructors are thinking hard about the way they're teaching,"" he said. ""In our classes, we start each topic by asking students to gather in small groups to solve some problems. While they work, we walk around the room to observe them and answer questions. Then we come together and give a short lecture targeted specifically at the misconceptions and struggles we saw during the problem-solving activity. So far we've transformed over a dozen classes to use this kind of active learning approach. It's extremely efficient -- we can cover just as much material as we would using lectures."" A pioneer in work on active learning, Professor of Physics Eric Mazur hailed the study as debunking long-held beliefs about how students learn. ""This work unambiguously debunks the illusion of learning from lectures,"" he said. ""It also explains why instructors and students cling to the belief that listening to lectures constitutes learning. I recommend every lecturer reads this article."" The work also earned accolades from Dean of Science Christopher Stubbs, Professor of Physics and of Astronomy, who was an early convert to this style of active learning. ""When I first switched to teaching using active learning, some students resisted that change,"" he said. ""This research confirms that faculty should persist and encourage active learning. Active engagement in every classroom, led by our incredible science faculty, should be the hallmark of residential undergraduate education at Harvard."" Ultimately, Deslauriers said, the study shows that it's important to ensure that both instructors and students aren't fooled into thinking that lectures -- even well-presented ones -- are the best learning option. ""A great lecture can get students to feel like they are learning a lot,"" he said. ""Students might give fabulous evaluations to an amazing lecturer based on this feeling of learning, even though their actual learning isn't optimal. This could help to explain why study after study shows that student evaluations seem to be completely uncorrelated with actual learning."" This research was supported with funding from the Harvard FAS Division of Science. "
Science Daily,High Blood Pressure Treatment May Slow Cognitive Decline,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905161414.htm,"   The findings are important because high blood pressure and cognitive decline are two of the most common conditions associated with aging, and more people are living longer worldwide. According to the American Heart Association's 2017 Hypertension Guidelines, high blood pressure is a global health threat, affecting approximately 80 million U.S. adults and one billion people globally. Moreover, the relationship between brain health and high blood pressure is a growing interest as researchers examine how elevated blood pressure affects the brain's blood vessels, which in turn, may impact memory, language and thinking skills. In this observational study, researchers from Columbia University analyzed data collected on nearly 11,000 adults from the China Health and Retirement Longitudinal Study (CHARLS) between 2011-2015, to assess how high blood pressure and its treatment may influence cognitive decline. High blood pressure was defined as having a systolic blood pressure of 140 mmHg or higher and a diastolic blood pressure of 90 mmHg or higher, and/or taking antihypertensive medications. (Note: The American Heart Association guidelines define high blood pressure as 130 mmHg or higher or a diastolic reading of 80 mmHg or higher.) Researchers in China interviewed study participants at home about their high blood pressure treatment, education level and noted if they lived in a rural or urban environment. They were also asked to perform cognitive tests, such as immediately recalling words as part of a memory quiz. Among the study's findings: Overall cognition scores declined over the four-year study; Participants ages 55 and older who had high blood pressure showed a more rapid rate of cognitive decline compared with participants who were being treated for high blood pressure and those who did not have high blood pressure; and The rate of cognitive decline was similar between those receiving high blood pressure treatment and those who did not have high blood pressure. The study did not evaluate why or how high blood pressure treatments may have contributed to slower cognitive decline or if some treatments were more effective than others. ""We think efforts should be made to expand high blood pressure screenings, especially for at-risk populations, because so many people are not aware that they have high blood pressure that should be treated,"" said presenting study author Shumin Rui, a biostatistician at the Mailman School of Public Health, Columbia University in New York. ""This study focused on middle-aged and older adults in China, however, we believe our results could apply to populations elsewhere as well. We need to better understand how high blood pressure treatments may protect against cognitive decline and look at how high blood pressure and cognitive decline are occurring together."" "
Science Daily,"MouseLight Project Maps 1,000 Neurons (and Counting) in the Mouse Brain",Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111640.htm,"   Researchers at the Howard Hughes Medical Institute's Janelia Research Campus have now carefully unraveled a tangle of more than 1,000 neurons, tracing each cell's branching route across the brain to pinpoint where it goes and to which cells it connects. If laid end-to-end, the neurons would stretch more than 80 meters, roughly the length of two school buses, the team reports September 5, 2019, in the journal Cell. When Jayaram Chandrashekar and his colleagues began their neural cartography effort two years ago, neuroscientists had a general idea about which areas of the mammalian brain talked to one other. But what the messaging infrastructure actually looked like was largely a mystery. A fleshed-out picture of that circuitry could help scientists better understand how the brain is wired and how messages travel through it. In October 2017, the neuron-tracing project team, called MouseLight, released data for the first 300 neurons. Now, they've vastly expanded the data set, adding over 700 more to the collection. ""It's by far the largest digital collection of such neurons,"" says Chandrashekar. Over time, the team has streamlined their neuron-tracing process. First, the team injects a virus into mouse brains that makes a handful of neurons glow. Then, they use a light microscope to capture high-resolution images of the illuminated neurons. A computer program stitches together the 20,000 resulting images to make a three-dimensional map of the brain. ""It's like putting together 20,000 Lego blocks,"" Chandrashekar says. Algorithms and software developed in collaboration with Janelia's Scientific Computing group then help scientists follow the intertwined paths of individual neurons. Currently, it takes about one day to trace a single neuron, but a few years ago, it took a week or two. The preliminary data are revealing new clues about how the mouse brain is wired. In some regions, neurons cluster into discrete categories. In other regions, neurons can't be easily delineated into specific types. What that means for the way messages travel around the brain isn't yet clear, Chandrashekar says, but it's a target for future research. Still, much of the mouse brain, which contains about 70 million neurons, remains uncharted wilderness. For even a fuzzy view of the whole brain's wiring scheme to emerge -- the equivalent of a tourist map that captures major landmarks -- you'd need to trace about 100,000 neurons, estimates Janelia Group Leader Karel Svoboda. So far, MouseLight has focused on reconstructing neurons in a few areas of the brain that Janelia scientists are studying: the motor cortex, the subiculum, the hypothalamus, and the thalamus. For example, Svoboda has used MouseLight data to identify distinct motor control pathways in mice. The project is still growing, though. The MouseLight team has shared their ever-expanding data set online, and Chandrashekar hopes that other scientists will join their neuron-tracing efforts. "
Science Daily,Protein Tangles Linked With Dementia Seen in Patients After Single Head Injury,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905103013.htm,"   This is the finding of a new study led by scientists from Imperial College London, published in the journal Science Translational Medicine. In the early-stage study, researchers studied 21 patients who had suffered a moderate to severe head injury at least 18 years earlier (mostly from traffic accidents), as well as 11 healthy individuals who had not experienced a head injury. The research, from scientists at Imperial's Dementia Research Institute as well as the University of Glasgow, showed some of these patients had clumps of protein in their brain called tau tangles. The team, who recruited patients from the Institute of Health and Wellbeing at the University of Glasgow and from Imperial College Healthcare NHS Trust, say the research may accelerate the development of treatments that breakdown tau tangles, by enabling medics to monitor the amount of the protein. Tau normally helps provide structural support to nerve cells in the brain - acting as a type of scaffolding, but when brain cells become damaged - for instance during a head injury, the protein may form clumps, or tangles. Tau tangles are found in Alzheimer's disease and other forms of dementia, and associated with progressive nerve damage. Scientists have known for some time that repeated head injury - such as those sustained in sports such as boxing, rugby and American Football - can lead to neurodegeneration and dementia in later life - with particularly strong links to a type of brain condition called chronic traumatic encephalopathy. However, this is the first time scientists have seen the protein tangles in living patients who have suffered a single, severe head injury, explains Dr Nikos Gorgoraptis, author of the paper from Imperial's Department of Brain Sciences. ""Scientists increasingly realise that head injuries have a lasting legacy in the brain - and can continue to cause damage decades after the initial injury. However, up until now most of the research has focussed on the people who have sustained multiple head injuries, such as boxers and American Football players. This is the first time we have seen in these protein tangles in patients who have sustained a single head injury."" Dr Gorgoraptis adds that although these tangles have been detected in the brains of patients in post-mortem examination - where findings suggest around one in three patients with a single head injury develop protein tangles - they have not before been seen in the brains of living patients. The study used a type of brain scan, called a PET scan, combined with a substance that binds to tau protein, called flortaucipir, to study the amount of tau protein in the brains of head injury patients. The results revealed that, collectively, patients with head injury were more likely to have tau tangles. The paper also showed that patients with tau tangles had higher levels of nerve damage, particular in the white matter of the brain. None of the healthy individuals had tau tangles. Interestingly, the results revealed patients with higher levels of tau tangles did not necessarily have any reduction in brain function, such as memory problems, compared to patients with fewer tangles. However, Dr Gorgoraptis adds these tangles can develop years before a person starts to develop symptoms such as memory loss. He explained there are still many questions to answer about the tau tangles and brain damage. ""This research adds a further piece in the puzzle of head injury and the risk of neurodegeneration. Not all patients with head injury develop these protein tangles, and some patients can have them for many years without developing symptoms. While we know tau tangles are associated with Alzheimer's and other forms of dementia, we are only beginning to understand how brain trauma might lead to their formation. What is exciting about this study is this is the first step towards a scan that can give a clear indication of how much tau is in the brain, and where it is located. As treatments develop over the coming years that might target tau tangles, these scans will help doctors select the patients who may benefit and monitor the effectiveness of these treatments.""  The research was funded by the Medical Research Council and the NIHR Imperial Biomedical Research Centre "
Science Daily,Discovery of Neuronal Ensemble Activities That Is Orchestrated to Represent One Memory,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905094052.htm,"   Japanese research group supervised by Dr Noriaki Ohkawa (Lecturer) and Professor Kaoru Inokuchi of University of Toyama succeeded to establish a system to investigate characteristic activity of cell ensemble acquiring memory and visualized ways for representation and consolidation of memory experienced novel episode in brain. We are exposed to many episodic events and then memorize their information through our life. This kind of memory, episodic memory, is processed in several brain regions, and one of the regions is hippocampus. It is authorized that, in the hippocampus, one specific episodic memory is stored within and retrieved from a neuronal ensemble composed of neurons, termed engram cells, that are activated during learning. Indeed, activation or inhibition of engram cell ensemble induces or inhibits memory retrieval, respectively, thus, engram cell ensemble represents the physiological manifestation of a specific memory trace. However, one episodic memory is composed of several components of episode, and each component should be encoded by specific substrate, e.g. engram sub-ensemble. Nevertheless, it had not been clear how activity in these engram cells is assembled to represent the corresponding event because of technical limitations mean that it is difficult to distinguish the population activity of engram cells from that of non-engram cells. To address how one episodic memory is represented and consolidated in engram cell ensemble, it is required to visualize the activity of engram and non-engram cells. Engram cells can be specifically targeted in c-fos-tTA mice because the neural activity associated with memory formation induces c-fos gene expression, which in turn induces activity-dependent tTA expression under the control of the c-fos promoter. In the absence of doxycycline, tTA can bind to the tetracycline responsive element (TRE), enabling downstream expression of the TRE-dependent transgene. When neuron activates, Ca2+ flows into their soma. Thy1-G-CaMP7 mice express a Ca2+ indicator, G-CaMP7, in pyramidal neurons of hippocampal CA1 in the mice. Thus, neuronal activity is transferred into G-CaMP7 fluorescence, called Ca2+ imaging. We developed a technique that combines a head-mounted, miniature fluorescent microscope, with Thy1-G-CaMP7/c-fos-tTA double transgenic mice. The hippocampal CA1 region in double transgenic mice was injected with LV expressing a fluorescent protein, Kikume Green Red (KikGR), under the control of TRE. Using this approach, engram cells can be identified with KikGR, and the Ca2+ signals corresponding to the activity of engram cells and non-engram cells can be tracked during experience of a novel episodic event. The data indicated that population activity of engram cell ensemble exhibited the characteristic trait of highly repetitive activity during novel episodic event. To address component of one memory, next, it was proposed to deconstruct population activity into sub-ensemble groups. Non-negative matrix factorization (NMF) decomposes population activity into a time series of coactivated neuronal ensembles. Each sub-ensemble is composed of the different cells, which are spatially intermingled, to make their synchronous activity even among the group of engram cells associated with a single event. These results suggest that the total information of one event is structured into sub-engram ensembles. In order to measure the activity of engram cells across different memory processing stages, recording of Ca2+ transients from novel experience through post-experience sleep to retrieval was conducted. Around 40% of the total number of sub-ensembles that appeared in a novel experience in engram reactivated during post-experience sleep and then preferentially reappeared in retrieval sessions, while almost none of sub-ensembles in non-engram did not show this feature. Thus, engram sub-ensembles formed during a novel experience and that were reactivated during sleep sessions were mostly reactivated during the retrieval session. By contrast, most non-engram ensembles that were activated during the novel experience were not reactivated in the later sessions. The findings reported in this study demonstrate that engram cells possess synchronous activity, formed by several sub-ensembles in the engram population. Only in engram cells does this synchronous activity survive through post-learning sleep sessions that contribute to the consolidation process. The present work sheds light on the relationship between ensemble activities and coding principles in learning and memory. "
Science Daily,New MRI Technique Can 'See' Molecular Changes in Brain,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905094046.htm,"   In a paper published today in Nature Communications, Dr. Aviv Mezer and his team at the Hebrew University of Jerusalem (HUJI)'s Edmond and Lily Safra Center for Brain Sciences successfully transformed an MRI from a diagnostic camera that takes into a device that can record changes in the biological makeup of brain tissue. This is especially important for doctors looking to understand whether a patient is merely getting older or developing a neurodegenerative disease, such as Alzheimer's or Parkinson's. ""Instead of images, our quantitative MRI model provides molecular information about the brain tissue we're studying. This could allow doctors to compare brain scans taken over time from the same patient, and to differentiate between healthy and diseased brain tissue, without resorting to invasive or dangerous procedures, such as brain tissue biopsies,"" explained Mezer. External signs of aging are familiar to us: gray hair, a stooped spine, occasional memory loss. However, how do we know if a patient's brain is aging normally or developing a disease? The answer is found on the biological level. Both normal aging and neurodegenerative diseases create biological ""footprints"" in the brain, changing the lipid and protein content of brain tissue. Whereas current MRI scans provide only pictures of the human brain, this new technique provides biological readouts of brain tissue -- the ability to see what's going on on a molecular level, and to direct a course of treatment accordingly. ""When we take a blood test, it shows us the exact number of white blood cells in our body and whether that number is higher than normal due to illness. MRI scans provide images of the brain but don't show changes in the composition of the human brain, changes that could potentially differentiate normal aging from the beginnings of Alzheimer's or Parkinson's,"" shared PhD student Shir Filo who worked on the study. Looking ahead, Mezer believes that the new MRI technique will also provide a crucial understanding into how our brains age, ""when we scanned young and old patients' brains, we saw that different brain areas ages differently. For example, in some white-matter areas, there is a decrease in brain tissue volume, whereas in the gray-matter, tissue volume remains constant. However, we saw major changes in the molecular makeup of the gray matter in younger versus older subjects."" All this bodes well for patients. Not only will MRI's be able to distinguish molecular signs of normal aging from the early signs of disease. Patients will more likely receive correct diagnoses earlier, speeding up when they begin treatment and maintaining an improved quality of life longer, all via a non-invasive technique. "
Science Daily,A Swifter Way Towards 3D-Printed Organs,Matter & Energy,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906172436.htm,"   Now, a new technique called SWIFT (sacrificial writing into functional tissue) created by researchers from Harvard's Wyss Institute for Biologically Inspired Engineering and John A. Paulson School of Engineering and Applied Sciences (SEAS), overcomes that major hurdle by 3D printing vascular channels into living matrices composed of stem-cell-derived organ building blocks (OBBs), yielding viable, organ-specific tissues with high cell density and function. The research is reported in Science Advances. ""This is an entirely new paradigm for tissue fabrication,"" said co-first author Mark Skylar-Scott, Ph.D., a Research Associate at the Wyss Institute. ""Rather than trying to 3D-print an entire organ's worth of cells, SWIFT focuses on only printing the vessels necessary to support a living tissue construct that contains large quantities of OBBs, which may ultimately be used therapeutically to repair and replace human organs with lab-grown versions containing patients' own cells."" SWIFT involves a two-step process that begins with forming hundreds of thousands of stem-cell-derived aggregates into a dense, living matrix of OBBs that contains about 200 million cells per milliliter. Next, a vascular network through which oxygen and other nutrients can be delivered to the cells is embedded within the matrix by writing and removing a sacrificial ink. ""Forming a dense matrix from these OBBs kills two birds with one stone:  not only does it achieve a high cellular density akin to that of human organs, but the matrix's viscosity also enables printing of a pervasive network of perfusable channels within it to mimic the blood vessels that support human organs,"" said co-first author Sébastien Uzel, PhD., a Research Associate at the Wyss Institute and SEAS. The cellular aggregates used in the SWIFT method are derived from adult induced pluripotent stem cells, which are mixed with a tailored extracellular matrix (ECM) solution to make a living matrix that is compacted via centrifugation. At cold temperatures (0-4oC), the dense matrix has the consistency of mayonnaise - soft enough to manipulate without damaging the cells, but thick enough to hold its shape - making it the perfect medium for sacrificial 3D printing. In this technique, a thin nozzle moves through this matrix depositing a strand of gelatin ""ink"" that pushes cells out of the way without damaging them. When the cold matrix is heated to 37 oC, it stiffens to become more solid (like an omelet being cooked) while the gelatin ink melts and can be washed out, leaving behind a network of channels embedded within the tissue construct that can be perfused with oxygenated media to nourish the cells. The researchers were able to vary the diameter of the channels from 400 micrometers to 1 millimeter, and seamlessly connected them to form branching vascular networks within the tissues. Organ-specific tissues that were printed with embedded vascular channels using SWIFT and perfused in this manner remained viable, while tissues grown without these channels experienced cell death in their cores within 12 hours. To see whether the tissues displayed organ-specific functions, the team printed, evacuated, and perfused a branching channel architecture into a matrix consisting of heart-derived cells and flowed media through the channels for over a week. During that time, the cardiac OBBs fused together to form a more solid cardiac tissue whose contractions became more synchronous and over 20 times stronger, mimicking key features of a human heart. ""Our SWIFT biomanufacturing method is highly effective at creating organ-specific tissues at scale from OBBs ranging from aggregates of primary cells to stem-cell-derived organoids,"" said corresponding author Jennifer Lewis, Sc.D., who is a Core Faculty Member at the Wyss Institute as well as the Hansjörg Wyss Professor of Biologically Inspired Engineering at SEAS. ""By integrating recent advances from stem-cell researchers with the bioprinting methods developed by my lab, we believe SWIFT will greatly advance the field of organ engineering around the world."" Collaborations are underway with Wyss Institute faculty members Chris Chen, M.D., Ph.D. at Boston University and Sangeeta Bhatia, M.D., Ph.D., at MIT to implant these tissues into animal models and explore their host integration, as part of the 3D Organ Engineering Initiative co-led by Lewis and Chris Chen. ""The ability to support living human tissues with vascular channels is a huge step toward the goal of creating functional human organs outside of the body,"" said Wyss Institute Founding Director Donald Ingber, M.D., Ph.D., who is also the Judah Folkman Professor of Vascular Biology at HMS, the Vascular Biology Program at Boston Children's Hospital, and Professor of Bioengineering at SEAS. ""We continue to be impressed by the achievements in Jennifer's lab including this research, which ultimately has the potential to dramatically improve both organ engineering and the lifespans of patients whose own organs are failing,"" "
Science Daily,Scientists Couple Magnetization to Superconductivity for Quantum Discoveries,Matter & Energy,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906134009.htm,"   Quantum computing promises to revolutionize the ways in which scientists can process and manipulate information. The physical and material underpinnings for quantum technologies are still being explored, and researchers continue to look for new ways in which information can be manipulated and exchanged at the quantum level. In a recent study, scientists at the U.S. Department of Energy's (DOE) Argonne National Laboratory have created a miniaturized chip-based superconducting circuit that couples quantum waves of magnetic spins called magnons to photons of equivalent energy. Through the development of this ""on chip"" approach that marries magnetism and superconductivity for manipulation of quantum information, this fundamental discovery could help to lay the foundation for future advancements in quantum computing. Magnons emerge in magnetically ordered systems as excitations within a magnetic material that cause an oscillation of the magnetization directions at each atom in the material -- a phenomenon called a spin wave. ""You can think of it like having an array of compass needles that are all magnetically linked together,"" said Argonne materials scientist Valentine Novosad, an author of the study. ""If you kick one in a particular direction, it will cause a wave that propagates through the rest."" Just as photons of light can be thought of as both waves and particles, so too can magnons. ""The electromagnetic wave represented by a photon is equivalent to the spin wave represented by a magnon -- the two are analogues of each other,"" said Argonne postdoctoral researcher Yi Li, another author of the study. Because photons and magnons share such a close relationship to each other, and both contain a magnetic field component, the Argonne scientists sought a way to couple the two together. The magnons and photons ""talk"" to each other through a superconducting microwave cavity, which carries microwave photons with an energy identical to the energy of magnons in the magnetic systems that could be paired to it. Using a superconducting resonator with a coplanar geometry proved effective because it allowed the researchers to transmit a microwave current with low loss. Additionally, it also allowed them to conveniently define the frequency of photons for coupling to the magnons. ""By pairing the right length of resonator with the right energy of our magnons and photons, we are in essence creating a kind of echo chamber for energy and quantum information,"" Novosad said. ""The excitations stay in the resonator for a much longer length of time, and when it comes to doing quantum computing, those are the precious moments during which we can perform operations."" Because the dimensions of the resonator determine the frequency of the microwave photon, magnetic fields are required to tune the magnon to match it. ""You can think of it like tuning a guitar or a violin,"" Novosad said. ""The length of your string -- in this case, our resonator of photons -- is fixed. Independently, for the magnons, we can tune the instrument by adjusting the applied magnetic field, which is similar to modifying the amount of tension on the string."" Ultimately, Li said, the combination of a superconducting and a magnetic system allows for precise coupling and decoupling of the magnon and photon, presenting opportunities for manipulating quantum information. "
Science Daily,Selenium Anchors Could Improve Durability of Platinum Fuel Cell Catalysts,Matter & Energy,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906092753.htm,"   Now researchers at the Georgia Institute of Technology have developed a new platinum-based catalytic system that is far more durable than traditional commercial systems and has a potentially longer lifespan. The new system could, over the long term, reduce the cost of producing fuel cells. In the study, which was published July 15 in the ACS journal Nano Letters, the researchers described a possible new way to solve one of the key causes of degradation of platinum catalysts, sintering, a process in which particles of platinum migrate and clump together, reducing the specific surface area of the platinum and causing the catalytic activity to drop. To reduce such sintering, the researchers devised a method to anchor the platinum particles to their carbon support material using bits of the element selenium. ""There are strategies out there to mitigate sintering, such as using platinum particles that are uniform in size to reduce chemical instability among them,"" said Zhengming Cao, a visiting graduate student at Georgia Tech. ""This new method using selenium results in a strong metal-support interaction between platinum and the carbon support material and thus remarkably enhanced durability. At the same time, the platinum particles can be used and kept at a small to attain high catalytic activity from the increased specific surface area."" The process starts by loading nanoscale spheres of selenium onto the surface of a commercial carbon support. The selenium is then melted under high temperatures so that it spreads and uniformly covers the surface of the carbon. Then, the selenium is reacted with a salt precursor to platinum to generate particles of platinum smaller than two nanometers in diameter and evenly distributed across the carbon surface. The covalent interaction between the selenium and platinum provides a strong link to stably anchor the platinum particles to the carbon. ""The resulting catalyst system was remarkable both for its high activity as a catalyst as well as its durability,"" said Younan Xia, professor and Brock Family Chair in the Wallace H. Coulter Department of Biomedical Engineering at Georgia Tech and Emory University. Because of the increased specific surface area of the nanoscale platinum, the new catalytic system initially showed catalytic activity three and a half times higher than the pristine value of a state-of-the-art commercial platinum-carbon catalyst. Then, the research team tested the catalytic system using an accelerated durability test. Even after 20,000 cycles of electropotential sweeping, the new system still provided a catalytic activity more than three times that of the commercial system. The researchers used transmission electron microscopy at different stages of the durability test to examine why catalytic activity remained so high. They found that the selenium anchors were effective in keeping most of the platinum particles in place. ""After 20,000 cycles, most of the particles remained on the carbon support without detachment or aggregation,"" Cao said. ""We believe this type of catalytic system holds great potential as a scalable way to increase the durability and activity of platinum catalysts and eventually improve the feasibility of using fuel cells for a wider range of applications."" "
Science Daily,Scientists Measure Precise Proton Radius to Help Resolve Decade-Old Puzzle,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145426.htm,"   Scientists thought they knew the size of the proton, but that changed in 2010 when a team of physicists measured the proton-radius value to be four percent smaller than expected, which confused the scientific community. Since then, the world's physicists have been scrambling to resolve the proton-radius puzzle -- the inconsistency between these two proton-radius values. This puzzle is an important unsolved problem in fundamental physics today. Now, a study to be published in the journal Science finds a new measurement for the size of the proton at 0.833 femtometres, which is just under one trillionth of a millimetre. This measurement is approximately five percent smaller than the previously-accepted radius value from before 2010. The study, led by researchers in York University's Faculty of Science, presents a new electron-based measurement of how far the proton's positive charge extends, and it confirms the 2010 finding that the proton is smaller than previously believed. ""The level of precision required to determine the proton size made this the most difficult measurement our laboratory has ever attempted,"" said Distinguished Research Professor Eric Hessels, Department of Physics & Astronomy, who led the study. ""After eight years of working on this experiment, we are pleased to record such a high-precision measurement that helps to solve the elusive proton-radius puzzle,"" said Hessels. The quest to resolve the proton-radius puzzle has far-reaching consequences for the understanding of the laws of physics, such as the theory of quantum electrodynamics, which describes how light and matter interact. Hessels, who is an internationally-recognized physicist and expert in atomic physics, says three previous studies were pivotal in attempting to resolve the discrepancy between electron-based and muon-based determinations of the proton size. The 2010 study was the first to use muonic hydrogen to determine the proton size, compared to prior experiments that used regular hydrogen. At the time, scientists studied an exotic atom in which the electron is replaced by a muon, the electron's heavier cousin. While a 2017 study using hydrogen agreed with the 2010 muon-based determination of the proton charge radius, a 2018 experiment, also using hydrogen, supported the pre-2010 value. Hessels and his team of scientists spent eight years focused on resolving the proton-radius puzzle and understanding why the proton radius took on a different value when measured with muons, rather than electrons. The York University team studied atomic hydrogen to understand the deviant value obtained from muonic hydrogen. They conducted a high-precision measurement using the frequency-offset separated oscillatory fields (FOSOF) technique, which they developed for this measurement. This technique is a modification of the separated oscillatory fields technique that has been around for almost 70 years and won Norman F. Ramsey a Nobel Prize. Their measurement used a fast beam of hydrogen atoms created by passing protons through a molecular hydrogen gas target. The method allowed them to make an electron-based measurement of the proton radius that is directly analogous to the muon-based measurement from the 2010 study. Their result agrees with the smaller value found in the 2010 study. "
Science Daily,Nanowires Replace Newton's Famous Glass Prism,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145342.htm,"   The device, made from a single nanowire 1000 times thinner than a human hair, is the smallest spectrometer ever designed. It could be used in potential applications such as assessing the freshness of foods, the quality of drugs, or even identifying counterfeit objects, all from a smartphone camera. Details are reported in the journal Science. In the 17th century, Isaac Newton, through his observations on the splitting of light by a prism, sowed the seeds for a new field of science studying the interactions between light and matter -- spectroscopy. Today, optical spectrometers are essential tools in industry and almost all fields of scientific research. Through analysing the characteristics of light, spectrometers can tell us about the processes within galactic nebulae, millions of light years away, down to the characteristics of protein molecules. However, even now, the majority of spectrometers are based around principles similar to what Newton demonstrated with his prism: the spatial separation of light into different spectral components. Such a basis fundamentally limits the size of spectrometers in respect: they are usually bulky and complex, and challenging to shrink to sizes much smaller than a coin. Four hundred years after Newton, University of Cambridge researchers have overcome this challenge to produce a system up to a thousand times smaller than those previously reported. The Cambridge team, working with colleagues from the UK, China and Finland, used a nanowire whose material composition is varied along its length, enabling it to be responsive to different colours of light across the visible spectrum. Using techniques similar to those used for the manufacture of computer chips, they then created a series of light-responsive sections on this nanowire. ""We engineered a nanowire that allows us to get rid of the dispersive elements, like a prism, producing a far simpler, ultra-miniaturised system than conventional spectrometers can allow,"" said first author Zongyin Yang from the Cambridge Graphene Centre. ""The individual responses we get from the nanowire sections can then be directly fed into a computer algorithm to reconstruct the incident light spectrum."" ""When you take a photograph, the information stored in pixels is generally limited to just three components -- red, green, and blue,"" said co-first author Tom Albrow-Owen. ""With our device, every pixel contains data points from across the visible spectrum, so we can acquire detailed information far beyond the colours which our eyes can perceive. This can tell us, for instance, about chemical processes occurring in the frame of the image."" ""Our approach could allow unprecedented miniaturisation of spectroscopic devices, to an extent that could see them incorporated directly into smartphones, bringing powerful analytical technologies from the lab to the palm of our hands,"" said Dr Tawfique Hasan, who led the study. One of the most promising potential uses of the nanowire could be in biology. Since the device is so tiny, it can directly image single cells without the need for a microscope. And unlike other bioimaging techniques, the information obtained by the nanowire spectrometer contains a detailed analysis of the chemical fingerprint of each pixel. The researchers hope that the platform they have created could lead to an entirely new generation of ultra-compact spectrometers working from the ultraviolet to the infrared range. Such technologies could be used for a wide range of consumer, research and industrial applications, including in lab-on-a-chip systems, biological implants, and smart wearable devices. The Cambridge team has filed a patent on the technology, and hopes to see real-life applications within the next five years. "
Science Daily,New Method for Imaging Biological Molecules,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145332.htm,"   The new method involves researchers in the first stage mixing up a cell or tissue sample with short sequences of single-stranded DNA, selected to attach to the specific molecules that are going to be studied. If it for example involves a specific protein that is going to be investigated, small DNA snippets are used that bind to this particular protein. In the next stage, enzymes are fed to the short DNA sequences to connect and form DNA molecules. By analysing these newly formed DNA molecules with so-called DNA sequencing, it is possible to see exactly which DNA snippets have ended up next to each other. Based on this information, you can add a puzzle that shows how all of the DNA sequences must be connected. Since DNA sequences are attached to the molecules that are being represented, it is possible to understand how abundant they occur and where they are in the cells. What researchers are now publishing is a mathematical model that makes it possible to calculate this as well as create images from such information. ""You can liken it to the table placement game at a wedding where each guest gets a note that matches the person next to them at the table. If you have all of these notes you can recreate the table placement. In our experiments, the notes represent the DNA-snippets and the guests are the molecules that the snippets are attached to,"" says Björn Högberg, professor at the Department of Medical Biochemistry and Biophysics at Karolinska Institutet. The advantage of this method, which scientists call DNA microscopy, is that it makes it possible to search for specific molecules within a larger material, such as a whole cell collection or a tissue sample. With traditional microscopy, where you have to look at one area at a time, it is very time-consuming. But with DNA microscopy it is possible to, for example, screen for certain molecules and examine how frequent they are. The method also makes it possible to see what role the immediate environment plays in the life of a cell, i.e. how micro-environment might affect possible disease development -- to mention only two of several possible useful areas when it comes to DNA microscopy. ""This is a tool that can be used to gain a better understanding of how biology works and how cells work together. This knowledge can provide us with a better picture of how different diseases develop. In the long run, this tool can also provide opportunities for safer diagnostics,"" says Ian Hoffecker, researcher in Björn Högberg's Research Group at the Department of Medical Biochemistry and Biophysics at Karolinska Institutet and the coordinator of the study. "
Science Daily,Exotic Physics Phenomenon Is Observed for First Time,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145344.htm,"   The new finding involves the non-Abelian Aharonov-Bohm Effect and is reported today in the journal Science by MIT graduate student Yi Yang, MIT visiting scholar Chao Peng (a professor at Peking University), MIT graduate student Di Zhu, Professor Hrvoje Buljan at University of Zagreb in Croatia, Francis Wright Davis Professor of Physics John Joannopoulos at MIT, Professor Bo Zhen at the University of Pennsylvania, and MIT professor of physics Marin Soljacic. The finding relates to gauge fields, which describe transformations that particles undergo. Gauge fields fall into two classes, known as Abelian and non-Abelian. The Aharonov-Bohm Effect, named after the theorists who predicted it in 1959, confirmed that gauge fields -- beyond being a pure mathematical aid -- have physical consequences. But the observations only worked in Abelian systems, or those in which gauge fields are commutative -- that is, they take place the same way both forward and backward in time. In 1975, Tai-Tsun Wu and Chen-Ning Yang generalized the effect to the non-Abelian regime as a thought experiment. Nevertheless, it remained unclear whether it would even be possible to ever observe the effect in a non-Abelian system. Physicists lacked ways of creating the effect in the lab, and also lacked ways of detecting the effect even if it could be produced. Now, both of those puzzles have been solved, and the observations carried out successfully. The effect has to do with one of the strange and counterintuitive aspects of modern physics, the fact that virtually all fundamental physical phenomena are time-invariant. That means that the details of the way particles and forces interact can run either forward or backward in time, and a movie of how the events unfold can be run in either direction, so there's no way to tell which is the real version. But a few exotic phenomena violate this time symmetry. Creating the Abelian version of the Aharonov-Bohm effects requires breaking the time-reversal symmetry, a challenging task in itself, Soljacic says. But to achieve the non-Abelian version of the effect requires breaking this time-reversal multiple times, and in different ways, making it an even greater challenge. To produce the effect, the researchers use photon polarization. Then, they produced two different kinds of time-reversal breaking. They used fiber optics to produce two types of gauge fields that affected the geometric phases of the optical waves, first by sending them through a crystal biased by powerful magnetic fields, and second by modulating them with time-varying electrical signals, both of which break the time-reversal symmetry. They were then able to produce interference patterns that revealed the differences in how the light was affected when sent through the fiber-optic system in opposite directions, clockwise or counterclockwise. Without the breaking of time-reversal invariance, the beams should have been identical, but instead, their interference patterns revealed specific sets of differences as predicted, demonstrating the details of the elusive effect. The original, Abelian version of the Aharonov-Bohm effect ""has been observed with a series of experimental efforts, but the non-Abelian effect has not been observed until now,"" Yang says. The finding ""allows us to do many things,"" he says, opening the door to a wide variety of potential experiments, including classical and quantum physical regimes, to explore variations of the effect. The experimental approach devised by this team ""might inspire the realization of exotic topological phases in quantum simulations using photons, polaritons, quantum gases, and superconducting qubits,"" Soljacic says. For photonics itself, this could be useful in a variety of optoelectronic applications, he says. In addition, the non-Abelian gauge fields that the group was able to synthesize produced a non-Abelian Berry phase, and ""combined with interactions, it may potentially one day serve as a platform for fault-tolerant topological quantum computation,"" he says. At this point, the experiment is primarily of interest for fundamental physics research, with the aim of gaining a better understanding of some basic underpinnings of modern physical theory. The many possible practical applications ""will require additional breakthroughs going forward,"" Soljacic says. For one thing, for quantum computation, the experiment would need to be scaled up from one single device to likely a whole lattice of them. And instead of the beams of laser light used in their experiment, it would require working with a source of single individual photons. But even in its present form, the system could be used to explore questions in topological physics, which is a very active area of current research, Soljacic says. "
Science Daily,Sum of Three Cubes for 42 Finally Solved -- Using Real Life Planetary Computer,Computers & Math,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906134011.htm,"   The original problem, set in 1954 at the University of Cambridge, looked for Solutions of the Diophantine Equation x^3+y^3+z^3=k, with k being all the numbers from one to 100. Beyond the easily found small solutions, the problem soon became intractable as the more interesting answers -- if indeed they existed -- could not possibly be calculated, so vast were the numbers required. But slowly, over many years, each value of k was eventually solved for (or proved unsolvable), thanks to sophisticated techniques and modern computers -- except the last two, the most difficult of all; 33 and 42. Fast forward to 2019 and Professor Andrew Booker's mathematical ingenuity plus weeks on a university supercomputer finally found an answer for 33, meaning that the last number outstanding in this decades-old conundrum, the toughest nut to crack, was that firm favourite of Douglas Adams fans everywhere. However, solving 42 was another level of complexity. Professor Booker turned to MIT maths professor Andrew Sutherland, a world record breaker with massively parallel computations, and -- as if by further cosmic coincidence -- secured the services of a planetary computing platform reminiscent of ""Deep Thought,"" the giant machine which gives the answer 42 in Hitchhiker's Guide to the Galaxy. Professors Booker and Sutherland's solution for 42 would be found by using Charity Engine; a 'worldwide computer' that harnesses idle, unused computing power from over 500,000 home PCs to create a crowd-sourced, super-green platform made entirely from otherwise wasted capacity. The answer, which took over a million hours of calculating to prove, is as follows: X = -80538738812075974 Y = 80435758145817515 Z = 12602123297335631 And with these almost infinitely improbable numbers, the famous Solutions of the Diophantine Equation (1954) may finally be laid to rest for every value of k from one to 100 -- even 42. Professor Booker, who is based at the University of Bristol's School of Mathematics, said: ""I feel relieved. In this game it's impossible to be sure that you'll find something. It's a bit like trying to predict earthquakes, in that we have only rough probabilities to go by. ""So, we might find what we're looking for with a few months of searching, or it might be that the solution isn't found for another century."" "
Science Daily,Scientists Couple Magnetization to Superconductivity for Quantum Discoveries,Computers & Math,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906134009.htm,"   Quantum computing promises to revolutionize the ways in which scientists can process and manipulate information. The physical and material underpinnings for quantum technologies are still being explored, and researchers continue to look for new ways in which information can be manipulated and exchanged at the quantum level. In a recent study, scientists at the U.S. Department of Energy's (DOE) Argonne National Laboratory have created a miniaturized chip-based superconducting circuit that couples quantum waves of magnetic spins called magnons to photons of equivalent energy. Through the development of this ""on chip"" approach that marries magnetism and superconductivity for manipulation of quantum information, this fundamental discovery could help to lay the foundation for future advancements in quantum computing. Magnons emerge in magnetically ordered systems as excitations within a magnetic material that cause an oscillation of the magnetization directions at each atom in the material -- a phenomenon called a spin wave. ""You can think of it like having an array of compass needles that are all magnetically linked together,"" said Argonne materials scientist Valentine Novosad, an author of the study. ""If you kick one in a particular direction, it will cause a wave that propagates through the rest."" Just as photons of light can be thought of as both waves and particles, so too can magnons. ""The electromagnetic wave represented by a photon is equivalent to the spin wave represented by a magnon -- the two are analogues of each other,"" said Argonne postdoctoral researcher Yi Li, another author of the study. Because photons and magnons share such a close relationship to each other, and both contain a magnetic field component, the Argonne scientists sought a way to couple the two together. The magnons and photons ""talk"" to each other through a superconducting microwave cavity, which carries microwave photons with an energy identical to the energy of magnons in the magnetic systems that could be paired to it. Using a superconducting resonator with a coplanar geometry proved effective because it allowed the researchers to transmit a microwave current with low loss. Additionally, it also allowed them to conveniently define the frequency of photons for coupling to the magnons. ""By pairing the right length of resonator with the right energy of our magnons and photons, we are in essence creating a kind of echo chamber for energy and quantum information,"" Novosad said. ""The excitations stay in the resonator for a much longer length of time, and when it comes to doing quantum computing, those are the precious moments during which we can perform operations."" Because the dimensions of the resonator determine the frequency of the microwave photon, magnetic fields are required to tune the magnon to match it. ""You can think of it like tuning a guitar or a violin,"" Novosad said. ""The length of your string -- in this case, our resonator of photons -- is fixed. Independently, for the magnons, we can tune the instrument by adjusting the applied magnetic field, which is similar to modifying the amount of tension on the string."" Ultimately, Li said, the combination of a superconducting and a magnetic system allows for precise coupling and decoupling of the magnon and photon, presenting opportunities for manipulating quantum information. "
Science Daily,Nanowires Replace Newton's Famous Glass Prism,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145342.htm,"   The device, made from a single nanowire 1000 times thinner than a human hair, is the smallest spectrometer ever designed. It could be used in potential applications such as assessing the freshness of foods, the quality of drugs, or even identifying counterfeit objects, all from a smartphone camera. Details are reported in the journal Science. In the 17th century, Isaac Newton, through his observations on the splitting of light by a prism, sowed the seeds for a new field of science studying the interactions between light and matter -- spectroscopy. Today, optical spectrometers are essential tools in industry and almost all fields of scientific research. Through analysing the characteristics of light, spectrometers can tell us about the processes within galactic nebulae, millions of light years away, down to the characteristics of protein molecules. However, even now, the majority of spectrometers are based around principles similar to what Newton demonstrated with his prism: the spatial separation of light into different spectral components. Such a basis fundamentally limits the size of spectrometers in respect: they are usually bulky and complex, and challenging to shrink to sizes much smaller than a coin. Four hundred years after Newton, University of Cambridge researchers have overcome this challenge to produce a system up to a thousand times smaller than those previously reported. The Cambridge team, working with colleagues from the UK, China and Finland, used a nanowire whose material composition is varied along its length, enabling it to be responsive to different colours of light across the visible spectrum. Using techniques similar to those used for the manufacture of computer chips, they then created a series of light-responsive sections on this nanowire. ""We engineered a nanowire that allows us to get rid of the dispersive elements, like a prism, producing a far simpler, ultra-miniaturised system than conventional spectrometers can allow,"" said first author Zongyin Yang from the Cambridge Graphene Centre. ""The individual responses we get from the nanowire sections can then be directly fed into a computer algorithm to reconstruct the incident light spectrum."" ""When you take a photograph, the information stored in pixels is generally limited to just three components -- red, green, and blue,"" said co-first author Tom Albrow-Owen. ""With our device, every pixel contains data points from across the visible spectrum, so we can acquire detailed information far beyond the colours which our eyes can perceive. This can tell us, for instance, about chemical processes occurring in the frame of the image."" ""Our approach could allow unprecedented miniaturisation of spectroscopic devices, to an extent that could see them incorporated directly into smartphones, bringing powerful analytical technologies from the lab to the palm of our hands,"" said Dr Tawfique Hasan, who led the study. One of the most promising potential uses of the nanowire could be in biology. Since the device is so tiny, it can directly image single cells without the need for a microscope. And unlike other bioimaging techniques, the information obtained by the nanowire spectrometer contains a detailed analysis of the chemical fingerprint of each pixel. The researchers hope that the platform they have created could lead to an entirely new generation of ultra-compact spectrometers working from the ultraviolet to the infrared range. Such technologies could be used for a wide range of consumer, research and industrial applications, including in lab-on-a-chip systems, biological implants, and smart wearable devices. The Cambridge team has filed a patent on the technology, and hopes to see real-life applications within the next five years. "
Science Daily,Exotic Physics Phenomenon Is Observed for First Time,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145344.htm,"   The new finding involves the non-Abelian Aharonov-Bohm Effect and is reported today in the journal Science by MIT graduate student Yi Yang, MIT visiting scholar Chao Peng (a professor at Peking University), MIT graduate student Di Zhu, Professor Hrvoje Buljan at University of Zagreb in Croatia, Francis Wright Davis Professor of Physics John Joannopoulos at MIT, Professor Bo Zhen at the University of Pennsylvania, and MIT professor of physics Marin Soljacic. The finding relates to gauge fields, which describe transformations that particles undergo. Gauge fields fall into two classes, known as Abelian and non-Abelian. The Aharonov-Bohm Effect, named after the theorists who predicted it in 1959, confirmed that gauge fields -- beyond being a pure mathematical aid -- have physical consequences. But the observations only worked in Abelian systems, or those in which gauge fields are commutative -- that is, they take place the same way both forward and backward in time. In 1975, Tai-Tsun Wu and Chen-Ning Yang generalized the effect to the non-Abelian regime as a thought experiment. Nevertheless, it remained unclear whether it would even be possible to ever observe the effect in a non-Abelian system. Physicists lacked ways of creating the effect in the lab, and also lacked ways of detecting the effect even if it could be produced. Now, both of those puzzles have been solved, and the observations carried out successfully. The effect has to do with one of the strange and counterintuitive aspects of modern physics, the fact that virtually all fundamental physical phenomena are time-invariant. That means that the details of the way particles and forces interact can run either forward or backward in time, and a movie of how the events unfold can be run in either direction, so there's no way to tell which is the real version. But a few exotic phenomena violate this time symmetry. Creating the Abelian version of the Aharonov-Bohm effects requires breaking the time-reversal symmetry, a challenging task in itself, Soljacic says. But to achieve the non-Abelian version of the effect requires breaking this time-reversal multiple times, and in different ways, making it an even greater challenge. To produce the effect, the researchers use photon polarization. Then, they produced two different kinds of time-reversal breaking. They used fiber optics to produce two types of gauge fields that affected the geometric phases of the optical waves, first by sending them through a crystal biased by powerful magnetic fields, and second by modulating them with time-varying electrical signals, both of which break the time-reversal symmetry. They were then able to produce interference patterns that revealed the differences in how the light was affected when sent through the fiber-optic system in opposite directions, clockwise or counterclockwise. Without the breaking of time-reversal invariance, the beams should have been identical, but instead, their interference patterns revealed specific sets of differences as predicted, demonstrating the details of the elusive effect. The original, Abelian version of the Aharonov-Bohm effect ""has been observed with a series of experimental efforts, but the non-Abelian effect has not been observed until now,"" Yang says. The finding ""allows us to do many things,"" he says, opening the door to a wide variety of potential experiments, including classical and quantum physical regimes, to explore variations of the effect. The experimental approach devised by this team ""might inspire the realization of exotic topological phases in quantum simulations using photons, polaritons, quantum gases, and superconducting qubits,"" Soljacic says. For photonics itself, this could be useful in a variety of optoelectronic applications, he says. In addition, the non-Abelian gauge fields that the group was able to synthesize produced a non-Abelian Berry phase, and ""combined with interactions, it may potentially one day serve as a platform for fault-tolerant topological quantum computation,"" he says. At this point, the experiment is primarily of interest for fundamental physics research, with the aim of gaining a better understanding of some basic underpinnings of modern physical theory. The many possible practical applications ""will require additional breakthroughs going forward,"" Soljacic says. For one thing, for quantum computation, the experiment would need to be scaled up from one single device to likely a whole lattice of them. And instead of the beams of laser light used in their experiment, it would require working with a source of single individual photons. But even in its present form, the system could be used to explore questions in topological physics, which is a very active area of current research, Soljacic says. "
Science Daily,Not All Meat Is Created Equal: How Diet Changes Can Sustain World's Food Production,Earth & Climate,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906172458.htm,"   Phosphorus is an essential mineral to grow food, but research suggests that this is being mined unsustainably. If reserves run low, food production will be constrained and starvation entirely possible. Now, David Vaccari, an environmental engineer at Stevens Institute of Technology, and colleagues have developed a model to describe how phosphorus flows through the global food system. The model, reported in the Sept. 4 issue of Environmental Science & Technology, can predict how several different conservation approaches could reduce demand for a nonrenewable resource that is absolutely vital for feeding the world. ""Phosphate is spread across the planet but hardly recycled,"" said Vaccari, a pioneer in phosphate research who led the work. ""The model allows us to answer specific 'what if' questions to see how certain changes in human behavior could significantly improve the conservation of this resource and by extension, help sustain the world's food production."" In the past, the phosphorus cycle was practically closed: crops were eaten by humans and livestock while their feces were used as natural fertilizers to grow crops again. These days, the cycle is broken. Each year phosphate rocks are mined and turned into fertilizer. This is converted into crops which are transported to cities for food. Some phosphorus is lost at every step along the way and winds up in the environment. Runoff from farms goes into waterways, food waste goes to landfills, and the human waste goes to the sewage disposal, most of which ultimately ends up in the sea. A cycle has become a linear process. Vaccari and colleagues model this cycle, with ""knobs"" that can be turned up or down to create different conservation scenarios. When a knob is turned -- e.g. fraction of animal meat in diet, fraction of food that is wasted, fraction of human waste recycled -- the model, which factors in leaks and losses from the food system, loops back to calculate the degree to which phosphate mining could be reduced. The model shows several interesting results: Collective diet changes can reduce the demand for phosphate mining substantially. Since different animals have a different footprint on phosphate mining, these changes could include reducing total meat consumption or switching to meats that require less feed to produce; for example, it takes 32 pounds of feed to produce edible beef; 11 pounds to produce edible pork; and four to produce edible chicken and around one or two for milk and eggs. The fewer pounds of feed needed, the less demand for phosphate mining. Dietary changes would reduce demand for mining phosphorus only up to a point; then, surprisingly, demand would increase. That's because eating less meat would necessarily lead to eating more crops and initially, crops could use phosphorus from non-mining resources such as natural mineral erosion in the soil. However, natural mineral erosion wouldn't be enough to sustain the increased demand for crops, so phosphate mines would again need to be tapped. Calculations show that reducing the amount of food we waste is about 80 times more effective at conserving phosphorus than recycling that same waste. Even if 100 percent of the phosphorus in our human waste was recycled, mining of phosphorus would only be reduced about 16 percent; recycling 100 percent of food waste would reduce mining by 5 percent. Recycling has such a low effect on conservation because recycled phosphorus is subject to much of the same losses in the food system as is fertilizer. Thus, although recycling is still part of the solution, it is much better to reduce the amount of waste we produce in the first place. If the world had to depend entirely upon phosphorus other than from mining, it would only be able to support about one-third of the current world population, using current levels of usage and recycling efficiency. However, if we substantially increased our efficiency, it would be possible to support about twice the current world population.The world, however, is not about to run out of phosphorus. At current usage rates, known phosphorus reserves could sustain agricultural production for several centuries, according to Vaccari. But unlike nitrogen, which makes up 78 percent of the atmosphere (and a main ingredient in fertilizer), phosphate is a finite resource -- and ""new sources of phosphorus will be hard to come by,"" said Vaccari. Reducing phosphate mining has other advantages. When phosphorus ""leaks"" from the agricultural, food production and waste disposal systems, it contributes to severe water pollution problems such as harmful algal blooms in lakes and in the coastal zone. Aside from the long known public health issues and economic impact of algal blooms in fishing communities, a deadly variety of algae has recently caused several dog deaths in the Southern United States. ""Phosphorus is essential for life,"" said Vaccari. ""So the plan is to keep it around for a long time by mining phosphate sustainably and responsibly -- and this model helps us look at optimal ways on how to do that."" "
Science Daily,Climate Change Water Variability Hurts Salamander Populations,Earth & Climate,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906172429.htm,"   UM biology Professor Winsor Lowe and his partners studied spring salamanders living in five New Hampshire streams. Like many streams around the globe, these waterways are experiencing greater fluctuations between low and high flows brought about by climate change. The researchers revealed that streamflow variability can kill salamanders while they are metamorphosing from larvae to adults. The work was published recently in the Proceedings of the National Academy of Sciences in an article titled ""Hydrologic variability contributes to reduced survival through metamorphosis in a stream salamander."" ""We feel this work is important because it expands our knowledge about the effects of climate change on a diverse group of species that are often overlooked because they spend most of their lives under rocks and logs in small, headwater streams,"" Lowe said. ""Increasing environmental variability may be especially challenging for species that undergo metamorphosis -- like many insects and amphibians -- because that's a vulnerable period when they rely on stable environments for survival."" He said the research suggests society shouldn't focus on average conditions as it tries to understand and manage the effects of climate change. Scientists and managers also must pay attention to changes in environmental variability, which may increase with climate change. ""Small headwater streams are home to diverse species and the source of clean water to downstream communities, but these ecosystems are also easy to overlook and are losing protection under proposed Clean Water Act revisions,"" Lowe said. ""Our work underscores the vulnerability of headwater ecosystems in this era of climate change, the need for protection of vulnerable headwater species, and the value of long-term monitoring efforts."" Using a 20-year dataset from Merrill Brook in New Hampshire, the researchers showed the abundance of spring salamander adults declined about 50% since 1999, but no trend was noted for larval abundance. Scientists then studied whether streamflow variability at Merrill Brook and streams in the nearby Hubbard Brook Experimental Forest affected the survival of salamanders metamorphosing from larvae to adults. They found that fewer salamanders survived metamorphosis during years when steamflow variability was high, leading to the decline in the adult population. "
Science Daily,Rare Deer Likely Lived 50 Years Beyond Declaration of Extinction,Earth & Climate,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906134015.htm,"   Schomburgk's deer (Rucervus schomburgki) was added to the extinction list in 1938. But new evidence, gleaned from antlers obtained in late 1990 or early 1991, shows that it survived for at least an additional half century and might still be around today. The research was published last week (Aug. 30) in the Journal of the Bombay Natural History Society. Gary Galbreath, professor of biological sciences at Northwestern University's Weinberg College of Arts and Sciences, was involved in the work. After the wild population died out from overhunting in 1932, the last known Schomburgk's deer died in captivity six years later. Or so we thought. A trucker in Laos found a set of antlers, seemingly in fresh condition, in the early 1990s. He then gave the antlers to a shop in the northern Laos province of Phongsali. In February 1991, United Nations agronomist Laurent Chazée photographed the antlers. Galbreath and his collaborator G.B. Schroering recently analyzed the antlers' physical condition in those photos. Based on the widely spreading, basket-shaped, hyper-branched structure of the antlers, the team determined the antlers belonged to a Schomburgk's deer. (Other Asian deer's antlers do not have the same signature basket shape.) Galbreath also confirmed that the antlers were fresh when photographed in 1991. The antlers -- spotted with dark red to reddish-brown dried blood -- had been excised from the deer's head. The color of the blood and condition of the exposed bone marrow offered clues into the antlers' age. ""The relative antiquity of the antler specimens can be assessed by the materials, such as dried marrow, still adhering to them,"" said Galbreath, an expert in Asian wildlife. ""Even the blood was still reddish; it would become black with increased age. In the tropics, the antlers would not continue to look this way even within a matter of months."" Before they were listed as ""extinct,"" the deer were well documented in Thailand. Galbreath believes a small population probably also lived in a remote area in central Laos, where they just might still be living today. "
Science Daily,Pain in the Asp: Bird-Deterring Nets Create Haven for Stinging Pests,Earth & Climate,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906134002.htm,"   Live oak trees lining sidewalks in the Texas Medical Center (TMC) -- which is visited by 10 million people seeking health care each year -- are routinely netted to discourage pesky birds such as grackles and pigeons. Now Rice researchers have learned the netting has an unintended consequence: Chasing away birds that eat insects has created a haven for a flourishing population of Megalopyge opercularis, commonly referred to as asps. The asps bristle with venomous spines that can cause severe pain for humans unlucky enough to come in contact with them. When a human is stung by an asp, it can cause localized pain that is compared to blunt-force trauma or a bone break. Symptoms include headaches, nausea, vomiting, fever, low blood pressure, inflammation of lymph nodes and, in severe cases, abdominal distress, muscle spasms, convulsions and respiratory stress. ""I've been stung by a lot of things and an asp sting definitely ranks high up there,"" said Mattheau Comerford, an ecology and evolutionary biology graduate student at Rice who is also a U.S. Army veteran. ""It takes about 10 minutes before the pain kicks in so you might not even realize you've been stung at first. It feels like a broken bone and the pain lasts for hours. I was stung on the wrist and the pain traveled up my arm, into my arm pit, and my jaw started to feel pain."" The TMC, in addition to hosting a vulnerable population seeking health care, is the eighth-largest business district in the country, employing more than 106,000 people. ""There are a lot of people that congregate in the green spaces of TMC,"" said Glen Hood, a research assistant professor of biological sciences at Wayne State University and a former member of the Rice Academy of Fellows. ""It becomes this scenario of what's worse -- bird guano or venomous asps -- and is there a happy medium?"" Scott Egan, an assistant professor of biosciences at Rice, has spent almost 20 years studying live oaks from Florida to Texas and is used to seeing the asps, which resemble tufts of fur. He said asp stings are common in live-oak country, even though the insects typically occur in low numbers under natural conditions. ""When we saw such high numbers of asps in the TMC, we knew this was something we needed to look into,"" he said. ""The Egan Lab addresses important conceptual issues in ecology, evolution and conservation biology, and we do that through the lens of specializing in one single system."" As specialists in the live oak system, Egan and his team can observe unique differences or slight changes that a general scientist might not be able to see. That's what sparked the study entitled ""Human-mediated disturbance in multitrophic interactions results in outbreak levels of North America's most venomous caterpillar,"" which is published in the journal Biology Letters. ""We thought this was a wonderful, natural, already-occurring experiment that we could exploit to understand what happens when trophic interactions get disrupted,"" Egan said. ""We observed that there were a higher number of asps in the TMC than on Rice's campus, so we performed an observational study across many sites in this area over a three-year span: trees that have massive nets and deter birds, versus naturally occurring systems such as live oak trees and the caterpillars that feed on them."" Egan's team discovered that caterpillar abundance, on average, was more than 7,300% higher on netted versus non-netted trees. ""The patterns in the data are overwhelming and striking,"" said Hood, the study's lead author and a former postdoctoral research fellow in Egan's lab. ""It's highly suggestive that when you don't take into account the natural interactions taking place within a community or ecosystem, even in an urban setting, it can cause unforeseen consequences."" Egan and his team was especially interested in the location of the netted trees -- lining the sidewalks, green spaces and streets of the TMC. ""In this area, there is a high density of people with allergies and compromised or depleted immune systems, putting them at a higher risk for the ill effects of the asp stings,"" Comerford said. Egan said nuisance birds are a widespread problem, and the kind of nets used in the TMC are a common sight in cities, fruit orchards and commercial nurseries. He said TMC staff helped his team with its research, giving students access to trees and opening up netting for sample collection. ""There is no 'bad guy' here,"" Egan said. ""Urban bird pests are a real problem, and birds can carry diseases and pose health risks, too. Netting trees is a way to address that problem, and we do not know if netting trees leads to an increase in asp stings. What our study shows is the complexity of the problem. Asps are yet another dimension to something that was already a multi-dimensional problem."" Humans may not be the only species at risk from asps. The Houston Zoo, located on the northeast edge of the TMC, houses more than 6,000 animals from 900 different species. Many zoos use netting to cover animal enclosures and observation areas, and Egan's team said that could facilitate similar asp outbreaks and increase animal and human exposure. Amanda Weaver noticed the surge of asps covering netted trees in the TMC while she was working on a separate project for her senior thesis as an undergrad in the Egan Lab. ""I thought this project was really interesting because I had never heard of asps before I got to Rice,"" said Weaver, now a graduate student at Columbia University. ""I remember getting the warning emails from Rice and seeing the rashes from my classmates who had been stung. Especially because they are not prevalent everywhere, my biggest concern is that people may not know they are venomous. In fact, they actually look cute."" Rice's status as a registered arboretum and its next-door proximity to the TMC allowed Egan's team to gather data from many sites over a three-year span. ""Like a lot of things in biology, this experiment happened serendipitously,"" Hood said. ""If you're inquisitive and paying attention to the details, you'll see an opportunity to collect data."" By spotlighting an unforeseen consequence of ecological disturbance, Egan and his team hope the study emphasizes the importance of considering ecology in urban planning. ""When we try to manage environments, it's very difficult for us to predict all of the implications or consequences,"" Comerford said. ""It's very easy to say the birds are a problem and we can fix it by removing their nesting sites. But if you don't think through the pipeline, you can ultimately create unintended downstream effects."" "
Science Daily,Selenium Anchors Could Improve Durability of Platinum Fuel Cell Catalysts,Earth & Climate,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906092753.htm,"   Now researchers at the Georgia Institute of Technology have developed a new platinum-based catalytic system that is far more durable than traditional commercial systems and has a potentially longer lifespan. The new system could, over the long term, reduce the cost of producing fuel cells. In the study, which was published July 15 in the ACS journal Nano Letters, the researchers described a possible new way to solve one of the key causes of degradation of platinum catalysts, sintering, a process in which particles of platinum migrate and clump together, reducing the specific surface area of the platinum and causing the catalytic activity to drop. To reduce such sintering, the researchers devised a method to anchor the platinum particles to their carbon support material using bits of the element selenium. ""There are strategies out there to mitigate sintering, such as using platinum particles that are uniform in size to reduce chemical instability among them,"" said Zhengming Cao, a visiting graduate student at Georgia Tech. ""This new method using selenium results in a strong metal-support interaction between platinum and the carbon support material and thus remarkably enhanced durability. At the same time, the platinum particles can be used and kept at a small to attain high catalytic activity from the increased specific surface area."" The process starts by loading nanoscale spheres of selenium onto the surface of a commercial carbon support. The selenium is then melted under high temperatures so that it spreads and uniformly covers the surface of the carbon. Then, the selenium is reacted with a salt precursor to platinum to generate particles of platinum smaller than two nanometers in diameter and evenly distributed across the carbon surface. The covalent interaction between the selenium and platinum provides a strong link to stably anchor the platinum particles to the carbon. ""The resulting catalyst system was remarkable both for its high activity as a catalyst as well as its durability,"" said Younan Xia, professor and Brock Family Chair in the Wallace H. Coulter Department of Biomedical Engineering at Georgia Tech and Emory University. Because of the increased specific surface area of the nanoscale platinum, the new catalytic system initially showed catalytic activity three and a half times higher than the pristine value of a state-of-the-art commercial platinum-carbon catalyst. Then, the research team tested the catalytic system using an accelerated durability test. Even after 20,000 cycles of electropotential sweeping, the new system still provided a catalytic activity more than three times that of the commercial system. The researchers used transmission electron microscopy at different stages of the durability test to examine why catalytic activity remained so high. They found that the selenium anchors were effective in keeping most of the platinum particles in place. ""After 20,000 cycles, most of the particles remained on the carbon support without detachment or aggregation,"" Cao said. ""We believe this type of catalytic system holds great potential as a scalable way to increase the durability and activity of platinum catalysts and eventually improve the feasibility of using fuel cells for a wider range of applications."" "
Science Daily,Tracking Sulfur-Based Metabolism in the Open Ocean,Earth & Climate,2019-09-06,-,https://www.sciencedaily.com/releases/2019/09/190906090608.htm,"   A study by University of Washington oceanographers, published this summer in Nature Microbiology, looks at how photosynthetic microbes and ocean bacteria use sulfur, a plentiful marine nutrient. Sulfur is the odorous element that gives beaches their distinctive smell. The new study focused on sulfonates, in which a sulfur atom is connected to three oxygen atoms and a carbon-based molecule. In the ocean, phytoplankton use energy from the sun to create sulfonate molecules. Bacteria then consume the sulfates to gain nutrients and energy. Bryndan Durham, then a postdoctoral researcher in oceanography and now an assistant professor at the University of Florida, drew on the recent genetic studies of soils to learn which microbial pathways are used to process sulfonates in the ocean. The study first focused on 36 marine microbes that the team cultured in the lab, using a UW-developed method to test which organisms produce sulfonates on their own in a lab environment. The study discovered ""some striking similarities between sulfonate pathways in terrestrial and ocean systems,"" Durham wrote in a ""Behind the Paper"" post in Nature Microbiology that discusses the project. Plants produce most of the sulfonates in soils. In the oceans most sulfonates are also produced by photosynthetic organisms, but in this case by unicellular phytoplankton. The study then considered microbes in the open ocean that cannot yet be bred in the lab. During a 2015 research cruise north of Hawaii co-led by a team of researchers including Virginia Armbrust and Anitra Ingalls, both professors of oceanography and senior authors on the new study, microbial samples were collected at different times of day and night. The researchers then froze the samples in order to analyze their genetic and chemical contents back in Seattle. ""We returned from sea with a freezer's worth of samples that generated over six terabytes of data for us to explore,"" Durham wrote, ""a major computational hurdle."" The team eventually succeeded in extracting the relevant data and found patterns that backed up the findings from the lab samples. They also detected a day-night rhythm in sulfonate metabolism that reflects the activity of photosynthetic organisms. ""Sulfonates are produced and consumed by certain groups of microbes, so we can use them to track specific relationships in seawater communities,"" Durham said. ""And because sulfonates contain a carbon-sulfur bond, they are part of the global carbon cycle which controls the flux of carbon dioxide into and out of the ocean. This is increasingly important to understand as the climate changes."" "
Science Daily,Tiny Airborne Particles from Wildfires Have Climate Change Implications,Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905161421.htm,"   The smoke from BB events produces large amounts of aerosol particles and gases. These emissions can cause major problems for visibility and health, as well as for local and global climate. BB emissions are expected to increase in the future as a result of climate change. Tarballs, which are microscopic organic BB particles, are estimated to contribute up to 30% of BB aerosol mass. Because tarballs are a dominant, light-absorbing type of aerosol particle in BB smoke, understanding their influence on climate is crucial. But details of how they form and influence climate change have been unclear. Senior researcher Kouji Adachi, currently working at the Meteorological Research Institute in Tsukuba, Japan, was a postdoctoral research associate from 2005 until 2011 with Professor Peter Buseck of Arizona State University's School of Molecular Sciences and School of Earth and Space Exploration. Their work attracted the attention of colleagues from the Department of Energy's Brookhaven National Lab in Upton, New York. Principal Investigators, Arthur Sedlacek III and Lawrence Kleinman, with support from the Atmospheric Sciences Program, were planning the Biomass Burning Operational Period (BBOP) field campaign, in which an instrumented airplane would measure rapid chemical changes in wildfire smoke. Sedlacek and Kleinman approached Buseck about participating in BBOP, as the sampling strategy provided an ideal laboratory in the sky to study tarball formation. The results, published online September 5, are in a Proceedings of the National Academy of Sciences paper titled ""Spherical tarball particles form through rapid chemical and physical changes of organic matter in biomass-burning smoke."" The team's observations show that tarballs form through a combination of chemical and physical changes of organic aerosols formed within the first hours following smoke production. ""I'm so pleased that tarballs, the subject of this paper, were first reported in 2003 papers in which an ASU chemistry graduate student, Li Jia, and postdoctoral research associate, Mihaly Posfai, were major contributors; thus the School of Molecular Sciences and the School of Earth and Space Exploration had an important role,"" said Buseck. Buseck, an ASU Regents Professor, also is being awarded the 2019 Roebling Medal this month, the highest award of the Mineralogical Society of America for outstanding original research in mineralogy. ""This study of tarball particles and the possible effects on climate change further shows the breadth and diversity of Buseck's research,"" said School of Earth and Space Exploration Director Meenakshi Wadhwa. ""From solid-state geochemistry and mineralogy, to atmospheric geochemistry, to cosmochemistry, he continually proves to be a pioneer in his field."" ""Peter Buseck and his group have developed the use of transmission electron microscopy to study minerals, meteorites and aerosol particles in a uniquely interesting way,"" said Professor Neal Woodbury, director of the School of Molecular Sciences. ""His team's current findings on tarball formation are a good example and will significantly improve assessments of biomass burning impacts on regional and global climate."" Tarballs used in this study were collected from large wildfires sampled during the BBOP campaign in the summer of 2013 in the northwestern United States. Using a Gulfstream-1 research airplane, the team collected wildfire aerosol particles on repeated flights through smoke plumes. Shapes and compositions of more than 10,000 particles were measured using transmission electron microscopy, with detailed chemical analysis of tarballs performed using scanning transmission X-ray spectroscopy. The analysis reveals that the fraction of aerosol particles that are tarballs increases with particle age. In addition, the tarball ratios of nitrogen and oxygen relative to potassium, and the particle roundness, also increase with particle age. In summary, BB emissions including tarballs are expected to increase in coming decades as a result of climate change. This study reveals their formation process through chemical and microphysical analyses. The findings can be used to improve interpretation of BB smoke from satellite data and ground-based observations by considering tarball shape, viscosity and compositional changes during aging and to provide better estimates of their effects in climate models. "
Science Daily,New Insights on Impacts of Crop Trading in China,Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145351.htm,"   In a recent study, scientists from the University of Maryland Center for Environmental Science used historical records to shed light on sustainability policies for balancing food demand, crop production, trade expenditure, and the environmental degradation associated with food production in China. ""It is critical to understand and quantify the trade-offs of using international trade as one of the strategies for resolving food demand and environmental challenges,"" said study co-author Xin Zhang of the University of Maryland Center for Environmental Science. ""The lack of systematic approaches for assessing the impacts of trade on sustainability has been preventing us from understanding the synergies and trade-offs among different environmental and socioeconomic concerns related to trade and crop production."" The study is among the few to consider both the socioeconomic and environmental impacts of crop trading, and is one of the pioneer studies to consider the impact of crop mixes in an import portfolio and domestic production. ""Economic costs to alleviate environmental pollutions caused by producing export crops could be comparable to the economic benefits brought by trade,"" said co-author Guolin Yao. Focusing on China's crop production and trade over 1986-2015, scientists evaluated the impacts of trade from several perspectives, including environmental (such as nitrogen pollution and land use), social (for example, crop self-sufficiency for a country) and economic (such as trade expenditure and environmental damage cost). Their findings show that crop imports can relieve nitrogen pollution and land-use pressure in China and the world but not without adding environmental burdens to other countries and exposing China's food availability to the risks of the international market or unstable bilateral trade relationships. They also found that the environmental damage costs of nitrogen pollution avoided by importing crops in China are less than current trade expenditure, but may reach or surpass it as China's economy develops. ""This paper proposes new concepts of 'alternative' nitrogen and land. If China has to produce imported crops domestically, then how much nutrients and land is needed? Since the nitrogen-use efficiency and crop yield is usually lower in China than countries with better technologies or more favorable environments, reallocating production in those countries can provide relief on environment in China and the world,"" Zhang said. ""The international food trade could mitigate environmental degradation by coordinating global food supply and demand. "" China increasingly relies on agricultural imports, driven by its rising population and income, as well as dietary shifts. International trade offers an opportunity to relieve pressures on resource depletion and pollution, while it poses multiple socioeconomic challenges, such as food availability. ""Food trade can reduce global nitrogen inputs by redistributing the production of commodities to regions more efficient in nitrogen use,"" said Zhang. Globalization of food trade can help alleviate the pressure of increasing food demands and subsequent nitrogen pollution, and a diverse and carefully designed crop trade portfolio can protect a country against local disruptions and shortfalls in production. Currently, 23% of the food produced for human consumption is traded internationally, tending to flow from regions with high production efficiency to less efficient regions. ""We are not only assessing what happened in last 30 years but also what are the potentials of China to relieve nitrogen pollution with adjusting their trade portfolio,"" she said. ""we found such potentials are less than but comparable to the nitrogen mitigation potentials by improving technologies and practices for nutrient management."" "
Science Daily,"Climate Change Could Bring Short-Term Gain, Long-Term Pain for Loggerhead Turtles",Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111658.htm,"   For the loggerhead turtle, whose vast range extends from the chilly shores of Newfoundland to the blistering beaches of Australia, the story isn't so cut and dried. New research from conservation biologists at Florida State University and their collaborators suggests that while some loggerheads will suffer from the effects of a changing climate, populations in certain nesting areas could stand to reap important short-term benefits from the shifting environmental conditions. In an investigation of 17 loggerhead turtle nesting beaches along the coast of Brazil, scientists found that hatchling production -- the rate of successful hatching and emergence of hatchling turtles -- could receive a boost in temperate areas forecasted to warm under climate change. But those improvements could be relatively short lived. ""Even though hatchling success is projected to increase by the year 2100 in areas that currently have lower temperatures, it is likely that as climate change progresses and temperatures and precipitation levels approach negative thresholds, hatchling production at these locations will start to decrease,"" said study author Mariana Fuentes, an assistant professor in FSU's Department of Earth, Ocean and Atmospheric Science. The study was published in the journal Scientific Reports. During the incubation process, marine turtle eggs are heavily influenced by their environments. Air and sand temperatures can determine the sex of hatchlings, spikes in moisture content can drown developing embryos, and excessive solar radiation exposure can affect turtles' morphology and reduce their chances of survival. In their study, the FSU researchers evaluated current and projected hatchling production under a variety of different environmental conditions throughout the expansive Brazilian coastline. In the northern equatorial nesting beaches where temperatures already soar, the team found persistent and accelerating climate change will increase air temperatures and escalate precipitation beyond the thresholds for healthy incubation -- a major hit to hatchling production. For the temperate beaches farther down the coast, climate change will bring similar increases in air temperature and precipitation. But, hundreds of miles from the equator, the effects of those changes look considerably different. ""These cooler beaches are also predicted to experience warming air temperatures; however, productivity is predicted to increase under both the extreme and conservative climate change scenarios,"" said former Florida State master's student Natalie Montero, who led the study. Over the coming decades, as the climate shifts and temperatures climb, these conventionally cooler beaches will become more suitable for healthy loggerhead incubation. But if climate change continues unabated, ""these beaches could also become too warm for successful production, much like the warmer beaches in our study,"" Montero said. The researchers also stress that changes associated with a warming climate -- beach erosion, unchecked coastal development and environmental degradation, for example -- pose urgent threats to marine turtle nesting beaches at all latitudes, regardless of air temperature or precipitation. And while contemporary and future shifts in climate conditions could benefit select loggerhead populations, well-documented warming trends suggest the long-term prospects of these and other ancient sea turtle species remain precarious. ""Sea turtles have been around for a long time and have survived many changes to the global climate,"" Montero said. ""However, climate changes of the past took a long time, allowing sea turtles to adapt to the changing conditions. Today's climate change is happening very quickly, and therefore sea turtles must adapt quickly or perish."" "
Science Daily,New Patterns of Key Ocean Nutrient,Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905103017.htm,"   ""This is a bit of a wake-up call that, as technology advances, we need to update the underlying data and processes that fuel ocean models to ensure they yield the best possible predictions,"" said Mike Lomas, senior research scientist at Bigelow Laboratory for Ocean Sciences and an author of the paper. ""Models are our best chance to predict the many ways the oceans will respond as climate changes, but our predictions are only as good as the data underlying them."" Lomas and his coauthors found that data collected using high-sensitivity measurements reveal that phosphate in the surface ocean is less abundant than traditional measurements and models suggest. They also discovered previously-unknown patterns in phosphate levels in major ocean basins in both the Atlantic and Pacific. Ocean algae depend upon the mineral phosphate, which is essential to all life on Earth. Lower phosphate levels pose challenges for algae, which are already predicted to suffer as climate change makes ocean nutrients scarcer. The research team compiled high-sensitivity phosphate datasets collected throughout the global ocean, including measurements Lomas collected over nearly two decades of intensive study in the Sargasso Sea. They mapped this new, more accurate dataset, and compared the result to that yielded by more common, lower-sensitivity methods. These methods are used widely because they require less time and less sophisticated equipment, but they don't allow researchers to detect phosphate levels as accurately. ""The reality is that we've been operating with a skewed view of ocean phosphate,"" Lomas said. ""With this improvement, we will be able to make our models a little bit more realistic and better able to predict this aspect of climate change's impact on our oceans."" The element phosphorus is a key ingredient in the fundamental molecules of life, including the genetic code DNA and the molecule ATP, which provides the energy to fuel cells. It is rare in the ocean, and, unlike most other essential nutrients, the amount of phosphorus on Earth is finite. Knowing the true availability of phosphorus in the ocean is essential to accurately describing how the base of the food web functions -- and how it will respond as climate change alters ocean chemistry. Lomas hopes to continue this research by studying how ocean algae take in organic phosphorus dissolved in seawater, another potentially important -- and poorly understood -- source of the element. He believes this phosphorus may help support algae in low-phosphate regions, and that understanding its role has the potential to improve ocean models even further. ""There is a vast sea of existing data to explore, and it likely holds as many undiscovered truths as the bottom of the ocean,"" Lomas said. ""As a global scientific community, we need to support more projects that help us capitalize on this trove for important insights into the ocean."" "
Science Daily,Taxing Sweetened Drinks by the Amount of Sugar Could Cut Obesity and Boost Economic Gains,Science & Society,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145434.htm,"   The analysis, by researchers at New York University, Harvard's TH Chan School of Public Health, the Wharton School at the University of Pennsylvania, and the University of California, Berkeley, appears in the journal Science. Seven U.S. cities currently tax sugar-sweetened beverages, or SSBs, by the volume of the beverage -- levies that don't take into account the amount of sugar these drinks contain. ""Despite their different sugar content and resulting different harms, all sugar-sweetened beverages are taxed at the same rate per liter under a volumetric tax,"" write Harvard's Anna Grummon, NYU's Hunt Allcott, Wharton's Benjamin Lockwood, and UC Berkeley's Dmitry Taubinsky. ""This tax structure gives consumers no incentive to substitute from high-sugar to low-sugar SSBs, even though the latter are less harmful. Thus, while a volumetric tax reduces consumption of SSBs in general, it does not provide the maximum possible health benefits."" ""A basic economic principle is that such corrective taxes should be proportional to the harm caused,"" the authors add. ""The harm from sugary drinks comes from the sugar, and SSBs vary substantially in sugar per unit volume."" The researchers note, however, that a tax on liquid volume is beneficial. They estimate, for instance, that a 34-cent per liter volumetric tax causes the average U.S. adult to drink 2.9 fewer ounces of SSBs per day, a 22-percent reduction. This decrease in sugar intake would help the average adult to lose 2.3 pounds. In addition, a nationwide volumetric SSB tax would reduce obesity rates by 2 percent -- a 2.1 million decline in adults with obesity -- and would lower the number of new Type 2 diabetes cases by 2.3 percent, or approximately 36,000 new cases per year. They add that such a tax would also result in economic gains -- primarily through savings in health care costs -- of about $1.4 billion per year nationwide. However, in their assessment, a tax on the amount of sugar in SSBs would yield even greater health and economic gains. Such a tax would cause U.S. adults to consume 2.3 fewer grams of sugar per day from SSBs than they would under a volumetric tax, helping the average adult lose an additional 0.7 pounds. Across the U.S., a sugar tax instead of a volumetric tax would reduce obesity rates by an additional 630,000 adults and would cut the number of new Type 2 diabetes cases by another 0.7 percent -- or approximately 11,000 people per year. Moreover, the additional annual economic gain would be another $400 million. ""Once there is agreement to tax SSBs, it seems natural to tax the harmful sugar, instead of the liquid that comes with the sugar,"" the authors conclude. ""Our calculations suggest that this idea offers valuable low-hanging fruit for improving public health."" A previous study by Allcott, Lockwood, and Taubinsky concluded that soda taxes serve as a ""net good,"" an assessment based on an examination of health benefits and consumer behavior. That analysis, which appeared in the Quarterly Journal of Economics earlier this year, estimated that a nationwide soda tax would yield $7 billion in net benefits to society each year. "
Science Daily,More Than a Billion Fewer Cigarettes Smoked Each Year in England as People Ditch the Cigs,Science & Society,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829115409.htm,"   Between 2011 and 2018, average monthly cigarette consumption fell by nearly a quarter, equating to around 118 million fewer cigarettes being smoked every month. This decline suggests that stricter tobacco laws and taking action to encourage people to quit smoking are working. The researchers, based at UCL, looked at cigarette sales data for England and compared this with the monthly self-reported cigarette use of over 135,000 individuals from the Smoking Toolkit Study. They found that the two different methods of looking at how many cigarettes people are smoking provided similar results. Over the whole period, the average number of cigarettes smoked monthly declined by 24.4% based on survey data and 24.1% based on sales data from 3.40 billion and 3.41 billion a month to 2.57 billion and 2.58 billion, respectively. Lead author, Dr Sarah Jackson from UCL's Tobacco and Alcohol Research Group, said: ""It's brilliant that over a billion fewer cigarettes are being sold and smoked in England every year. The decline in national cigarette consumption has been dramatic and exceeded the decline in smoking prevalence, which, over the same time period, was around 15%. This means that not only are fewer people smoking, but those who continue to smoke are smoking less. ""Studies like this help to give us an accurate picture of cigarette consumption so we know where we're at and what more needs to be done."" Currently 16% of English adults smoke cigarettes. George Butterworth, senior policy manager at Cancer Research UK, said: ""It's great news that fewer cigarettes are being sold and smoked. Big tobacco said that introducing stricter regulation wouldn't work and campaigned against it, but this is proof that smoking trends are heading in the right direction. But smoking is still the biggest preventable cause of cancer, and certain groups have much higher rates of smoking, such as routine and manual workers, so we can't stop here and think job done. ""Last month the government committed to making the UK smokefree by 2030. But stop smoking services, which give smokers the best chance of quitting, have been subject to repeated cuts in recent years. We need the government to fix the funding crisis in local stop smoking services. The tobacco industry could be made to pay for these services to clean up the mess their products have created.""   "
Science Daily,"Kids Wore Video Cameras in Their Preschool Class, for Science",Education & Learning,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828140041.htm,"   The researchers documented these different experiences using a novel technique in the classroom: They had children wear a video camera on their head for two hours on one day to see what the class was like from the child's perspective. In this study, published recently in PLOS ONE, the researchers were interested in the linguistic environment -- how were children exposed to language in the class? ""We found that the duration and frequency of kids' interactions with teachers and peers was very different among kids,"" said Leydi Johana Chaparro-Moreno, lead author of the study and a doctoral student in educational psychology at The Ohio State University. ""Overall, children interacted more with the teacher than their peers, but for some kids their peers were an additional source of hearing language."" This was something that researchers could not easily learn from traditional studies, she said. Most previous studies observed how teachers behaved in the class, such as how they talked to kids. ""What was missing was the perspective of the kids. Especially in preschool classrooms, each child may not be listening to the teacher at the same time or interacting in the same way,"" Chaparro-Moreno said. The study involved 13 children, ages 3 and 4, who attended an urban nonprofit childcare center full-day preschool located in an urban community. The children, whose parents agreed to let them join the study, wore the wireless head-mounted camera on a randomly assigned day during a four-day period. They wore it for one hour in the morning, during which they participated in different kinds of activities: some involving the whole class, and some where the kids were free to choose what to do in various centers around the classroom. Four children wore a camera each day, as did the master teacher. Afterward, the researchers analyzed the videos to see how many times each child interacted with the teacher or a peer, how long each interaction lasted, and a variety of aspects of the interaction. Overall, about 60 percent of the interactions caught on video were with the teacher. And the total duration of interactions with teachers was almost three times longer than the interactions with peers. But there was wide variation. Six of the 13 children interacted with their peers almost as many times as they interacted with their teachers. In addition, four children interacted for longer periods of time with their peers, relative to the average interaction length among all the children. When someone was talking directly to a child wearing a camera in class, 81 percent of the time it was the teacher. Kids also heard nearly all sophisticated language from their teachers -- 92 percent of all complex sentences came from teachers. But again, there was a lot of variability, Chaparro-Moreno said. For example, for every type of word and complex sentence that one child heard per minute from teachers, another child heard five types of words and four complex sentences. ""Teachers may adapt how they interact with kids to meet the students' learning needs,"" she said. ""It is also possible that children's language ability and personality may play a role in how they interact with the teacher and their peers."" This study is part of a larger five-year, $4.5 million project called Early Learning Ohio funded by the federal Institute of Education Sciences, said Laura Justice, co-author of the PLOS ONE paper and principal investigator for the project. The project, which involves students from preschool to grade 3, is ""trying to identify those aspects of the classroom experience that are most important to academic development and social development,"" said Justice, who is also director of Ohio State's Crane Center for Early Childhood Research and Policy. The data from the camera study will be used in future studies to model the social network in the classroom and examine how kids resolve conflicts with each other. Researchers participating in the project are experimenting with other technologies, such as location sensors to see how kids move around classrooms and interact with their peers. ""Our goal is to map the dimensions of the classroom experience, including relationships with teachers and peers, how much kids enjoy school and whether they are victimized by other children,"" Justice said. "
