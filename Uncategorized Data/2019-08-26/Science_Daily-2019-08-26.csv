Source,Heading,Category,Date,Time,URL,Text
Science Daily,Hiring Committees That Don't Believe in Gender Bias Promote Fewer Women,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112653.htm,"   Opinions vary, but a new study by a UBC psychologist and researchers in France reveals that hiring committees who denied it's a problem were less likely to promote women. ""Our evidence suggests that when people recognize women might face barriers, they are more able to put aside their own biases,"" said Toni Schmader, a UBC psychology professor and Canada Research Chair in social psychology. ""We don't see any favourability for or against male or female candidates among those committees who believe they need to be vigilant to the possibility that biases could be creeping in to their decision-making."" The study was unique in that findings were based on actual decisions made by 40 hiring committees in France, charged with filling elite research positions with the National Committee for Scientific Research (CNRS) for two consecutive years. Past research in this area has relied mostly on hypothetical scenarios, such as presenting a large sample of participants with identical resum√©s bearing either male or female names and asking who they would hire. By contrast, the decisions made during this study had real impact on scientists' careers. With cooperation from the CNRS, the researchers were able to first measure how strongly hiring committee members associated men with science. They did this using an ""implicit association test"" that flashes words on a computer screen and measures how quickly participants are able to assign those words to a particular category. People who make a strong association between men and science have to think a bit longer, and react more slowly, when challenged to pair female-related words with science concepts. Both men and women on the hiring committees tended to show the science = male association, which is difficult to hide in such a test. ""There's research suggesting that you can document a 'think science, think male' implicit association showing up with kids as early as elementary school,"" Schmader said. ""We learn associations from what we see in our environment. If we don't see a lot of women who are role models in science, then we learn to associate science more with men than women."" These implicit associations are distinct from people's explicit beliefs about women in science. In a separate survey that asked panellists directly whether women in science careers are impacted by such things as discrimination and family constraints, some hiring committees minimized those issues. Others acknowledged them. When the researchers compared these implicit and explicit beliefs with the actual hiring outcomes, they learned that committees attuned to the barriers women face were more likely to overcome their implicit science/male associations when selecting candidates for the job. Among committees that believed ""science isn't sexist,"" those which implicitly associated science more with men promoted fewer women. The difference was especially pronounced in Year 2 of the study, when committee members would have been less conscious of the fact that their selections were being studied. The findings show that awareness and acknowledgement of the barriers women face might be key to making sure implicit biases don't affect hiring decisions. They also point to the importance of educating hiring committees about gender bias and how to guard against it, Schmader said. The study was published today in Nature Human Behaviour. "
Science Daily,Breaching the Brain's Defense Causes Epilepsy,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112644.htm,"   The study found that just before an epileptic seizure, nerve cells were abnormally active but only in a localized area of the brain. Instead, glial cells showed large burst of synchronous activity that are widely dispersed across the brain. During the actual seizure, the neuronal activity increased abruptly. The functional connections between the nerve cells and glial cells became vigorous. When this happened, generalized seizure spread like a storm of electrical activity across the entire brain due to a strong increase in the level of glutamate, a chemical compound that transmits signals between neuronal cells. Glutamate was secreted by glial cells, which convert themselves from a friend to a foe. The findings indicate that epilepsy may occur not only due to anomalies in neurons, but also in glial cells. ""Our results provide a direct evidence that the interactions between glial cells and neurons change during the transition from a pre-seizure state to a generalized seizure. It will be interesting to see if this phenomenon is generalizable across different types of epilepsies,"" says Prof. Emre Yaksi. Normally, the glial cells absorb the excess glutamate that is excreted during the increased activity of the nerve cells. This study assumes that the secretion process of the glial cells that we observed in combination with their hyperactivity just before a seizure is a defence mechanism of the brain. "
Science Daily,Even Scientists Have Gender Stereotypes ... Which Can Hamper the Career of Women Researchers,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826110354.htm,"   Women remain underrepresented in scientific research: at the French National Centre for Scientific Research (CNRS), across all disciplines, the average percentage of female researchers is 35%. And the higher the scientific research position, the more this percentage declines. Several reasons have been cited to explain these disparities: differences in levels of motivation, self-censorship ... but is discrimination also part of the story? To find out, scientists in social and cognitive psychology studied 40 evaluation committees (1) tasked with evaluating applications for research director (2) positions at the CNRS over a period of two years. This is the first time that a research institution has carried out such a scientific study of its practices in the course of an annual nationwide competition covering the entire scientific spectrum. This study shows that, from particle physics to the social sciences, most scientists, whether male or female, associate ""science"" and ""masculine"" in their semantic memory (the memory of concepts and words). This stereotype is implicit, which is to say that most often it is not detectable at the level of discourse. And it is equivalent to that observed among the general population. Yet does this implicit stereotype have consequences on the decisions made by evaluation committees? Yes, when committees deny or minimise the existence of bias against women.(3) Here, this is the case for around half of the committees. In these committees, the stronger the implicit stereotypes, the less often women are promoted. In contrast, when committees acknowledge the possibility of bias, implicit stereotypes, however strong they may be, have no influence. Even if disparities between men and women in science have multiple causes and start at school (as the same authors have shown in other publications), this study indicates for the first time the existence of implicit gender stereotypes among male and female researchers across all disciplines -- stereotypes that can harm the careers of women scientists. Since 2019, at the instigation of the CNRS Mission for the place of women, members of evaluation committees have been invited to participate in training sessions on gender stereotypes and each committee has appointed a reference person in charge of gender equality issues. However, the authors of the study emphasise that, in order to be fully effective, this process must be accompanied by other measures aiming, on the one hand, to enlighten committee members on the exact conditions in which implicit stereotypes influence their decisions, and, on the other, to explain strategies likely to control this influence. Notes: (1) In total, 414 people participated to the study. The committees considered in this study have since come to the end of their commission. (2) A senior researcher. (3) They more often attribute gender disparities in science to the choices made by women or gender differences in ability than to the existence of discrimination or the constraints of family life. "
Science Daily,Exercise Is Good for the Aging Brain,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826110409.htm,"   University of Iowa researchers have found that a single bout of exercise improves cognitive functions and working memory in some older people. In experiments that included physical activity, brain scans, and working memory tests, the researchers also found that participants experienced the same cognitive benefits and improved memory from a single exercise session as they did from longer, regular exercise. ""One implication of this study is you could think of the benefits day by day,"" says Michelle Voss, assistant professor in the Department of Psychological and Brain Sciences and the study's corresponding author. ""In terms of behavioral change and cognitive benefits from physical activity, you can say, 'I'm just going to be active today. I'll get a benefit.' So, you don't need to think of it like you're going to train for a marathon to get some sort of optimal peak of performance. You just could work at it day by day to gain those benefits."" Previous research has shown exercise can confer a mental boost. But the benefits vary: One person may improve cognitively and have improved memory, while another person may show little to no gain. Limited research has been done on how a single bout of physical activity may affect cognition and working memory specifically in older populations, despite evidence that some brain functions slip as people age. Voss wanted to tease out how a single session of exercise may affect older individuals. Her team enrolled 34 adults between 60 and 80 years of age who were healthy but not regularly active. Each participant rode a stationary bike on two separate occasions -- with light and then more strenuous resistance when pedaling -- for 20 minutes. Before and after each exercise session, each participant underwent a brain scan and completed a memory test. In the brain scan, the researchers examined bursts of activity in regions known to be involved in the collection and sharing of memories. In the working memory tests, each participant used a computer screen to look at a set of eight young adult faces that rotated every three seconds -- flashcard style -- and had to decide when a face seen two ""cards"" previously matched the one they were currently viewing. After a single exercise session, the researchers found in some individuals increased connectivity between the medial temporal (which surrounds the brain's memory center, the hippocampus) and the parietal cortex and prefrontal cortex, two regions involved in cognition and memory. Those same individuals also performed better on the memory tests. Other individuals showed little to no gain. The boost in cognition and memory from a single exercise session lasted only a short while for those who showed gains, the researchers found. ""The benefits can be there a lot more quickly than people think,"" Voss says. ""The hope is that a lot of people will then keep it up because those benefits to the brain are temporary. Understanding exactly how long the benefits last after a single session, and why some benefit more than others, are exciting directions for future research."" The participants also engaged in regular exercise, pedaling on a stationary bike for 50 minutes three times a week for three months. One group engaged in moderate-intensity pedaling, while another group had a mostly lighter workout in which the bike pedals moved for them. Most individuals in the moderate and lighter-intensity groups showed mental benefits, judging by the brain scans and working memory tests given at the beginning and at the end of the three-month exercise period. But the brain gains were no greater than the improvements from when they had exercised a single time. ""The result that a single session of aerobic exercise mimics the effects of 12 weeks of training on performance has important implications both practically and theoretically,"" the authors write. The researchers note their study had a small participant pool, with a homogenous population that excluded anyone with chronic health conditions or who were taking beta-blockers. To address those limitations, Voss has expanded her participant pool in a current, five-year study to confirm the initial findings and learn more about how exercise alters older people's brains. The participants are healthy older individuals who are not physically active, similar to the participants' profile in the study's results reported here. The National Institute on Aging, part of the National Institutes of Health, funded the research. "
Science Daily,Medicare Patients With Multiple Sclerosis Bear the Burden of Rising Drug Prices,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826110402.htm,"   Using Medicare claims data from 2006-2016, the researchers looked at trends in multiple sclerosis drug prices over time. Not only did they find steep increases in list prices -- the starting point before rebates, coupons or insurance kicks in -- but also in the ultimate costs to both Medicare and its recipients. ""We wanted to see how increases in list prices translated to increases in out-of-pocket spending, and we discovered that actual price increases do get passed down to patients, and that can negatively affect access,"" said study senior author Inmaculada Hernandez, Pharm.D., Ph.D., assistant professor of pharmacy at Pitt. Several drugs on the market reduce the frequency and severity of multiple sclerosis flare-ups, which can involve a variety of disabling neurological symptoms, such as vision loss, pain, fatigue and muscle weakness. From 2006-2016, the annual list prices of these drugs more than quadrupled, ballooning from about $18,000 to nearly $76,000 per patient per year. Some of the most popular drugs for treating multiple sclerosis are Copaxone, Tecfidera and Avonex, and, despite increased market competition over time, prices have been rising steadily for nearly all of them. ""One of the most significant findings was that the prices of these drugs have increased in parallel,"" said lead author Alvaro San-Juan-Rodriguez, Pharm.D., a pharmacy fellow at Pitt. ""Only a couple exceptions deviate from that general trend."" Although this trend among list prices is alarming on its own, critics have argued that since some of the cost is canceled out by manufacturer rebates and other kinds of discounts, rising list prices may not be translating into increased spending. But since Medicare claims provide a detailed cost breakdown, the researchers were able to measure changes in what Medicare Part D beneficiaries actually paid out of pocket for multiple sclerosis drugs, as well as what Medicare itself paid. What they found was that from 2006-2016, Medicare spending increased by more than tenfold, and the patients themselves saw more than a sevenfold increase in their share of the bill. ""We're not talking about patients without health insurance here,"" Hernandez said. ""We're talking about insured patients, under Medicare. Still, they are paying much more for multiple sclerosis drugs than they were 10 years ago."" Additional authors on the study include Chester Good, M.D., M.P.H., Natasha Parekh, M.D., M.S., and William Shrank, M.D., M.S.H.S., from the UPMC Health Plan; and Rock Heyman, M.D., from Pitt and UPMC. Funding was provided by the Myers Family Foundation and the National Heart, Lung, and Blood Institute (grant number K01HL142847). Hernandez disclosed fees paid to her personally by Pfizer, for services unrelated to the scope of this work. "
Science Daily,Stable Home Lives Improve Prospects for Preemies,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104830.htm,"   Researchers at Washington University School of Medicine in St. Louis who have been trying to determine what puts such children at risk for these problems have found that their mental health may be related less to medical challenges they face after birth than to the environment the babies enter once they leave the newborn intensive care unit (NICU). In a new study, the children who were most likely to have overcome the complications of being born so early and who showed normal psychiatric and neurodevelopmental outcomes also were those with healthier, more nurturing mothers and more stable home lives. The findings are published Aug. 26 in The Journal of Child Psychology and Psychiatry. ""Home environment is what really differentiated these kids,"" said first author Rachel E. Lean, PhD, a postdoctoral research associate in child psychiatry. ""Preterm children who did the best had mothers who reported lower levels of depression and parenting stress. These children received more cognitive stimulation in the home, with parents who read to them and did other learning-type activities with their children. There also tended to be more stability in their families. That suggests to us that modifiable factors in the home life of a child could lead to positive outcomes for these very preterm infants."" The researchers evaluated 125 5-year-old children. Of them, 85 had been born at least 10 weeks before their due dates. The other 40 children in the study were born full-term, at 40 weeks' gestation. The children completed standardized tests to assess their cognitive, language and motor skills. Parents and teachers also were asked to complete checklists to help determine whether a child might have issues indicative of ADHD or autism spectrum disorder, as well as social or emotional problems or behavioral issues. It turned out the children who had been born at 30 weeks of gestation or sooner tended to fit into one of four groups. One group, representing 27% of the very preterm children, was found to be particularly resilient. ""They had cognitive, language and motor skills in the normal range, the range we would expect for children their age, and they tended not to have psychiatric issues,"" Lean said. ""About 45% of the very preterm children, although within the normal range, tended to be at the low end of normal. They were healthy, but they weren't doing quite as well as the more resilient kids in the first group."" The other two groups had clear psychiatric issues such as ADHD, autism spectrum disorder or anxiety. A group of about 13% of the very preterm kids had moderate to severe psychiatric problems. The other 15% of children, identified via surveys from teachers, displayed a combination of problems with inattention and with hyperactive and impulsive behavior. The children in those last two groups weren't markedly different from other kids in the study in terms of cognitive, language and motor skills, but they had higher rates of ADHD, autism spectrum disorder and other problems. ""The children with psychiatric problems also came from homes with mothers who experienced more ADHD symptoms, higher levels of psychosocial stress, high parenting stress, just more family dysfunction in general,"" said senior investigator Cynthia E. Rogers, MD, an associate professor of child psychiatry. ""The mothers' issues and the characteristics of the family environment were likely to be factors for children in these groups with significant impairment. In our clinical programs, we screen mothers for depression and other mental health issues while their babies still are patients in the NICU."" Rogers and Lean believe the findings may indicate good news because maternal psychiatric health and family environment are modifiable factors that can be targeted with interventions that have the potential to improve long-term outcomes for children who are born prematurely. ""Our results show that it wasn't necessarily the clinical characteristics infants faced in the NICU that put them at risk for problems later on,"" Rogers said. ""It was what happened after a baby went home from the NICU. Many people have thought that babies who are born extremely preterm will be the most impaired, but we really didn't see that in our data. What that means is in addition to focusing on babies' health in the NICU, we need also to focus on maternal and family functioning if we want to promote optimal development."" The researchers are continuing to follow the children from the study. This work was supported by the Eunice Kennedy Shriver National Institute of Child Health and Human Development, the National Institute of Neurological Disorders and Stroke and the National Institute of Mental Health of the National Institutes of Health (NIH). Grant numbers R01 HD057098, R01 MH113570, K02 NS089852, UL1 TR000448, K23-MH105179 and U54-HD087011. Additional funding was provided by the Cerebral Palsy International Research Foundation, the Dana Foundation, the Child Neurology Foundation and the Doris Duke Charitable Foundation. "
Science Daily,Simple Blood Test Unmasks Concussions Absent on CT Scans,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092334.htm,"   In a study led by UC San Francisco, researchers tracked 450 patients with suspected traumatic brain injury (TBI) -- which includes concussion or mild TBI -- who had been admitted to one of 18 level 1 trauma centers throughout the nation. The patients, whose injuries were mainly attributed to traffic accidents or falls, all had normal CT scans, according to the study publishing in The Lancet Neurology on Aug. 23, 2019. Within 24 hours of their accidents, the patients had their blood drawn to measure for glial fibrillary acidic protein, a marker correlating to TBI. The study used a device by Abbott Laboratories called i-STAT‚Ñ¢ Alinity‚Ñ¢, a handheld portable blood analyzer, currently unavailable in the U.S., that produces test results in minutes. The researchers later confirmed the blood test results against MRI, a more sensitive and expensive scan that is not as widely available as CT but offers a more definitive diagnosis of TBI. They found that 120 of these 450 patients (27 percent) had an MRI that was positive for TBI. Patients with TBI Are Not Even Getting a Diagnosis ""Our earlier research has shown that even in the best trauma centers, patients with TBI are not getting the care they need,"" said Geoffrey Manley, MD, PhD, senior author of the study, professor of neurosurgery at UCSF and a member of the Weill Institutes for Neurosciences. ""Now we know that many of these patients with TBI are not even getting a diagnosis."" Manley is also the principal investigator of TRACK TBI, which has analyzed clinical data on more than 3,300 patients and comparison participants, and previously has linked concussion with major depression, post-traumatic stress disorder and cognitive deficits. Work by other UCSF faculty has found correlations between TBI and Parkinson's disease and TBI and dementia. To assess the accuracy of the blood test, researchers compared the results of the patients whose CT-negative TBIs were confirmed by MRI, with a group of healthy participants as well as a cohort of patients with orthopedic injuries. They found that the average protein value of the blood samples of patients with positive MRIs was 31.6 times higher than those with orthopedic injuries and nearly 52 times that of the healthy participants. The protein was elevated even in the patients with normal MRIs, suggesting that the test may be sensitive to injury undetectable by MRI, the researchers noted. In the future, the blood test may help clinicians decide who can safely avoid a CT scan, with the advantage of not exposing patients to radiation from a CT, said first author John Yue, MD, of the UCSF Department of Neurological Surgery. Additionally, the blood test may be a useful tool for those patients in trauma centers and emergency departments, whose symptoms may be altered by substance use, he said. ""Patients with concussion may present as confused and disoriented, and may repeat themselves -- symptoms that are similar in people with intoxication. With the blood test, we may be able to discern whether their symptoms are primarily due to brain injury and treat accordingly."" The blood test may also clarify diagnosis in patients with co-existing conditions or those who take medications that may impact speech and behavior, said Yue. ""These blood-based biomarkers are the next step in the evolution of diagnosing and treating TBI,"" said Manley. ""We are finding that not only are they more sensitive than CT in identifying TBI, but they may be more accurate than the current standard of MRI."" The study follows an earlier TRACK-TBI pilot study that found approximately 30 percent of concussion patients with negative CTs and positive MRIs had disability three months post-injury. "
Science Daily,Blood Test Detects Concussion and Subconcussive Injuries in Children and Adults,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092315.htm,"   Subconcussive injuries often show no symptoms or immediate effects, but can cause wear and tear on the brain over time with repeated injuries. The latest study, published in the journal BMJ Paediatrics Open, includes more than 700 emergency room patients -- children and adults. The study gets us closer to developing a standard blood test to spot these injuries as early as possible. ""A unique feature of this study is that it includes patients who hit their heads but have no symptoms,"" said Linda Papa, MD, lead author of the study and emergency medicine doctor at Orlando Health. ""This group is rarely -- if ever -- included in biomarker studies."" The blood test looks for two proteins (GFAP and UCH-L1) found in our brains and released into blood after an injury -- higher levels of which could indicate a concussion or subconcussive injury. Dr. Papa has been studying these biomarkers for more than a decade. Some of her previous studies have focused on athletes, but now she's expanding her research on subconcussive injuries to the general population and all age groups. Historically, people who suffer head trauma without concussion symptoms may have been classified as having ""no injury."" Plus, there are very few studies addressing the impact of subconcussive injuries following head trauma in the civilian population, as opposed to military members or athletes. ""It is estimated that up to 3.8 million concussions occur in the U.S. annually from organized and recreational sports -- and there are more than 2 million ER visits for traumatic brain injuries and concussions,"" said Papa. ""It is a significant health problem in both athletes and non-athletes."" The study looked at patients with concussions, those with head trauma without overt signs of concussion and those with body trauma without head trauma or concussion. Elevated levels of both biomarkers were found in patients with nonconcussive head trauma, potentially signaling a subconcussive brain injury. Furthermore, this blood test goes even deeper than a routine CT scan. Previous studies using the two biomarkers have focused on detecting brain lesions, but subconcussive injuries don't necessarily result in lesions -- and even the vast majority of patients with concussions tend to have a normal CT scan. ""The study includes an array of patients with different injury mechanisms, including car crashes, falls and bicycle accidents in addition to recreational and sports injuries,"" said Papa. ""It is not limited to just one group of injury types."" A number of companies are now working on developing a bench-top device for the hospital lab -- along with a point-of-care handheld device that can be used to detect subconcussive injuries in a variety of settings -- including sporting events, in the ambulance, at the scene of car crashes, in military settings or even after a simple bump to the head. ""The technology is only a year or two away,"" said Papa. "
Science Daily,A Lack of Background Knowledge Can Hinder Reading Comprehension,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092303.htm,"   ""Background knowledge plays a key role in students' reading comprehension -- our findings show that if students don't have sufficient related knowledge, they'll probably have difficulties understanding text,"" says lead researcher Tenaha O'Reilly of Educational Testing Service (ETS)'s Center for Research on Human Capital in Education. ""We also found that it's possible to measure students' knowledge quickly by using natural language processing techniques. If a student scores below the knowledge threshold, they'll probably have trouble comprehending the text."" Previous research has shown that students who lack sufficient reading skills, including decoding and vocabulary, fare poorly relative to their peers. But the research of O'Reilly and ETS colleagues Zuowei Wang and John Sabatini suggests that a knowledge threshold may also be an essential component of reading comprehension. The researchers examined data from 3,534 high-school students at 37 schools in the United States. The students completed a test that measured their background knowledge on ecosystems. For the topical vocabulary section of the test, the students saw a list of 44 words and had to decide which were related to the topic of ecosystems. They also completed a multiple-choice section that was designed to measure their factual knowledge. Then, after reading a series of texts on the topic of ecosystems, the students completed 34 items designed to measure how well they understood the texts. These comprehension items tapped into their ability to summarize what they had read, recognize opinions and incorrect information, and apply what they had read to reason more broadly about the content. The researchers used a statistical technique called broken-line regression -- often used to identify an inflection point in a data set -- to analyze the students' performance. The results revealed that a background-knowledge score of about 33.5, or about 59% correct, functioned as a performance threshold. Below this score, background knowledge and comprehension were not noticeably correlated; above the threshold score, students' comprehension appeared to increase as their background knowledge increased. Additional results indicated that the pattern could not be fully explained by the level of students' knowledge on a different topic -- what mattered was their background knowledge of ecosystems. The researchers found that students' ability to identify specific keywords was a fairly strong predictor whether they would perform above or below the threshold. That is, correctly identifying ecosystems, habitat, and species as topically relevant was more strongly linked with students' comprehension than was identifying bioremediation, densities, and fauna. The findings underscore the importance of having reached a basic knowledge level to be able to read and comprehend texts across different subjects: ""Reading isn't just relevant to English Language Arts classes but also to reading in the content areas,"" says O'Reilly. ""The Common Core State Standards highlight the increasing role of content area and disciplinary reading. We believe that the role of background knowledge in students' comprehension and learning might be more pronounced when reading texts in the subject areas."" The researchers plan to explore whether a similar kind of knowledge threshold emerges in other topic areas and domains; they note that it will be important to extend the research by focusing on diverse measures and populations. If the pattern holds, the findings could have important applications for classroom teaching, given the availability of knowledge assessments that can be administered without taking valuable time away from instruction. ""If we can identify whether a given student does not have sufficient knowledge to comprehend a text, then teachers can provide background material -- for example, knowledge maps -- so that students have a context for the texts they are about to read,"" O'Reilly concludes.. "
Science Daily,Early Disease Detection: Individual Exosomes Identified,Mind & Brain,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092254.htm,"   Exosomes are released from all cells in the body. They convey protein and nucleic acid cargos between the cells as a form of intercellular communication, and they represent potential circulating biomarkers for tumor progression and metastasis, as well as for early detection of neurodegenerative disease. In order to use exosomes as biomarkers of diseases in different tissues it is vital to distinguish them according to their surface protein complements. Researchers at Uppsala University and Vesicode AB, along with collaborators, have developed a method that can map surface protein complements on large numbers of individual exosomes. The novel proximity-dependent barcoding assay (PBA) reveals the surface protein composition of individual exosomes using antibody-DNA conjugates and next-generation sequencing. The method identifies proteins on individual exosomes using micrometer-sized, uniquely tagged single-stranded DNA clusters generated by rolling circle amplification. ""This technology will not only benefit researchers studying exosomes, but also enable high-throughput biomarker discovery. We will further develop and validate the PBA technology and provide service to researchers starting later this year. We believe single exosome analysis will allow this exciting class of biomarkers to reach its full potential,"" says Di Wu, researcher and inventor of the PBA technology and founder of Vesicode AB, commercializing the technique. ""This new technology will allow large-scale screens for biomarkers in disease, complementing a panel of methods for sensitive and specific detection of exosomes that we have previously established,"" says Masood Kamali-Moghaddam one of the group leaders at the Molecular Tools unit at Uppsala University. "
Science Daily,"Physicists Mash Quantum and Gravity and Find Time, but Not as We Know It",Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826122010.htm,"   UQ physicist Dr Magdalena Zych said the discovery arose from an experiment the team designed to bring together elements of the two big -- but contradictory -- physics theories developed in the past century. ""Our proposal sought to discover: what happens when an object massive enough to influence the flow of time is placed in a quantum state?"" Dr Zych said. She said Einstein's theory described how the presence of a massive object slowed time. ""Imagine two space ships, asked to fire at each other at a specified time while dodging the other's attack,"" she said. ""If either fires too early, it will destroy the other."" ""In Einstein's theory, a powerful enemy could use the principles of general relativity by placing a massive object -- like a planet -- closer to one ship to slow the passing of time."" ""Because of the time lag, the ship furthest away from the massive object will fire earlier, destroying the other."" Dr Zych said the second theory, of quantum mechanics, says any object can be in a state of ""superposition"" ""This means it can be found in different states -- think Schrodinger's cat,"" she said. Dr Zych said using the theory of quantum mechanics, if the enemy put the planet into a state of ""quantum superposition,"" then time also should be disrupted. ""There would be a new way for the order of events to unfold, with neither of the events being first or second -- but in a genuine quantum state of being both first and second,"" she said. UQ researcher Dr Fabio Costa said although ""a superposition of planets"" as described in the paper -- may never be possible, technology allowed a simulation of how time works in the quantum world -- without using gravity. ""Even if the experiment can never be done, the study is relevant for future technologies,"" Dr Costa said. ""We are currently working towards quantum computers that -- very simply speaking -- could effectively jump through time to perform their operations much more efficiently than devices operating in fixed sequence in time, as we know it in our 'normal' world."" "
Science Daily,Quantum Criticality Could Be a Boon for Qubit Designers,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112651.htm,"   In a study in the Proceedings of the National Academy of Sciences, researchers from Rice University and the Vienna University of Technology (TU Wien) in Austria examined the behavior of an intermetallic crystal of cerium, palladium and silicon as it was subjected to extreme cold and a strong magnetic field. To their surprise, they found they could transform the quantum behavior of the material in two unique ways, one in which electrons compete to occupy orbitals and another where they compete to occupy spin states. ""The effect is so pronounced with one degree of freedom that it ends up liberating the other one,"" said Rice's Qimiao Si, co-corresponding author of the study and the director of the Rice Center for Quantum Materials (RCQM). ""You can essentially tune the system to maximize damage to one of these, leaving the other well-defined."" Si said the result could be important for companies like Google, IBM, Intel and others who are competing to develop quantum computers. Unlike today's digital computers, which use electricity or light to encode bits of information, quantum computers use the quantum states of subatomic particles like electrons to store information in qubits. A practical quantum computer could outperform its digital counterpart in many ways, but the technology is still in its infancy, and one of the chief obstacles is the fragility of the quantum states inside the qubits. ""You need a well-defined quantum state if you wish to be assured that the information that is stored in a qubit will not change due to background interference,"" Si said. Every electron acts like a spinning magnet, and its spin is described in one of two values, up or down. In many qubit designs, information is encoded in these spins, but these states can be so fragile that even tiny amounts of light, heat, vibration or sound can cause them to flip from one state to another. Minimizing the information that's lost to such ""decoherence"" is a major concern in qubit design, Si said. In the new study, Si worked with longtime collaborator Silke Paschen of TU Wien to study a material where the quantum states of electrons were scrambled not just in terms of their spins but also in terms of their orbitals. ""We designed a system, realized in some theoretical models and concurrently realized in a material, where spins and orbitals are almost on an equal footing and are strongly coupled together,"" he said. From previous research in 2012, Si, Paschen and colleagues knew that electrons in the compound could be made to interact so strongly that the material would undergo a dramatic change at a critically cold temperature. On either side of this ""quantum critical point,"" electrons in key orbitals would arrange themselves in a completely different way, with the shift occurring solely due to the quantum interactions between them. The earlier study invoked a well-known theory Si and collaborators developed in 2001 that prescribes how the spins of these localized electrons, which are part of atoms inside the alloy, strongly couple with free-flowing conduction electrons at the quantum critical point. According to this ""local quantum critical"" theory, as the material is cooled and approaches the critical point, the spins of localized electrons and conduction electrons begin to compete to occupy particular spin states. The quantum critical point is the tipping point where this competition destroys the ordered arrangement of the localized electrons and they instead become completely entangled with the conduction electrons. Even though Si has studied quantum criticality for almost 20 years, he was surprised by the results of Paschen's latest experiments. ""The new data was completely baffling to all of us,"" he said. ""That is, until we realized that the system contained not only spins but also orbitals as active degrees of freedom."" With that realization, Si's team, including Rice graduate student Ang Cai, built a theoretical model that contains both the spins and orbitals. Their detailed analysis of the model revealed a surprising form of quantum criticality that provided a clear understanding of the experiments. ""It was a shock to me, both from the theoretical model perspective and the experiments,"" he said. ""Even though this is a soup of things -- spins, orbitals that are all strongly coupled to each other and to background conduction electrons -- we could resolve two quantum critical points in this one system under the tuning of one parameter, which is the magnetic field. And at each one of the quantum critical points, only the spin or the orbital is driving the quantum criticality. The other one is more or less a bystander."" Si is the Harry C. and Olga K. Wiess Professor in Rice's Department of Physics and Astronomy. The study's co-lead authors are Cai and Valentina Martelli, formerly of TU Wien and now with the University of S√£o Paulo in Brazil. Additional co-authors include Chia-Chuan Liu and Hsin-Hua Lai, both of Rice; Emilian Nica, formerly of Rice and currently at the University of British Columbia; Rong Yu, formerly of Rice and currently at Renmin University of China; Mathieu Taupin, Andrey Prokofiev, Diana Geiger, Jonathan Haenel and Julio Larrea, all of TU Wien; Kevin Ingersent of the University of Florida; Robert K√ºchler of the Max Planck Institute for Chemical Physics of Solids in Dresden, Germany; and Andre Strydom of the University of Johannesburg in South Africa. The research was supported by the National Science Foundation (DMR-1920740, CNS-1338099, PHY-1607611, DMR-1508122), the Robert A. Welch Foundation (C-1411), the Army Research Office (ARO-W911NF-14-1-0525, ARO-W911NF-14-1-0496), the Austrian Science Fund (P29296-N27, DK W1243), the European Research Council (Advanced Grant 227378), the Carlos Chagas Filho Foundation for Research Support of the State of Rio de Janeiro (201.755/2015), the National Natural Science Foundation of China (11674392), the Ministry of Science and Technology of China (2016YFA0300504), the South African National Research Foundation (93549), the University of Johannesburg and RCQM. RCQM leverages global partnerships and the strengths of more than 20 Rice research groups to address questions related to quantum materials. RCQM is supported by Rice's offices of the provost and the vice provost for research, the Wiess School of Natural Sciences, the Brown School of Engineering, the Smalley-Curl Institute and the departments of Physics and Astronomy, Electrical and Computer Engineering, and Materials Science and NanoEngineering. "
Science Daily,Flame Retardants -- From Plants,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092330.htm,"   The researchers will present their results at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""The best flame-retardant chemicals have been organohalogen compounds, particularly brominated aromatics,"" says Bob Howell, Ph.D., the project's principal investigator. ""The problem is, when you throw items away, and they go into a landfill, these substances can leach into the environment."" Most organohalogen flame retardants are very stable. Microorganisms in the soil or water can't degrade them, so they persist for many years in the environment, working their way up the food chain. In addition, some of the compounds can migrate out of items to which they are added, such as electronics, and enter household dust. Although the health effects of ingesting or breathing organohalogen flame retardants are largely unknown, some studies suggest they could be harmful, prompting California to ban the substances in children's products, mattresses and upholstered furniture in 2018. ""A number of flame retardants are no longer available because of toxicity concerns, so there is a real need to find new materials that, one, are nontoxic and don't persist, and two, don't rely upon petroleum,"" Howell says. His solution was to identify compounds from plants that could easily be converted into flame retardants by adding phosphorus atoms, which are known to quench flames. ""We're making compounds that are based on renewable biosources,"" he says. ""Very often they are nontoxic; some are even food ingredients. And they're biodegradable -- organisms are accustomed to digesting them."" To make their plant-derived compounds, Howell and colleagues at the Center for Applications in Polymer Science at Central Michigan University began with two substances: gallic acid, commonly found in fruits, nuts and leaves; and 3,5-dihydroxybenzoic acid from buckwheat. Using a fairly simple chemical reaction, the researchers converted hydroxyl groups on these compounds to flame-retardant phosphorus esters. Then, the team added the various phosphorus esters individually to samples of an epoxy resin, a polymer often used in electronics, automobiles and aircraft, and examined the different esters' properties with several tests. In one of these tests, the researchers showed that the new flame retardants could strongly reduce the peak heat release rate of the epoxy resin, which reflects the intensity of the flame and how quickly it is going to spread. The plant-derived substances performed as well as many organohalogen flame retardants on the market. ""As a matter of fact, they may be better,"" Howell says. ""Because gallic acid has three hydroxyl groups within the same molecule that can be converted to phosphorus esters, you don't have to use as much of the additive, which reduces cost."" The researchers also studied how the new compounds quench flames, finding that the level of oxygenation at the phosphorus atom determined the mode of action. Compounds with a high level of oxygenation (phosphates) decomposed to a substance that promoted char formation on the polymer surface, starving the flame of fuel. In contrast, compounds with a low level of oxygenation (phosphonates) decomposed to species that scavenged combustion-promoting radicals. Howell's team hasn't yet performed toxicity tests, but he says that other groups have done such studies on similar compounds. ""In general, phosphorus compounds are much less harmful than the corresponding organohalogens,"" he notes. In addition, the plant-derived substances are not as volatile and are less likely to migrate from items into household dust. Howell hopes that the new flame retardants will attract the attention of a company that could help bring them to market, he says. "
Science Daily,A 2 Nm Sized Nanomachine Able to Spin and Transfer Its Rotational Energy,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104854.htm,"   Nature has proven exceptional at designing similar machines by using molecules that can convert optical, chemical or electrical energy into interactions with the surface to generate motion. ""For many nanomachines, we look at nature as our model. There are many examples of propellers with which organisms move in dynamic environments. Surprisingly, these natural nanomachines take the shape of large-scale propellers,"" says NAIST Professor Gw√©na√´l Rapenne, who contributed to the new study. Consistently, the propeller Rapenne and his colleagues designed consists of three components: three blades each composed of an indazole, a stator consisting of five phenyl groups, and a ruthenium atom that binds to the two and allow the rotation like a ball bearing. One of the major differences is the conditions in which artificial nanopropellers work. Where natural nanomachines tend to work in environments comfortable for life, artificial nanomachines can work in much harsher conditions. Rapenne demonstrates this point by attaching his machine to a gold surface and observing that some begin swirling at extremely cold temperatures (near -200 oC). At the same time no propellers move at -275 oC, verifying their ability to convert thermal energy into movement. The propellers also showed the capacity to rotate in different directions in a controlled manner, but never to switch directions. This was the result of how the propeller was attached to the gold surface, which caused a slight tilt in the stator. The direction of the tilt determined the direction of the spin. This feature is reminiscent to macroscopic propellers we see in the real world. ""The stator acts as a ratchet-shaped gear that imposes a unidirectional rotation,"" notes Rapenne. This is not the first time Rapenne has used gold to prove the capabilities of his nanomachines. Two years ago, he and colleagues organized the world's first nanocar competition using gold tracks. While he does not expect to follow that effort with the first nano single propeller competition, he does believe the new machines will serve an important purpose in the nanoworld. ""Our propellers can displace nearby molecules, showing that they can be used to move molecular loads for faster transfer of energy or information,"" he says. "
Science Daily,Insights Into High Quality Fabrication of Nanocomposites,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104827.htm,"   Selective laser melting (SLM), also known as laser powder bed fusion (L-PBF), is an additive manufacturing (AM) technology applied to metals and ceramics, and has shown promising potential for fabrication of unique structures and properties such as MMNCs. Using high power laser, SLM allows for quick production of three-dimensional (3D) parts with complicated shapes directly from powder materials without the time-consuming mold design process. This reduces production cost and lead time while delivering customized MMNCs parts for automotive, aerospace, electronics and biomedical industries. However, due to the lack of comprehensive understanding of the defects unique to SLM as well as the fabrication and performance of nanocomposites with SLM, researchers from Singapore University of Technology and Design (SUTD) and their research collaborators set out to gain a thorough understanding of the scientific and technological knowledge. They reviewed state of the art research from the perspective of materials and SLM processing parameters. Their paper was published in Progress in Materials Science, a journal that publishes authoritative reviews of recent advances in the science of materials. An in-depth review of the fabrication considerations related to nanocomposites was also conducted including the materials and SLM processing parameters, emphasizing on physical properties and preparation of powders (refer to image). Thereafter, mechanical properties of MMNCs and the corresponding enhancing mechanisms were addressed to provide a deeper understanding of MMNCs. ""MMNCs have always been a huge interest for material scientists. With the advancement in advanced manufacturing, particularly additive manufacturing, there is now greater potential in achieving high quality MMNCs. In our review, laser powder bed fusion is chosen as the process in focus as it has proven its capabilities in fabricating functional parts from metals and ceramics,"" explained principal investigator and co-author Professor Chua Chee Kai from SUTD. The review paper also addressed the defects unique to SLM technology associated with nanoparticles. The applications of MMNCs especially those fabricated with SLM processing were also listed and compared. ""One of the key challenges in AM is the lack of 'printable' materials. We believe this comprehensive review provides a timely overview and understanding of SLM for MMNCs by focusing on the merits while not ignoring the limitations. This hopefully will encourage more researchers to explore this highly interesting area,"" said co-author Dr Sing Swee Leong from Nanyang Technological University. "
Science Daily,Cleaning Pollutants from Water With Pollen and Spores -- Without the 'Achoo!',Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092326.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""Even very low levels of certain compounds, such as hormones, pharmaceuticals or those in household and personal care products, can cause toxic effects. However, they often can escape normal cleanup processes at wastewater treatment plants,"" says Andrew Boa, Ph.D., whose lab is working on the pollen project. ""We're trying to find alternative ways to remove these chemicals from water so we can reduce the amount going into the environment."" The project is part of the larger ""Sullied Sediments"" program in which Boa and many other scientists are assessing pollutant levels in sediments from European waterways, with a view to assessing dredged sediment, managing sediment reuse and reducing future contaminations. These contaminants include pharmaceuticals such as the pain reliever diclofenac and household chemicals such as triclosan, an antimicrobial compound used in toothpaste, and other personal care products. Some of these chemicals, including triclosan, are either banned or their use heavily restricted. The European Union will begin officially monitoring levels of all of these ""Watch List"" chemicals from 2020 onwards. The spore grains used in the study are extracted from Lycopodium clavatum -- the common club moss. In their natural state, each of these microscopic grains carries genetic material inside a hard shell that's coated with an outer layer of wax and proteins, explains Aimilia Meichanetzoglou, a doctoral student in Boa's lab at the University of Hull. Boa first became interested in pollen thanks to his work with Grahame Mackenzie, Ph.D., a Hull professor (now emeritus) who developed the original method to form non-allergenic, hollowed-out pollen and spore shells. Mackenzie's company, Sporomex, uses the inert shells to encapsulate active ingredients for controlled release in pharmaceutical, food, cosmetic and medical applications. Boa has taken the concept in an entirely different direction. When he and Meichanetzoglou were studying the empty shells' interactions with a variety of chemicals, they noticed that some of the compounds became adsorbed, or stuck to, the surface of the shells. Boa realized this stickiness could potentially be used to grab low levels of pollutants, and so he pursued this type of application. Meichanetzoglou uses hydrolysis to rid the pollen of its genetic cargo and waxy coat, which makes the grains hypoallergenic. To target particular pollutants, she can vary the hydrolysis conditions and make modifications to the surface of the grains. For example, to remove phosphate, which is used in many fertilizers, Meichanetzoglou deposits iron oxide on the surface of the shells. Iron oxide reacts with the phosphate to form insoluble iron phosphate, which precipitates out of the water and gets adsorbed onto the grains. The researchers found that the grains could remove almost all of the phosphate from water samples and nearly 80% of several other pollutants. Treating wastewater will require consideration of various factors, such as scale and the degree of contamination. For example, homes that use a septic tank; particular buildings with a high level of pharmaceuticals in their waste water, such as hospitals or care homes for the elderly; or municipal waste water treatment plants that serve a whole city will all have different requirements. Boa is exploring options with local water authorities for implementation of this technology. Boa has purposely tried to keep process costs low to make the method commercially feasible. The moss is already harvested for other applications, Boa notes, and it can grow on poor-quality soil, so it won't compete with food crops for arable land. His collaborators have also begun testing the bioavailability of pollutants captured by the grains. "
Science Daily,Making Polyurethane Degradable Gives Its Components a Second Life,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092324.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""Millions of tons of polyurethanes are produced every day, and they're widely used in foams, plastics, sneakers, insulation and other products,"" says Ephraim Morado, a doctoral student who is presenting the work at the meeting. ""But when people finish using them, these materials are usually discarded."" Waste polyurethane either ends up in landfills, or it's incinerated, which requires a lot of energy and generates toxic byproducts, he notes. ""As an alternative, we want to develop the next generation of polyurethane that can degrade easily and be reprocessed into a new material that can then be commercialized, such as adhesives or paint,"" he says. Of course, Morado isn't alone in seeking ways to reuse polymers. ""A lot of people interested in recycling are trying to make polymers that will break down into their original starting materials and then remake the same polymer,"" says Steven Zimmerman, Ph.D., the project's principal investigator. ""We're taking a very different, intermediate approach, which industry might be more interested in pursuing in the short term because it would be easier and cheaper,"" adds Zimmerman, whose lab is based at the University of Illinois at Urbana-Champaign. ""We're trying to break our polymers down into some other starting materials that are familiar to industry."" The key difference between standard polyurethane and Morado's version is the incorporation of a hydroxy acetal as one of the monomers, alongside the traditional monomers. Zimmerman's team had first used a special iodine-containing acetal to make degradable polymers and polyacrylamide gels. In that earlier work, the polymer could be dissolved in slightly acidic water. Morado invented a new type of acetal to incorporate in his unconventional polyurethane so he could dissolve the polymer in the absence of water. After months of investigation, he discovered that a solution of trichloroacetic acid in dichloromethane, an organic solvent, could dissolve the polyurethane at room temperature in just three hours. That's in contrast to the harsher conditions of the typical incineration method, which requires more than 1,400 F to avoid toxic gas formation. Unlike water, dichloromethane causes the material to swell. That expansion enables the acid to reach the backbone of polyurethane's molecular chains, which it can break at positions where the acetal groups are located. Degradation releases alcohol monomers that can then be used to make new products such as adhesives whose performance rivals superglue. Morado created other acetal-containing polyurethanes that can be triggered to degrade when exposed to light. He used these materials to make microcapsules that could contain herbicides or even biocides for killing barnacles and other creatures that stick to ship hulls. He and Zimmerman are also developing adhesives that dissolve when treated with a few drops of acid in dichloromethane solvent. One potential application is on circuit boards, where a chip that had been securely glued to the board could be swapped out for a replacement if the original chip had failed. In addition, the team is working on polyurethanes that can degrade under even milder conditions, such as exposure to vinegar. That would be particularly useful for, say, degradable sutures or household applications such as removable picture hangers. "
Science Daily,"Producing Protein Batteries for Safer, Environmentally Friendly Power Storage",Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092322.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition.  ""The trend in the battery field right now is to look at how the electrons are transported within a polymer network,"" says Tan Nguyen, a Ph.D. student who helped develop the project. ""The beauty of polypeptides is that we can control the chemistry on their side chains in 3D without changing the geometry of the backbone, or the main part of the structure. Then we can systematically examine the effect of changing different aspects of the side chains."" Current lithium-ion batteries can harm the environment, and because the cost of recycling them is higher than manufacturing them from scratch, they often accumulate in landfills. At the moment, there is no safe way of disposing of them. Developing a protein-based, or organic, battery would change this situation. ""The amide bonds along the peptide backbone are pretty stable -- so the durability is there, and we can then trigger when they break down for recycling,"" says Karen Wooley, Ph.D., who leads the team at Texas A&M University. She envisions that polypeptides could eventually be used in applications such as flow batteries for storing electrical energy. ""The other advantage is that by using this protein-like architecture, we're building in the kinds of conformations that are found in proteins in nature that already transport electrons efficiently,"" Wooley says. ""We can also optimize this to control battery performance."" The researchers built the system using electrodes made of composites of carbon black, constructing polypeptides that contain either viologen or 2,2,6,6-tetramethylpiperidine 1-oxyl (TEMPO). They attached viologens to the matrix used for the anode, which is the negative electrode, and used a TEMPO-containing polypeptide for the cathode, which is the positive electrode. The viologens and TEMPO are redox-active molecules. ""What we've measured so far for the range, the potential window between the two materials, is about 1.5 volts, suitable for low-energy requirement applications, such as biosensors,"" Nguyen says. For potential use in an organic battery, Nguyen has synthesized several polymers that adopt different conformations, such as a random coil, an alpha helix and a beta sheet, to investigate their electrochemical characteristics. With these peptides in hand, Nguyen is now collaborating with Alexandra Danielle Easley, a Ph.D. student in the laboratory of Jodie Lutkenhaus, Ph.D., also at Texas A&M University, to build the battery prototypes. Part of that work will include testing to better understand how the polymers function when they're organized on a substrate. While this early stage research has far to go before organic-based batteries are commercially available, the flexibility and variety of structures that proteins can provide promise wide potential for sustainable energy storage that is safer for the environment. "
Science Daily,Disappearing Act: Device Vanishes on Command After Military Missions,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092302.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""This is not the kind of thing that slowly degrades over a year, like the biodegradable plastics that consumers might be familiar with,"" says Paul Kohl, Ph.D., whose team developed the material. ""This polymer disappears in an instant when you push a button to trigger an internal mechanism or the sun hits it."" The disappearing polymers were developed for the Department of Defense, which is interested in deploying electronic sensors and delivery vehicles that leave no trace of their existence after use, thus avoiding discovery and alleviating the need for device recovery. The key to making a polymer disappear, or break apart, is ""ceiling temperature."" Below the ceiling temperature, a polymer configuration is favored, but above that temperature, the polymer will break apart into its component monomers. Common polymers, like polystyrene, have a ceiling temperature above ambient temperature and are very stable. And even when they are warmed above their ceiling temperature, some of these materials can take a long time to decompose. For example, thousands of chemical bonds link all of the monomers together in polystyrene, and all of these bonds must be broken for the materials to decompose. But with low ceiling-temperature polymers, such as the cyclic ones Kohl is using, only one bond needs to break, and then all of the other bonds come apart, so the depolymerization happens quickly. The process can be initiated by a temperature spike from an outside or embedded source, or by a light-sensitive catalyst. For many years, researchers have attempted to make these polymers, but were unsuccessful because of the materials' instability at room temperature. Kohl's research group at the Georgia Institute of Technology discovered that they could overcome this issue if they were careful to remove all impurities formed during the synthesis. In addition, they found a number of aldehydes, including phthalaldehyde, that readily form cyclic polymers. Once they had optimized this polymer's synthesis, they focused on ways to make it disappear. To do this, the researchers incorporated into the polymer a photosensitive additive, which absorbs light and catalyzes depolymerization. ""Initially, we made it photosensitive to just ultraviolet light so we could make the parts in a well-lit room with fluorescent lighting, and it was just fine; it was stable,"" Kohl says. But when the polymer was placed outside, exposure to sunlight vaporized it (or reverted it back to a liquid, in some cases). A vehicle deployed at night would, therefore, disappear with the sunrise. Kohl's group has since discovered new additives that can trigger depolymerization at different wavelengths of visible light, so the polymer can decompose indoors. ""We have polymers designed for applications in which you come in the room, you turn the light on, and the thing disappears,"" Kohl says. The group has also determined how to stall depolymerization. ""We have a way to delay the depolymerization for a specific amount of time -- one hour, two hours, three hours,"" he says. ""You would keep it in the dark until you were going to use it, but then you would deploy it during the day, and you would have three hours before it decomposes."" The team has considered chemical methods to start the decomposition process, as well. In addition, they are testing various copolymers that can be added to phthalaldehyde to change the material's properties without altering its ability to vanish. Kohl says that this ""James Bond""-like material is already being incorporated in military devices by other researchers. But he also sees the potential of the materials for non-military applications. For example, the researchers have made a disappearing epoxy for a temporary adhesive that could be used in building materials. They also imagine the material could be used as sensors for environmental monitoring. Once the sensors are finished collecting data, there is no risk of littering the environment since they can be triggered to vaporize. The material can also be used for delivery vehicles in remote areas where recovery is difficult. "
Science Daily,Augmented Reality Glasses May Help People With Low Vision Better Navigate Their Environment,Matter & Energy,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092252.htm,"   In a new study of patients with retinitis pigmentosa, an inherited degenerative eye disease that results in poor vision, Keck School of Medicine of USC researchers found that adapted augmented reality (AR) glasses can improve patients' mobility by 50% and grasp performance by 70%. ""Current wearable low vision technologies using virtual reality are limited and can be difficult to use or require patients to undergo extensive training,"" said Mark Humayun, MD, PhD, director of the USC Dr. Allen and Charlotte Ginsburg Institute for Biomedical Therapeutics, codirector of the USC Roski Eye Institute and University Professor of Ophthalmology at the Keck School. ""Using a different approach -- employing assistive technology to enhance, not replace, natural senses -- our team adapted AR glasses that project bright colors onto patients' retinas, corresponding to nearby obstacles,"" Humayun said. Patients with retinitis pigmentosa wore adapted AR glasses as they navigated through an obstacle course based on a U.S. Food and Drug Administration-validated functional test. Using video of each test, researchers recorded the number of times patients collided with obstacles, as well as the time taken to complete the course. Patients averaged 50% fewer collisions with the adapted AR glasses. Patients also were asked to grasp a wooden peg against a black background -- located behind four other wooden pegs -- without touching the front items. Patients demonstrated a 70% increase in grasp performance with the AR glasses. ""Patients with retinitis pigmentosa have decreased peripheral vision and trouble seeing in low light, which makes it difficult to identify obstacles and grasp objects. They often require mobility aids to navigate, especially in dark environments,"" said Anastasios N. Angelopoulos, study project lead in Humayun's research laboratory at the Keck School. ""Through the use of AR, we aim to improve the quality of life for low vision patients by increasing their confidence in performing basic tasks, ultimately allowing them to live more independent lives,"" Angelopoulos says. How the AR system works The AR system overlays objects within a 6-foot wireframe with four bright, distinct colors. In doing so, the glasses provide visual color cues that help people with constricted peripheral vision interpret complex environments, such as avoiding obstacles in dimly lit environments. To accomplish this, researchers used a process called simultaneous location and mapping, allowing the AR glasses to fully render the 3D structure of a room in real time. The glasses then translated this information into a semitransparent colored visual overlay, which highlighted potential obstacles with bright colors to help patients with spatial understanding and depth perception. This technology can work on commercially available devices. According to Humayun, while major cost and technical issues remain, this type of assistive technology could eventually become more practical for everyday use in the near future. "
Science Daily,"Physicists Mash Quantum and Gravity and Find Time, but Not as We Know It",Space & Time,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826122010.htm,"   UQ physicist Dr Magdalena Zych said the discovery arose from an experiment the team designed to bring together elements of the two big -- but contradictory -- physics theories developed in the past century. ""Our proposal sought to discover: what happens when an object massive enough to influence the flow of time is placed in a quantum state?"" Dr Zych said. She said Einstein's theory described how the presence of a massive object slowed time. ""Imagine two space ships, asked to fire at each other at a specified time while dodging the other's attack,"" she said. ""If either fires too early, it will destroy the other."" ""In Einstein's theory, a powerful enemy could use the principles of general relativity by placing a massive object -- like a planet -- closer to one ship to slow the passing of time."" ""Because of the time lag, the ship furthest away from the massive object will fire earlier, destroying the other."" Dr Zych said the second theory, of quantum mechanics, says any object can be in a state of ""superposition"" ""This means it can be found in different states -- think Schrodinger's cat,"" she said. Dr Zych said using the theory of quantum mechanics, if the enemy put the planet into a state of ""quantum superposition,"" then time also should be disrupted. ""There would be a new way for the order of events to unfold, with neither of the events being first or second -- but in a genuine quantum state of being both first and second,"" she said. UQ researcher Dr Fabio Costa said although ""a superposition of planets"" as described in the paper -- may never be possible, technology allowed a simulation of how time works in the quantum world -- without using gravity. ""Even if the experiment can never be done, the study is relevant for future technologies,"" Dr Costa said. ""We are currently working towards quantum computers that -- very simply speaking -- could effectively jump through time to perform their operations much more efficiently than devices operating in fixed sequence in time, as we know it in our 'normal' world."" "
Science Daily,Study Models New Method to Accelerate Nanoparticles,Space & Time,2019-08-23,-,https://www.sciencedaily.com/releases/2019/08/190823094829.htm,"   The team simulated a system that uses light to generate an electromagnetic field. Neutral nanoparticles made from glass or some other material that insulates rather than conducts electric charges are used. The nanoparticles become polarized. All of the positive charges are displaced in the direction of the field and negative charges shift in the opposite direction. It creates an internal electric field that produces a force to move the particles from a reservoir, funneled through an injector, then shot out of an accelerator to produce thrust. The study, that has been about eight years in the making, analytically showed that the technique can work, and suggested parameters for success. ""The challenge is in selecting the right permittivity of the medium, the right amount of charge, in which all of this happens,"" said Joshua Rovey, associate professor in the Department of Aerospace Engineering in The Grainger College of Engineering at the U of I. ""You have to choose the right materials for the nanoparticles themselves as well as the material surrounding the nanoparticles as they move through the structure."" The technique is based on a field of physics called plasmonics that studies how optical light or optical electromagnetic waves, interact with nanoscale structures, such as a bar or prism. Rovey explained when the light hits the nanoscale structure, a resonant interaction occurs. It creates strong electromagnetic fields right next to that structure. And those electromagnetic fields can manipulate particles by applying forces to nanoscale particles that are near those structures. The study focused on how to feed the nanoparticles into the accelerator structure, or injector and how the angles of the plates in the injector affect the forces on these nanoparticles. ""One of the main motivating factors for the concept was the absence of or lack of a power supply in space,"" Rovey said. ""If we can just harness the sun directly, have the sun shine directly on the nanostructures themselves, there's no need for an electrical power supply or solar panel to provide power."" Rovey said this study was a numerical simulation. The next step will be to create nanoscale structures in a lab, load then into the system, apply a light source, and observe how the nanoparticles move. The study, ""Nanoparticle injector for photonic manipulators using dielectrophoresis,"" was written by Jaykob Maser and Joshua L. Rovey. It appears in AIP Advances. This project was supported by the Air Force Office of Scientific Research, a grant from the NASA Innovative Advanced Concepts program, and Missouri University of Science and Technology through the Chancellor's Fellowship. "
Science Daily,Helping NASA Spacecraft Travel Faster and Farther With Math,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822165020.htm,"   Randy Paffenroth, associate professor of mathematical sciences, computer science, and data science, has a multi-part mission in this research project. Using machine learning, neural networks, and an old mathematical equation, he has developed an algorithm that will significantly enhance the resolution of density scanning systems that are used to detect flaws in carbon nanotube materials. Higher resolution scans provide more accurate images (nine times ""super resolution"") of the material's uniformity, detecting imperfections in Miralon¬Æ materials -- a strong, lightweight, flexible nanomaterial produced by Nanocomp Technologies, Inc. Miralon¬Æ yarns, which can be as thin as a human hair, can be wrapped around structures like rocket fuel tanks, giving them the strength to withstand high pressures. Imperfections and variations in thickness can cause weak spots in the yarn and the resulting composite. Paffenroth, with a team of graduate students, is analyzing data from the manufacturing process to help ensure a more consistent end product. Nanocomp uses a modified commercial ""basis weight"" scanning system that scans the nanomaterial for mass uniformity and imperfections, creating a visual image of density; Paffenroth and his team are using machine learning to train algorithms to increase the resolution of the images, allowing the machine to detect more minute variations in the material. They have developed a unique mathematical ""compressed sensing / super resolution"" algorithm that has increased the resolution by nine times. Built with the Python programming language and based on an artificial neural network, the algorithm was ""trained"" on thousands of sets of nanomaterial images in which Paffenroth had already identified and located flaws. He essentially gave the algorithm a series of practice tests where he already knew the answers (known as ""ground truth""). Then, he gave it other tests without the answers. ""I give it a sheet of material. I know the imperfections going in but the algorithm doesn't. If it finds those imperfections, I can trust its accuracy,"" said Paffenroth. To make the machine learning algorithm more effective at making a high-resolution image out of a low-resolution image, he combined it with the Fourier Transform, a mathematical tool devised in the early 1800s that can be used to break down an image into its individual components. ""We take this fancy, cutting-edge neural network and add in 250-year-old mathematics and that helps the neural network work better,"" said Paffenroth. ""The Fourier Transform makes creating a high-resolution image a much easier problem by breaking down the data that makes up the image. Think of the Fourier Transform as a set of eyeglasses for the neural network. It makes blurry things clear to the algorithm. We're taking computer vision and virtually putting glasses on it. ""It's exciting to use this combination of modern machine learning and classic math for this kind of work,"" he added. Paffenroth's work is funded by an $87,353 grant WPI received from Nanocomp Technologies, a New Hampshire-based subsidiary of Huntsman Corporation that makes advanced carbon-nanotube materials for aerospace, defense, and the automotive industry. WPI is a sub-contractor to Nanocomp, which received an $8.1 million contract from NASA to advance its carbon nanotube sheets and yarns. Miralon¬Æ has already been proven in space. For instance, it was wrapped around structural supports in NASA's Juno probe orbiting the planet Jupiter to help a challenging problem with vibration damping and static discharge. NASA has also used Miralon¬Æ nanomaterials to make and test prototypes of new carbon composite pressure vessels, the precursors to next generation rocket fuel tanks. NASA spacecraft will need that added strength and durability as they travel farther from home and deeper into space. As part of its current NASA contract, Nanocomp is trying to make Miralon¬Æ yarns that are three times stronger, and the work by Paffenroth's team is a big part of making that happen. ""Randy is helping us achieve this goal of tripling our strength by improving the tools in our toolbox so that we can make stronger, better, next-generation materials to be used in space applications,"" said Bob Casoni, Quality Manager at Nanocomp. ""If NASA needs to build a new rocket system strong enough to get to Mars and back, it has a big set of challenges to face. Better materials are needed to allow NASA to design rockets that can go farther, faster and survive longer."" Casoni noted that with the higher resolution from WPI's algorithm, Nanocomp can see patterns and variations in its materials that they couldn't see before. ""We can not only pick up features, but we also have a better idea of the magnitude of those features,"" he said. ""Before, it was like seeing a blurry satellite image. You might think you're seeing the rolling hills of Pennsylvania, but with better resolution you see it's really Mount Washington or the Colorado Rockies. It's pretty amazing stuff."" And with better measurement tools, Nanocomp also will be able to improve its manufacturing process by testing whether changes in factors like temperature, tension control, pressure, and flow rates create better materials. ""We can use better measurements to optimize our ultimate product performance,"" said Casoni. ""Randy is helping us understand our manufacturing process better. He's doing his ""magic math"" to help us better understand variations in our product. The uniformity of that material plays a big part in its ultimate strength."" Paffenroth and his team will also develop algorithms to be used in active feedback control systems to predict how good a particular piece of material will be as it's first being made, helping to ensure a more consistent end product. The algorithm analyzes the properties measured at the beginning of the manufacturing run to effectively predict the properties at the end of the run, including mechanical properties and length of run. ""We can use machine learning to predict that Nanocomp won't get a useful length of material out of a particular production run,"" said Paffenroth. ""It helps them with waste. If they can tell in the first few meters of the run that there will be a problem, they can stop and start over. The Holy Grail of process engineering is that the more you understand about your process, the better your process is."" WPI will present its findings on Aug. 25 at the 2019 International Conference on Image, Video Processing and Artificial Intelligence in Shanghai, China. "
Science Daily,"In a Quantum Future, Which Starship Destroys the Other?",Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822165018.htm,"   ""The sequence of events can become quantum mechanical,"" said co-author Igor Pikovski, a physicist at the Center for Quantum Science and Engineering at Stevens Institute of Technology. "" We looked at quantum temporal order where there is no distinction between one event causing the other or vice versa."" The work, reported in the August 22 issue of Nature Communications, is among the first to reveal the quantum properties of time, whereby the flow of time doesn't observe a straight arrow forward, but one where cause and effect can co-exist both in the forward and backward direction. In the upcoming era of quantum computers, the work holds particular promise: quantum computers that exploit the quantum order of performing operations might beat devices that operate using only fixed sequences. To show this scenario, Pikovski and colleagues merged two seemingly conflicting theories -- quantum mechanics and general relativity -- to conduct a Gedanken experiment, a way of using the imagination to investigate the nature of things. The team, consisting of Pikovski, Magdalena Zych, Fabio Costa and Caslav Brukner, started by asking the question, ""what would a clock measure if it was influenced by a massive object in a quantum superposition state, i.e. both near and far at the same time?"" According to general relativity, the presence of a massive object slows down the flow of time, such that a clock placed close to a massive object will run slower compared to an identical one that is farther away. To illustrate what happens, imagine a pair of starships training for a mission. They are asked to fire at each other at a specified time and dodge the fire at another time, whereby each ship knows the exact time when to fire and when to dodge. If either ship fires too early, it will destroy the other, and this establishes an unmistakable time order between the firing events. However, if a powerful agent could place a sufficiently massive object, say a planet, closer to one ship it would slow down its flow of time. As a result, the ship would dodge the fire too late and would be destroyed. Quantum mechanics complicates the matter. When placing the planet in a state of superposition near one ship or the other, both can be destroyed or survive at the same time. The sequence of events exists in a state of superposition, such that each starship simultaneously destroys the other. The authors illustrate for the first time how this quantum scenario can occur and how it can be verified. ""Moving planets around is hard,"" said Pikovski. ""But imagining it helped us examine a quantum aspect of time that was previously unknown."" "
Science Daily,Mission to Jupiter's Icy Moon Confirmed,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822143218.htm,"   ""We are all excited about the decision that moves the Europa Clipper mission one key step closer to unlocking the mysteries of this ocean world,"" said Thomas Zurbuchen, associate administrator for the Science Mission Directorate at NASA Headquarters in Washington. ""We are building upon the scientific insights received from the flagship Galileo and Cassini spacecraft and working to advance our understanding of our cosmic origin, and even life elsewhere."" The mission will conduct an in-depth exploration of Jupiter's moon Europa and investigate whether the icy moon could harbor conditions suitable for life, honing our insights into astrobiology. To develop this mission in the most cost-effective fashion, NASA is targeting to have the Europa Clipper spacecraft complete and ready for launch as early as 2023. The agency baseline commitment, however, supports a launch readiness date by 2025. NASA's Jet Propulsion Laboratory in Pasadena, California, leads the development of the Europa Clipper mission in partnership with the Johns Hopkins University Applied Physics Laboratory for the Science Mission Directorate. Europa Clipper is managed by the Planetary Missions Program Office at NASA's Marshall Space Flight Center in Huntsville, Alabama. "
Science Daily,Storms on Jupiter Are Disturbing the Planet's Colorful Belts,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822130438.htm,"   Thanks to coordinated observations of the planet in January 2017 by six ground-based optical and radio telescopes and NASA's Hubble Space Telescope, a University of California, Berkeley, astronomer and her colleagues have been able to track the effects of these storms -- visible as bright plumes above the planet's ammonia ice clouds -- on the belts in which they appear. The observations will ultimately help planetary scientists understand the complex atmospheric dynamics on Jupiter, which, with its Great Red Spot and colorful, layer cake-like bands, make it one of the most beautiful and changeable of the giant gas planets in the solar system. One such plume was noticed by amateur astronomer Phil Miles in Australia a few days before the first observations by the Atacama Large Millimeter/Submillimeter Array (ALMA) in Chile, and photos captured a week later by Hubble showed that the plume had spawned a second plume and left a downstream disturbance in the band of clouds, the South Equatorial Belt. The rising plumes then interacted with Jupiter's powerful winds, which stretched the clouds east and west from their point of origin. Three months earlier, four bright spots were seen slightly north of the North Equatorial Belt. Though those plumes had disappeared by 2017, the belt had since widened northward, and its northern edge had changed color from white to orangish brown. ""If these plumes are vigorous and continue to have convective events, they may disturb one of these entire bands over time, though it may take a few months,"" said study leader Imke de Pater, a UC Berkeley professor emerita of astronomy. ""With these observations, we see one plume in progress and the aftereffects of the others."" The analysis of the plumes supports the theory that they originate about 80 kilometers below the cloud tops at a place dominated by clouds of liquid water. A paper describing the results has been accepted for publication in the Astronomical Journal and is now online. Into the stratosphere Jupiter's atmosphere is mostly hydrogen and helium, with trace amounts of methane, ammonia, hydrogen sulfide and water. The top-most cloud layer is made up of ammonia ice and comprises the brown belts and white zones we see with the naked eye. Below this outer cloud layer sits a layer of solid ammonium hydrosulfide particles. Deeper still, at around 80 kilometers below the upper cloud deck, is a layer of liquid water droplets. The storm clouds de Pater and her team studied appear in the belts and zones as bright plumes and behave much like the cumulonimbus clouds that precede thunderstorms on Earth. Jupiter's storm clouds, like those on Earth, are often accompanied by lightning. Optical observations cannot see below the ammonia clouds, however, so de Pater and her team have been probing deeper with radio telescopes, including ALMA and also the Very Large Array (VLA) in New Mexico, which is operated by the National Science Foundation-funded National Radio Astronomy Observatory. ALMA array's first observations of Jupiter were between Jan. 3 and 5 of 2017, a few days after one of these bright plumes was seen by amateur astronomers in the planet's South Equatorial Belt. A week later, Hubble, the VLA, the Gemini, Keck and Subaru observatories in Hawaii and the Very Large Telescope (VLT) in Chile captured images in the visible, radio and mid-infrared ranges. De Pater combined the ALMA radio observations with the other data, focused specifically on the newly brewed storm as it punched through the upper deck clouds of ammonia ice. The data showed that these storm clouds reached as high as the tropopause -- the coldest part of the atmosphere -- where they spread out much like the anvil-shaped cumulonimbus clouds that generate lightning and thunder on Earth. ""Our ALMA observations are the first to show that high concentrations of ammonia gas are brought up during an energetic eruption,"" de Pater said. The observations are consistent with one theory, called moist convection, about how these plumes form. According to this theory, convection brings a mix of ammonia and water vapor high enough -- about 80 kilometers below the cloud tops -- for the water to condense into liquid droplets. The condensing water releases heat that expands the cloud and buoys it quickly upward through other cloud layers, ultimately breaking through the ammonia ice clouds at the top of the atmosphere. The plume's momentum carries the supercooled ammonia cloud above the existing ammonia-ice clouds until the ammonia freezes, creating a bright, white plume that stands out against the colorful bands encircling Jupiter. ""We were really lucky with these data, because they were taken just a few days after amateur astronomers found a bright plume in the South Equatorial Belt,"" said de Pater. ""With ALMA, we observed the whole planet and saw that plume, and since ALMA probes below the cloud layers, we could actually see what was going on below the ammonia clouds."" Hubble took images a week after ALMA and captured two separate bright spots, which suggests that the plumes originate from the same source and are carried eastward by the high altitude jet stream, leading to the large disturbances seen in the belt. Coincidentally, three months before, bright plumes had been observed north of the Northern Equatorial Belt. The January 2017 observations showed that that belt had expanded in width, and the band where the plumes had first been seen turned from white to orange. De Pater suspects that the northward expansion of the North Equatorial Belt is a result of gas from the ammonia-depleted plumes falling back into the deeper atmosphere. De Pater's colleague and co-author Robert Sault of the University of Melbourne in Australia used special computer software to analyze the ALMA data to obtain radio maps of the surface that are comparable to visible-light photos taken by Hubble. ""Jupiter's rotation once every 10 hours usually blurs radio maps, because these maps take many hours to observe,"" Sault said. ""In addition, because of Jupiter's large size, we had to 'scan' the planet, so we could make a large mosaic in the end. We developed a technique to construct a full map of the planet."" The VLT data were contributed by Leigh Fletcher and Padraig Donnelly of the University of Leicester in the United Kingdom, while Glenn Orton and James Sinclair of the Jet Propulsion Laboratory in California and Yasuma Kasaba of Tokyo University in Japan supplied the SUBARU data. Gordon Bjoraker of the NASA Goddard Space Flight Center in Maryland and M√°t√© √Åd√°mkovics of Clemson University in South Carolina analyzed the Keck data. The work was supported by a NASA Planetary Astronomy award (NNX14AJ43G) and a Solar System Observations award (80NSSC18K1001). "
Science Daily,Maximum Mass of Lightest Neutrino Revealed Using Astronomical Big Data,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822113407.htm,"   It's important to better understand neutrinos and the processes through which they obtain their mass as they could reveal secrets about astrophysics, including how the universe is held together, why it is expanding and what dark matter is made of. First author, Dr Arthur Loureiro (UCL Physics & Astronomy), said: ""A hundred billion neutrinos fly through your thumb from the Sun every second, even at night. These are very weakly interactive ghosts that we know little about. What we do know is that as they move, they can change between their three flavours, and this can only happen if at least two of their masses are non-zero."" ""The three flavours can be compared to ice cream where you have one scoop containing strawberry, chocolate and vanilla. Three flavours are always present but in different ratios, and the changing ratio-and the weird behaviour of the particle-can only be explained by neutrinos having a mass."" The concept that neutrinos have mass is a relatively new one with the discovery in 1998 earning Professor Takaaki Kajita and Professor Arthur B. McDonald the 2015 Nobel Prize in Physics. Even so, the Standard Model used by modern physics has yet to be updated to assign neutrinos a mass. The study, published today in Physical Review Letters by researchers from UCL, Universidade Federal do Rio de Janeiro, Institut d'Astrophysique de Paris and Universidade de Sao Paulo, sets an upper limit for the mass of the lightest neutrino for the first time. The particle could technically have no mass as a lower limit is yet to be determined. The team used an innovative approach to calculate the mass of neutrinos by using data collected by both cosmologists and particle physicists. This included using data from 1.1 million galaxies from the Baryon Oscillation Spectroscopic Survey (BOSS) to measure the rate of expansion of the universe, and constraints from particle accelerator experiments. ""We used information from a variety of sources including space- and ground-based telescopes observing the first light of the Universe (the cosmic microwave background radiation), exploding stars, the largest 3D map of galaxies in the Universe, particle accelerators, nuclear reactors, and more,"" said Dr Loureiro. ""As neutrinos are abundant but tiny and elusive, we needed every piece of knowledge available to calculate their mass and our method could be applied to other big questions puzzling cosmologists and particle physicists alike."" The researchers used the information to prepare a framework in which to mathematically model the mass of neutrinos and used UCL's supercomputer, Grace, to calculate the maximum possible mass of the lightest neutrino to be 0.086 eV (95% CI), which is equivalent to 1.5 x 10-37 Kg. They calculated that three neutrino flavours together have an upper bound of 0.26 eV (95% CI). Second author, PhD student Andrei Cuceu (UCL Physics & Astronomy), said: ""We used more than half a million computing hours to process the data; this is equivalent to almost 60 years on a single processor. This project pushed the limits for big data analysis in cosmology."" The team say that understanding how neutrino mass can be estimated is important for future cosmological studies such as DESI and Euclid, which both involve teams from across UCL. The Dark Energy Spectroscopic Instrument (DESI) will study the large scale structure of the universe and its dark energy and dark matter contents to a high precision. Euclid is a new space telescope being developed with the European Space Agency to map the geometry of the dark Universe and evolution of cosmic structures. Professor Ofer Lahav (UCL Physics & Astronomy), co-author of the study and chair of the UK Consortiums of the Dark Energy Survey and DESI said: ""It is impressive that the clustering of galaxies on huge scales can tell us about the mass of the lightest neutrino, a result of fundamental importance to physics. This new study demonstrates that we are on the path to actually measuring the neutrino masses with the next generation of large spectroscopic galaxy surveys, such as DESI, Euclid and others."" The research was funded by National Council for Scientific and Technological Development (CNPq) Science without Borders (Brazil), the Royal Astronomical Society, the UK Science and Technology Facilities Council (STFC), the Royal Society and the European Research Council. "
Science Daily,Temperatures of 800 Billion Degrees in the Cosmic Kitchen,Space & Time,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822101419.htm,"   Simulation of electromagnetic radiation Collisions between stars cannot be directly observed -- not least of all because of their extreme rarity. According to estimates, none has ever happened in our galaxy, the Milky Way. The densities and temperatures in merging processes of neutron stars are similar to those occurring in heavy ion collisions, however. This enabled the HADES team to simulate the conditions in merging stars at the microscopic level in the heavy ion accelerator at the Helmholtzzentrum f√ºr Schwerionenforschung (GSI) in Darmstadt. As in a neutron star collision, when two heavy ions are slammed together at close to the speed of light, electromagnetic radiation is produced. It takes the form of virtual photons that turn back into real particles after a very short time. However, the virtual photons occur very rarely in experiments using heavy ions. ""We had to record and analyze about 3 billion collisions to finally reconstruct 20,000 measurable virtual photons,"" says Dr. J√ºrgen Friese, the former spokesman of the HADES collaboration and researcher at Laura Fabbietti's Professorship on Dense and Strange Hadronic Matter at TUM. Photon camera shows collision zone To detect the rare and transient virtual photons, researchers at TUM developed a special 1.5 square meter digital camera. This instrument records the Cherenkov effect: the name given to certain light patterns generated by decay products of the virtual photons. ""Unfortunately the light emitted by the virtual photons is extremely weak. So the trick in our experiment was to find the light patterns,"" says Friese. ""They could never be seen with the naked eye. We therefore developed a pattern recognition technique in which a 30,000 pixel photo is rastered in a few microseconds using electronic masks. That method is complemented with neural networks and artificial intelligence."" Observing the material properties in the laboratory The reconstruction of thermal radiation from compressed matter is a milestone in the understanding of cosmic forms of matter. It enabled the scientists to place the temperature of the new system resulting from the merger of stars at 800 billion degrees celsius. As a result, the HADES team was able to show that the merging processes under consideration are in fact the cosmic kitchens for the fusion of heavy nucleii. "
Science Daily,Quantum Criticality Could Be a Boon for Qubit Designers,Computers & Math,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112651.htm,"   In a study in the Proceedings of the National Academy of Sciences, researchers from Rice University and the Vienna University of Technology (TU Wien) in Austria examined the behavior of an intermetallic crystal of cerium, palladium and silicon as it was subjected to extreme cold and a strong magnetic field. To their surprise, they found they could transform the quantum behavior of the material in two unique ways, one in which electrons compete to occupy orbitals and another where they compete to occupy spin states. ""The effect is so pronounced with one degree of freedom that it ends up liberating the other one,"" said Rice's Qimiao Si, co-corresponding author of the study and the director of the Rice Center for Quantum Materials (RCQM). ""You can essentially tune the system to maximize damage to one of these, leaving the other well-defined."" Si said the result could be important for companies like Google, IBM, Intel and others who are competing to develop quantum computers. Unlike today's digital computers, which use electricity or light to encode bits of information, quantum computers use the quantum states of subatomic particles like electrons to store information in qubits. A practical quantum computer could outperform its digital counterpart in many ways, but the technology is still in its infancy, and one of the chief obstacles is the fragility of the quantum states inside the qubits. ""You need a well-defined quantum state if you wish to be assured that the information that is stored in a qubit will not change due to background interference,"" Si said. Every electron acts like a spinning magnet, and its spin is described in one of two values, up or down. In many qubit designs, information is encoded in these spins, but these states can be so fragile that even tiny amounts of light, heat, vibration or sound can cause them to flip from one state to another. Minimizing the information that's lost to such ""decoherence"" is a major concern in qubit design, Si said. In the new study, Si worked with longtime collaborator Silke Paschen of TU Wien to study a material where the quantum states of electrons were scrambled not just in terms of their spins but also in terms of their orbitals. ""We designed a system, realized in some theoretical models and concurrently realized in a material, where spins and orbitals are almost on an equal footing and are strongly coupled together,"" he said. From previous research in 2012, Si, Paschen and colleagues knew that electrons in the compound could be made to interact so strongly that the material would undergo a dramatic change at a critically cold temperature. On either side of this ""quantum critical point,"" electrons in key orbitals would arrange themselves in a completely different way, with the shift occurring solely due to the quantum interactions between them. The earlier study invoked a well-known theory Si and collaborators developed in 2001 that prescribes how the spins of these localized electrons, which are part of atoms inside the alloy, strongly couple with free-flowing conduction electrons at the quantum critical point. According to this ""local quantum critical"" theory, as the material is cooled and approaches the critical point, the spins of localized electrons and conduction electrons begin to compete to occupy particular spin states. The quantum critical point is the tipping point where this competition destroys the ordered arrangement of the localized electrons and they instead become completely entangled with the conduction electrons. Even though Si has studied quantum criticality for almost 20 years, he was surprised by the results of Paschen's latest experiments. ""The new data was completely baffling to all of us,"" he said. ""That is, until we realized that the system contained not only spins but also orbitals as active degrees of freedom."" With that realization, Si's team, including Rice graduate student Ang Cai, built a theoretical model that contains both the spins and orbitals. Their detailed analysis of the model revealed a surprising form of quantum criticality that provided a clear understanding of the experiments. ""It was a shock to me, both from the theoretical model perspective and the experiments,"" he said. ""Even though this is a soup of things -- spins, orbitals that are all strongly coupled to each other and to background conduction electrons -- we could resolve two quantum critical points in this one system under the tuning of one parameter, which is the magnetic field. And at each one of the quantum critical points, only the spin or the orbital is driving the quantum criticality. The other one is more or less a bystander."" Si is the Harry C. and Olga K. Wiess Professor in Rice's Department of Physics and Astronomy. The study's co-lead authors are Cai and Valentina Martelli, formerly of TU Wien and now with the University of S√£o Paulo in Brazil. Additional co-authors include Chia-Chuan Liu and Hsin-Hua Lai, both of Rice; Emilian Nica, formerly of Rice and currently at the University of British Columbia; Rong Yu, formerly of Rice and currently at Renmin University of China; Mathieu Taupin, Andrey Prokofiev, Diana Geiger, Jonathan Haenel and Julio Larrea, all of TU Wien; Kevin Ingersent of the University of Florida; Robert K√ºchler of the Max Planck Institute for Chemical Physics of Solids in Dresden, Germany; and Andre Strydom of the University of Johannesburg in South Africa. The research was supported by the National Science Foundation (DMR-1920740, CNS-1338099, PHY-1607611, DMR-1508122), the Robert A. Welch Foundation (C-1411), the Army Research Office (ARO-W911NF-14-1-0525, ARO-W911NF-14-1-0496), the Austrian Science Fund (P29296-N27, DK W1243), the European Research Council (Advanced Grant 227378), the Carlos Chagas Filho Foundation for Research Support of the State of Rio de Janeiro (201.755/2015), the National Natural Science Foundation of China (11674392), the Ministry of Science and Technology of China (2016YFA0300504), the South African National Research Foundation (93549), the University of Johannesburg and RCQM. RCQM leverages global partnerships and the strengths of more than 20 Rice research groups to address questions related to quantum materials. RCQM is supported by Rice's offices of the provost and the vice provost for research, the Wiess School of Natural Sciences, the Brown School of Engineering, the Smalley-Curl Institute and the departments of Physics and Astronomy, Electrical and Computer Engineering, and Materials Science and NanoEngineering. "
Science Daily,Augmented Reality Glasses May Help People With Low Vision Better Navigate Their Environment,Computers & Math,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092252.htm,"   In a new study of patients with retinitis pigmentosa, an inherited degenerative eye disease that results in poor vision, Keck School of Medicine of USC researchers found that adapted augmented reality (AR) glasses can improve patients' mobility by 50% and grasp performance by 70%. ""Current wearable low vision technologies using virtual reality are limited and can be difficult to use or require patients to undergo extensive training,"" said Mark Humayun, MD, PhD, director of the USC Dr. Allen and Charlotte Ginsburg Institute for Biomedical Therapeutics, codirector of the USC Roski Eye Institute and University Professor of Ophthalmology at the Keck School. ""Using a different approach -- employing assistive technology to enhance, not replace, natural senses -- our team adapted AR glasses that project bright colors onto patients' retinas, corresponding to nearby obstacles,"" Humayun said. Patients with retinitis pigmentosa wore adapted AR glasses as they navigated through an obstacle course based on a U.S. Food and Drug Administration-validated functional test. Using video of each test, researchers recorded the number of times patients collided with obstacles, as well as the time taken to complete the course. Patients averaged 50% fewer collisions with the adapted AR glasses. Patients also were asked to grasp a wooden peg against a black background -- located behind four other wooden pegs -- without touching the front items. Patients demonstrated a 70% increase in grasp performance with the AR glasses. ""Patients with retinitis pigmentosa have decreased peripheral vision and trouble seeing in low light, which makes it difficult to identify obstacles and grasp objects. They often require mobility aids to navigate, especially in dark environments,"" said Anastasios N. Angelopoulos, study project lead in Humayun's research laboratory at the Keck School. ""Through the use of AR, we aim to improve the quality of life for low vision patients by increasing their confidence in performing basic tasks, ultimately allowing them to live more independent lives,"" Angelopoulos says. How the AR system works The AR system overlays objects within a 6-foot wireframe with four bright, distinct colors. In doing so, the glasses provide visual color cues that help people with constricted peripheral vision interpret complex environments, such as avoiding obstacles in dimly lit environments. To accomplish this, researchers used a process called simultaneous location and mapping, allowing the AR glasses to fully render the 3D structure of a room in real time. The glasses then translated this information into a semitransparent colored visual overlay, which highlighted potential obstacles with bright colors to help patients with spatial understanding and depth perception. This technology can work on commercially available devices. According to Humayun, while major cost and technical issues remain, this type of assistive technology could eventually become more practical for everyday use in the near future. "
Science Daily,Tech Time Not to Blame for Teens' Mental Health Problems,Computers & Math,2019-08-23,-,https://www.sciencedaily.com/releases/2019/08/190823140736.htm,"   The study tracked young adolescents on their smartphones to test whether more time spent using digital technology was linked to worse mental health outcomes. The researchers -- Candice Odgers, professor of psychological science at the University of California, Irvine; Michaeline Jensen, assistant professor of psychology at the University of North Carolina at Greensboro; Madeleine George, postdoctoral researcher at Purdue University; and Michael Russell, assistant professor of behavioral health at Pennsylvania State University -- found little evidence of longitudinal or daily linkages between digital technology use and adolescent mental health. ""It may be time for adults to stop arguing over whether smartphones and social media are good or bad for teens' mental health and start figuring out ways to best support them in both their offline and online lives,"" Odgers said. ""Contrary to the common belief that smartphones and social media are damaging adolescents' mental health, we don't see much support for the idea that time spent on phones and online is associated with increased risk for mental health problems,"" Jensen said. The study surveyed more than 2,000 youth and then intensively tracked a subsample of nearly 400 teens on their smartphones multiple times a day for two weeks. Adolescents in the study were between 10 and 15 years old and represented the economically and racially diverse population of youth attending North Carolina public schools. The researchers collected reports of mental health symptoms from the adolescents three times a day and they also reported on their daily technology usage each night. They asked whether youth who engaged more with digital technologies were more likely to experience later mental health symptoms and whether days that adolescents spent more time using digital technology for a wide range of purposes were also days when mental health problems were more common. In both cases, increased digital technology use was not related to worse mental health. When associations were observed, they were small and in the opposite direction that would be expected given all of the recent concerns about digital technology damaging adolescents' mental health. For instance, teens who reported sending more text messages over the study period actually reported feeling better (less depressed) than teens who were less frequent texters. "
Science Daily,Helping NASA Spacecraft Travel Faster and Farther With Math,Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822165020.htm,"   Randy Paffenroth, associate professor of mathematical sciences, computer science, and data science, has a multi-part mission in this research project. Using machine learning, neural networks, and an old mathematical equation, he has developed an algorithm that will significantly enhance the resolution of density scanning systems that are used to detect flaws in carbon nanotube materials. Higher resolution scans provide more accurate images (nine times ""super resolution"") of the material's uniformity, detecting imperfections in Miralon¬Æ materials -- a strong, lightweight, flexible nanomaterial produced by Nanocomp Technologies, Inc. Miralon¬Æ yarns, which can be as thin as a human hair, can be wrapped around structures like rocket fuel tanks, giving them the strength to withstand high pressures. Imperfections and variations in thickness can cause weak spots in the yarn and the resulting composite. Paffenroth, with a team of graduate students, is analyzing data from the manufacturing process to help ensure a more consistent end product. Nanocomp uses a modified commercial ""basis weight"" scanning system that scans the nanomaterial for mass uniformity and imperfections, creating a visual image of density; Paffenroth and his team are using machine learning to train algorithms to increase the resolution of the images, allowing the machine to detect more minute variations in the material. They have developed a unique mathematical ""compressed sensing / super resolution"" algorithm that has increased the resolution by nine times. Built with the Python programming language and based on an artificial neural network, the algorithm was ""trained"" on thousands of sets of nanomaterial images in which Paffenroth had already identified and located flaws. He essentially gave the algorithm a series of practice tests where he already knew the answers (known as ""ground truth""). Then, he gave it other tests without the answers. ""I give it a sheet of material. I know the imperfections going in but the algorithm doesn't. If it finds those imperfections, I can trust its accuracy,"" said Paffenroth. To make the machine learning algorithm more effective at making a high-resolution image out of a low-resolution image, he combined it with the Fourier Transform, a mathematical tool devised in the early 1800s that can be used to break down an image into its individual components. ""We take this fancy, cutting-edge neural network and add in 250-year-old mathematics and that helps the neural network work better,"" said Paffenroth. ""The Fourier Transform makes creating a high-resolution image a much easier problem by breaking down the data that makes up the image. Think of the Fourier Transform as a set of eyeglasses for the neural network. It makes blurry things clear to the algorithm. We're taking computer vision and virtually putting glasses on it. ""It's exciting to use this combination of modern machine learning and classic math for this kind of work,"" he added. Paffenroth's work is funded by an $87,353 grant WPI received from Nanocomp Technologies, a New Hampshire-based subsidiary of Huntsman Corporation that makes advanced carbon-nanotube materials for aerospace, defense, and the automotive industry. WPI is a sub-contractor to Nanocomp, which received an $8.1 million contract from NASA to advance its carbon nanotube sheets and yarns. Miralon¬Æ has already been proven in space. For instance, it was wrapped around structural supports in NASA's Juno probe orbiting the planet Jupiter to help a challenging problem with vibration damping and static discharge. NASA has also used Miralon¬Æ nanomaterials to make and test prototypes of new carbon composite pressure vessels, the precursors to next generation rocket fuel tanks. NASA spacecraft will need that added strength and durability as they travel farther from home and deeper into space. As part of its current NASA contract, Nanocomp is trying to make Miralon¬Æ yarns that are three times stronger, and the work by Paffenroth's team is a big part of making that happen. ""Randy is helping us achieve this goal of tripling our strength by improving the tools in our toolbox so that we can make stronger, better, next-generation materials to be used in space applications,"" said Bob Casoni, Quality Manager at Nanocomp. ""If NASA needs to build a new rocket system strong enough to get to Mars and back, it has a big set of challenges to face. Better materials are needed to allow NASA to design rockets that can go farther, faster and survive longer."" Casoni noted that with the higher resolution from WPI's algorithm, Nanocomp can see patterns and variations in its materials that they couldn't see before. ""We can not only pick up features, but we also have a better idea of the magnitude of those features,"" he said. ""Before, it was like seeing a blurry satellite image. You might think you're seeing the rolling hills of Pennsylvania, but with better resolution you see it's really Mount Washington or the Colorado Rockies. It's pretty amazing stuff."" And with better measurement tools, Nanocomp also will be able to improve its manufacturing process by testing whether changes in factors like temperature, tension control, pressure, and flow rates create better materials. ""We can use better measurements to optimize our ultimate product performance,"" said Casoni. ""Randy is helping us understand our manufacturing process better. He's doing his ""magic math"" to help us better understand variations in our product. The uniformity of that material plays a big part in its ultimate strength."" Paffenroth and his team will also develop algorithms to be used in active feedback control systems to predict how good a particular piece of material will be as it's first being made, helping to ensure a more consistent end product. The algorithm analyzes the properties measured at the beginning of the manufacturing run to effectively predict the properties at the end of the run, including mechanical properties and length of run. ""We can use machine learning to predict that Nanocomp won't get a useful length of material out of a particular production run,"" said Paffenroth. ""It helps them with waste. If they can tell in the first few meters of the run that there will be a problem, they can stop and start over. The Holy Grail of process engineering is that the more you understand about your process, the better your process is."" WPI will present its findings on Aug. 25 at the 2019 International Conference on Image, Video Processing and Artificial Intelligence in Shanghai, China. "
Science Daily,"In a Quantum Future, Which Starship Destroys the Other?",Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822165018.htm,"   ""The sequence of events can become quantum mechanical,"" said co-author Igor Pikovski, a physicist at the Center for Quantum Science and Engineering at Stevens Institute of Technology. "" We looked at quantum temporal order where there is no distinction between one event causing the other or vice versa."" The work, reported in the August 22 issue of Nature Communications, is among the first to reveal the quantum properties of time, whereby the flow of time doesn't observe a straight arrow forward, but one where cause and effect can co-exist both in the forward and backward direction. In the upcoming era of quantum computers, the work holds particular promise: quantum computers that exploit the quantum order of performing operations might beat devices that operate using only fixed sequences. To show this scenario, Pikovski and colleagues merged two seemingly conflicting theories -- quantum mechanics and general relativity -- to conduct a Gedanken experiment, a way of using the imagination to investigate the nature of things. The team, consisting of Pikovski, Magdalena Zych, Fabio Costa and Caslav Brukner, started by asking the question, ""what would a clock measure if it was influenced by a massive object in a quantum superposition state, i.e. both near and far at the same time?"" According to general relativity, the presence of a massive object slows down the flow of time, such that a clock placed close to a massive object will run slower compared to an identical one that is farther away. To illustrate what happens, imagine a pair of starships training for a mission. They are asked to fire at each other at a specified time and dodge the fire at another time, whereby each ship knows the exact time when to fire and when to dodge. If either ship fires too early, it will destroy the other, and this establishes an unmistakable time order between the firing events. However, if a powerful agent could place a sufficiently massive object, say a planet, closer to one ship it would slow down its flow of time. As a result, the ship would dodge the fire too late and would be destroyed. Quantum mechanics complicates the matter. When placing the planet in a state of superposition near one ship or the other, both can be destroyed or survive at the same time. The sequence of events exists in a state of superposition, such that each starship simultaneously destroys the other. The authors illustrate for the first time how this quantum scenario can occur and how it can be verified. ""Moving planets around is hard,"" said Pikovski. ""But imagining it helped us examine a quantum aspect of time that was previously unknown."" "
Science Daily,The Technology Behind Bitcoin May Improve the Medications of the Future,Computers & Math,2019-08-23,-,https://www.sciencedaily.com/releases/2019/08/190823091740.htm,"   Big data. Machine Learning. Internet of Things. Blockchain. Futuristic concepts from the world of technology will likely soon find their way into your medicine cabinet -- and onto your mobile phone. Using a prototype app for smartphones, researchers from the University of Copenhagen have taken the next step in the dosing, production and distribution of the pharmaceutical products of the future. And the time for innovation is more than ripe, says Professor Jukka Rantanen of the Department of Pharmacy: '200 years ago, the first patent on making tablets was filed and the products have not changed much since. We are still having the same tablets. What we are doing now is suggesting a totally new type of product', he says. 'By rethinking the product design principles, related manufacturing solutions and distribution models for the pharmaceutical products, it is possible to dramatically reduce the overall price of medicine while also improving the safety and efficacy of the medication'. App-othecary The core of Jukka Rantanen and his research group's wager for a future solution for pharmaceutical products is the new concept of cryptopharmaceuticals, embodying the mentioned prototype of an app for smartphones. The app is called 'MedBlockChain' and has been developed by the group's former MSc-student Lasse N√∏rfeldt. It is, among other things, based on the research group's earlier work on digitalisation of pharmaceutical products, for example in the form of printing medications as edible QR codes. With the app, patients will be able to scan a medication and receive confirmation that it is a genuine product and not a fake item. A problem that, according to Jukka Rantanen, is particularly serious in countries with less structured medicines regulatory agencies. At the same time, patients can choose to provide access to a range of personal data -- everything from heart rate monitor watches, pedometers and internet-connected bath scales to genetic profiles, screen time and social media usage -- all contributing with knowledge that can enable computer systems based on artificial intelligence to gradually pin down the optimal dose for each patient. 'This type of data already exist in our information-rich society. It would be logical to employ this big data for something useful. Not just for sharing on Facebook, your exercise app or something like that, but also for defining your optimal dose of given medicine', says Jukka Rantanen. Builds on Blockchain With the growing mass of personal data, data security is also gaining importance, Jukka Rantanen points out. To guarantee data security, the app uses the so-called blockchain technology, which is probably best known in connection with the cryptocurrency Bitcoin. With blockchain, information -- or data blocks -- are linked in a chain that cannot be changed without simultaneously altering all other links of information in the chain. Thus, all changes will be detected and may be traced. If something looks suspicious, the system can also generate an alarm. As an example, a patient who scans a QR code on his medication may be alerted by an alarm if the code does not match the one that the pharmaceutical company has entered into the system, or if the medication does not match with the prescription. Conversely, the pharmaceutical company may be alerted if an otherwise unique medication code is registered more than once. Likewise, an absence of registrations may form the basis for alarms as it may reveal that the patient is not taking his or her medication as planned. This information may for example be shared with the patient's doctor or relatives. Cryptopharmaceuticals The blockchain concept may still seem distant to most people, but in fact, the technology is already being used in similar ways for everything from insurance and finance to shipping and food, explains Jukka Rantanen. As an example, Chinese consumers have already become accustomed to scanning items in the supermarket to confirm that the product they are buying is, for example, indeed bacon produced in Denmark, and not a counterfeit product. 'All of this is technologically possible. Now, the big question is how we should handle all of this data and who should get access to it. That is the discussion we hope to start with this new concept of cryptopharmaceuticals', says Jukka Rantanen. He emphasises Denmark as an obvious candidate as a pioneer country for the technology. Among other things based on the country's existing tradition of storing citizens' health data and prominent pharmaceutical industry. 'I think it has huge potential for Denmark to be among the first movers on this type of product. It is not limited to only one clinical condition. There could be a completely new type of product family coming out of this', says Jukka Rantanen. For the research team at the University of Copenhagen, the next step is to test the app on a test group of patients. This could for example be diabetes, where patients are most often accustomed to taking medication and measuring their personal blood sugar on a regular basis. The 'MedBlockChain' app may be downloaded from the App Store and Google PLAY. Note that this product is not final, but an illustrative prototype. * What is Blockchain? * A blockchain is a growing chain of data where each link -- or block -- is connected by means of a special, encrypted code. Each block has its own timestamp and contains information on previous blocks. Therefore, you can always go back and trace what has happened along the way. A network of computers, often in a huge number, shares the chain of data blocks. When new blocks are added, it will be confirmed by all the computers in the network, using a consensus mechanism. The design makes blockchains almost impossible to manipulate. It is thus a very secure way to process and store data. "
Science Daily,"Lasers Enable Engineers to Weld Ceramics, No Furnace Required",Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822141946.htm,"   The process, published in the Aug. 23 issue of Science, uses an ultrafast pulsed laser to melt ceramic materials along the interface and fuse them together. It works in ambient conditions and uses less than 50 watts of laser power, making it more practical than current ceramic welding methods that require heating the parts in a furnace. Ceramics have been fundamentally challenging to weld together because they need extremely high temperatures to melt, exposing them to extreme temperature gradients that cause cracking, explained senior author Javier E. Garay, a professor of mechanical engineering and materials science and engineering at UC San Diego, who led the work in collaboration with UC Riverside professor and chair of mechanical engineering Guillermo Aguilar. Ceramic materials are of great interest because they are biocompatible, extremely hard and shatter resistant, making them ideal for biomedical implants and protective casings for electronics. However, current ceramic welding procedures are not conducive to making such devices. ""Right now there is no way to encase or seal electronic components inside ceramics because you would have to put the entire assembly in a furnace, which would end up burning the electronics,"" Garay said. Garay, Aguilar and colleagues' solution was to aim a series of short laser pulses along the interface between two ceramic parts so that heat builds up only at the interface and causes localized melting. They call their method ultrafast pulsed laser welding. To make it work, the researchers had to optimize two aspects: the laser parameters (exposure time, number of laser pulses, and duration of pulses) and the transparency of the ceramic material. With the right combination, the laser energy couples strongly to the ceramic, allowing welds to be made using low laser power (less than 50 watts) at room temperature. ""The sweet spot of ultrafast pulses was two picoseconds at the high repetition rate of one megahertz, along with a moderate total number of pulses. This maximized the melt diameter, minimized material ablation, and timed cooling just right for the best weld possible,"" Aguilar said. ""By focusing the energy right where we want it, we avoid setting up temperature gradients throughout the ceramic, so we can encase temperature-sensitive materials without damaging them,"" Garay said. As a proof of concept, the researchers welded a transparent cylindrical cap to the inside of a ceramic tube. Tests showed that the welds are strong enough to hold vacuum. ""The vacuum tests we used on our welds are the same tests that are used in industry to validate seals on electronic and optoelectronic devices,"" said first author Elias Penilla, who worked on the project as a postdoctoral researcher in Garay's research group at UC San Diego. The process has so far only been used to weld small ceramic parts that are less than two centimeters in size. Future plans will involve optimizing the method for larger scales, as well as for different types of materials and geometries. "
Science Daily,Materials Scientists Build a Synthetic System With Compartments Like Real Cells,Computers & Math,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822113405.htm,"   Now a research team led by Thomas Russell at the University of Massachusetts Amherst and the Lawrence Berkeley National Laboratory, with postdoctoral researcher Ganhua Xie and others, describe in a new paper how they take advantage of differences in electrical charge to create an ""all aqueous,"" water-in-water construct that achieves compartmentalization in a synthetic system. ""Our results point to new opportunities for manipulating and improving continuous separation and compartmentalized reactions. I feel we have developed a strategy to mimic the behavior of living cells,"" Russell notes. ""People have tried before to build synthetic systems that mimic nature and haven't done it, but we have. I think this is the first time this has been demonstrated."" Details appear in the current issue of Chem. Evan Runnerstrom, program manager in materials design at the Army Research Office, which supported this work with the U.S. Department of Energy, says, ""This ability to program stable structure and chemical functionality in all-aqueous systems that are environmentally friendly and bio-compatible will potentially provide unprecedented future capabilities for the Army. The knowledge generated by this project could be applicable to future technologies for all-liquid batteries, water purification or wound treatment and drug delivery in the field."" Russell and colleagues have been interested in liquid interfaces for several years and earlier conducted many oil-and-water experiments to observe results under various conditions. ""This led us to start looking at water-in-water liquid interfaces,"" he notes. For this work, Xie used two polymer aqueous solutions, one of polyethylene glycol (PEG) and water, the other dextran and water, with different electrical charges; they can be combined but do not mix. It's a ""classic example"" of coacervation, they suggest -- the solution undergoes liquid-liquid phase separation and forms two separate domains, like the non-mixing wax-and-water in a lava lamp. Next, Xie used a needle to send a high velocity jet of the dextran-plus-water solution into the PEG-plus-water solution, something Russell calls ""3D printing water-in-water."" This operation creates a coacervate-membrane-stabilized aqueous or water-filled tubule where the path-length of the tube can be kilometers long, he says. This 3D water-on-water printing forms a membranous layer of a coacervate that separates the two solutions. Another feature of the water tube formed this way is that electrical charge regulates whether and in which direction a material can pass through the coacervate membrane, the authors explain. A negatively charged dye or other molecule can only pass through a negatively charged wall of the asymmetrical membrane, and likewise for positively charged materials. Xie says, ""It effectively forms a diode, a one-sided gate. We can do a reaction inside this tube or sac that will generate a positively charged molecule that can only diffuse into the positive phase through the coacervate."" He adds, ""If we design the system right, we can separate things out easily by charge, so it can be used for separations media in all-aqueous compartmentalized reaction systems. We can also trigger one reaction that will allow a coordinated reaction cascade, just as it happens in our bodies."" Xie explains that the 3D water-on-water printing allows them to direct where they put these domains. ""We can build multi-layered structures with positive/negative/positive layers. We can use the sac-shaped ones as reaction chambers,"" he says. Advantages of separating functions and materials in cells by compartmentalization include allowing many processes to occur at once, many different chemical environments to coexist and otherwise incompatible components to work side by side. Among other tests and experiments, the researchers report on how they designed an all-aqueous tubular system and attached needles and syringe pumps at each end to allow water to pump through the entire structure without leakage, creating a flow-through coordinated reaction system. ""Once we'd done it, we looked at the biological mimicry,"" Russell says. ""There have been lots of efforts to mimic biological systems, and a biologist might object and say this is too simple. But I do think that even though it involves simple materials, it works. It's treading very close to vasculature, and it mimics any place where chemicals flow through a membrane. Is it in the body? No, but it does mimic a real metabolic process, a compartmental reaction."" The Army Research Office is an element of the U.S. Army Combat Capabilities Development Command's Army Research Laboratory. "
Science Daily,New Technique Could Streamline Design of Intricate Fusion Device,Computers & Math,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821173718.htm,"   ""Our main result is that we came up with a new method of identifying the irregular magnetic fields produced by stellarator coils,"" said physicist Caoxiang Zhu, lead author of a paper reporting the results in Nuclear Fusion. ""This technique can let you know in advance which coil shapes and placements could harm the plasma's magnetic confinement, promising a shorter construction time and reduced costs."" Fusion, the power that drives the sun and stars, is the fusing of light elements in the form of plasma -- the hot, charged state of matter composed of free electrons and atomic nuclei -- that generates massive amounts of energy. Twisty, cruller-shaped stellarators are an alternative to doughnut-shaped tokamaks that are more commonly used by scientists seeking to replicate fusion on Earth for a virtually inexhaustible supply of power to generate electricity. A key benefit of stellarators is their production of highly stable plasmas that are less liable to the damaging disruptions that tokamaks can incur. But the complexity of stellarator coils has been a factor holding back development of such facilities. The coils of a stellarator must be constructed and arranged around the vacuum chamber very precisely, since deviations from the best coil arrangement create bumps and wiggles in the magnetic field that degrade the magnetic confinement and allow the plasma to escape. These problematic magnetic fields can easily be caused by misplacement of the magnetic coils, so engineers stipulate strict tolerances for these components. ""The big challenge of building stellarators is figuring out how to make them simply and economically,"" said PPPL Chief Scientist Michael Zarnstorff. ""Zhu's research is important because he is trying to look more carefully and quantitatively at some of the drivers of the cost. His results suggest that we can simplify the construction of stellarators and thereby make them easier and less expensive to build, by not insisting on tight tolerances for things that don't matter."" In the past, scientists have used computer simulations to determine which coil placements would be best, checking the plasma's reactions to all possible magnetic configurations before the stellarator was built. But because there are many ways for the coils to vary, ""this approach requires massive computation resources and man-hours,"" said Zhu. ""In this paper, we propose a new mathematical method to rapidly identify dangerous coil deviations that could appear during fabrication and assembly."" The method relies on a Hessian matrix, a mathematical tool that allows researchers to determine which variations of the magnetic coils can make the plasma change its properties. ""The idea is to figure out which perturbations you really have to control or avoid, and which you can ignore,"" Zhu said. The team recently confirmed the accuracy of the new method by using it to analyze coil placements for a configuration similar to the Columbia Non-Neutral Torus, a small fusion facility operated by Columbia University. They compared the results to those produced by past studies relying on conventional methods and found that they agreed. The team is now collaborating with researchers in China to use the method to optimize coil placement on the Chinese First Quasi-axisymmetric Stellarator (CFQS), currently under construction. The new technique could help scientists design better stellarators, Zhu said. It could make possible ways to identify an optimal coil arrangement that no one had considered before. Included on the research team were scientists from China's Southwest Jiaotong University and Japan's National Institute for Fusion Science. The research was supported by the DOE's Office of Science and the Max Planck Princeton Center for Plasma Physics. "
Science Daily,Physicists Create World's Smallest Engine,Computers & Math,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821125519.htm,"   Work performed by Professor John Goold's QuSys group in Trinity's School of Physics describes the science behind this tiny motor. The research, published today in international journal Physical Review Letters, explains how random fluctuations affect the operation of microscopic machines. In the future, such devices could be incorporated into other technologies in order to recycle waste heat and thus improve energy efficiency. The engine itself -- a single calcium ion -- is electrically charged, which makes it easy to trap using electric fields. The working substance of the engine is the ion's ""intrinsic spin"" (its angular momentum). This spin is used to convert heat absorbed from laser beams into oscillations, or vibrations, of the trapped ion. These vibrations act like a ""flywheel,"" which captures the useful energy generated by the engine. This energy is stored in discrete units called ""quanta,"" as predicted by quantum mechanics. ""The flywheel allows us to actually measure the power output of an atomic-scale motor, resolving single quanta of energy, for the first time,"" said Dr Mark Mitchison of the QuSys group at Trinity, and one of the article's co-authors. Starting the flywheel from rest -- or, more precisely, from its ""ground state"" (the lowest energy in quantum physics) -- the team observed the little engine forcing the flywheel to run faster and faster. Crucially, the state of the ion was accessible in the experiment, allowing the physicists to precisely assess the energy deposition process. Assistant Professor in Physics at Trinity, John Goold said: ""This experiment and theory ushers in a new era for the investigation of the energetics of technologies based on quantum theory, which is a topic at the core of our group's research. Heat management at the nanoscale is one of the fundamental bottlenecks for faster and more efficient computing. Understanding how thermodynamics can be applied in such microscopic settings is of paramount importance for future technologies."" "
Science Daily,Wildfires Could Permanently Alter Alaska's Forest Composition,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112658.htm,"   Using a well-tested ecosystem model called ecosys, they predicted that by the year 2100 the relative dominance of evergreen conifer trees (black spruce) will decline by 25% and non-woody herbaceous plants such as moss and lichen will decline by 66%, while broadleaf deciduous trees (aspen) will become dominant, nearly doubling in prevalence. With such large declines, this shift in vegetation will highly likely have reverberations for the entire ecosystem and climate. ""Expansion of the deciduous broadleaf forests in a warmer climate may result in several ecological and climatic feedbacks that affect the carbon cycle of northern ecosystems,"" said Zelalem Mekonnen, a Berkeley Lab postdoctoral fellow who was first author of the study. The paper, ""Expansion of High-Latitude Deciduous Forests Driven by Interactions Between Climate Warming and Fire,"" was published today in Nature Plants. The study was funded as part of DOE's Office of Science through the Next-Generation Ecosystem Experiment -- Arctic project and included co-authors from UC Irvine, the University of Alberta, and Woods Hole Research Center. NGEE-Arctic seeks to gain a predictive understanding of the Arctic terrestrial ecosystem's feedback to climate and is a collaboration among scientists at Oak Ridge National Laboratory, Berkeley Lab, Los Alamos National Laboratory, Brookhaven National Laboratory, and the University of Alaska Fairbanks. ""We predict the forest system will remain a net sink for carbon, meaning it will absorb more carbon than it emits,"" said co-author William J. Riley, a senior scientist in Berkeley Lab's Earth & Environmental Sciences Area. ""But will it be more or less of a sink? Our next study will quantify the carbon and surface energy budgets. This study focused more on how vegetation types are expected to change."" Changes in forest cover type will affect many important ecosystem processes. For example, an increase in deciduous broadleaf trees, which lose their leaves every year, unlike evergreens, could result in more rapid microbial decomposition and increased transpiration (the loss of moisture through leaves); both of these processes introduce amplifying feedbacks to climate warming. On the other hand, higher surface reflectance may have a cooling effect when more snow is exposed because of fewer evergreen trees; what's more, deciduous trees are less flammable than evergreen trees. The researchers predicted modest effects on net carbon budgets and will analyze that further in future work. Riley added that the study included many steps to confirm that the results from ecosys were valid. ""We evaluated model performance against many current observations of forest cover and carbon cycling measurements, and against long-term changes under natural climate variation,"" he said. Combo of fire plus climate warming could alter forests in 40 years Climate change is hitting the northern latitudes especially hard due to the phenomenon of Arctic amplification, a positive feedback that causes temperatures to rise faster than the global average. While average global temperatures are projected to rise about 4 degrees Celsius by 2100 in a ""business as usual"" scenario, some recent studies are predicting much larger increases for the Arctic. The extent to which fires will increase is even more uncertain. So the researchers modeled four scenarios, from a zero increase in burn area up to a 150% increase by 2100. The scenarios were taken from published studies that accounted for factors such as warmer temperatures and increases in lightning strikes. What is known about fires are the impacts they have on the forest ecosystem. ""Fires deepen the active layer, which is the zone of soil that remains unfrozen,"" said Riley. ""That leads to an increase in soil nutrients available for plants. Increases in soil nutrients favor deciduous plants, which is one reason why we predict they will do so well under a warming climate. Higher deciduous tree cover has happened under previous climates; paleoecological studies of the last 10,000 years suggest that Alaskan forests have undergone similar shifts in dominant tree species."" Another factor that favors broadleaf deciduous over evergreen conifer trees is that their leaves decompose more rapidly, leading to more rapid carbon turnover, which determines the available nutrients in the ecosystem. ""As you get more rapid turnover, you get more deciduous plants,"" Riley said. ""It's a self-reinforcing mechanism."" Although previous studies have examined how climate change will impact boreal forests, Riley said this was the first to consider the complex interactions among plants, soil, and nutrients -- both above and below ground -- and how they evolve over time. ""This study is a more detailed and mechanistic explanation of these processes,"" he said. Other factors that favor broadleaf deciduous trees in a future warmer climate are their greater ability for post-fire seedling regeneration and their ability to grow fast and thus compete for light. ""Plants have different strategies to survive under different environmental conditions,"" Mekonnen said. The study found that both climate change and increased fire were required to produce broadleaf deciduous trees' dominance. Across the fire scenarios tested where fires increased, that shift was projected to occur around the year 2058. If warming occurred without increased fire or vice versa, the model found that evergreen conifers remained the dominant Alaskan tree type through the 21st century. Another forest component that will be affected is wildlife. ""Broadleaf deciduous trees have a large canopy which covers underlying vegetation, potentially decreasing herbaceous plant cover. Those plants, especially moss, are very important forage for wildlife,"" Mekonnen said. What's more, the modeling technique can be used to study how climate change and fire will affect other geographic areas. ""Our modeling approach is applicable to other northern regions because the fundamental mechanisms that control these dynamics are similar everywhere,"" Mekonnen said. "
Science Daily,Big Increase in Ocean Carbon Dioxide Absorption Along West Antarctic Peninsula,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112656.htm,"   The study, led by scientists at Rutgers University-New Brunswick, is published in the journal Nature Climate Change. The West Antarctic Peninsula is experiencing some of the most rapid climate change on Earth, featuring dramatic increases in temperatures, retreats in glaciers and declines in sea ice. The Southern Ocean absorbs nearly half of the carbon dioxide -- the key greenhouse gas linked to climate change -- that is absorbed by all the world's oceans. ""Understanding how climate change will affect carbon dioxide absorption by the Southern Ocean, especially in coastal Antarctic regions like the West Antarctic Peninsula, is critical to improving predictions of the global impacts of climate change,"" said lead author Michael Brown, an oceanography doctoral student in the Center for Ocean Observing Leadership in the Department of Marine and Coastal Sciences at the School of Environmental and Biological Sciences. The study tapped an unprecedented 25 years of oceanographic measurements in the Southern Ocean and highlights the need for more monitoring in the region. The research revealed that carbon dioxide absorption by surface waters off the West Antarctic Peninsula is linked to the stability of the upper ocean, along with the amount and type of algae present. A stable upper ocean provides algae with ideal growing conditions. During photosynthesis, algae remove carbon dioxide from the surface ocean, which in turn draws carbon dioxide out of the atmosphere. From 1993 to 2017, changes in sea ice dynamics off the West Antarctic Peninsula stabilized the upper ocean, resulting in greater algal concentrations and a shift in the mix of algal species. That's led to a nearly five-fold increase in carbon dioxide absorption during the summertime. The research also found a strong north-south difference in the trend of carbon dioxide absorption. The southern portion of the peninsula, which to date has been less impacted by climate change, experienced the most dramatic increase in carbon dioxide absorption, demonstrating the poleward progression of climate change in the region. The results also demonstrate the often counterintuitive impacts of climate change. The scientists hypothesize that upper ocean stability off the West Antarctic Peninsula may ultimately decrease in the coming decades as sea ice continues to decline. Once sea ice reaches a critically low level, there won't be enough of it to prevent wind-driven mixing of the upper ocean, or to supply a sufficient amount of stabilizing meltwater. And that could result in reduced carbon dioxide absorption in the Southern Ocean over the long run. A decrease in the ocean's ability to absorb carbon dioxide could lead to more warming worldwide by allowing more of the heat-trapping gas to remain in the atmosphere. "
Science Daily,Flame Retardants -- From Plants,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092330.htm,"   The researchers will present their results at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""The best flame-retardant chemicals have been organohalogen compounds, particularly brominated aromatics,"" says Bob Howell, Ph.D., the project's principal investigator. ""The problem is, when you throw items away, and they go into a landfill, these substances can leach into the environment."" Most organohalogen flame retardants are very stable. Microorganisms in the soil or water can't degrade them, so they persist for many years in the environment, working their way up the food chain. In addition, some of the compounds can migrate out of items to which they are added, such as electronics, and enter household dust. Although the health effects of ingesting or breathing organohalogen flame retardants are largely unknown, some studies suggest they could be harmful, prompting California to ban the substances in children's products, mattresses and upholstered furniture in 2018. ""A number of flame retardants are no longer available because of toxicity concerns, so there is a real need to find new materials that, one, are nontoxic and don't persist, and two, don't rely upon petroleum,"" Howell says. His solution was to identify compounds from plants that could easily be converted into flame retardants by adding phosphorus atoms, which are known to quench flames. ""We're making compounds that are based on renewable biosources,"" he says. ""Very often they are nontoxic; some are even food ingredients. And they're biodegradable -- organisms are accustomed to digesting them."" To make their plant-derived compounds, Howell and colleagues at the Center for Applications in Polymer Science at Central Michigan University began with two substances: gallic acid, commonly found in fruits, nuts and leaves; and 3,5-dihydroxybenzoic acid from buckwheat. Using a fairly simple chemical reaction, the researchers converted hydroxyl groups on these compounds to flame-retardant phosphorus esters. Then, the team added the various phosphorus esters individually to samples of an epoxy resin, a polymer often used in electronics, automobiles and aircraft, and examined the different esters' properties with several tests. In one of these tests, the researchers showed that the new flame retardants could strongly reduce the peak heat release rate of the epoxy resin, which reflects the intensity of the flame and how quickly it is going to spread. The plant-derived substances performed as well as many organohalogen flame retardants on the market. ""As a matter of fact, they may be better,"" Howell says. ""Because gallic acid has three hydroxyl groups within the same molecule that can be converted to phosphorus esters, you don't have to use as much of the additive, which reduces cost."" The researchers also studied how the new compounds quench flames, finding that the level of oxygenation at the phosphorus atom determined the mode of action. Compounds with a high level of oxygenation (phosphates) decomposed to a substance that promoted char formation on the polymer surface, starving the flame of fuel. In contrast, compounds with a low level of oxygenation (phosphonates) decomposed to species that scavenged combustion-promoting radicals. Howell's team hasn't yet performed toxicity tests, but he says that other groups have done such studies on similar compounds. ""In general, phosphorus compounds are much less harmful than the corresponding organohalogens,"" he notes. In addition, the plant-derived substances are not as volatile and are less likely to migrate from items into household dust. Howell hopes that the new flame retardants will attract the attention of a company that could help bring them to market, he says. "
Science Daily,Beaver Reintroduction Key to Solving Freshwater Biodiversity Crisis,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104844.htm,"   New research from the Faculty of Natural Sciences has provided further support to previous work that has shown beavers have an important impact on the variety of plant and animal life. The latest study, led by Dr Alan Law and Professor Nigel Willby, found that the number of species only found in beaver-built ponds was 50 percent higher than other wetlands in the same region. Dr Law, Lecturer in Biological and Environmental Sciences, said: ""Beavers make ponds that, at first glance, are not much different from any other pond. However, we found that the biodiversity -- predominantly water plants and beetles -- in beaver ponds was greater than and surprisingly different from that found in other wetlands in the same region. ""Our results also emphasise the importance of natural disturbance by big herbivores -- in this case, tree felling, grazing and digging of canals by beavers -- in creating habitat for species which otherwise tend to be lost. ""Reintroducing beavers where they were once native should benefit wider biodiversity and should be seen as an important and bold step towards solving the freshwater biodiversity crisis."" Beavers are one of the only animals that can profoundly engineer the environment that they live in -- using sticks to build dams across small rivers, behind which ponds form. Beavers do this to raise water levels to avoid predators, such as wolves and bears: however, numerous other plants and animals also benefit from their work. The research team surveyed water plants and beetles in 20 wetlands in a small area of southern Sweden -- 10 created by beavers and 10 that were not -- to understand whether beavers might provide a solution to the current biodiversity crisis by creating novel habitats. Professor Willby added: ""The loss of large mammals from modern landscapes is a global conservation concern. These animals are important in their own right, but our research emphasises the added biodiversity benefits that go with them. ""We are best reminded of this effect when large herbivores, such as beavers, are reintroduced to places where they have been lost."" This research follows the team's 2018 study that found that 33 percent more plant species and 26 percent more beetles were living in wetlands created by beavers, compared to those that were not. Another previous study, from 2017, showed that -- over a period of 12 years -- local plant richness in a Tayside wetland rose by 46 percent following the introduction of beavers. They created 195 metres of dams, 500 metres of canals and a hectare of ponds. "
Science Daily,Northern White Rhino Eggs Successfully Fertilized,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104835.htm,"   ""We were surprised by the high rate of maturation achieved as we do not get such high rate (comparable to what we get with horse oocytes) with southern white rhino females in European zoos. The semen of Saut was very difficult to work with and to find three live sperms needed for the eggs of Najin we had to thaw two batches of semen. Now the injected oocytes are incubated and we need to wait to see if any viable embryo develop to the stage where it can be cryopreserved for later transfer,"" said Cesare Galli of Avantea in Cremona (Italy) who led the fertilization procedure. The international research consortium to save the northern white rhino from extinction is led by Thomas Hildebrandt from the Leibniz Institute for Zoo and Wildlife Research (IZW). Avantea is responsible for maturing the egg cells and creating viable embryos, further key project partners are Dvur Kralove Zoo, Ol Pejeta Conservancy and the Kenya Wildlife Service. The results of possible embryo development are to be announced around September 10th. The research program (BioRescue) is funded by the German Federal Ministry of Education and Research (BMBF). "
Science Daily,First Direct Evidence for Mantle Plume Origin of Jurassic Flood Basalts in Southern Africa,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826104821.htm,"   The great Jurassic lava flows that flooded across southern Africa and parts of East Antarctica prior to the splitting of the Pangea supercontinent make up one of the largest volcanic systems on Earth. The magma eruptions caused global environmental turmoil and the extinctions of species. The rapid origin of this Karoo flood basalt province in southern Africa has been frequently associated with the melting of a large plume that ascended from the deep mantle around 180 million years ago. However, the plume model has lacked confirmation from lava compositions that preserve a geochemical 'plume signature'. ""To our knowledge, the Luenha picrites are the first lava samples that could originate from the plume source that has been previously inferred from various geological and geophysical data on the Karoo province. Therefore they allow compositional analysis of this source,"" says Sanni Turunen, the leading author and a doctoral student at the Finnish Museum of Natural History, which is part of the University of Helsinki. In the case of the Luenha picrites, named after the research area near the Luenha River, the geochemical compositions indicate a hot magma source that is in many respects different from previously reported magma sources in the Karoo province. They show compositional similarities to magmas formed in other deep mantle plume-related volcanic provinces worldwide. ""It is very important to realise that in huge and complex volcanic systems, such as the Karoo province, large amounts of magmas may be produced from several magma sources,"" explains Da√∫d Jamal, professor at the Eduardo Mondlane University, in Mozambique. ""Previous studies of Karoo picrites in Africa and Antarctica by us and by other groups have suggested the generation of magmas in the upper mantle, but our new results indicate plume sources were also involved,"" adds Jussi Heinonen, an Academy of Finland fellow at the Department of Geosciences and Geography at the University of Helsinki. Importantly, the Luenha picrites appear to represent the main source of the voluminous flood basalts of southern Africa. ""We were fascinated to realise that the Luenha picrites revealed a type of magma source that was recently predicted using lava compositions, but which had not been confirmed by observational evidence,"" as characterised by Arto Luttinen, senior curator at the Finnish Museum of Natural History. According to the study, the presently available data are compatible with a plume source that has retained the composition of Earth's primitive mantle remarkably well. This is quite unusual because of the 4.5 billion year evolution of the convecting mantle. Confirmation of the age and evolution of the primitive mantle-like source of the Luenha picrites requires further constraints from future isotopic studies. ""Whatever the exact nature of the Luenha source turns out to be, we feel confident that we have uncovered rocks that help to address the complex origin of large eruptions in new detail,"" Turunen concludes. "
Science Daily,Salt Marshes' Capacity to Sink Carbon May Be Threatened by Nitrogen Pollution,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092340.htm,"   However, a new study indicates that a common pollutant of coastal waters, nitrate, stimulates the decomposition of organic matter in salt marsh sediments that normally would have remained stable over long periods of time. This increase in decomposition, which releases CO2, could alter the capacity of salt marshes to sequester carbon over the long term. The study, led by scientists at the Marine Biological Laboratory (MBL), Woods Hole, and Northeastern University, is published in Global Change Biology. ""Traditionally, we have viewed salt marshes as resilient to nitrogen pollution, because the microbes there remove much of the nitrogen as gas through a process called denitrification,"" writes first author Ashley Bulseco, a postdoctoral scientist at the MBL. ""But this research suggests that when nitrate is abundant, a change occurs in the microbial community in salt marsh sediments that increases the microbes' capacity to degrade organic matter. This potentially reduces the ability of the marsh to store carbon,"" Bulseco writes. As global temperatures continue to rise, a number of carbon capture strategies have been proposed, including sequestering CO2 in ""blue carbon"" habitats such as salt marshes, mangroves and seagrass meadows. However, coastal nitrogen pollution is also still rising in many areas due to agricultural and urban runoff, and sewage. ""Given the extent of nitrogen loading along our coastlines, it is imperative that we better understand the resilience of salt marsh systems to nitrate, especially if we hope to rely on salt marshes and other blue carbon systems for long-term carbon storage,"" the authors write. The next phase of this research, already in progress, is to analyze the microbial community responsible for degrading carbon in a salt marsh ecosystems, especially when exposed to high concentrations of nitrate. "
Science Daily,Monster Tumbleweed: Invasive New Species Is Here to Stay,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092336.htm,"   The species, Salsola ryanii, is significantly larger than either of its parent plants, which can grow up to 6 feet tall. A new study from UC Riverside supports the theory that the new tumbleweed grows more vigorously because it is a hybrid with doubled pairs of its parents' chromosomes. Findings from the study are detailed in a new paper published in the Oxford University-produced journal AoB Plants. ""Salsola ryanii is a nasty species replacing other nasty species of tumbleweed in the U.S.,"" said study co-author Norman Ellstrand, UCR Distinguished Professor of Genetics. ""It's healthier than earlier versions, and now we know why."" Humans are diploid organisms, with one set of chromosomes donated by the mother and one set from the father. Sometimes a mother's egg contains two sets of chromosomes rather than just the one she is meant to pass on. If this egg is fertilized, the offspring would be triploid, with three sets of chromosomes. Most humans do not survive this. Plants with parents closely related enough to mate can produce triploid offspring that survive but are unable to reproduce themselves. However, a hybrid plant that manages to get two copies from the mother and two from the father will be fertile. Some species can have more than four sets of chromosomes. They can even have ""hexaploidy,"" with six sets of chromosomes. Scientists have long assumed there must be some kind of evolutionary advantage to polyploidy, the term for hybrids that have multiple sets of chromosomes, since it poses some immediate difficulties for the new hybrids. ""Typically, when something is new, and it's the only one of its kind, that's a disadvantage. There's nobody exactly like you to mate with,"" said study co-author Shana Welles, the graduate student in Ellstrand's laboratory that conducted the study as part of her Ph.D. research. She is now a postdoctoral fellow at Chapman University. The advantage to having multiple sets of chromosomes, according to the study, is that the hybrid plant grows more vigorously than either of its parents. This has been suggested as the reason polyploidy is so common in plants. However, it has not, until now, been demonstrated experimentally. Polyploidy is associated with our favorite crops; domesticated peanuts have four sets of chromosomes, and the wheat we eat has six. Though tumbleweeds are often seen as symbols of America's old West, they are also invasive plants that cause traffic accidents, damage agricultural operations, and cause millions in property damage every year. Last year, the desert town of Victorville, California, was buried in them, piling up to the second story of some homes. Currently, Salsola ryanii has a relatively small but expanding geographic range. Since the new study determined it is even more vigorous than its progenitors, which are invasive in 48 states, Welles said it is likely to continue to expand its range. Additionally, Welles said climate change could increase its territory takeover. Though this tumbleweed is an annual, it tends to grow on the later side of winter. ""It's one of the only things that's still green in late summer,"" Welles said. ""They may be well positioned to take advantage of summer rains if climate changes make those more prevalent."" Given its potential for damage, the knowledge now available about Salsola ryanii could be important for helping to suppress it, and Ellstrand believes that is what should happen before it takes over. ""An ounce of prevention is a pound of cure,"" he said. "
Science Daily,Cleaning Pollutants from Water With Pollen and Spores -- Without the 'Achoo!',Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092326.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""Even very low levels of certain compounds, such as hormones, pharmaceuticals or those in household and personal care products, can cause toxic effects. However, they often can escape normal cleanup processes at wastewater treatment plants,"" says Andrew Boa, Ph.D., whose lab is working on the pollen project. ""We're trying to find alternative ways to remove these chemicals from water so we can reduce the amount going into the environment."" The project is part of the larger ""Sullied Sediments"" program in which Boa and many other scientists are assessing pollutant levels in sediments from European waterways, with a view to assessing dredged sediment, managing sediment reuse and reducing future contaminations. These contaminants include pharmaceuticals such as the pain reliever diclofenac and household chemicals such as triclosan, an antimicrobial compound used in toothpaste, and other personal care products. Some of these chemicals, including triclosan, are either banned or their use heavily restricted. The European Union will begin officially monitoring levels of all of these ""Watch List"" chemicals from 2020 onwards. The spore grains used in the study are extracted from Lycopodium clavatum -- the common club moss. In their natural state, each of these microscopic grains carries genetic material inside a hard shell that's coated with an outer layer of wax and proteins, explains Aimilia Meichanetzoglou, a doctoral student in Boa's lab at the University of Hull. Boa first became interested in pollen thanks to his work with Grahame Mackenzie, Ph.D., a Hull professor (now emeritus) who developed the original method to form non-allergenic, hollowed-out pollen and spore shells. Mackenzie's company, Sporomex, uses the inert shells to encapsulate active ingredients for controlled release in pharmaceutical, food, cosmetic and medical applications. Boa has taken the concept in an entirely different direction. When he and Meichanetzoglou were studying the empty shells' interactions with a variety of chemicals, they noticed that some of the compounds became adsorbed, or stuck to, the surface of the shells. Boa realized this stickiness could potentially be used to grab low levels of pollutants, and so he pursued this type of application. Meichanetzoglou uses hydrolysis to rid the pollen of its genetic cargo and waxy coat, which makes the grains hypoallergenic. To target particular pollutants, she can vary the hydrolysis conditions and make modifications to the surface of the grains. For example, to remove phosphate, which is used in many fertilizers, Meichanetzoglou deposits iron oxide on the surface of the shells. Iron oxide reacts with the phosphate to form insoluble iron phosphate, which precipitates out of the water and gets adsorbed onto the grains. The researchers found that the grains could remove almost all of the phosphate from water samples and nearly 80% of several other pollutants. Treating wastewater will require consideration of various factors, such as scale and the degree of contamination. For example, homes that use a septic tank; particular buildings with a high level of pharmaceuticals in their waste water, such as hospitals or care homes for the elderly; or municipal waste water treatment plants that serve a whole city will all have different requirements. Boa is exploring options with local water authorities for implementation of this technology. Boa has purposely tried to keep process costs low to make the method commercially feasible. The moss is already harvested for other applications, Boa notes, and it can grow on poor-quality soil, so it won't compete with food crops for arable land. His collaborators have also begun testing the bioavailability of pollutants captured by the grains. "
Science Daily,Making Polyurethane Degradable Gives Its Components a Second Life,Earth & Climate,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092324.htm,"   The researchers will present their results today at the American Chemical Society (ACS) Fall 2019 National Meeting & Exposition. ""Millions of tons of polyurethanes are produced every day, and they're widely used in foams, plastics, sneakers, insulation and other products,"" says Ephraim Morado, a doctoral student who is presenting the work at the meeting. ""But when people finish using them, these materials are usually discarded."" Waste polyurethane either ends up in landfills, or it's incinerated, which requires a lot of energy and generates toxic byproducts, he notes. ""As an alternative, we want to develop the next generation of polyurethane that can degrade easily and be reprocessed into a new material that can then be commercialized, such as adhesives or paint,"" he says. Of course, Morado isn't alone in seeking ways to reuse polymers. ""A lot of people interested in recycling are trying to make polymers that will break down into their original starting materials and then remake the same polymer,"" says Steven Zimmerman, Ph.D., the project's principal investigator. ""We're taking a very different, intermediate approach, which industry might be more interested in pursuing in the short term because it would be easier and cheaper,"" adds Zimmerman, whose lab is based at the University of Illinois at Urbana-Champaign. ""We're trying to break our polymers down into some other starting materials that are familiar to industry."" The key difference between standard polyurethane and Morado's version is the incorporation of a hydroxy acetal as one of the monomers, alongside the traditional monomers. Zimmerman's team had first used a special iodine-containing acetal to make degradable polymers and polyacrylamide gels. In that earlier work, the polymer could be dissolved in slightly acidic water. Morado invented a new type of acetal to incorporate in his unconventional polyurethane so he could dissolve the polymer in the absence of water. After months of investigation, he discovered that a solution of trichloroacetic acid in dichloromethane, an organic solvent, could dissolve the polyurethane at room temperature in just three hours. That's in contrast to the harsher conditions of the typical incineration method, which requires more than 1,400 F to avoid toxic gas formation. Unlike water, dichloromethane causes the material to swell. That expansion enables the acid to reach the backbone of polyurethane's molecular chains, which it can break at positions where the acetal groups are located. Degradation releases alcohol monomers that can then be used to make new products such as adhesives whose performance rivals superglue. Morado created other acetal-containing polyurethanes that can be triggered to degrade when exposed to light. He used these materials to make microcapsules that could contain herbicides or even biocides for killing barnacles and other creatures that stick to ship hulls. He and Zimmerman are also developing adhesives that dissolve when treated with a few drops of acid in dichloromethane solvent. One potential application is on circuit boards, where a chip that had been securely glued to the board could be swapped out for a replacement if the original chip had failed. In addition, the team is working on polyurethanes that can degrade under even milder conditions, such as exposure to vinegar. That would be particularly useful for, say, degradable sutures or household applications such as removable picture hangers. "
Science Daily,Hiring Committees That Don't Believe in Gender Bias Promote Fewer Women,Science & Society,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112653.htm,"   Opinions vary, but a new study by a UBC psychologist and researchers in France reveals that hiring committees who denied it's a problem were less likely to promote women. ""Our evidence suggests that when people recognize women might face barriers, they are more able to put aside their own biases,"" said Toni Schmader, a UBC psychology professor and Canada Research Chair in social psychology. ""We don't see any favourability for or against male or female candidates among those committees who believe they need to be vigilant to the possibility that biases could be creeping in to their decision-making."" The study was unique in that findings were based on actual decisions made by 40 hiring committees in France, charged with filling elite research positions with the National Committee for Scientific Research (CNRS) for two consecutive years. Past research in this area has relied mostly on hypothetical scenarios, such as presenting a large sample of participants with identical resum√©s bearing either male or female names and asking who they would hire. By contrast, the decisions made during this study had real impact on scientists' careers. With cooperation from the CNRS, the researchers were able to first measure how strongly hiring committee members associated men with science. They did this using an ""implicit association test"" that flashes words on a computer screen and measures how quickly participants are able to assign those words to a particular category. People who make a strong association between men and science have to think a bit longer, and react more slowly, when challenged to pair female-related words with science concepts. Both men and women on the hiring committees tended to show the science = male association, which is difficult to hide in such a test. ""There's research suggesting that you can document a 'think science, think male' implicit association showing up with kids as early as elementary school,"" Schmader said. ""We learn associations from what we see in our environment. If we don't see a lot of women who are role models in science, then we learn to associate science more with men than women."" These implicit associations are distinct from people's explicit beliefs about women in science. In a separate survey that asked panellists directly whether women in science careers are impacted by such things as discrimination and family constraints, some hiring committees minimized those issues. Others acknowledged them. When the researchers compared these implicit and explicit beliefs with the actual hiring outcomes, they learned that committees attuned to the barriers women face were more likely to overcome their implicit science/male associations when selecting candidates for the job. Among committees that believed ""science isn't sexist,"" those which implicitly associated science more with men promoted fewer women. The difference was especially pronounced in Year 2 of the study, when committee members would have been less conscious of the fact that their selections were being studied. The findings show that awareness and acknowledgement of the barriers women face might be key to making sure implicit biases don't affect hiring decisions. They also point to the importance of educating hiring committees about gender bias and how to guard against it, Schmader said. The study was published today in Nature Human Behaviour. "
Science Daily,Tech Time Not to Blame for Teens' Mental Health Problems,Science & Society,2019-08-23,-,https://www.sciencedaily.com/releases/2019/08/190823140736.htm,"   The study tracked young adolescents on their smartphones to test whether more time spent using digital technology was linked to worse mental health outcomes. The researchers -- Candice Odgers, professor of psychological science at the University of California, Irvine; Michaeline Jensen, assistant professor of psychology at the University of North Carolina at Greensboro; Madeleine George, postdoctoral researcher at Purdue University; and Michael Russell, assistant professor of behavioral health at Pennsylvania State University -- found little evidence of longitudinal or daily linkages between digital technology use and adolescent mental health. ""It may be time for adults to stop arguing over whether smartphones and social media are good or bad for teens' mental health and start figuring out ways to best support them in both their offline and online lives,"" Odgers said. ""Contrary to the common belief that smartphones and social media are damaging adolescents' mental health, we don't see much support for the idea that time spent on phones and online is associated with increased risk for mental health problems,"" Jensen said. The study surveyed more than 2,000 youth and then intensively tracked a subsample of nearly 400 teens on their smartphones multiple times a day for two weeks. Adolescents in the study were between 10 and 15 years old and represented the economically and racially diverse population of youth attending North Carolina public schools. The researchers collected reports of mental health symptoms from the adolescents three times a day and they also reported on their daily technology usage each night. They asked whether youth who engaged more with digital technologies were more likely to experience later mental health symptoms and whether days that adolescents spent more time using digital technology for a wide range of purposes were also days when mental health problems were more common. In both cases, increased digital technology use was not related to worse mental health. When associations were observed, they were small and in the opposite direction that would be expected given all of the recent concerns about digital technology damaging adolescents' mental health. For instance, teens who reported sending more text messages over the study period actually reported feeling better (less depressed) than teens who were less frequent texters. "
Science Daily,The Case for Retreat in the Battle Against Climate Change,Science & Society,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822141856.htm,"   That's the case for carefully planned ""managed retreat"" made by three environmental researchers in an article published Aug. 22 in the Policy Forum section of the journal Science. The article was written by lead author A.R. Siders of the University of Delaware, with co-authors Miyuki Hino and Katharine J. Mach of Stanford University and the University of Miami. ""We need to stop picturing our relationship with nature as a war,"" said Siders, who is a core faculty member of UD's Disaster Research Center and an assistant professor of public policy and administration and of geography. ""We're not winning or losing; we're adjusting to changes in nature. Sea levels rise, storms surge into flood plains, so we need to move back."" Moving away from coastal and other endangered areas usually occurs after disaster strikes, she said, with emergency evacuations and their aftermath often handled inefficiently and haphazardly. Instead, the researchers argue that retreating from those areas should be done thoughtfully, with planning that is strategic as well as managed. ""Retreat is a tool that can help achieve societal goals like community revitalization, equity and sustainability if it is used purposefully,"" Siders said. ""People sometimes see retreat as defeatist, but I see it as picking your battles."" In the Science paper, the researchers point out that retreat is a difficult and complex issue for many reasons, including the short-term economic gains of coastal development, subsidized insurance rates and disaster recovery costs, and people's attachment to the place where they live and to the status quo. Also, when disaster strikes, the more affluent residents are more able to relocate, often leaving behind those who don't have the financial resources to move. ""No matter the circumstances, moving is hard,"" Hino said. ""People have chosen where to live for a reason, and it is often difficult to find a place to move to that meets all their social, cultural and financial requirements. ""One major challenge with retreat is that we're so focused on getting people out of harm's way, we miss the chance to help them move to opportunity."" The researchers take the long view, noting that retreat may be the answer to climate change in some areas, but it may not be a step that's necessary this year or even this decade. ""The challenge is to prepare for long-term retreat by limiting development in at-risk areas,"" they write, and making plans for further action based on responding to specific triggers and constantly monitoring and evaluating conditions. ""The story of retreat as a climate response is just beginning,"" Mach said. ""Retreat is compelling because it brings together so many aspects of how societies work, what individuals are trying to achieve and what it takes to ensure preparedness and resilience in a changing climate."" The paper makes note of a variety of areas where additional work is needed, including coordination of various levels of government and support for relocation assistance programs. First, Siders said, communities must identify which areas they most want to protect and how to encourage and assist relocation. ""Managed retreat needs to be embedded in larger conversations and social programs,"" she said. ""Retreat can't be just about avoiding risk. It needs to be about moving toward something better."" "
Science Daily,Shocking Rate of Plant Extinctions in South Africa,Science & Society,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822141918.htm,"   According to a study published in the journal Current Biology this week, this represents a shocking 45.4% of all known plant extinctions from 10 of the world's 36 biodiversity hotspots. Biodiversity hotspots are areas that harbour exceptionally high numbers of unique species, but at the same time they are under severe threat from human disturbance. South Africa is remarkable in that, for a relatively small country, it is home to three of these hotspots. An international team of researchers, led by Prof Jaco Le Roux and Dr Heidi Hirsch, affiliated with the Centre for Invasion Biology (CIB) at Stellenbosch University (SU), analysed a comprehensive dataset of 291 plant extinctions since 1700 in ten biodiversity hotspots and six coldspots, covering about 15% of Earth's land surface. The main drivers for extinctions in South Africa were found to be agriculture (49.4%), urbanisation (38%) and invasive species (22%). Variability in predictions on the rate of plant extinctions The results of their analysis show that, since the 1990s, extinction rates for plants over the past 300 years appear to have settled at about 1.26 extinctions per year. At its peak, however, it was at least 350 times that of historical background rates during pre-human times. At this rate, they predict that, in the areas they studied, an additional 21 plant species will go extinct by 2030, 47 species by 2050 and 110 species by 2100. However, these findings stand in sharp contrast to predictions from other studies that as much as half of Earth's estimated 390,000 plant species may disappear within the remainder of this century. ""This would translate into more than 49,000 extinctions in the regions we studied over the next 80 years, which seems unlikely, bar a cataclysmic event such as an asteroid strike!"" they argue. Prof Le Roux says regional datasets provide valuable data to make general inferences around plant extinctions and the drivers underlying these extinctions. There are, however, still many regions in the world without a Red List of Plants, or with outdated lists, such as Madagascar and Hawaii. These 'hottest' of hotspots were therefore not included in their analysis. ""A lack of up-to-date lists prevents us from gaining a more complete and precise picture of what we are losing, and at exactly what rate,"" Dr Hirsch adds. They believe the only way to better understand the magnitude of the extinction crisis faced by plants, and biodiversity in general, is to urgently initiate regional or at least country-level biodiversity assessments. ""While our study suggests that modern plant extinctions are relatively low, it is important to keep in mind that plants are exceptionally good at 'hanging in there'. Some of them are among the longest living organisms on earth today and many can persist in low densities, even under prolonged periods of unfavourable environmental conditions. A recent report, for example, indicated that 431 plant species, previously thought to be extinct, have been rediscovered,"" Le Roux explains. This means that many plant species may technically not be extinct, even though they only have one or a few living individuals remaining in the wild. Claiming extinction rates for plant species therefore remains a particularly challenging exercise. ""We need comprehensive and up-to-date datasets to make informative forecasts about the future and preservation of Earth's flora,"" they emphasise. Lost plant species in South Africa's biodiversity hotspots The first recorded species to be lost to forestry in South Africa in the 1700s was a type of fountainbush that used to grow next to streams in the Tulbagh region -- Psoralea cataracta. In 2008 it was listed as extinct on the Red List of South African Plants. The next species to be confirmed extinct was one of the African daisies, Osteospermum hirsutum, last seen in 1775, followed by the honeybush, Cyclopia laxiflora, last seen around 1800. The reasons for their extinction are listed as agriculture, forestry and urbanisation. More recently in 2012, an extremely rare species of vygie, Jordaaniella anemoniflora, was declared extinct in the wild after losing its battle against sprawling urbanisation and coastal developments around Strand, Macassar and Hermanus. The Succulent Karoo has seen three confirmed plant extinctions -- a vygie, Lampranthus vanzijliae (extinct in 1921, due to agriculture and urbanisation), the legume, Leobordea magnifica (extinct in 1947 due to agriculture and grazing) and the 'knopie' Conophytum semivestitum, lost to urbanisation and mining. For the Maputuland-Pondoland-Albany corridor, twenty species have been confirmed extinct, mainly due to agriculture and utilisation, and include Adenia natalensis (1865), Barleria natalensis (1890) and more recently, Pleiospilos simulans (2007). In conclusion The researchers emphasise that biodiversity loss, together with climate change, are the biggest threats faced by humanity: ""Along with habitat destruction, the effects of climate change are expected to be particularly severe on those plants not capable of dispersing their seeds over long distances,"" they conclude. "
Science Daily,An Unreported Zika Outbreak in 2017 Detected Through Travel Surveillance and Genetics,Science & Society,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822113356.htm,"   Three years ago, a well-publicized Zika epidemic in Brazil was tied to microcephaly and other congenital abnormalities. Zika is also associated with neurologic disorders, including Guillain-Barr√© syndrome. However, disparities in data make it unclear whether the virus -- transmitted by the mosquito Aedes aegypti -- is still a threat in the Americas. ""As the larger Zika epidemic in the Americas appeared to be waning towards the end of 2016, we became interested in understanding whether the epidemic was truly gone or whether 'hidden' transmissions could still be occurring,"" says study co-author Scott F. Michael, a biologist at Florida Gulf Coast University. Accurately pinpointing cases of Zika can be challenging: its symptoms mimic those of other diseases; and regions with inadequate health care systems often lack reliable, inexpensive diagnostic tools. Early and rapid pathogen detection is critical in preventing outbreaks from spinning into large-scale epidemics, says co-author Kristian Andersen, an infectious disease researcher at Scripps Research. Based on travel incidence rates reported by other countries, in 2017, Cuba would have experienced 1,000 to 20,000 Zika cases, Andersen says. However, only 187 cases were reported in 2016 and none in 2017. Between June 2017 and October 2018, more than 98 percent of the travel-associated cases reported in Florida and Europe came from Cuba. The timing of this outbreak was a mystery: conditions in Cuba could have supported a large Zika outbreak in 2016. Why did Cuba's cases jump in 2017? The main resource for reporting Zika cases is the Pan-American Health Organization (PAHO), which relies on local case reporting from member countries. To check the accuracy of PAHO results, the researchers sequenced Zika virus genomes from infected travelers in Florida and detected the unreported spike in cases in 2017. The researchers determined that the delay was likely caused by a successful mosquito eradication campaign that had taken place in Cuba in 2016. ""We show that the 2017 Zika outbreak was sparked by long-lived lineages of Zika virus introduced a year prior,"" Grubaugh says. ""Our data suggest that while mosquito control in Cuba may initially have been effective at mitigating Zika virus transmission, such measures may need to be maintained to be effective."" "
Science Daily,Rising Summer Heat Could Soon Endanger Travelers on Annual Muslim Pilgrimage,Science & Society,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822103834.htm,"   Hajj, or Muslim Pilgrimage, is one of the five pillars of the Muslim faith. It is an annual pilgrimage to Mecca, Saudi Arabia, that involves living in the hot weather conditions of Saudi Arabia. Muslims are expected to make the pilgrimage at least once in their lifetimes. Islam follows a lunar calendar, so the dates for Hajj change every year. But for five to seven years at a time, the trip falls over summer. A new study projecting future summer temperatures in the region around Mecca finds that as soon as 2020, summer days in Saudi Arabia could surpass the United States National Weather Service's extreme danger heat-stress threshold, at a wet-bulb temperature of 29.1 degrees C (84.3 degrees Fahrenheit). Wet-bulb temperature is a measurement combining temperature with the amount of moisture in the air. At the extreme danger threshold defined by the National Weather Service, sweat no longer evaporates efficiently, so the human body cannot cool itself and overheats. Exposure to these conditions for long periods of time, such as during Hajj, could cause heat stroke and possibly death. ""When the Hajj happens in summer, you can imagine with climate change and increasing heat-stress levels conditions could be unfavorable for outdoor activity,"" said Elfatih Eltahir, a civil and environmental engineer at Massachusetts Institute of Technology and co-author of the new study in the AGU journal Geophysical Research Letters. ""Hajj may be the largest religious tourism event,"" Eltahir said. ""We are trying to bring in the perspective of what climate change could do to such large-scale outdoor activity."" Adapting to rising temperatures Middle Eastern temperatures are rising because of climate change and scientists project them to keep rising in the coming decades. In the new study, Eltahir and his colleagues wanted to know how soon and how frequently temperatures during summer Hajj would pass the extreme danger threshold. The researchers examined historical climate models and used past data to create a projection for the future. In the past 30 years, they found that wet-bulb temperature surpassed the danger threshold 58 percent of the time, but never the extreme danger threshold. At the danger threshold, heat exhaustion is likely and heat stroke is a potential threat from extended exposure. Passing the extreme danger threshold for extended periods of time means heat stroke is highly likely. The researchers then calculated how climate change is likely to impact wet-bulb temperature in Saudi Arabia in the future. They found that in the coming decades, pilgrims will have to endure extremely dangerous heat and humidity levels in years when Hajj falls over summer. Their projections estimate heat and humidity levels during Hajj will exceed the extreme danger threshold six percent of the time by 2020, 20 percent of the time from 2045 and 2053, and 42 percent of the time between 2079 and 2086. Climate change mitigation initiatives make passing the threshold during these years less frequent, projecting one percent by 2020, 15 percent of the time between 2045 and 2053, and 19 percent of the time between 2079 and 2086, according to the study. The study authors stress that their projections are meant not to cause anxiety among pilgrims but instead to help them adapt, and to help authorities plan for safe Hajj. ""These results are not meant to spread any fears, but they are meant to inform policies about climate change, in relation to both mitigation and adaptation"" Eltahir said. ""There are ways people could adapt, including structural changes by providing larger facilities to help people perform Hajj as well as nonstructural changes by controlling the number of people who go."" ""They've provided a very compelling example of an iconic way that 2 to 3 million people per year that can be really vulnerable to what to me is the biggest underrated climate hazard -- this combination of high temp and high humidity,"" said Radley Horton, a climate scientist at Columbia University Lamont Doherty Earth Observatory who was not involved with the study. ""I believe as the century progresses if we don't reduce our greenhouse gases [this] could become every much as an existential threat as sea level rising and coastal flooding."" "
Science Daily,Australian Men's Life Expectancy Tops Other Men's,Science & Society,2019-08-22,-,https://www.sciencedaily.com/releases/2019/08/190822094020.htm,"   The study introduces a new way of measuring life expectancy, accounting for the historical mortality conditions that today's older generations lived through. By this measure, Australian men, on average, live to 74.1. The news is good for Australian women too; the study shows they're ranked second, behind their Swiss counterparts. Dr Collin Payne co-led the study, which used data from 15 countries across Europe, North America and Asia with high life expectancies. ""Popular belief has it that Japan and the Nordic countries are doing really well in terms of health, wellbeing, and longevity. But Australia is right there,"" Dr Payne said. ""The results have a lot to do with long term stability and the fact Australia's had a high standard of living for a really, really long time. Simple things like having enough to eat, and not seeing a lot of major conflict play a part."" Dr Payne's study grouped people by year of birth, separating 'early' deaths from 'late' deaths, to come up with the age at which someone can be considered an 'above-average' survivor. ""Most measures of life expectancy are just based on mortality rates at a given time,"" Dr Payne said. ""It's basically saying if you took a hypothetical group of people and put them through the mortality rates that a country experienced in 2018, for example, they would live to an average age of 80. ""But that doesn't tell you anything about the life courses of people, as they've lived through to old age. ""Our measure takes the life course into account, including mortality rates from 50, 60, or 70 years ago. ""What matters is we're comparing a group of people who were born in the same year, and so have experienced similar conditions throughout their life."" Dr Payne says this method allows us to clearly see whether someone is reaching their cohort's life expectancy. ""For example, any Australian man who's above age 74 we know with 100 per cent certainty has outlived half of his cohort -- he's an above average survivor compared to his peers born in the same year,"" he said. ""And those figures are higher here than anywhere else that we've measured life expectancy. ""On the other hand, any man who's died before age 74 is not living up to their cohort's life expectancy."" Dr Payne says there are a number of factors which might've contributed to Australia jumping ahead in these new rankings. ""Mortality was really high in Japan in the 30s, 40s and 50s. In Australia, mortality was really low during that time,"" Dr Payne said. ""French males, for example, drop out because a lot of them died during WW2, some from direct conflict, others from childhood conditions."" Dr Payne is now hoping to get enough data to look at how rankings have changed over the last 30 or 40 years. The research has been published in the journal Population Studies. "
Science Daily,China's Two-Child Policy Has Led to 5.4 Million Extra Births,Science & Society,2019-08-21,-,https://www.sciencedaily.com/releases/2019/08/190821185332.htm,"   It is the first study to use national data to estimate the impacts of the policy change and shows that births increased in response to the policy, but not as much as some policymakers had hoped. China's two-child policy, announced in October 2015, was enacted to reverse the nation's stagnant population growth, ageing population, and shrinking workforce. The policy targeted some 90 million women of reproductive age who already had a child, and now would be allowed to have a second child. There has been much speculation about the impact of the policy, with projections ranging from slightly over 1 million to more than 20 million annually. But, so far, studies have been limited. So a team of researchers based in China and the US set out to measure changes in births and health-related birth characteristics associated with the policy change. Using two national databases, they compared the number of births in two phases: ""baseline period"" (up to and including June 2016, 9 months after the announcement) and ""effective period"" (July 2016 to December 2017). Their findings are based on 67.8 million births in 28 out of 31 provinces of mainland China, an average of 1.41 million births per month. The researchers estimate an additional 5.4 million births as a result of the new policy during the first 18 months that it was in effect. And for the first time, the number of births to those who had given birth previously (multiparous mothers) exceeded births to first-time (primiparous) mothers. The policy was also associated with a 59% average increase in births to older mothers (35 years or older), but there was no accompanying increase in premature births. Finally, there was a slight decrease in caesarean deliveries to primiparous mothers, which might signal a favourable trend towards vaginal birth in first-time mothers in China, explain the researchers. However, they point out that many of the changes associated with the policy, including the increase in births, appeared to diminish at the end of the study period, raising questions about whether the policy's effects will be sustained. This is an observational study, so can't establish cause, and the researchers outline some limitations relating to the validity and representativeness of the data. But they say their findings clearly show that births increased in response to the policy, albeit not as much as some policymakers had hoped. Although they found no significant increased premature births, they say ""more work is needed to document and ensure the health of an increasingly older maternal population of second-time mothers in a nation where caesarean delivery rates are high."" Further research is also needed ""to develop a more nuanced understanding of the sustained impact of this historic change on the world's largest nation,"" they conclude. "
Science Daily,A Lack of Background Knowledge Can Hinder Reading Comprehension,Education & Learning,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826092303.htm,"   ""Background knowledge plays a key role in students' reading comprehension -- our findings show that if students don't have sufficient related knowledge, they'll probably have difficulties understanding text,"" says lead researcher Tenaha O'Reilly of Educational Testing Service (ETS)'s Center for Research on Human Capital in Education. ""We also found that it's possible to measure students' knowledge quickly by using natural language processing techniques. If a student scores below the knowledge threshold, they'll probably have trouble comprehending the text."" Previous research has shown that students who lack sufficient reading skills, including decoding and vocabulary, fare poorly relative to their peers. But the research of O'Reilly and ETS colleagues Zuowei Wang and John Sabatini suggests that a knowledge threshold may also be an essential component of reading comprehension. The researchers examined data from 3,534 high-school students at 37 schools in the United States. The students completed a test that measured their background knowledge on ecosystems. For the topical vocabulary section of the test, the students saw a list of 44 words and had to decide which were related to the topic of ecosystems. They also completed a multiple-choice section that was designed to measure their factual knowledge. Then, after reading a series of texts on the topic of ecosystems, the students completed 34 items designed to measure how well they understood the texts. These comprehension items tapped into their ability to summarize what they had read, recognize opinions and incorrect information, and apply what they had read to reason more broadly about the content. The researchers used a statistical technique called broken-line regression -- often used to identify an inflection point in a data set -- to analyze the students' performance. The results revealed that a background-knowledge score of about 33.5, or about 59% correct, functioned as a performance threshold. Below this score, background knowledge and comprehension were not noticeably correlated; above the threshold score, students' comprehension appeared to increase as their background knowledge increased. Additional results indicated that the pattern could not be fully explained by the level of students' knowledge on a different topic -- what mattered was their background knowledge of ecosystems. The researchers found that students' ability to identify specific keywords was a fairly strong predictor whether they would perform above or below the threshold. That is, correctly identifying ecosystems, habitat, and species as topically relevant was more strongly linked with students' comprehension than was identifying bioremediation, densities, and fauna. The findings underscore the importance of having reached a basic knowledge level to be able to read and comprehend texts across different subjects: ""Reading isn't just relevant to English Language Arts classes but also to reading in the content areas,"" says O'Reilly. ""The Common Core State Standards highlight the increasing role of content area and disciplinary reading. We believe that the role of background knowledge in students' comprehension and learning might be more pronounced when reading texts in the subject areas."" The researchers plan to explore whether a similar kind of knowledge threshold emerges in other topic areas and domains; they note that it will be important to extend the research by focusing on diverse measures and populations. If the pattern holds, the findings could have important applications for classroom teaching, given the availability of knowledge assessments that can be administered without taking valuable time away from instruction. ""If we can identify whether a given student does not have sufficient knowledge to comprehend a text, then teachers can provide background material -- for example, knowledge maps -- so that students have a context for the texts they are about to read,"" O'Reilly concludes.. "
