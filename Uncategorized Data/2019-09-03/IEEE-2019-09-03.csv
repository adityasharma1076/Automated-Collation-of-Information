Source,Heading,Category,Date,Time,URL,Text
IEEE,Construction Robots Learn to Excavate by Mimicking Humans,Robotics,2019-09-03,-,https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans,"      Pavel Savkin remembers the first time he watched a robot imitate his movements. Minutes earlier, the engineer had finished “showing” the robotic excavator its new goal by directing its movements manually. Now, running on software Savkin helped design, the robot was reproducing his movements, gesture for gesture. “It was like there was something alive in there—but I knew it was me,” he said.     As Quinn manipulated blocks in a virtual 3D space, the software learned a set of ordered instructions to be carried out in the real world. That order is essential for remote operations, says Quinn. To build remotely, developers need a way to communicate instructions to robotic builders on location. In the age of digital construction and industrial robotics, giving a computer a blueprint for what to build is a well-explored art. But operating on a distant object––especially under conditions that humans haven’t experienced themselves––presents challenges that only real-time communication with operators can solve. The problem is that, in an unpredictable setting, even simple tasks require not only instruction from an operator, but constant feedback from the changing environment. Five years ago, the Swedish fiber network provider umea.net (part of the private Umeå Energy utility) took advantage of the virtual reality boom to promote its high-speed connections with the help of a viral video titled “Living with Lag: An Oculus Rift Experiment.” The video is still circulated in VR and gaming circles.  In the experiment, volunteers donned headgear that replaced their real-time biological senses of sight and sound with camera and audio feeds of their surroundings—both set at a 3-second delay. Thus equipped, volunteers attempt to complete everyday tasks like playing ping-pong, dancing, cooking, and walking on a beach, with decidedly slapstick results. At outer-orbit intervals, including SE4’s dream of construction projects on Mars, the limiting factor in communication speed is not an artificial delay, but the laws of physics. The shifting relative positions of Earth and Mars mean that communications between the planets—even at the speed of light—can take anywhere from 3 to 22 minutes. A long-distance relationship Imagine trying to manage a construction project from across an ocean without the benefit of intelligent workers: sending a ship to an unknown world with a construction crew and blueprints for a log cabin, and four months later receiving a letter back asking how to cut down a tree. The parallel problem in long-distance construction with robots, according to SE4 CEO Lochlainn Wilson, is that automation relies on predictability. “Every robot in an industrial setting today is expecting a controlled environment.”  Platforms for applying AR and VR systems to teach tasks to artificial intelligences, as SE4 does, are already proliferating in manufacturing, healthcare, and defense. But all of the related communications systems are bound by physics and, specifically, the speed of light.  The same fundamental limitation applies in space. “Our communications are light-based, whether they’re radio or optical,” says Laura Seward Forczyk, a planetary scientist and consultant for space startups. “If you’re going to Mars and you want to communicate with your robot or spacecraft there, you need to have it act semi- or mostly-independently so that it can operate without commands from Earth.” Semantic control  That’s exactly what SE4 aims to do. By teaching robots to group micro-movements into logical units—like all the steps to building a tower of blocks—the Tokyo-based startup lets robots make simple relational judgments that would allow them to receive a full set of instruction modules at once and carry them out in order. This sidesteps the latency issue in real-time bilateral communications that could hamstring a project or at least make progress excruciatingly slow.  The key to the platform, says Wilson, is the team’s proprietary operating software, “Semantic Control.” Just as in linguistics and philosophy, “semantics” refers to meaning itself, and meaning is the key to a robot’s ability to make even the smallest decisions on its own. “A robot can scan its environment and give [raw data] to us, but it can’t necessarily identify the objects around it and what they mean,” says Wilson. That’s where human intelligence comes in. As part of the demonstration phase, the human operator of an SE4-controlled machine “annotates” each object in the robot’s vicinity with meaning. By labeling objects in the VR space with useful information—like which objects are building material and which are rocks—the operator helps the robot make sense of its real 3D environment before the building begins.  Giving robots the tools to deal with a changing environment is an important step toward allowing the AI to be truly independent, but it’s only an initial step. “We’re not letting it do absolutely everything,” said Quinn. “Our robot is good at moving an object from point A to point B, but it doesn’t know the overall plan.” Wilson adds that delegating environmental awareness and raw mechanical power to separate agents is the optimal relationship for a mixed human-robot construction team; it “lets humans do what they’re good at, while robots do what they do best.”    Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Video Friday: This Robotic Thread Could One Day Travel Inside Your Brain,Robotics,2019-08-31,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain,"      Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (send us your events!): Let us know if you have suggestions for next week, and enjoy today’s videos. Eight engineering students from ETH Zurich are working on a year-long focus project to develop a multimodal robot called Dipper, which can fly, swim, dive underwater, and manage that difficult air-water transition:    The robot uses one motor to selectively drive either a propeller or a marine screw depending on whether it’s in flight or not. We’re told that getting the robot to autonomously do the water to air transition is still a work in progress, but that within a few weeks things should be much smoother. [ Dipper ] Thanks Simon!   Giving a jellyfish a hug without stressing them out is exactly as hard as you think, but Harvard’s robot will make sure that all jellyfish get the emotional (and physical) support that they need.    The gripper’s six “fingers” are composed of thin, flat strips of silicone with a hollow channel inside bonded to a layer of flexible but stiffer polymer nanofibers. The fingers are attached to a rectangular, 3D-printed plastic “palm” and, when their channels are filled with water, curl in the direction of the nanofiber-coated side. Each finger exerts an extremely low amount of pressure — about 0.0455 kPA, or less than one-tenth of the pressure of a human’s eyelid on their eye. By contrast, current state-of-the-art soft marine grippers, which are used to capture delicate but more robust animals than jellyfish, exert about 1 kPA. The gripper was successfully able to trap each jellyfish against the palm of the device, and the jellyfish were unable to break free from the fingers’ grasp until the gripper was depressurized. The jellyfish showed no signs of stress or other adverse effects after being released, and the fingers were able to open and close roughly 100 times before showing signs of wear and tear. [ Harvard ]   MIT engineers have developed a magnetically steerable, thread-like robot that can actively glide through narrow, winding pathways, such as the labyrinthine vasculature of the brain. In the future, this robotic thread may be paired with existing endovascular technologies, enabling doctors to remotely guide the robot through a patient’s brain vessels to quickly treat blockages and lesions, such as those that occur in aneurysms and stroke.    [ MIT ]   See NASA’s next Mars rover quite literally coming together inside a clean room at the Jet Propulsion Laboratory. This behind-the-scenes look at what goes into building and preparing a rover for Mars, including extensive tests in simulated space environments, was captured from March to July 2019. The rover is expected to launch to the Red Planet in summer 2020 and touch down in February 2021.    The Mars 2020 rover doesn’t have a name yet, but you can give it one! As long as you’re not too old! Which you probably are!  [ Mars 2020 ]   I desperately wish that we could watch this next video at normal speed, not just slowed down, but it’s quite impressive anyway.    Here’s one more video from the Namiki Lab showing some high speed tracking with a pair of very enthusiastic robotic cameras:  [ Namiki Lab ]   Normally, tedious modeling of mechanics, electronics, and information science is required to understand how insects’ or robots’ moving parts coordinate smoothly to take them places. But in a new study, biomechanics researchers at the Georgia Institute of Technology boiled down the sprints of cockroaches to handy principles and equations they then used to make a test robot amble about better.    [ Georgia Tech ]   More magical obstacle-dodging footage from Skydio’s still secret new drone.    We’ve been hard at work extending the capabilities of our upcoming drone, giving you ways to get the control you want without the stress of crashing. The result is you can fly in ways, and get shots, that would simply be impossible any other way. How about flying through obstacles at full speed, backwards? [ Skydio ]   This is a cute demo with Misty:    [ Misty Robotics ]   We’ve seen pieces of hardware like this before, but always made out of hard materials—a soft version is certainly something new.    Utilizing vacuum power and soft material actuators, we have developed a soft reconfigurable surface (SRS) with multi-modal control and performance capabilities. The SRS is comprised of a square grid array of linear vacuum-powered soft pneumatic actuators (linear V-SPAs), built into plug-and-play modules which enable the arrangement, consolidation, and control of many DoF. [ RRL ]   The EksoVest is not really a robot, but it’ll make you a cyborg! With super strength!    ""This is NOT intended to give you super strength but instead give you super endurance and reduce fatigue so that you have more energy and less soreness at the end of your shift."" Drat! [ EksoVest ]   We have created a solution for parents, grandparents, and their children who are living separated. This is an amazing tool to stay connected from a distance through the intimacy that comes through interactive play with a child. For parents who travel for work, deployed military, and families spread across the country, the Cushybot One is much more than a toy; it is the opportunity for maintaining a deep connection with your young child from a distance.    Hmm. I think the concept here is great, but it’s going to be a serious challenge to successfully commercialize. [ Indiegogo ]   What happens when you equip RVR with a parachute and send it off a cliff? Watch this episode of RVR Launchpad to find out – then go Behind the Build to see how we (eventually) accomplished this high-flying feat.    [ Sphero ]   These omnidirectional crawler robots aren’t new, but that doesn’t keep them from being fun to watch.    [ NEDO ] via [ Impress ]     We’ll finish up the week with a couple of past ICRA and IROS keynote talks—one by Gill Pratt on The Reliability Challenges of Autonomous Driving, and the other from Peter Hart, on Making Shakey.     [ IEEE RAS ]    Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,ETH Zurich Demonstrates PuppetMaster Robot,Robotics,2019-08-30,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot,"      As far as I know, the universe does not have a desperate need for robot puppeteers, and considering the difficulty of making even a halfway decent robot puppeteer, you’d think that any sensible roboticist would keep well clear of the problem. But some folks over at ETH Zurich decided that they’d have a crack at it anyway, and they started by describing why they’d likely be better off if they hadn’t: Marionettes are underactuated, high-dimensional, highly non-linear coupled pendulum systems. They are driven by gravity, the tension forces generated by a small number of cables, and the internal forces arising from mechanical articulation constraints. As such, the map between the actions of a puppeteer and the motions performed by the marionette is notoriously unintuitive, and mastering this unique art form takes unfaltering dedication and a great deal of practice. Our goal is to enable autonomous robots to animate marionettes with a level of skill that approaches that of human puppeteers.   I’m not much of a puppeteer myself, but this looks not bad at all, considering that the ABB YuMi robot is missing quite a few degrees of freedom in its hands. For context, here’s someone who has mastered this unique artform through unfaltering dedication and a great deal of practice, master puppeteer Scott Land:   The ETH Zurich project can’t yet animate a complex marionette, but it’s a respectable showing with the dragon, I think. As input, all the robot needs to know is the design of the puppet at the target motion you want the puppet to make. While moving the puppet in real life, the robot is continuously simulating its motions over the next second while iteratively optimizing to try to get the puppet to move the way it’s supposed to. The usefulness of this research, thankfully, is not constrained to puppets: Our long term goal is to enable robots to manipulate various types of complex physical systems – clothing, soft parcels in warehouses or stores, flexible sheets and cables in hospitals or on construction sites, plush toys or bedding in our homes, etc – as skillfully as humans do. We believe the technical framework we have set up for robotic puppeteering will also prove useful in beginning to address this very important grand-challenge. [ Paper ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,"SpaceX, OneWeb, or Kepler Communications: Who Really Launched the First Ku-band Satellite?",Aerospace,2019-08-30,-,https://spectrum.ieee.org/tech-talk/aerospace/satellites/spacex-oneweb-or-kepler-communications-who-really-launched-the-first-kuband-satellite,"      No one denies that the Soviet Union put the first man-made object into orbit nor, a few wackos aside, that American astronauts were first to reach the moon. But deciding which of three companies’ new broadband internet satellites was first to launch is proving somewhat more contentious. Elon Musk’s SpaceX, SoftBank’s OneWeb and Canadian start-up Kepler Communications are all claiming that they launched the first satellites capable of delivering high-speed internet using Ku-band (12-18 GHz) frequencies. At stake is far more than just bragging rights or national pride. According to US law, the first operator to launch gets first choice of back-up spectrum should there be any interference between the rival systems—a near certainty given the more than 10,000 satellites they intend to deploy. The Federal Communications Commission (FCC) now finds itself in the bizarre position of having to rule on something that might seem utterly obvious but that will affect the future of all three companies. It all seemed much simpler around the turn of the millennium, when two companies, Teledesic and Skybridge, announced plans for a few hundred low earth orbit (LEO) internet satellites. The FCC quickly settled on a plan for them to share the available spectrum. All the satellites could use the entire frequency range in the US, only switching to non-interfering frequencies during in-line events—when a ground station happened to line up with satellites from both systems. This would happen just six percent of the time, the FCC concluded. In those rare cases, the first operator to have launched would get to choose which half of the spectrum it would prefer to use. “But as there is no difference in quantity or kind between the halves of the spectrum,” the FCC wrote, “This first choice, a kind of coordination priority, has little significance.” In the end, it didn’t matter at all, as Teledesic and Skybridge both ran out of money before launching a single commercial satellite. But fast forward to 2019, and the significance of the FCC’s decision is looming much larger. “The rules were made for two systems and for just a few hundred satellites,” says Tim Farrar, president of satellite and telecommunications consulting firm TMF Associates. “If you’ve got thousands going around, you’ll have conjunctions happening almost all the time, and then you’re effectively splitting the band in two almost everywhere.” In a worst case scenario, operators would be operating with half the spectrum, and effectively half the capacity, their systems were designed for. Suddenly, the quality of an operator’s “home spectrum”—the dedicated frequencies it can retreat to—looks far more important. “Some portions of the Ku-band have more terrestrial incumbent users than other portions,” wrote OneWeb in a letter to the FCC. “Home spectrum matters... because it allows an operator to maximize network capacity and, in turn, service to customers by choosing the portion of the frequency band in which it prefers to operate.” Another issue is that only the first two systems can choose home spectrum bands anchored to either end of the available spectrum. “If there’s going to be more than two operators, you really, really want to be one of the first two,” says Farrar. “If you’re the third, you could be all hopping all over the place.” The home spectrum rules only come into play if the operators cannot agree beforehand how to coordinate with one another’s systems. But while everyone agrees that would be the best solution for all concerned, the challenges of such complex negotiations between multiple operators are considerable. “With operators all coming online at different times, we anticipate conversations will always be ongoing,” OneWeb told IEEE Spectrum. In order to hedge their bets, SpaceX, OneWeb and Kepler would all very much like to be first in line to choose their home spectrum. After launching its initial six satellites in February, OneWeb sent a letter to the FCC asserting victory. It wrote: “OneWeb hereby notifies the Commission that the first space station in the OneWeb System has met the requirement to be launched and capable of operating.... [Therefore] OneWeb hereby claims first priority in home spectrum selection in the Ku-band.” In May, Kepler put in its own claim, noting that its KIPP spacecraft reached orbit more than a year earlier, in January 2018, and had since been carrying out commercial operations. “As far as Kepler is aware, this launch represented the first deployment of a Ku-band satellite within the Processing Round, and as such it should have first priority in any selection of home spectrum within Ku-band,” the company wrote to the FCC. “Needless to say, it is disappointing to see OneWeb try and undercut Kepler’s position.” A few weeks after that, SpaceX made its own case. Despite launching its first experimental satellite in February 2018 (after Kepler), and its first 60 commercial satellites in May 2019 (after OneWeb), SpaceX believes that under FCC regulations, it still officially launched first. “The scope of this rule makes clear that to be considered ‘capable of operating,’ an operator must not only launch satellites but must also communicate with a U.S.-licensed earth station in the specific frequency band,” it wrote to the Commission. Although OneWeb applied for an FCC license for its earth stations before SpaceX, it has not yet been granted. SpaceX, however, applied for and was granted a special temporary license to communicate with its first batch of satellites shortly before their launch. OneWeb and Kepler dispute this interpretation, calling it “flawed” and “extraordinary,” with both sides insisting that their readings embody common sense. SpaceX notes that a foreign operator that launched first but had no intention of offering service in the US could hold domestic systems hostage. Kepler, on the other hand, points out that “to preserve fairness... home spectrum selection order cannot be based...on arbitrary barriers such as approval delays.” SpaceX has another Starlink mission scheduled for September, and OneWeb hopes to launch 30 satellites at a time on future rockets. With Tim Farrar estimating that the FCC could take 6 to 12 months to rule on home spectrum priority, the skies could be full of satellites by the time we find out, officially, who actually launched first.  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,AI at the Speed of Light,Semiconductors,2019-08-29,-,https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light,"      Neural networks shine for solving tough problems such as facial and voice recognition, but conventional electronic versions are limited in speed and hungry for power. In theory, optics could beat digital electronic computers in the matrix calculations used in neural networks. However, optics had been limited by their inability to do some complex calculations that had required electronics. Now new experiments show that all-optical neural networks can tackle those problems.  The key attraction of neural networks is their massive interconnections among processors, comparable to the complex interconnections among neurons in the brain. This lets them perform many operations simultaneously, like the human brain does when looking at faces or listening to speech, making them more efficient for facial and voice recognition than traditional electronic computers that execute one instruction at a time. Today  electronic neural networks have reached eight million neurons, but their future use in artificial intelligence may be limited by their high power usage and limited parallelism in connections. Optical connections through lenses are inherently parallel. The lens in your eye simultaneously focuses light from across your field of view onto the retina in the back of your eye, where an array of light-detecting nerve cells detects the light. Each cell then relays the signal it receives to neurons in the brain that process the visual signals to show us an image. Glass lenses process optical signals by focusing light, which performs a complex mathematical operation called a Fourier transform that preserves the information in the original scene but rearranges is completely. One use of Fourier transforms is converting time variations in signal intensity into a plot of the frequencies present in the signal. The military used this trick in the 1950s to convert raw radar return signals recorded by an aircraft in flight into a three-dimensional image of the landscape viewed by the plane. Today that conversion is done electronically, but the vacuum-tube computers of the 1950s were not up to the task. Development of neural networks for artificial intelligence started with electronics, but their AI applications have been limited by their slow processing and need for extensive computing resources. Some researchers have developed hybrid neural networks, in which optics perform simple linear operations, but electronics perform more complex nonlinear calculations. Now two groups have demonstrated simple all-optical neural networks that do all processing with light. In May, Wolfram Pernice of the Institute of Physics at the University of Münster in Germany and colleagues reported testing an all-optical ""neuron"" in which signals change target materials between liquid and solid states, an effect that has been used for optical data storage. They demonstrated nonlinear processing, and produced output pulses like those from organic neurons. They then produced an integrated photonic circuit that incorporated four optical neurons operating at different wavelengths, each of which connected to 15 optical synapses. The photonic circuit contained more than 140 components and could recognize simple optical patterns. The group wrote that their device is scalable, and that the technology promises ""access to the high speed and high bandwidth inherent to optical systems, thus enabling the direct processing of optical telecommunication and visual data.”  Now a group at the Hong Kong University of Science and Technology reports in Optica that they have made an all-optical neural network based on a different process, electromagnetically induced transparency, in which incident light affects how atoms shift between quantum-mechanical energy levels. The process is nonlinear and can be triggered by very weak light signals, says Shengwang Du, a physics professor and coauthor of the paper. In their demonstration, they illuminated rubidium-85 atoms cooled by lasers to about 10 microKelvin (10 microdegrees above absolute zero). Although the technique may seem unusually complex, Du said the system was the most accessible one in the lab that could produce the desired effects. ""As a pure quantum atomic system [it] is ideal for this proof-of-principle experiment,"" he says. Next, they plan to scale up the demonstration using a hot atomic vapor center, which is less expensive, does not require time-consuming preparation of cold atoms, and can be integrated with photonic chips. Du says the major challenges are reducing cost of the nonlinear processing medium and increasing the scale of the all-optical neural network for more complex tasks. ""Their demonstration seems valid,"" says Volker Sorger, an electrical engineer at George Washington University in Washington who was not involved in either demonstration. He says the all-optical approach is attractive because it offers very high parallelism, but the update rate is limited to about 100 hertz because of the liquid crystals used in their test, and he is not completely convinced their approach can be scaled error-free.    Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Algorithms Aid Search for Source of Spacetime Rumbles,Aerospace,2019-08-28,-,https://spectrum.ieee.org/tech-talk/aerospace/astrophysics/new-algorithms-lead-search-for-origins-of-spacetime-rumbles,"      One night last week around 7 p.m., Michael Lundquist was at his home in Tucson, Arizona when his cellphone rang. He knew from the ringtone that it was a robocall. So he took it immediately. He lives for moments like this. “A LIGO/VIRGO alert has been received,” the automated voice told him (generated by a Python script that he’d written). The LIGO gravitational wave detector in Louisiana and Washington state had just seconds earlier picked up a ripple in spacetime. Lundquist opened his email app to see the details of LIGO’s gravitational wave alert. The gravitational wavefront from a collision of two neutron stars some 115 million light years away had, LIGO reported, just passed through the earth. And it looked like this potential “kilonova” event could be visible in Tucson that night. (The alert arrived, as it happened, perfectly timed to kick off the evening’s telescope observations, under partly cloudy but clearing skies.) Lundquist is one of a handful of pioneers of what’s called “multi-messenger” astronomy, using in this case gravitational wave detections to trigger astronomical observations via an optical telescope. His group’s SAGUARO system (standing for Searches After Gravitational-waves Using ARizona Observatories) has automated access to a 1.5-meter survey telescope in the Catalina mountains outside Tucson. Within an hour of receiving notification of the gravitational wave candidate event “S190822c,” Lundquist and others in the SAGUARO loop began plotting their night’s observing run of the region of the sky where this gravitational wave source might be found. In this case, the LIGO data analysis programs had concluded from the shape of the gravitational wave front that two in-spiraling and ultimately colliding neutron stars had produced the spacetime blip. LIGO is also well tuned to detect neutron star-black hole collisions and—the predominant, vanilla flavor in LIGO-land—black hole-black hole collisions. The fact that LIGO is able to point in any direction to the potential source of the gravitational waves is remarkable in itself. A single gravitational wave interferometer provides precious little information about where the candidate wave originated. However, with two LIGO interferometers in different regions of the United States and one European gravitational wave interferometer (VIRGO) outside Pisa, Italy, careful timing of the wavefront’s arrival at multiple detectors allows astronomers to triangulate the direction of the signal. As it turns out, S190822c was ultimately retracted. Detecting changes in laser beam path length down to fractions of the width of a proton—which is how much spacetime wobbles and warps when a gravitational wave passes through—is often a game of false positives. S190822c was one such false positive. Yet, as Lundquist and co-authors describe in their recent paper on SAGUARO, published in Astrophysical Journal Letters, they have had three candidate gravitational wave events on which to test their increasingly automated system. The closest they’ve yet come to discovering the optical counterpart to a gravitational wave source came on 25 April of this year. In that case, the source was in a galaxy some 500 million light years distant. Lundquist says that, knowing the region of the sky where the gravitational wave had just been found, SAGUARO’s algorithms get telescope time as soon as possible to take pictures of between 12 and 24 tiled segments of the sky. The team already has on file previous images from that same telescope of 5,069 patches of sky, covering most of the sky that’s visible from Tucson, Arizona’s latitude. So the algorithm automatically aligns and then subtracts that archived image from the new image. Which creates a negative picture that only registers any changes in stars or galaxies (or asteroids or comets or planets) in the frame. Through this process, the 25 April event, Lundquist says, generated 2,711 candidate sources. Which would be impossible to sort out by hand in any reasonable timeframe, when the optical flare-up in the distant galaxy might only be visible for a day or two after the gravitational wave signal arrives. So Lundquist has also set up a machine learning pipeline that automatically considers each potential candidate source for more trivial explanations—a known variable star, for instance, or an over-saturated image that caused noisy pixel data. This way, the team can whittle the list of possible gravitational wave sources down to one or two dozen objects that could then be reasonably followed up on by astronomers. The thing they’re ultimately looking for looks like a supernova in a far away galaxy, only brighter. Called a kilonova, this recently discovered astronomical event (thanks in large part to the observations of another multi-messenger gravitational wave detection from 2017) is now believed to be the source of most of the universe’s heavy elements. That gold in your watch or ring or necklace is in fact very likely the byproduct of two colliding neutron stars somewhere nearby, sometime around or before 4.6 billion years ago, when our solar system formed. “We have it in a nice spot where we’re waiting for a really good event,” Lundquist says. “But in the meantime we’re just trying to improve the machine learning as much as possible—to make it just that much easier to find the kilonova.”  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,A Carbon Nanotube Microprocessor Mature Enough to Say Hello,Semiconductors,2019-08-28,-,https://spectrum.ieee.org/nanoclast/semiconductors/processors/modern-microprocessor-built-using-carbon-nanotubes,"      Engineers at MIT and Analog Devices have created the first fully-programmable 16-bit carbon nanotube microprocessor. It’s the most complex integration of carbon nanotube-based CMOS logic so far, with nearly 15,000 transistors, and it was done using technologies that have already been proven to work in a commercial chip-manufacturing facility. The processor, called RV16X-NANO, is a milestone in the development of beyond-silicon technologies, its inventors say.  Unlike silicon transistors, nanotube devices can easily be made in multiple layers with dense 3D interconnections. The Defense Advanced Research Projects Agency is hoping this 3D aspect will lead to commercial carbon nanotube (CNT) chips with the performance of today’s cutting-edge silicon but without the high design and manufacturing cost. Some of the same researchers created a modest one-bit, 178-transistor processor back in 2013. In contrast, the new one, which is based on the open source RISC-V instruction set, is capable of working with 16-bit data and 32-bit instructions. Naturally, the team, led by MIT assistant professor Max Shulaker, tested the chip by running a version of the obligatory “Hello, World!” program. They reported the achievement this week in Nature. “Ten years ago, we hoped this was possible,” says Shulaker. “Now we know it is possible… and we know it can be done in commercial facilities.” Shulaker’s team, along with engineers at Analog Devices and, later, Skywater Technology Foundry, developed three commercially-viable techniques to create the RV16X-NANO. Two dealt with stubborn issues of carbon nanotube purity and uniformity, and the third allowed for the creation of both n-type and p-type transistors to form complementary logic circuits. 1. When making CNT transistors, the nanotubes are first put into a solution and spread across a silicon wafer. Most of the nanotubes lie uniformly on the silicon, but every once in a while, they ball up into bundles of a thousand or more. These bundles can’t form transistors. When building small-scale test circuits, this was no big deal, Shulaker explains, because even if they killed one circuit, another would work. But for a large-scale integration like for the RV16X-NANO these nanotube-pile-ups would be common enough to mess up the whole processor. RINSE, a solution one of Shulaker’s students, Christian Lau, arrived at, relies on the fact that individual nanotubes are stuck to the substrate by Van Der Waals forces more strongly than bundles are. By first coating the nanotube-covered substrate with a photo resist and then carefully washing it away—under just the right conditions—the process selectively removes the bundles but leaves the individual CNTs. 2. While RINSE dealt with one carbon-nanotube impurity, another purity problem nearly crashed the whole project. CNTs have always come in two basic flavors, metallic and semiconducting. Having some metallic nanotubes in a CNT-based logic gate means the circuit will waste power and produce a noisy signal. But how many metallic nanotubes is too many when you’re trying to build a full-scale processor? “It’s a very basic question,” says Shulaker. And to his surprise, it hadn’t been answered. The answer his team came up with was “pretty depressing.” The best today’s commercial processes could produce is 99.99 percent semiconducting nanotubes and 0.01 percent metallic. But what’s needed is 99.999999 percent purity—impossibly far out of reach. “We thought, if we can’t process our way out of this… then somehow we had to design our way around it,” says Shulaker. The team found that, by far, the main driver for the needed purity was not the power issue but the noise. Amongst the many logic circuits they’d made, they found a pattern that suggested some combinations were much more susceptible to the noise problem than others. “So the solution at that point was simple: We’ll just design circuits with the good combinations of logic gates and avoid using the bad combinations.” DREAM, the set of design rules post-doctoral researcher Gage Hill came up with, allows large-scale integration using carbon nanotubes you can purchase off-the-shelf. 3. The third big breakthrough, called MIXED, allowed for the creation of the two types of transistors needed for CMOS logic, the kind in use in all-kinds of processors for decades. For that you need both electron-conducting (NMOS) and hole conducting (PMOS) transistors. Previous attempts at nanotube processors, such as the one-bit system Shulaker built as a graduate student, used only PMOS. In silicon, the distinction is achieved by doping the transistor’s channel region with different atoms to effectively add electrons to the silicon crystal lattice or steal some. But such “substitutional doping” doesn’t work for carbon nanotubes. “It’s difficult to swap out an atom without destroying the properties of the nanotube,” says Shulaker. So instead they turned to “electrostatic doping.” Here, a dielectric oxide is engineered to add or subtract electrons from the nanotube. Using a common semiconductor manufacturing technology called atomic layer deposition, the team was able to deposit dielectrics, such as halfnium dioxide, one atomic layer at a time. By manipulating the exact composition of the layer, say to have slightly fewer oxygens or a bit more, the oxide “wants to either donate electrons to the nanotube or steal from the nanotube,” explains Shulaker. Between careful selection of the metal electrodes involved and the ALD process, the researchers were able to reliably build PMOS and NMOS devices together. Crucially, MIXED is a low-temperature process, so the transistors can be built on top of other layers of circuitry without damaging them. In fact, the transistors in RV16X-NANO were built in between a layer of interconnects that provide power to the transistors and another layer that connects the transistors into logic gates and larger systems. Engineers are interested in such “buried power line” schemes in order to free up space that would allow for better-performing or smaller systems. But they are more difficult to achieve in silicon, in part because of high processing temperatures.  Monthly newsletter about how new materials, designs, and processes drive the chip industry.  IEEE Spectrum’s nanotechnology blog, featuring news and analysis about the development, applications, and future of science and technology at the nanoscale. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Eben Upton on the Raspberry Pi’s Industrial Crossover and Why There Will Never Be a Pi 9,Semiconductors,2019-08-28,-,https://spectrum.ieee.org/semiconductors/processors/eben-upton-on-the-raspberry-pis-industrial-crossover-and-why-there-will-never-be-a-pi-9,     
IEEE,New Double 3 Robot Makes Telepresence Easier than Ever,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever,"      Today, Double Robotics is announcing Double 3, the latest major upgrade to its line of consumer(ish) telepresence robots. We had a (mostly) fantastic time testing out Double 2 back in 2016. One of the things that we found out back then was that it takes a lot of practice to remotely drive the robot around. Double 3 solves this problem by leveraging the substantial advances in 3D sensing and computing that have taken place over the past few years, giving their new robot a level of intelligence that promises to make telepresence more accessible for everyone. Double 2’s iPad has been replaced by “a fully integrated solution”—which is a fancy way of saying a dedicated 9.7-inch touchscreen and a whole bunch of other stuff. That other stuff includes an NVIDIA Jetson TX2 AI computing module, a beamforming six-microphone array, an 8-watt speaker, a pair of 13-megapixel cameras (wide angle and zoom) on a tilting mount, five ultrasonic rangefinders, and most excitingly, a pair of Intel RealSense D430 depth sensors.  It’s those new depth sensors that really make Double 3 special. The D430 modules each uses a pair of stereo cameras with a pattern projector to generate 1280 x 720 depth data with a range of between 0.2 and 10 meters away. The Double 3 robot uses all of this high quality depth data to locate obstacles, but at this point, it still doesn’t drive completely autonomously. Instead, it presents the remote operator with a slick, augmented reality view of drivable areas in the form of a grid of dots. You just click where you want the robot to go, and it will skillfully take itself there while avoiding obstacles (including dynamic obstacles) and related mishaps along the way.  This effectively offloads the most stressful part of telepresence—not running into stuff—from the remote user to the robot itself, which is the way it should be. That makes it that much easier to encourage people to utilize telepresence for the first time. The way the system is implemented through augmented reality is particularly impressive, I think. It looks like it’s intuitive enough for an inexperienced user without being restrictive, and is a clever way of mitigating even significant amounts of lag.  Otherwise, Double 3’s mobility system is exactly the same as the one featured on Double 2. In fact, that you can stick a Double 3 head on a Double 2 body and it instantly becomes a Double 3. Double Robotics is thoughtfully offering this to current Double 2 owners as a significantly more affordable upgrade option than buying a whole new robot. For more details on all of Double 3  new features, we spoke with the co-founders of Double Robotics, Marc DeVidts and David Cann. IEEE Spectrum: Why use this augmented reality system instead of just letting the user click on a regular camera image? Why make things more visually complicated, especially for new users? Marc DeVidts and David Cann: One of the things that we realized about nine months ago when we got this whole thing working was that without the mixed reality for driving, it was really too magical of an experience for the customer. Even us—we had a hard time understanding whether the robot could really see obstacles and understand where the floor is and that kind of thing. So, we said “What would be the best way of communicating this information to the user?” And the right way to do it ended up drawing the graphics directly onto the scene. It’s really awesome—we have a full, real time 3D scene with the depth information drawn on top of it. We’re starting with some relatively simple graphics, and we’ll be adding more graphics in the future to help the user understand what the robot is seeing. How robust is the vision system when it comes to obstacle detection and avoidance? Does it work with featureless surfaces, IR absorbent surfaces, in low light, in direct sunlight, etc? We’ve looked at all of those cases, and one of the reasons that we’re going with the RealSense is the projector that helps us to see blank walls. We also found that having two sensors—one facing the floor and one facing forward—gives us a great coverage area. Having ultrasonic sensors in there as well helps us to detect anything that we can't see with the cameras. They're sort of a last safety measure, especially useful for detecting glass.  It seems like there’s a lot more that you could do with this sensing and mapping capability. What else are you working on? We're starting with this semi-autonomous driving variant, and we're doing a private beta of full mapping. So, we’re going to do full SLAM of your environment that will be mapped by multiple robots at the same time while you're driving, and then you'll be able to zoom out to a map and click anywhere and it will drive there. That  where we're going with it, but we want to take baby steps to get there. It  the obvious next step, I think, and there are a lot more possibilities there. Do you expect developers to be excited for this new mapping capability? We're using a very powerful computer in the robot, a NVIDIA Jetson TX2 running Ubuntu. There  room to grow. It’s actually really exciting to be able to see, in real time, the 3D pose of the robot along with all of the depth data that gets transformed in real time into one view that gives you a full map. Having all of that data and just putting those pieces together and getting everything to work has been a huge feat in of itself.  We have an extensive API for developers to do custom implementations, either for telepresence or other kinds of robotics research. Our system isn't running ROS, but we're going to be adding ROS adapters for all of our hardware components. Telepresence robots depend heavily on wireless connectivity, which is usually not something that telepresence robotics companies like Double have direct control over. Have you found that connectivity has been getting significantly better since you first introduced Double? When we started in 2013, we had a lot of customers that didn’t have WiFi in their hallways, just in the conference rooms. We very rarely hear about customers having WiFi connectivity issues these days. The bigger issue we see is when people are calling into the robot from home, where they don't have proper traffic management on their home network. The robot doesn't need a ton of bandwidth, but it does need consistent, low latency bandwidth. And so, if someone else in the house is watching Netflix or something like that, it’s going to saturate your connection. But for the most part, it’s gotten a lot better over the last few years, and it’s no longer a big problem for us. Do you think 5G will make a significant difference to telepresence robots? We’ll see. We like the low latency possibilities and the better bandwidth, but it  all going to be a matter of what kind of reception you get. LTE can be great, if you have good reception; it’s all about where the tower is. I’m pretty sure that WiFi is going to be the primary thing for at least the next few years. DeVidts also mentioned that an unfortunate side effect of the new depth sensors is that hanging a t-shirt on your Double to give it some personality will likely render it partially blind, so that  just something to keep in mind. To make up for this, you can switch around the colorful trim surrounding the screen, which is nowhere near as fun. When the Double 3 is ready for shipping in late September, US $2,000 will get you the new head with all the sensors and stuff, which seamlessly integrates with your Double 2 base. Buying Double 3 straight up (with the included charging dock) will run you $4,ooo. This is by no means an inexpensive robot, and my impression is that it’s not really designed for individual consumers. But for commercial, corporate, healthcare, or education applications, $4k for a robot as capable as the Double 3 is really quite a good deal—especially considering the kinds of use cases for which it’s ideal. [ Double Robotics ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Blue Ocean Robotics Acquires Beam Telepresence Robot From Suitable Technologies,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot,"      Today, Blue Ocean Robotics, a Danish robotics company, is announcing the acquisition of Suitable Technologies’ Beam telepresence robot business. Blue Ocean has been a Beam partner for five years, but now they’re taking things over completely. The Beam robot began its life as an internal project within Willow Garage. It was spun out in 2012 as Suitable Technologies, which produced a couple different versions of the Beam. As telepresence platforms go, Beam is on the powerful and expensive side, designed primarily for commercial and enterprise customers.  The most recent news from Suitable was the introduction of the BeamPro 2, which was announced over a year ago at CES 2018. The Suitable Tech website still lists it as “coming soon,” and our guess is that it’s now up to Blue Ocean to decide whether to go forward with this new version. Blue Ocean calls itself a “robot venture factory.” I’m not entirely sure what a “robot venture factory” is but Blue Ocean describes itself thusly: The company is known for developing professional service robots from the problem, idea and design phase to the development, commercialization and scaling phase. Every robot is placed in its own subsidiary which is responsible for scaling sales, customer service, support and everything else oriented towards global markets and customers. The parent company handles all development and production of robots across the organization.  Ah, that explains it! Blue Ocean does already have a couple portfolio companies making very specific robots, including a UV disinfection robot for hospitals and a sort of mobile patient lift also for hospitals. They’re working on some kind of agriculture robot, too. I’d love to be able to tell you more, but the press release doesn’t offer much: With the acquisition, Blue Ocean Robotics sees an opportunity to generate additional synergy: “Our development of robots is based on our own in-house created toolbox with reusable technology components. This means that we can build all of our robots fast, inexpensively, and better than others,” says Blue Ocean Robotics’ CTO John Erland Østergaard. “Some of our robots, for example the UVD disinfection robot, are already equipped with remote controls. With the Beam technology being a big seller in the healthcare sector, we can continue to grow our business within this industry by having our distributors present both UVD and Beam when they visit customers.” The press release is very specific that Blue Ocean isn’t acquiring Suitable Technologies itself—they’re acquiring the “assets and rights associated with the robot Beam” from Suitable, which I guess means that Suitable is still around somehow. But it’s really not clear what Suitable is without Beam, which (as far as we can make out) is the entirety of what the company does. Anyway, we’re glad that there’s enough interest in high-end telepresence robots to support this acquisition, and we hope that Blue Ocean will be investing in BeamPro 2 and further generations of the robot. It’s come a long way from the original Texai robot from Willow Garage, and still has a lot of potential. For more information, visit the new Beam website that Blue Ocean has just launched. [ Beam ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,All of the Winners in the DARPA Subterranean Challenge Tunnel Circuit,Robotics,2019-08-27,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit,"      The first competitive event in the DARPA Subterranean Challenge concluded last week—hopefully you were able to follow along on the livestream, on Twitter, or with some of the articles that we’ve posted about the event. We’ll have plenty more to say about how things went for the SubT teams, but while they take a bit of a (well earned) rest, we can take a look at the winning teams as well as who won DARPA’s special superlative awards for the competition.  With their rugged, reliable robots featuring giant wheels and the ability to drop communications nodes, Team Explorer was in the lead from day 1, scoring in double digits on every single run.  Team CoSTAR had one of the more diverse lineups of robots, and they switched up which robots they decided to send into the mine as they learned more about the course.   While many teams came to SubT with DARPA funding, Team CTU-CRAS was self-funded, making them eligible for a special $200,000 Tunnel Circuit prize.   DARPA also awarded a bunch of “superlative awards” after SubT: To score a point, teams had to submit the location of an artifact that was correct to within 5 meters of the artifact itself. However, DARPA was tracking the artifact locations with much higher precision—for example, the “zero” point on the backpack artifact was the center of the label on the front, which DARPA tracked to the millimeter. Team Explorer managed to return the location of a backpack with an error of just 0.18 meter, which is kind of amazing. With just an hour to find as many artifacts as possible, teams had to find the right balance between sending robots off to explore and bringing them back into communication range to download artifact locations. Team CSIRO Data61 cut their last point pretty close, sliding their final point in with a mere 22 seconds to spare.  Team Robotika had some of the quirkiest and most recognizable robots, which DARPA recognized with the “Most Distinctive” award. Robotika told us that part of the reason for that distinctiveness was practical—having a robot that was effectively in two parts meant that they could disassemble it so that it would fit in the baggage compartment of an airplane, very important for a team based in the Czech Republic. Kevin Knoedler, who won NASA’s Space Robotics Challenge entirely by himself, brought his own personal swarm of drones to SubT. With a ratio of seven robots to one human, Kevin was almost certainly the hardest working single human at the challenge. The Fan Favorite award went to the team that was most popular on Twitter (with the #SubTChallenge hashtag), and it may or may not be the case that I personally tweeted enough about Team NCTU’s blimp to win them this award. It’s also true that whenever we asked anyone on other teams what their favorite robot was (besides their own, of course), the blimp was overwhelmingly popular. So either way, the award is well deserved.  DARPA shared this little behind-the-scenes clip of the blimp in action (sort of), showing what happened to the poor thing when the mine ventilation system was turned on between runs and DARPA staff had to chase it down and rescue it: The thing to keep in mind about the results of the Tunnel Circuit is that unlike past DARPA robotics challenges (like the DRC), they don’t necessarily indicate how things are going to go for the Urban or Cave circuits because of how different things are going to be. Explorer did a great job with a team of rugged wheeled vehicles, which turned out to be ideal for navigating through mines, but they’re likely going to need to change things up substantially for the rest of the challenges, where the terrain will be much more complex. DARPA hasn’t provided any details on the location of the Urban Circuit yet; all we know is that it’ll be sometime in February 2020. This gives teams just six months to take all the lessons that they learned from the Tunnel Circuit and update their hardware, software, and strategies. What were those lessons, and what do teams plan to do differently next year? Check back next week, and we’ll tell you. [ DARPA SubT ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
