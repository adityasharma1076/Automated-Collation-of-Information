Source,Heading,Category,Date,Time,URL,Text
IEEE,OpenAI Teaches Robot Hand to Solve Rubik's Cube,Robotics,2019-10-15,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving,"      In-hand manipulation is a skill that, as far as I’m aware, humans in general don’t actively learn. We just sort of figure it out by doing other, more specific tasks with our fingers and hands. This makes it particularly tricky to teach robots to solve in-hand manipulation tasks because the way we do it is through experimentation and trial and error. Robots can learn through trial and error as well, but since it usually ends up being mostly error, it takes a very, very long time. Last June, we wrote about OpenAI’s approach to teaching a five-fingered robot hand to manipulate a cube. The method that OpenAI used leveraged the same kind of experimentation and trial and error, but in simulation rather than on robot hardware. For complex tasks that take a lot of finesse, simulation generally translates poorly into real-world skills, but OpenAI made their system super robust by introducing a whole bunch of randomness into the simulation during the training process. That way, even if the simulation didn’t perfectly match reality (which it didn’t), the system could still handle the kinds of variations that it experienced on the real-world hardware. In a preprint paper published online today, OpenAI has managed to teach its robot hand to solve a much more difficult version of in-hand cube manipulation: single-handed solving of a 3x3 Rubik’s cube. The new work is also based on the idea of solving a problem using advanced simulations and then transferring the solution to a real-world system, or what researchers call “sim2real.” In the paper, OpenAI says the new approach “vastly improved sim2real transfer.”  The initial step was to break down the robot manipulation of the Rubik’s cube into two different tasks: 1. rotating a single face of the cube 90 degrees in either direction, and 2. flipping the cube to bring a different face to the top. Since rotating the top face is much simpler for the robot than rotating other faces, the most reliable strategy is to just do a 90-degree flip to get the face you want to rotate on top. The actual process of solving the cube is computationally straightforward, although the solving process is optimized for the motions that the robot can perform rather than the solve that would take the least number of steps. The physical setup that’s doing the real-world cube solving is a Shadow Dexterous E Series Hand with a PhaseSpace motion capture system, plus RGB cameras for visual pose estimation. The cube that’s being manipulated is also pretty fancy: It’s stuffed with sensors that report the orientation of each face with an accuracy of five degrees, which is necessary because it’s otherwise very difficult to know the state of a Rubik’s cube when some of its faces are occluded.  While the video makes it easy to focus on the physical robot, the magic is mostly happening in simulation, and transferring things learned in simulation to the real world. Again, the key to this is domain randomization—jittering parts of the simulation around so that your system has to adapt to different situations similar to those that might be encountered in the real-world. For example, maybe you slightly alter the weight of the cube, or change the friction of the fingertips a little bit, or turn down the lighting. If your system can handle these simulated variations, it’ll be more robust to real-world operation. The physical setup includes the Shadow Dexterous Hand, a PhaseSpace motion capture system, and RGB cameras. OpenAI modified the Shadow Dexterous Hand by moving the PhaseSpace LEDs and cables inside the fingers and by adding rubber to the fingertips. When we spoke to last year to Jonas Schneider (one of the authors of the cube manipulation work) and asked him where he thought that system was the weakest, he said that the biggest problem at that point was that the randomizations were both task-specific and hand designed. It’s probably not surprising, then, that one of the big contributions of the Rubik’s cube work is “a novel method for automatically generating a distribution over randomized environments for training reinforcement learning policies and vision state estimators,” which the researchers call automatic domain randomization (ADR). Here’s why ADR is important, according to the paper: Our main hypothesis that motivates ADR is that training on a maximally diverse distribution over environments leads to transfer via emergent meta-learning. More concretely, if the model has some form of memory, it can learn to adjust its behavior during deployment to improve performance on the current environment over time, i.e. by implementing a learning algorithm internally. We hypothesize that this happens if the training distribution is so large that the model cannot memorize a special-purpose solution per environment due to its finite capacity. ADR is a first step in this direction of unbounded environmental complexity: it automates and gradually expands the randomization ranges that parameterize a distribution over environments.   Special-purpose solutions per environment are bad, because they work for that environment, but not for other environments. You can think of each little tweak to a simulation as creating a new environment, and the idea behind ADR is to automate these tweaks to create so many new environments that the system is forced to instead come up with general solutions that can work for many different environments all at once. This reflects the robustness required for real-world operation, where no two environments are ever exactly alike. It turns out that ADR is both better and more efficient than the previous manual tuning, say the researchers: ADR clearly leads to improved transfer with much less need for hand-engineered randomizations. We significantly outperformed our previous best results, which were the result of multiple months of iterative manual tuning. In terms of results, the researchers were mostly concerned with how many flips and rotations the system could do in a row without failing, rather than how many complete solves it was capable of. It sounds like a complete solve was a bit of an outlier—the starting configuration of the cube could be solved by the system in 43 successful moves, while the average successful run of the best trained policy (continuously trained over multiple months) was about 27 moves. Sixty percent of the time, the system could get halfway to a complete solve, and it made it the entire way 20 percent of the time. The researchers point out that the method they’ve developed here is general purpose, and you can train a real-world robot to do pretty much any task that you can adequately simulate. You don’t need any real-world training at all, as long as your simulations are diverse enough, which is where the automatic domain randomization comes in. The long-term goal is to reduce the task specialization that’s inherent to most robots, which will help them be more useful and adaptable in real-world applications. Lastly, just for reference, here’s what (I think) is the current 3x3 cube world record is, set just a few days ago by Max Park:  Wow. It’s interesting that it appears to be faster and/or more efficient for a human to use table contact to augment their own dexterity. We’ve seen other robots make use of environmental contact for manipulation; it would be cool if OpenAI threw a surface into the simulation to see if their system could make use of it. [ OpenAI ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Labrador Systems Developing Affordable Assistive Robots for the Home,Robotics,2019-10-15,-,https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home,"      Developing robots for the home is still a challenge, especially if you want those robots to interact with people and help them do practical, useful things. However, the potential markets for home robots are huge, and one of the most compelling markets is for home robots that can assist humans who need them. Today, Labrador Systems, a startup based in California, is announcing a pre-seed funding round of $2 million (led by SOSV’s hardware accelerator HAX with participation from Amazon’s Alexa Fund and iRobot Ventures, among others) with the goal of expanding development and conducting pilot studies of  “a new [assistive robot] platform for supporting home health.” Labrador was founded two years ago by Mike Dooley and Nikolai Romanov. Both Mike and Nikolai have backgrounds in consumer robotics at Evolution Robotics and iRobot, but as an ’80s gamer, Mike’s bio (or at least the parts of his bio on LinkedIn) caught my attention: From 1995 to 1997, Mike worked at Brøderbund Software, helping to manage play testing for games like Myst and Riven and the Where in the World is Carmen San Diego series. He then spent three years at Lego as the product manager for MindStorms. After doing some marginally less interesting things, Mike was the VP of product development at Evolution Robotics from 2006 to 2012, where he led the team that developed the Mint floor sweeping robot. Evolution was acquired by iRobot in 2012, and Mike ended up as the VP of product development over there until 2017, when he co-founded Labrador. I was pretty much sold at Where in the World is Carmen San Diego (the original version of which I played from a 5.25” floppy on my dad’s Apple IIe)*, but as you can see from all that other stuff, Mike knows what he’s doing in robotics as well. And according to Labrador’s press release, what they’re doing is this: Labrador Systems is an early stage technology company developing a new generation of assistive robots to help people live more independently. The company’s core focus is creating affordable solutions that address practical and physical needs at a fraction of the cost of commercial robots. … Labrador’s technology platform offers an affordable solution to improve the quality of care while promoting independence and successful aging.  Labrador’s personal robot, the company’s first offering, will enter pilot studies in 2020. That’s about as light on detail as a press release gets, but there’s a bit more on Labrador’s website, including: The only hardware we’ve actually seen from Labrador at this point is a demo that they put together for Amazon’s re:MARS conference, which took place a few months ago, showing a “demonstration project” called Smart Walker:  This isn’t the home assistance robot that Labrador got its funding for, but rather a demonstration of some of their technology. So of course, the question is, what’s Labrador working on, then? It’s still a secret, but Mike Dooley was able to give us a few more details. IEEE Spectrum: Your website shows a smart walker concept—how is that related to the assistive robot that you’re working on? Mike Dooley: The smart walker was a request from a major senior living organization to have our robot (which is really good at navigation) guide residents from place to place within their communities. To test the idea with residents, it turned out to be much quicker to take the navigation system from the robot and put it on an existing rollator walker. So when you see the clips of the technology in the smart walker video on our website, that’s actually the robot’s navigation system localizing in real time and path planning in an environment. “Assistive robot” can cover a huge range of designs and capabilities—can you give us any more detail about your robot, and what it’ll be able to do? One of the core features of our robot is to help people move things where they have difficulty moving themselves, particularly in the home setting. That may sound trivial, but to someone who has impaired mobility, it can be a major daily challenge and negatively impact their life and health in a number of ways. Some examples we repeatedly hear are people not staying hydrated or taking their medication on time simply because there is a distance between where they are and the items they need. Once we have those base capabilities, i.e. the ability to navigate around a home and move things within it, then the robot becomes a platform for a wider variety of applications. What made you decide to develop assistive robots, and why are robots a good solution for seniors who want to live independently? Supporting independent living has been seen as a massive opportunity in robotics for some time, but also as something off in the future. The turning point for me was watching my mother enter that stage in her life and seeing her transition to using a cane, then a walker, and eventually to a wheelchair. That made the problems very real for me. It also made things much clearer about how we could start addressing specific needs with the tools that are becoming available now. In terms of why robots can be a good solution, the basic answer is the level of need is so overwhelming that even helping with “basic” tasks can make an appreciable difference in the quality of someone’s daily life. It’s also very much about giving individuals a degree of control back over their environment. That applies to seniors as well as others whose world starts getting more complex to manage as their abilities become more impaired. What are the particular challenges of developing assistive robots, and how are you addressing them? Why do you think there aren’t more robotics startups in this space? The setting (operating in homes and personal spaces) and the core purpose of the product (aiding a wide variety of individuals) bring a lot of complexity to any capability you want to build into an assistive robot. Our approach is to put as much structure as we can into the system to make it functional, affordable, understandable and reliable. I think one of the reasons you don’t see more startups in the space is that a lot of roboticists want to skip ahead and do the fancy stuff, such as taking on human-level capabilities around things like manipulation. Those are very interesting research topics, but we think those are also very far away from being practical solutions you can productize for people to use in their homes. How do you think assistive robots and human caregivers should work together? The ideal scenario is allowing caregivers to focus more of their time on the high-touch, personal side of care. The robot can offload the more basic support tasks as well as extend the impact of the caregiver for the long hours of the day they can’t be with someone at their home. We see that applying to both paid care providers as well as the 40 million unpaid family members and friends that provide assistance. The robot is really there as a tool, both for individuals in need and the people that help them. What’s promising in the research discussions we’ve had so far, is that even when a caregiver is present, giving control back to the individual for simple things can mean a lot in the relationship between them and the caregiver. What should we look forward to from Labrador in 2020? Our big goal in 2020 is to start placing the next version of the robot with individuals with different types of needs to let them experience it naturally in their own homes and provide feedback on what they like, what don’t like and how we can make it better. We are currently reaching out to companies in the healthcare and home health fields to participate in those studies and test specific applications related to their services. We plan to share more detail about those studies and the robot itself as we get further into 2020. If you’re an organization (or individual) who wants to possibly try out Labrador’s prototype, the company encourages you to connect with them through their website. And as we learn more about what Labrador is up to, we’ll have updates for you, presumably in 2020. [ Labrador Systems ] * I just lost an hour of my life after finding out that you can play Where in the World is Carmen San Diego in your browser for free.  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Agility Robotics Unveils Upgraded Digit Walking Robot,Robotics,2019-10-14,-,https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot,"      Last time we saw Agility Robotics’ Digit biped, it was picking up a box from a Ford delivery van and autonomously dropping it off on a porch, while at the same time managing to not trip over stairs, grass, or small children. As a demo, it was pretty impressive, but of course there’s an enormous gap between making a video of a robot doing a successful autonomous delivery and letting that robot out into the semi-structured world and expecting it to reliably do a good job. Agility Robotics is aware of this, of course, and over the last six months they’ve been making substantial improvements to Digit to make it more capable and robust. A new video posted today shows what’s new with the latest version of Digit—Digit v2.  We appreciate Agility Robotics foregoing music in the video, which lets us hear exactly what Digit sounds like in operation. The most noticeable changes are in Digit’s feet, torso, and arms, and I was particularly impressed to see Digit reposition the box on the table before grasping it to make sure that it could get a good grip. Otherwise, it’s hard to tell what’s new, so we asked Agility Robotics’ CEO Damion Shelton to get us up to speed. IEEE Spectrum: Can you summarize the differences between Digit v1 and v2? We’re particularly interested in the new feet. Damion Shelton: The feet now include a roll degree of freedom, so that Digit can resist lateral forces without needing to side step. This allows Digit v2 to balance on one foot statically, which Digit v1 and Cassie could not do. The larger foot also dramatically decreases load per unit area, for improved performance on very soft surfaces like sand. The perception stack includes four Intel RealSense cameras used for obstacle detection and pick/place, plus the lidar. In Digit v1, the perception systems were brought up incrementally over time for development purposes. In Digit v2, all perception systems are active from the beginning and tied to a dedicated computer. The perception system is used for a number of additional things beyond manipulation, which we’ll start to show in the next few weeks. The torso changes are a bit more behind-the-scenes. All of the electronics in it are now fully custom, thermally managed, and environmentally sealed. We’ve also included power and ethernet to a payload bay that can fit either a NUC or Jetson module (or other customer payload). What exactly are we seeing in the video in terms of Digit’s autonomous capabilities? At the moment this is a demonstration of shared autonomy. Picking and placing the box is fully autonomous. Balance and footstep placement are fully autonomous, but guidance and obstacle avoidance are under local teleop. It’s no longer a radio controller as in early videos; we’re not ready to reveal our current controller design but it’s a reasonably significant upgrade. This is v2 hardware, so there’s one more full version in development prior to the 2020 launch, which will expand the autonomy envelope significantly. What are some unique features or capabilities of Digit v2 that might not be obvious from the video? For those who’ve used Cassie robots, the power-up and power-down ergonomics are a lot more user friendly. Digit can be disassembled into carry-on luggage sized pieces (give or take) in under 5 minutes for easy transport. The battery charges in-situ using a normal laptop-style charger. I’m curious about this “stompy” sort of gait that we see in Digit and many other bipedal robots—are there significant challenges or drawbacks to implementing a more human-like (and presumably quieter) heel-toe gait? There are no drawbacks other than increased complexity in controls and foot design. With Digit v2, the larger surface area helps with the noise, and v2 has similar or better passive-dynamic performance as compared to Cassie or Digit v1. The foot design is brand new, and new behaviors like heel-toe are an active area of development. How close is Digit v2 to a system that you’d be comfortable operating commercially? We’re on track for a 2020 launch for Digit v3. Changes from v2 to v3 are mostly bug-fix in nature, with a few regulatory upgrades like full battery certification. Safety is a major concern for us, and we have launch customers that will be operating Digit in a safe environment, with a phased approach to relaxing operational constraints. Digit operates almost exclusively under force control (as with cobots more generally), but at the moment we’ll err on the side of caution during operation until we have the stats to back up safety and reliability. The legged robot industry has too much potential for us to screw it up by behaving irresponsibly. It will be a while before Digit (or any other humanoid robot) is operating fully autonomously in crowds of people, but there are so many large market opportunities (think indoor factory/warehouse environments) to address prior to that point that we expect to mature the operational safety side of things well in advance of having saturated the more robot-tolerant markets. [ Agility Robotics ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Wanted: A Bomb Detector as Sensitive as a Dog's Nose,Semiconductors,2019-10-12,-,https://spectrum.ieee.org/tech-talk/semiconductors/devices/using-a-twopronged-approach-to-detect-explosive-substances-from-bombs,"      If a suicide bomber lurks in the public with an explosive device, bomb-sniffing dogs can often detect the explosive chemicals from the tiniest whiff—these canine superheroes can sense the presence of the explosive triacetone triperoxide (TATP) if just a few molecules are present, on the scale of parts per trillion. Researchers at the University of Rhode Island are striving to make a comparable device for detecting TATP in its vapor form. Their new detection system, which pairs a conductance sensor with a traditional thermodynamic sensor, confirms the presence of TATP at the level of parts per billion (ppb). Their work is described in a study published on 2 October in IEEE Sensors Letters. TATP is a common choice of ingredient for terrorists who make explosives. It was used in the 2015 Paris attacks, the 2016 Brussels airport bombings, the 2017 concert bombing in Manchester, and most recently the 2018 bombings in Surabaya, Indonesia. “All of the IEDs used in these attacks relied on TATP as the detonator and often times the energetic material itself,” explains Otto Gregory, a researcher involved in the study. But, he notes, “No electronic trace detection system currently exists that is capable of continuously monitoring TATP or its precursors that can compete with a dog’s nose.” His team, which has received funding for TATP sensors from the U.S. Department of Homeland Security for the last 10 years, hopes to change that. The researchers first developed a thermodynamic sensor that detects TATP at 78 ppb, and can detect 2,4-DNT (a decomposition product of the explosive substance TNT) at 2 ppb. The thermodynamic sensor uses two microheaters: one coated with a metal oxide catalyst, and one without a catalyst coating. When explosive substances like TATP or 2,4-DNT come in contact with the microheaters, the device analyzes the thermodynamic difference between the catalyst and non-catalyst reactions—which reveals the nature of the substance. To make the device more accurate at confirming the presence of these explosives, the researchers added a conductance sensor. The conductance sensor analyzes the same catalyst reactions as the thermodynamic sensor, but instead measures the resistivity changes that occur. “Combining a thermodynamic platform with a conductometric platform provides a built-in redundancy that can mitigate false positives and negatives. Both platforms operate as independent systems looking for a unique ‘fingerprint’ or response to same explosive molecule,” explains Peter Ricci, a co-author and chemical engineer at the University of Rhode Island. The researchers tested mircoheaters made from different catalyst metals, including tin oxide, zinc oxide, and copper oxide. In terms of measuring conductance, the copper oxide was particularly sensitive to detecting 2,4-DNT. Copper oxide was more than three times as sensitive as the other materials in measuring the heat effect and 13 times as sensitive for conductance. Gregory notes that implementing this device in a real-world setting depends on how well they can downsize the system and make it portable. “Our current detection system employs both a thermodynamic and conductometric platform that fits into a small toolbox,” he says. “The final version of our device would have a much smaller footprint, in the form of a handheld or even a wearable [device].” The catalyst coatings are currently layered on ultrathin alumina ceramic substrates, which are responsible for the systems extraordinary sensitivity. But these substrates are still thicker than desired. “Sensors fabricated on much thinner substrates would drastically reduce operating temperature and would also lower the power requirements of the system, thus enabling a smaller portable detection system. We are working towards that goal everyday,” says Ricci.  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Video Friday: This Humanoid Robot Will Serve You Ice Cream,Robotics,2019-10-11,-,https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream,"      Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (send us your events!): Let us know if you have suggestions for next week, and enjoy today’s videos. What’s better than a robotics paper with “dynamic” in the title? A robotics paper with “highly dynamic” in the title. From Sangbae Kim’s lab at MIT, the latest exploits of Mini Cheetah:    Yes I’d very much like one please. Full paper at the link below. [ Paper ] via [ MIT ] A humanoid robot serving you ice cream—on his own ice cream bike: What a delicious vision!    [ Roboy ]  The Roomba “i” series and “s” series vacuums have just gotten an update that lets you set “keep out” zones, which is super useful. Tell your robot where not to go!    I feel bad, that Roomba was probably just hungry :( [ iRobot ]   We wrote about Voliro’s tilt-rotor hexcopter a couple years ago, and now it’s off doing practical things, like spray painting a building pretty much the same color that it was before.    [ Voliro ] Thanks Mina!   Here’s a clever approach for bin-picking problematic objects, like shiny things: Just grab a whole bunch, and then sort out what you need on a nice robot-friendly table.    It might take a little bit longer, but what do you care, you’re probably off sipping a cocktail with a little umbrella in it on a beach somewhere. [ Harada Lab ]   A unique combination of the IRB 1200 and YuMi industrial robots that use vision, AI and deep learning to recognize and categorize trash for recycling.    [ ABB ]   Measuring glacial movements in-situ is a challenging, but necessary task to model glaciers and predict their future evolution. However, installing GPS stations on ice can be dangerous and expensive when not impossible in the presence of large crevasses. In this project, the ASL develops UAVs for dropping and recovering lightweight GPS stations over inaccessible glaciers to record the ice flow motion. This video shows the results of first tests performed at Gorner glacier, Switzerland, in July 2019.    [ EPFL ]   Turns out Tertills actually do a pretty great job fighting weeds.    Plus, they leave all those cute lil’ Tertill tracks. [ Franklin Robotics ]   The online autonomous navigation and semantic mapping experiment presented [below] is conducted with the Cassie Blue bipedal robot at the University of Michigan. The sensors attached to the robot include an IMU, a 32-beam LiDAR and an RGB-D camera. The whole online process runs in real-time on a Jetson Xavier and a laptop with an i7 processor. The resulting map is so precise that it looks like we are doing real-time SLAM (simultaneous localization and mapping). In fact, the map is based on dead-reckoning via the InvEKF.    [ GTSAM ] via [ University of Michigan ]   UBTECH has announced an upgraded version of its Meebot, which is 30 percent bigger and comes with more sensors and programmable eyes.    [ UBTECH ]   ABB’s research team will be working with medical staff, scientist and engineers to develop non-surgical medical robotics systems, including logistics and next-generation automated laboratory technologies. The team will develop robotics solutions that will help eliminate bottlenecks in laboratory work and address the global shortage of skilled medical staff.    [ ABB ]     In this video, Ian and Chris go through Misty’s SDK, discussing the languages we’ve included, the tools that make it easy for you to get started quickly, a quick rundown of how to run the skills you build, plus what’s ahead on the Misty SDK roadmap.     [ Misty Robotics ]   My guess is that this was not one of iRobot’s testing environments for the Roomba.    You know, that’s actually super impressive. And maybe if they threw one of the self-emptying Roombas in there, it would be a viable solution to the entire problem. [ How Farms Work ]   Part of WeRobotics’ Flying Labs network, Panama Flying Labs is a local knowledge hub catalyzing social good and empowering local experts. Through training and workshops, demonstrations and missions, the Panama Flying Labs team leverages the power of drones, data, and AI to promote entrepreneurship, build local capacity, and confront the pressing social challenges faced by communities in Panama and across Central America.    [ Panama Flying Labs ]   Go on a virtual flythrough of the NIOSH Experimental Mine, one of two courses used in the recent DARPA Subterranean Challenge Tunnel Circuit Event held 15-22 August, 2019. The data used for this partial flythrough tour were collected using 3D LIDAR sensors similar to the sensors commonly used on autonomous mobile robots.    [ SubT ]   Special thanks to PBS, Mark Knobil, Joe Seamans and Stan Brandorff and many others who produced this program in 1991. It features Reid Simmons (and his 1 year old son), David Wettergreen, Red Whittaker, Mac Macdonald, Omead Amidi, and other Field Robotics Center alumni building the planetary walker prototype called Ambler. The team gets ready for an important demo for NASA.    [ CMU RI ]   As art and technology merge, roboticist Madeline Gannon explores the frontiers of human-robot interaction across the arts, sciences and society, and explores what this could mean for the future.    [ Sonar+D ]    Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Nuclear Weapons Inspection: Encryption System Could Thwart Spies and Expose Hoaxes,Aerospace,2019-10-11,-,https://spectrum.ieee.org/tech-talk/aerospace/military/encrypting-nuclear-weapons-inspection-could-a-system-that-defeats-both-spies-and-hoaxes-have-been-invented,"      A new nuclear weapons inspection technology could enhance inspectors’ ability to verify that a nuclear warhead has been dismantled without compromising state secrets behind the weapon’s design. This new non-proliferation tool, its inventors argue, would greatly assist the often delicate dance of nuclear weapons inspectors—who want to know they haven’t been hoaxed but are also sensitive to a military’s fear that spies may have infiltrated their ranks. While nuclear non-proliferation treaties have historically verified the dismantlement of weapons delivery systems like ICBMs and cruise missiles, there have in fact never been any verified dismantlements of nuclear warheads themselves (in part for the reasons described above). Yet there are 13,000 nuclear warheads in the world, meaning the entire globe is still just a hair trigger away from apocalypse—even as we approach the thirtieth anniversary of the Berlin Wall’s collapse. As UN Secretary-General Antonio Guterres told world leaders last month, “I worry that we are slipping back into bad habits that will once again hold the entire world hostage to the threat of nuclear annihilation.” How, then, to verifiably dismantle a nuclear bomb? The challenge, says Areg Danagoulian, assistant professor of nuclear science and engineering at MIT, is finding a way for both sides of a nuclear disarmament treaty to arrive at a point of confidence. Any party to such a treaty, he says, must verifiably destroy the warheads they say they will destroy. But their military should also be confident that none of their weapons design secrets have leaked out during the inspection and verification process. Previous research has put forward systems that have strong cryptography but could conceivably be hoaxed with some sleight-of-hand—or which are sensitive to hoaxes but cannot guarantee the security of information about the design and composition of warheads. Danagoulian says he and his coauthor Ezra Engel (a former student who is now in the U.S. Army) relied on two innovations to create their new system. The first is the neutron beam that the system emits to study an individual warhead. These neutrons, at energies in the range of 5 to 50 electron volts (eV), are “slower than fast neutrons,” Danagoulian says. They sit within a range of neutron energies that probe nuclear resonances for most of the heavy elements found in nuclear warheads—including, of course, the relevant isotopes of plutonium and uranium. This means the inspection technology can potentially spot the difference between weapons-grade and reactor-grade uranium in a sample, as well as detect other elements like tungsten and molybdenum. This capability could reduce the chance of being hoaxed: A deceitful party could claim they’re dismantling a tranche of warheads—but those supposed warheads could in fact be lookalikes that are loaded instead with lower purity uranium or other heavy elements. Or the warheads could “look” to a neutron beam like a real warhead but only when viewed in one particular direction. Which is why Danagoulian and Engel’s method, described in a recent issue of the journal Nature Communications, also exposes candidate warheads to their neutron beam from multiple viewing angles. The team’s experiment—performed on fake warheads composed of non-weaponized heavy elements via a neutron beam from a linear accelerator at Rensselaer Polytechnic Institute—were able to verify bonafide “warheads” and pick out the hoaxes in both scenarios described above. The other innovation behind Danagoulian’s technology involved adding what the team calls an “encrypting filter” to the neutron beam’s path. The neutron beam passes through the warhead, then through the “filter,” and proceeds on to the neutron detectors. The filter in this case is just a slab of various heavy elements whose composition is unknown to the inspectors. The country whose weapons are being inspected, Danagoulian says, could even create the filter themselves. So long as the inspectors cannot perform any separate experiments on the encrypting filter, the ultimate composition and geometry of the warhead will be obscured to the neutron beam. “Inspectors cannot learn anything useful about the warhead’s composition,” Danagoulian says. However, if inspectors are able to take a snapshot of a “gold-standard” warhead whose genuineness has independently been established, they now have an encrypted picture of what a legitimate warhead looks like to their system. Then they can place other warheads of the same design into the encrypted neutron beam and compare the snapshots they take of candidate warheads: If the candidate has the same nuclear resonance spectrum as their “gold-standard” warhead, then the candidate warhead is real. If the candidate warhead’s spectrum is different from the gold standard, they may have just detected a hoax. Between the encrypting filter and the sensitivity of their setup, a hoaxer or spy on either side of the exchange would have to work a lot harder to deceive inspectors. Danagoulian says that his group hopes to adapt its method to a commercial neutron source and a portable detection system. The total pricetag, he says, should be around US $100,000. “You wouldn’t need this at every ICBM site,” he says. “We think we can build instruments that are 5 meters [in size]. You could put it in a truck or a van. The inspectors could bring it themselves—set up at some particular base or some particular facility. They could do these measurements and leave.”  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Forget Moore’s Law—Chipmakers Are More Worried About Heat and Power Issues,Semiconductors,2019-10-11,-,https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/power-problems-might-drive-chip-specialization,"      Power consumption and heat generation: these hurdles impede progress toward faster, cheaper chips, and are worrying semiconductor industry veterans far more than the slowing of Moore’s Law. That was the takeaway from several discussions about current and future chip technologies held in Silicon Valley this week. John Hennessy—president emeritus of Stanford University, Google chairman, and MIPS Computer Systems founder—says Moore’s Law “was an ambition, a goal. It wasn’t a law; it was something to shoot for.”  “It is definitely slowing down,” he says, “but to say it’s dead is premature.” That slowing, at this point, isn’t his biggest concern. The real problem, Hennessy says, is the failure of Dennard scaling, an observation that as transistors get smaller and circuits become faster, a chip’s power consumption stays the same. “Who would have thought,” he says, “that microprocessors would have to slow down clock speeds or turn off cores to keep from burning up?” Hennessy spoke as part of a panel at a Churchill Club forum held Monday in Menlo Park. The power consumption of microprocessors was also a hot topic at Arm TechCon 2019 on Tuesday, with Sha Rabii, Facebook’s head of silicon and technology engineering, indicating that the energy used by microprocessors and the heat that chips give off is a major roadblock on the road to augmented reality glasses. How to tackle the problem? The key, several industry veterans suggested, may be specialization. “Either we keep going on [an] evolutionary path, with faster CPUs and everything happening in software, or we look at it as a systems problem and think about what we would do differently” if the industry were designing from the ground up, said Navin Chaddha, managing director of the Mayfield Fund, speaking at the Churchill Club event. “I believe the world is moving to specialization,” Chaddha says, instead of focusing on doubling raw processing power every 18 to 24 months. The recent rush of startups producing processors designed to do deep learning—such as Cerebras Systems, Mythic, and Syntiant—are examples of this kind of thinking. But there may be limits to just how much specialization can help.   Arm Holdings CEO Simon Segars would seem to agree with Chaddha. Segars kicked off Arm TechCon by announcing that the company will, for the first time, allow developers to insert custom instructions into the core of Arm chips, allowing more efficient processing of algorithms. This capability will permit at least some degree of specialization, which is a big departure for a company that has always focused on standardized products.   IEEE Spectrum’s blog featuring the people, places, and passions of the world of technologists in Silicon Valley and its environs. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,U.S. Semiconductor Industry Veterans Keep Wary Eyes on China,Semiconductors,2019-10-11,-,https://spectrum.ieee.org/view-from-the-valley/semiconductors/devices/semiconductor-industry-veterans-keep-wary-eyes-on-china,"      How might the U.S. chip industry solve a problem like China?  A panel of semiconductor industry veterans took up this question at a Churchill Club event this week. The group generally expressed worry about the impact China will have on the future of the U.S. chip industry, and the lack of good ideas about how the U.S. industry can respond to threats posed by China.  “China is the ultimate conundrum,” says Stanford president emeritus and MIPS Computer Systems founder John Hennessy. “It’s a large market that U.S. companies need access to, together with being what will become a major technical competitor. We have never faced that.”  The consolidation of silicon manufacturing into two main foundries raises the threat level, pointed out Diane Bryant, former Intel and Google Cloud executive.  “You really just have TSMC and Samsung left,” she said. “And TSMC is in Taiwan, so you have to be thinking about China and the threat to Taiwan, and what will happen to TSMC.”  China will take over Taiwan “the same time North Korea takes over South Korea,” quipped Hennessy, giving it control over most of the world’s semiconductor manufacturing capabilities.  “What do you do tomorrow if TSMC and Samsung are off limits?” he asked his fellow panel members.  “You can’t go to Global Foundries,” which indeed has some U.S. semiconductor manufacturing capability, said Bryant, “unless you really want Moore’s Law to be dead.” (Global Foundries recently stopped developing the most advanced semiconductor processes.)   Rodrigo Liang, CEO of SambaNova Systems, argued that fixing this problem can only be done at the level of the U.S. government.  Pradeep Sindhu, founder of Juniper Networks and founder and CEO of Fungible, agreed. “The U.S. government needs an industrial policy,” he said, “and it doesn’t have one.”  The foundry issue is a long-term problem. Perhaps a nearer term question is how the growing capability of China’s tech industry will impact U.S.-based companies.       “China is talking about becoming tech independent, becoming net exporters,” said Bryant. “We can talk about how many years [it will take], but it is inevitable.”  Companies in China will catch up for several reasons, panelists indicated. For one, said Sindhu, they are very hungry to learn.   For another, said Navin Chaddha, managing director of the Mayfield Fund, China’s huge market gives Chinese companies a boost. “Usually innovation happens when you are close to a market,” he said. To date, the U.S. companies and Samsung have benefitted from the boom” in the Chinese tech market, but now “we are seeing Chinese companies benefitting from their local market… and China is the biggest market when it comes to broadband users.”  A solution?  “Invest in that market,” says Chaddha.  That strategy is not without pitfalls, Hennessy indicated. “What happens to your technology when you ship it over there?” he asked.  “To the extent that we can protect it, we will,” Sindhu said.  Hennessy remained skeptical. “Just wait until you sign the deal and send it over,” he said.  “This isn’t a redo of semiconductor wars with Japan in the 80s,” he concluded.  “This is a country that has scale, that has entrepreneurial zeal. They will give us a run for the money.”    IEEE Spectrum’s blog featuring the people, places, and passions of the world of technologists in Silicon Valley and its environs. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Watch Astrobee's First Autonomous Flight on the International Space Station,Robotics,2019-10-10,-,https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station,"      NASA’s Astrobee robots have come a long, long way since we first met them at NASA Ames back in 2017. In fact, they’ve made it all the way to the International Space Station: Bumble, Honey, and Queen Bee are up there right now. While Honey and Queen Bee are still packed away in a case (and quite unhappy about it, I would imagine), Bumble has been buzzing around, getting used to its new home. To be ready to fly solo, all Bumble needed was some astronaut-assisted mapping of its environment, and last month, the little robotic cube finally embarked on its first fully autonomous ISS adventure.  We cut together the above video from about an hour’s worth of raw footage (without audio) of Astrobee testing, which took place in the Japanese Experiment Module (JEM), also known as Kibo, on the ISS on August 28. Astronaut Christina Koch had been working with roboticists at NASA Ames on earlier Astrobee start-up activities, which hadn’t gone as perfectly as everyone hoped they would, and was (understandably) excited that the robot was able to successfully fly itself though the JEM. Christina and another astronaut, off camera in the Harmony node attached to the JEM, do a little dance to celebrate (with what is now officially the “Astrobee Jig,” we’re told), and apparently Astrobee now has a standing invitation to join in on all future ISS dance parties.  Astrobee’s goal for its first autonomous mission was to undock itself, follow a flight plan consisting of a list of waypoints and objectives that was uploaded to the robot from the ground, and then return to the dock. All of this was done without any direct intervention from the ground controllers or from the astronauts. As you can see in the video, Christina is mostly just following Bumble around as it does its thing, keeping out of the way of the navigation camera but otherwise just making sure the robot didn’t get into any trouble.  So far, the difficult part for Astrobee has been getting its localization to work robustly. While the robot does navigate visually, it’s dependent on preexisting maps rather than doing SLAM. Putting together those initial maps involved hand-carrying Bumble around the JEM to collect images, which were then processed offline (back on Earth) to identify features in the images and correlate them with locations to build up the map that Bumble uses to navigate. With maps like these, you have to find the right mix of features to include for navigation to work optimally. If your maps are too rich in features, there will be too much data for your robot to manage, and if the maps are too sparse, the robot won’t be able to localize accurately. This was a little bit tricky for Astrobee, as deputy group lead Maria Bualat from the Intelligent Systems Division at NASA Ames explained to us: It turned out that our maps needed to be richer. We tried to cull them down to make them fast and efficient, but we weren’t keeping enough features to enable the robot to localize robustly, so it would get lost a lot. During some of our earlier activities when we were trying to fly even basic motions, the robot would tend to drift as it would lose lock. This last activity that we had was great, because it was our first time using the more enriched map, and the localization worked really well. It was kind of nice because [Christina] saw us through those struggles—she saw how tough it was to get the robot to fly. Besides this little bit of software optimization, Bualat says that Astrobee has been working well, without any other software issues or hardware issues of any kind. This is impressive for any robot, and especially so for a robot that was developed entirely on the ground and is now being used in space. And as for the astronauts whose job it is to test Astrobee, it sounds like they’re actually having some fun with it. There was a bit of concern initially that Astrobee’s impellers would be overly loud, but that might be a feature rather than a bug, as Bualat explains: “We’ve asked them if they found it noisy or annoying, and they said no—in fact, they said that you can certainly hear it, but they actually liked it because it means that Astrobee can’t sneak up on them.” Astrobee will be continuing its commissioning activities over the next few months, which includes tuning Bumble so that it can fly as robustly as possible. For example, Astrobee needs to be able to navigate if an astronaut moves in front of its navigation camera, blocking some of the view. Bumble will then get its perching arm installed and tested, after which the goal is to start working with some of the science payloads, like a gecko gripper, a RFID tracker, and a microphone array, which you can read more about here and here. Honey and Queen still need to go through their own start-up tests and calibrations, and Maria Bualat says the goal is to have multiple Astrobees buzzing around the ISS together “not too far in the future.” [ Astrobee ]  Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Next-Gen AR Glasses Will Require New Chip Designs,Semiconductors,2019-10-09,-,https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/dramatic-changes-in-chip-design-will-be-necessary-to-make-ar-glasses-a-reality,"      What seems like a simple task—building a useful form of augmented reality into comfortable, reasonably stylish, eyeglasses—is going to need significant technology advances on many fronts, including displays, graphics, gesture tracking, and low-power processor design. That was the message of Sha Rabii, Facebook’s head of silicon and technology engineering. Rabii, speaking at Arm TechCon 2019 in San Jose, Calif., on Tuesday, described a future with AR glasses that enable wearers to see at night, improve overall eyesight, translate signs on the fly, prompt wearers with the names of people they meet, create shared whiteboards, encourage healthy food choices, and allow selective hearing in crowded rooms. This type of AR will be, he said, “an assistant, connected to the Internet, sitting on your shoulders, and feeding you useful information to your ears and eyes when you need it.” This vision, he indicated, isn’t arriving anytime soon, but it is achievable. The biggest roadblock, he said, is lowering the energy consumption of the hardware, along with reducing the heat that today’s processors emit. “The low-power design community is uniquely positioned to take the mantle and create the tools that let us realize this vision,” he said. Rabii had a couple of suggestions for approaches that chip developers could take. For one, he said, chips have to be better tailored to their intended uses. “Our use case,” he said, speaking about AR glasses, “is moderate performance but high-power efficiency, form factors that support stylish and lightweight designs, [and chip designs that are] mindful of temperature for user comfort.” Designers also need to think more realistically about how chips use energy. Energy consumption, he says, is mostly “determined by memory access and data movement. Data transfer is far more expensive than compute.” For example, he indicated, “fetching 1 byte from DRAM takes 12,000 times more energy than performing an 8-bit addition; sending 1 byte wirelessly takes 300,000 times more energy.” Hardware designers need to keep these differences in mind in the way they implement AI, Rabii says. “The prevalent model is to have a monolithic accelerator as a discrete compute element, with all AI workloads transferred to this element,” he said. “But this is a data transfer intensive architecture, which has implications for power consumption.” Better, he suggested, would be to “treat AI as a deeply embedded function and distribute it across all the compute” in a system. This type of architecture, he said, brings compute to data, so data doesn’t have to move around as much, dramatically saving power. There are other ways AI can be designed to use less energy, Rabii says. “Not every AI function needs the same precision,” he says. “A large percentage of the computational effort is required for the last percents of accuracy,” so breaking up workloads and reducing precision when possible can make AI systems far more efficient. That’s what designers can do now. In the future, he said, Facebook is looking forward to improvements in semiconductor process technologies that will lead to better performance per watt, as well as specialized accelerators that focus on specific types of AI for higher performance and better energy efficiency. Some of those advances, he hopes, will come from Arm Holdings and the Arm ecosystem.  IEEE Spectrum’s blog featuring the people, places, and passions of the world of technologists in Silicon Valley and its environs. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,From Mainframes to PCs: What Robot Startups Can Learn From the Computer Revolution,Robotics,2019-10-09,-,https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution,"      This is a guest post. The views expressed here are solely those of the author and do not represent positions of IEEE Spectrum or the IEEE. Autonomous robots are coming around slowly. We already got autonomous vacuum cleaners, autonomous lawn mowers, toys that bleep and blink, and (maybe) soon autonomous cars. Yet, generation after generation, we keep waiting for the robots that we all know from movies and TV shows. Instead, businesses seem to get farther and farther away from the robots that are able to do a large variety of tasks using general-purpose, human anatomy-inspired hardware. Although these are the droids we have been looking for, anything that came close, such as Willow Garage’s PR2 or Rethink Robotics’ Baxter has bitten the dust. With building a robotic company being particularly hard, compounding business risk with technological risk, the trend goes from selling robots to selling actual services like mowing your lawn, provide taxi rides, fulfilling retail orders, or picking strawberries by the pound. Unfortunately for fans of R2-D2 and C-3PO, these kind of business models emphasize specialized, room- or fridge-sized hardware that is optimized for one very specific task, but does not contribute to a general-purpose robotic platform.  We have actually seen something very similar in the personal computer (PC) industry. In the 1950s, even though computers could be as big as an entire room and were only available to a selected few, the public already had a good idea of what computers would look like. A long list of fictional computers started to populate mainstream entertainment during that time. In a 1962 New York Times article titled “Pocket Computer to Replace Shopping List,” visionary scientist John Mauchly stated that “there is no reason to suppose the average boy or girl cannot be master of a personal computer.” In 1968, Douglas Engelbart gave us the “mother of all demos,” browsing hypertext on a graphical screen and a mouse, and other ideas that have become standard only decades later. Now that we have finally seen all of this, it might be helpful to examine what actually enabled the computing revolution to learn where robotics is really at and what we need to do next.  In the 1970s, mainframes were about to be replaced by the emerging class of mini-computers, fridge-sized devices that cost less than US $25,000 ($165,000 in 2019 dollars). These computers did not use punch-cards, but could be programmed in Fortran and BASIC, dramatically expanding the ease with which potential applications could be created. Yet it was still unclear whether mini-computers could ever replace big mainframes in applications that require fast and efficient processing of large amounts of data, let alone enter every living room. This is very similar to the robotics industry right now, where large-scale factory robots (mainframes) that have existed since the 1960s are seeing competition from a growing industry of collaborative robots that can safely work next to humans and can easily be installed and programmed (minicomputers). As in the ’70s, applications for these devices that reach system prices comparable to that of a luxury car are quite limited, and it is hard to see how they could ever become a consumer product.  Yet, as in the computer industry, successful architectures are quickly being cloned, driving prices down, and entirely new approaches on how to construct or program robotic arms are sprouting left and right. Arm makers are joined by manufacturers of autonomous carts, robotic grippers, and sensors. These components can be combined, paving the way for standard general purpose platforms that follow the model of the IBM PC, which built a capable, open architecture relying as much on commodity parts as possible.  General purpose robotic systems have not been successful for similar reasons that general purpose, also known as “personal,” computers took decades to emerge. Mainframes were custom-built for each application, while typewriters got smarter and smarter, not really leaving room for general purpose computers in between. Indeed, given the cost of hardware and the relatively little abilities of today’s autonomous robots, it is almost always smarter to build a special purpose machine than trying to make a collaborative mobile manipulator smart.  A current example is e-commerce grocery fulfillment. The current trend is to reserve underutilized parts of a brick-and-mortar store for a micro-fulfillment center that stores goods in little crates with an automated retrieval system and a (human) picker. A number of startups like Alert Innovation, Fabric, Ocado Technology, TakeOff Technologies, and Tompkins Robotics, to just name a few, have raised hundreds of millions of venture capital recently to build mainframe equivalents of robotic fulfillment centers. This is in contrast with a robotic picker, which would drive through the aisles to restock and pick from shelves. Such a robotic store clerk would come much closer to our vision of a general purpose robot, but would require many copies of itself that crowd the aisles to churn out hundreds of orders per hour as a microwarehouse could. Although eventually more efficient, the margins in retail are already low and make it unlikely that this industry will produce the technological jump that we need to get friendly C-3POs manning the aisles.   Mainframes were also attacked from the bottom. Fascination with the new digital technology has led to a hobbyist movement to create microcomputers that were sold via mail order or at RadioShack. Initially, a large number of small businesses was selling tens, at most hundreds, of devices, usually as a kit and with wooden enclosures. This trend culminated into the “1977 Trinity” in the form of the Apple II, the Commodore PET, and the Tandy TRS-80, complete computers that were sold for prices around $2500 (TRS) to $5000 (Apple) in today’s dollars. The main application of these computers was their programmability (in BASIC), which would enable consumers to “learn to chart your biorhythms, balance your checking account, or even control your home environment,” according to an original Apple advertisement. Similarly, there exists a myriad of gadgets that explore different aspects of robotics such as mobility, manipulation, and entertainment.  As in the fledgling personal computing industry, the advertised functionality was at best a model of the real deal. A now-famous milestone in entertainment robotics was the original Sony’s Aibo, a robotic dog that was advertised to have many properties that a real dog has such as develop its own personality, play with a toy, and interact with its owner. Released in 1999, and re-launched in 2018, the platform has a solid following among hobbyists and academics who like its programmability, but probably only very few users who accept the device as a pet stand-in. There also exist countless “build-your-own-robotic-arm” kits. One of the more successful examples is the uArm, which sells for around $800, and is advertised to perform pick and place, assembly, 3D printing, laser engraving, and many other things that sound like high value applications. Using compelling videos of the robot actually doing these things in a constrained environment has led to two successful crowd-funding campaigns, and have established the robot as a successful educational tool. Finally, there exist platforms that allow hobbyist programmers to explore mobility to construct robots that patrol your house, deliver items, or provide their users with telepresence abilities. An example of that is the Misty II. Much like with the original Apple II, there remains a disconnect between the price of the hardware and the fidelity of the applications that were available.   For computers, this disconnect began to disappear with the invention of the first electronic spreadsheet software VisiCalc that spun out of Harvard in 1979 and prompted many people to buy an entire microcomputer just to run the program. VisiCalc was soon joined by WordStar, a word processing application, that sold for close to $2000 in today’s dollars. WordStar, too, would entice many people to buy the entire hardware just to use the software. The two programs are early examples of what became known as “killer application.”  With factory automation being mature, and robots with the price tag of a minicomputer being capable of driving around and autonomously carrying out many manipulation tasks, the robotics industry is somewhere where the PC industry was between 1973—the release of the Xerox Alto, the first computer with a graphical user interface, mouse, and special software—and 1979—when microcomputers in the under $5000 category began to take off. So what would it take for robotics to continue to advance like computers did? The market itself already has done a good job distilling what the possible killer apps are. VCs and customers alike push companies who have set out with lofty goals to reduce their offering to a simple value proposition. As a result, companies that started at opposite ends often converge to mirror images of each other that offer very similar autonomous carts, (bin) picking, palletizing, depalletizing, or sorting solutions. Each of these companies usually serves a single application to a single vertical—for example bin-picking clothes, transporting warehouse goods, or picking strawberries by the pound. They are trying to prove that their specific technology works without spreading themselves too thin.  Very few of these companies have really taken off. One example is Kiva Systems, which turned into the logistic robotics division of Amazon. Kiva and others are structured around sound value propositions that are grounded in well-known user needs. As these solutions are very specialized, however, it is unlikely that they result into any economies of scale of the same magnitude that early computer users who bought both a spreadsheet and a word processor application for their expensive minicomputer could enjoy. What would make these robotic solutions more interesting is when functionality becomes stackable. Instead of just being able to do bin picking, palletizing, and transportation with the same hardware, these three skills could be combined to model entire processes. A skill that is yet little addressed by startups and is historically owned by the mainframe equivalent of robotics is assembly of simple mechatronic devices. The ability to assemble mechatronic parts is equivalent to other tasks such as changing a light bulb, changing the batteries in a remote control, or tending machines like a lever-based espresso machine. These tasks would involve the autonomous execution of complete workflows possible using a single machine, eventually leading to an explosion of industrial productivity across all sectors.  For example, picking up an item from a bin, arranging it on the robot, moving it elsewhere, and placing it into a shelf or a machine is a process that equally applies to a manufacturing environment, a retail store, or someone’s kitchen. Even though many of the above applications are becoming possible, it is still very hard to get a platform off the ground without added components that provide “killer app” value of their own. Interesting examples are Rethink Robotics or the Robot Operating System (ROS). Rethink Robotics’ Baxter and Sawyer robots pioneered a great user experience (like the 1973 Xerox Alto, really the first PC), but its applications were difficult to extend beyond simple pick-and-place and palletizing and depalletizing items. ROS pioneered interprocess communication software that was adapted to robotic needs (multiple computers, different programming languages) and the idea of software modularity in robotics, but—in the absence of a common hardware platform—hasn’t yet delivered a single application, e.g. for navigation, path planning, or grasping, that performs beyond research-grade demonstration level and won’t get discarded once developers turn to production systems. At the same time, an increasing number of robotic devices, such as robot arms or 3D perception systems that offer intelligent functionality, provide other ways to wire them together that do not require an intermediary computer, while keeping close control over the real-time aspects of their hardware.  Robotic Materials GPR-1 combines a MIR-100 autonomous cart with an UR-5 collaborative robotic arm, an onRobot force/torque sensor and Robotic Materials’ SmartHand to perform out-of-the-box mobile assembly, bin picking, palletizing, and depalletizing tasks.  At my company, Robotic Materials Inc., we have made strides to identify a few applications such as bin picking and assembly, making them configurable with a single click by combining machine learning and optimization with an intuitive user interface. Here, users can define object classes and how to grasp them using a web browser, which then appear as first-class objects in a robot-specific graphical programming language. We have also done this for assembly, allowing users to stack perception-based picking and force-based assembly primitives by simply dragging and dropping appropriate commands together. While such an approach might answer the question of a killer app for robots priced in the “minicomputer” range, it is unclear how killer app-type value can be generated with robots in the less-than-$5000 category. A possible answer is two-fold: First, with low-cost arms, mobility platforms, and entertainment devices continuously improving, a confluence of technology readiness and user innovation, like with the Apple II and VisiCalc, will eventually happen. For example, there is not much innovation needed to turn Misty into a home security system; the uArm into a low-cost bin-picking system; or an Aibo-like device into a therapeutic system for the elderly or children with autism. Second, robots and their components have to become dramatically cheaper. Indeed, computers have seen an exponential reduction in price accompanied by an exponential increase in computational power, thanks in great part to Moore’s Law. This development has helped robotics too, allowing us to reach breakthroughs in mobility and manipulation due to the ability to process massive amounts of image and depth data in real-time, and we can expect it to continue to do so. One might ask, however, how a similar dynamics might be possible for robots as a whole, including all their motors and gears, and what a “Moore’s Law” would look like for the robotics industry. Here, it helps to remember that the perpetuation of  Moore’s Law is not the reason, but the result of the PC revolution. Indeed, the first killer apps for bookkeeping, editing, and gaming were so good that they unleashed tremendous consumer demand, beating the benchmark on what was thought to be physically possible over and over again. (I vividly remember 56 kbps to be the absolute maximum data rate for copper phone lines until DSL appeared.) That these economies of scale are also applicable to mechatronics is impressively demonstrated by the car industry. A good example is the 2020 Prius Prime, a highly computerized plug-in hybrid, that is available for one third of the cost of my company’s GPR-1 mobile manipulator while being orders of magnitude more complex, sporting an electrical motor, a combustion engine, and a myriad of sensors and computers. It is therefore very well conceivable to produce a mobile manipulator that retails at one tenth of the cost of a modern car, once robotics enjoy similar mass-market appeal. Given that these robots are part of the equation, actively lowering cost of production, this might happen as fast as never before in the history of industrialization.  There is one more driver that might make robots exponentially more capable: the cloud. Once a general purpose robot has learned or was programmed with a new skill, it could share it with every other robot. At some point, a grocer who buys a robot could assume that it already knows how to recognize and handle 99 percent of the retail items in the store. Likewise, a manufacturer can assume that the robot can handle and assemble every item available from McMaster-Carr and Misumi. Finally, families could expect a robot to know every kitchen item that Ikea and Pottery Barn is selling. Sounds like a labor intense problem, but probably more manageable than collecting footage for Google’s Street View using cars, tricycles, and snowmobiles, among other vehicles. While we are waiting for these two trends—better and better applications and hardware with decreasing cost—to converge, we as a community have to keep exploring what the canonical robotic applications beyond mobility, bin picking, palletizing, depalletizing, and assembly are. We must also continue to solve the fundamental challenges that stand in the way of making these solutions truly general and robust.  For both questions, it might help to look at the strategies that have been critical in the development of the personal computer, which might equally well apply to robotics:  Start with a solution to a problem your customers have. Unfortunately, their problem is almost never that they need your sensor, widget, or piece of code, but something that already costs them money or negatively affects them in some other way. Example: There are many more people who had a problem calculating their taxes (and wanted to buy VisiCalc) than writing their own solution in BASIC.  Build as little of your own hardware as necessary. Your business model should be stronger than the margin you can make on the hardware. Why taking the risk? Example: Why build your own typewriter if you can write the best typewriting application that makes it worth buying a computer just for that?   If your goal is a platform, make sure it comes with a killer application, which alone justifies the platform cost. Example: Microcomputer companies came and went until the “1977 Trinity” intersected with the killer apps spreadsheet and word processors. Corollary: You can also get lucky.   Use an open architecture, which creates an ecosystem where others compete on creating better components and peripherals, while allowing others to integrate your solution into their vertical and stack it with other devices. Example: Both the Apple II and the IBM PC were completely open architectures, enabling many clones, thereby growing the user and developer base.   It’s worthwhile pursuing this. With most business processes already being digitized, general purpose robots will allow us to fill in gaps in mobility and manipulation, increasing productivity at levels only limited by the amount of resources and energy that are available, possibly creating a utopia in which creativity becomes the ultimate currency. Maybe we’ll even get R2-D2.   Biweekly newsletter on advances and news in robotics, automation, control systems, interviews with leading roboticists, and more.  IEEE Spectrum’s award-winning robotics blog, featuring news, articles, and videos on robots, humanoids, automation, artificial intelligence, and more. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,Drones as Detectives: Surveying Crime Scenes for Evidence,Robotics,2019-10-08,-,https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence,"      When detectives and other forensics specialists arrive at a crime scene, there is a pressing need to survey the area quickly. Environmental disturbances such as wind or an incoming tide could ruin valuable evidence, and even the investigators themselves are at risk of contaminating the crime scene. Could a fleet of evidence-surveying drones be of help? Pompílio Araújo, a criminal expert for the Federal Police of Brazil, is responsible for recording crime scenes exactly as found. In his other role as a researcher at the Intelligent Vision Research Lab at Federal University of Bahia, he is trying to make his first job easier by developing drones that can—very quickly—home in on a piece of evidence and record it from multiple angles. The drone system, dubbed AirCSI, starts by scanning a crime scene with one big sweep, using a stereo camera and a visual self-localization and mapping (SLAM) system to monitor the drone’s position. “Initially, the drone [flies] at a height that can take a broad view of the crime scene and detect some larger pieces of evidence,” explains Araújo, who published this preliminary stage of research in a previous study. The drone system was first trained to detect guns, but could be trained to identify other types of weapons or evidence such as blood stains. In his latest work, described in IEEE Geoscience and Remote Sensing Letters, Araújo and his colleagues developed a secondary system for the drone, which includes a second camera, trained to image evidence from multiple angles. In this newer version, AirCSI calculates a circular area for each piece of evidence that’s initially identified, considering its relevance and size. The drone system then calculates a zigzag trajectory and makes a series of additional sweeps in order to collect more detailed data on each piece of evidence. “As a result, AirCSI provides a sketch with the localization of the evidences, as well as a detailed crime scene imagery,” says Araújo. His team used simulation software to test this newer version of AirCSI, and found that using multiple angles to detect evidence is up to 18 percent more effective than using only one angle. While the researchers have yet to test the new, multi-angle approach beyond simulations, they expect to try it out in a real environment by the end of this year or early next year. Meanwhile, they are working to overcome a few of the system’s limitations. For example, AirCSI can currently operate only in open environments with no obstacles. “In the future, we plan to do anti-collision routines and training for more criminal evidences,” Araújo says. He also plans on developing a way to completely reconstruct crime scenes using the drone footage, creating a virtual environment that investigators can explore indefinitely—or at least until the crime is solved.  Receive latest technology science and technology news & analysis from IEEE Spectrum every Thursday.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
IEEE,X-Ray Tech Lays Chip Secrets Bare,Semiconductors,2019-10-07,-,https://spectrum.ieee.org/nanoclast/semiconductors/design/xray-tech-lays-chip-secrets-bare,"      Scientists and engineers in Switzerland and California have come up with a technique that can reveal the 3D design of a modern microprocessor without destroying it. Typically today, such reverse engineering is a time-consuming process that involves painstakingly removing each of a chip’s many nanometers-thick interconnect layers and mapping them using a hierarchy of different imaging techniques, from optical microscopy for the larger features to electron microscopy for the tiniest features.  The inventors of the new technique, called ptychographic X-ray laminography, say it could be used by integrated circuit designers to verify that manufactured chips match their designs, or by government agencies concerned about “kill switches” or hardware trojans that could have secretly been added to ICs they depend on. “It’s the only approach to non-destructive reverse engineering of electronic chips—[and] not just reverse engineering but assurance that chips are manufactured according to design,” says Anthony F. J. Levi, professor of electrical and computer engineering at University of Southern California, who led the California side of the team. “You can identify the foundry, aspects of the design, who did the design. It’s like a fingerprint.” The new technique is an improvement on technology unveiled by the same team in 2017 called ptychographic computed tomography. That process used a coherent beam of X-rays from a synchrotron to illuminate a 10-micrometer pillar that had been cut away from the rest of the chip. The team then recorded how the X-rays diffract and scatter from the pillar at a variety of angles and computed what the internal structures must be in order to create that pattern. With the new technique, “the goal was obviously to avoid having to do any cutting at all,” explains Gabriel Aeppli, head of the photonic science division at the Paul Scherrer Institute (PSI) in Switzerland and professor of physics at the Swiss Federal Institutes of Technology in Zürich and Lausanne, who led the research. “A modern chip with a billion transistors is a bigger footprint than 10 microns.” The group wanted a single technology that would allow them to image a whole chip and also zoom in on points of interest. The prior technique needed the pillar, because trying to see through a whole chip, edge-on, absorbs too many of the X-rays to produce a useful diffraction pattern. But shooting the X-rays through the chip at an angle creates a small enough cross-section. However, it also produces a gap in the information. Some of that information can be regained by making some assumptions about what you’re looking at, explains Aeppli. For example, we know that real interconnects can’t have certain shapes. Finding the right angle for the X-rays—which turned out to be 61 degrees—was a matter of balancing absorption and information loss, says Aeppli. In the new technique, the bare chip is polished down to a thickness of 20 micrometers and then placed on a scanning stage at a 61-degree tilt. The stage then rotates the chip as the X-ray beam is focused on it. A photon-counting camera receives the resulting diffraction pattern. Using the technique in low-resolution mode, the team scanned a 300-by-300-micrometer area in 30 hours. They then used it to zoom in on a 40-micrometer-diameter section to produce a 3D image with 18.9-nanometer resolution, requiring another 60 hours. Using the high-resolution mode, the researchers could identify the parts of an individual inverter circuit in a chip made using 16-nanometer node technology. This first laminography microscope engineered by PSI’s Mirko Holler, can image a maximum of 12 by 12 millimeters—easily accommodating many chips, such as the iPhone processor Apple A12, but not large enough for an entire Nvidia Volta GPU. Though the group tested the technique on a chip made using a 16-nanometer process technology, it will be able to comfortably handle those made using the new 7-nanometer process technology, where the minimum distance between metal lines is around 35 to 40 nanometers. Future versions of the laminography technique could reach a resolution of just 2 nanometers or reduce the time for a low-resolution inspection of that 300-by-300-micrometer segment to less than an hour, the researchers say. Those improvements will come from a new generation of synchrotron light sources. The synchrotron at PSI is considered a 3rd generation machine. But 4th generation machines are already starting up, such as Sweden’s MAX IV. With a higher flux of X-ray photons flowing through the chip, the system can collect more usable data per unit of time, leading to higher resolution and faster processing. “We’re looking at improvements of 1,000 to 10,000 over the next five or six years in terms of the pixels we’re collecting per unit [of] time,” says Aeppli. Ptychographic X-ray laminography can be further sped up by starting with more information about the chip. Knowing the design rules ahead of time allows the system to come to a conclusion about what it’s seeing with fewer photons. In fact, Aeppli suspects one of the main uses of the technology will be to look for deviations from the design that could be indicative of manufacturing errors or something more sinister. “Looking for deviation from design is an easier problem than reverse engineering the entire design,” he says. The team is seeing “a lot of interest from the [United States] on the national security side.” However, Aeppli expects chipmakers to use the laminography technique, as well. “Every region that has major chip foundries nearby has some national lab with a synchrotron,” he points out. Aeppli, Levi, and their teams reported the technique this week in Nature Electronics.  Monthly newsletter about how new materials, designs, and processes drive the chip industry.  IEEE Spectrum’s nanotechnology blog, featuring news and analysis about the development, applications, and future of science and technology at the nanoscale. Featured Jobs © Copyright 2019 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.  A not-for-profit organization, IEEE is the world  largest technical professional organization dedicated to advancing technology for the benefit of humanity."
