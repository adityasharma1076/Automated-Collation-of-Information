Source,Heading,Category,Date,Time,URL,Text
Science Daily,Widely Available Drug Reduces Head Injury Deaths,Mind & Brain,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015113316.htm,"   Led by the London School of Hygiene & Tropical Medicine, the global randomised trial included more than 12,000 head injury patients who were given either intravenous tranexamic acid or a placebo. [2] It found that administration of TXA within three hours of injury reduced the number of deaths. This effect was greatest in patients with mild and moderate traumatic brain injury (20% reduction in deaths), while no clear benefit was seen in the most severely injured patients. The trial found no evidence of adverse effects and there was no increase in disability in survivors when the drug was used. [3,4,5] Traumatic brain injury (TBI) is a leading cause of death and disability worldwide with an estimated 69 million new cases each year. [6] The CRASH-3 (Clinical Randomisation of an Antifbrinolytic in Significant Head Injury) trial is one of the largest clinical trials ever conducted into head injury. Patients were recruited from 175 hospitals across 29 countries. Bleeding in or around the brain due to tearing of blood vessels is a common complication of TBI and can lead to brain compression and death. Although patients with very severe head injuries are unlikely to benefit from tranexamic acid treatment because they often have extensive brain bleeding prior to hospital admission and treatment, the study found a substantial benefit in patients with less severe injuries who comprise the majority (over 90%) of TBI cases. [7] Ian Roberts, Professor of Clinical Trials at the London School of Hygiene & Tropical Medicine, who co-led the study, said: ""We already know that rapid administration of tranexamic acid can save lives in patients with life threatening bleeding in the chest or abdomen such as we often see in victims of traffic crashes, shootings or stabbings. This hugely exciting new result shows that early treatment with TXA also cuts deaths from head injury. It's an important breakthrough and the first neuroprotective drug for patients with head injury. ""Traumatic brain injury can happen to anyone at any time, whether it's through an incident like a car crash or simply falling down the stairs. We believe that if our findings are widely implemented they will boost the chances of people surviving head injuries in both high income and low income countries around the world."" Because TXA prevents bleeds from getting worse, but cannot undo damage already done, early treatment is critical. The trial data showed a 10% reduction in treatment effectiveness for every 20-minute delay, suggesting that patients should be treated with TXA as soon as possible after head injury. [8,9]    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Antoni Belli, Neurosurgeon and Professor of Trauma Neurosurgery at the University of Birmingham and co-investigator for trial, said: ""This is a landmark study. After decades of research and many unsuccessful attempts, this is the first ever clinical trial to show that a drug can reduce mortality after traumatic brain injury. Not only do we think this could save hundreds of thousands of lives worldwide, but it will no doubt renew the enthusiasm for drug discovery research for this devastating condition."" Dr Ben Bloom, Consultant in Emergency Medicine at Barts Health NHS Trust, the UK's largest recruiter into the trial with more than 500 patients enrolled, said: ""Treating traumatic brain injury is extremely challenging with very few treatment options available for patients. Thanks to these latest results, which are applicable to patients with head injuries of any cause and of all demographics, clinicians now have a potentially powerful new treatment available to them."" The most common causes of TBI worldwide are road traffic crashes (which predominantly affect young adults) or falls (which are a major problem in older adults), and the incidence is increasing. In both cases, patients can experience permanent disability or death. Representatives from the charity that supports roach crash victims in the UK, Roadpeace, were involved in the design of the trial. Amy Aeron-Thomas, Justice and Advocacy Manager from Roadpeace and co-author on the paper said: ""It's always better to prevent road crashes in the first place, but these results show that if a crash can't be prevented, death can still be avoided. Given the time to treatment implications, it's more important than ever that the post-crash response is as efficient as possible."" CRASH-3 follows successful previous research involving 20,000 trauma patients, which showed that TXA reduced deaths due to bleeding outside of the skull by almost a third if given within three hours. Based on those trial results, tranexamic acid was included in guidelines for the pre-hospital care of trauma patients. However, patients with isolated traumatic brain injury were specifically excluded. [10] The authors noted some limitations of the trial, including wide confidence intervals despite the large trial size, and the fact that more patients with un-survivable head injuries were included in the trial than anticipated, which diluted the treatment effect.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The trial was jointly funded by the Department for International Development (DFID), the Medical Research Council (MRC), the National Institute for Health Research (NIHR), (through the Department of Health and Social Care), and Wellcome. The early phase of the trial was funded was funded by The JP Moulton Charitable Foundation. [11] Notes to Editors1. Tranexamic acid is a low cost and widely available drug as many different companies sell it. Costs vary slightly per country. In the UK, 500mg is roughly £1.55, so the total dose used in CRASH-3 is about £6.20 (https://bnf.nice.org.uk/medicinal-forms/tranexamic-acid.html). In Malaysia, 500mg is 3.30 Malaysian Ringitt (64p) so around £2.50 for the CRASH-3 dose https://www.pharmacy.gov.my/v2/en/apps/drug-price 2. Patients were randomly allocated to receive a loading dose of 1 g of tranexamic acid infused over 10 minutes, started immediately after randomisation, followed by an intravenous infusion of 1 g over 8 hours, or matching placebo. 3. Among patients treated within 3 hours of injury, there was a reduction in the risk of head injury death with tranexamic acid in mild to moderate head injury (RR=0·78 95%CI 0·64-0·95), numbers of deaths can be seen in figure 3 of the paper (TXA group = 166 / 2846 (5.8%), placebo group = 207 / 2769 (7.5%). In severe head injury (RR=0·99, 95%CI 0·91-1·07) there was no clear evidence of a reduction (p-value for heterogeneity 0·030). The impact of baseline GCS in a regression analysis showed evidence (p=0·007) that tranexamic acid is more effective in less severely injured patients. 4. The most common classification system for TBI severity is based on the Glasgow Coma Scale (GCS) score determined at the time of injury. A total score of 3-8 indicates severe TBI, a score of 9-12 indicates moderate TBI, and a score of 13-15 indicates mild TBI. 5. The risk of deep vein thrombosis, pulmonary embolism, stroke and myocardial infarction was similar in the tranexamic acid and placebo groups. There was no evidence that tranexamic acid increased fatal or non-fatal stroke (RR=1.08). The risk of seizures was similar between groups (RR=1.09). 6. Sixty-nine million (95% CI 64-74 million) individuals are estimated to suffer TBI from all causes each year (https://www.ncbi.nlm.nih.gov/pubmed/29701556). 7. Mild TBI occurs with far greater frequency than moderate or severe TBI -- nearly 10-fold the burden of both moderate and severe injury. Of the estimated 69 million TBIs that occur each year, 81% will be mild, 11% will be moderate, and 8% will be severe (https://www.ncbi.nlm.nih.gov/pubmed/29701556). 8. Early treatment was more effective in patients with mild and moderate head injury (p=0·005) but there was no obvious impact of time to treatment in severe head injury (p=0·73). This is consistent with the hypothesis that tranexamic acid improves outcome by reducing intracranial bleeding. 9. The left hand graph of Figure 4 marked ""Mild and Moderate GCS score"" shows how treatment benefit is related to time on the risk ratio scale. When the treatment effect for mild and moderate patients is modelled using logistic regression with a time treatment interaction term adjusting for GCS, age and systolic blood pressure, the odds ratio is reduced by approximately 10% for every 20 minute delay. 10. A previous trial (CRASH-2) of 20,211 bleeding trauma patients from hospitals in 40 countries showed that TXA reduces bleeding deaths by a third if given soon after injury (https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(10)60835-5/fulltext) 11. Funds to support the drug and placebo costs in the run-in phase of the trial were provided by Pfizer. "
Science Daily,"Happy, Angry or Neutral Expressions? Eyes React Just as Fast",Mind & Brain,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110655.htm,"   In everyday life, people are exposed to numerous stimuli, just as when walking through a town, there are people and faces, clothes and shop windows. From this onslaught of information, relevant content must be filtered and reacted to. ""This shift in attention is often accompanied by a movement of the eyes,"" says Kulke, a researcher in the Department of Affective Neuroscience and Psychophysiology at the University of Göttingen. In the current study, she combined two methods to investigate what happens in the brain during this attention shift: eye-tracking and EEG. With the Eye-Tracker, research volunteers sat in front of a device that records eye movements. Kulke then showed them standardised faces with different emotional expressions. At the same time, EEG measured brain waves via electrodes placed on their head. ""We investigated how quickly our test subjects look at the faces that appear on the screen in different places,"" says Kulke. The result shows that quick and immediate eye movements were occurring independently of the facial expression. ""We quickly look at people in our environment with the same speed, regardless of whether they look cheerful, angry or indifferent. The study also showed that the first reactions of the brain are independent of facial expression. Only later -- after the eye movement is completed -- do the reactions of the brain show strong responses to the emotional expression of the face."" According to Kulke, the result is also interesting for follow-up studies. The question is whether the processing of emotions also takes place later, when the eyes do not react reflexively, but are consciously controlled using a tangible task. Kulke explains: ""For example, do we process facial expressions that we see out of the corner of our eye before we move our eyes? When given the explicit goal of only looking at certain faces, such as our friendly fellow student, and ignoring others, such as our annoying neighbour, how does that affect our reactions?"" "
Science Daily,Repeated Febrile Convulsions Linked to Epilepsy and Psychiatric Disorders,Mind & Brain,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103408.htm,"   The register-based study is based on two million Danish children born between 1977 and 2011. The researchers have identified health data from approx. 17,000 children with more than a single febrile convulsion. This makes the register-based study the most comprehensive so far to study the long-term consequences of repeated febrile convulsions. The study has just been published in the scientific journal JAMA Pediatrics. ""Though previous research has documented an increased occurrence of epilepsy among children with febrile convulsions, this is still one of the first studies to demonstrate such a convincing correlation between febrile convulsions and psychiatric disorders. Not least due to the size of the study, the long period of time that the study covers and the valid Danish data,"" says the study's lead author, Postdoc Julie Werenberg Dreyer from the National Centre for Register-based Research. The researcher emphasises that although the study demonstrates a clear correlation, this is not the same as concluding that febrile convulsions in themselves cause epilepsy or psychiatric disorders. ""A statistical correlation does not necessarily mean that one thing causes the other and that it is the febrile convulsions themselves which have a damaging effect on the brain. But the study's results are so significant that looking into this more closely is more than relevant when it comes to possibly being able to provide the best possible prevention and treatment,"" says Julie Werenberg Dreier. According to Julie Werenberg Dreier, a future study could look into the significance of genetics for the child's risk of suffering febrile convulsions and subsequent epilepsy or psychiatric disorders. ""There are still many unknown factors that we don't know enough about. As we learn more about the importance of genes for health and disease, it may be that it is here we will find an explanation for why some children suffer repeated febrile convulsions and then later in life also develop epilepsy and psychiatric disorders,"" she says. The study shows that among children who have three or more attacks of febrile convulsions, the risk of developing epilepsy within thirty years is approximately fifteen per cent. The risk of a psychiatric disorder that requires treatment is approx. thirty per cent. In comparison, children without prior febrile convulsions have a risk of developing epilepsy of approx. two per cent and children without prior febrile convulsions have a seventeen per cent risk of developing a psychiatric disorder. The study points towards new correlations that can in the long-term improve the possibilities of prevention and treatment of patients with epilepsy and psychiatric disorders. This is according to another of the project contributors, Jakob Christensen. He is a clinical associate professor at Aarhus University and consultant at the Department of Neurology at Aarhus University Hospital. He has conducted intensive research into epilepsy over many years. ""Both epilepsy and psychiatric disorder can be extremely serious and associated with high morbidity and mortality -- so in this way the diseases have major consequences for both the individual patient, their family and society,"" says Jakob Christensen. Both researchers hope that the study will help to provide impetus for an intensified effort to clarify the cause of the correlation between the febrile convulsions and the long-term consequences. ""Our results may be frightening reading for parents who have a child that suffers from repeated attacks of febrile convulsions. But these are families who are already deeply concerned about their children. The new knowledge can help them and healthcare professionals to be extra aware of these children's health and development,"" says Julie Werenberg Dreier. "
Science Daily,Potential Therapy to Treat Detrimental Effects of Marijuana,Mind & Brain,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092247.htm,"   A University of Maryland School of Medicine study using a preclinical animal model suggests that prenatal exposure to THC, the psychoactive component of cannabis, makes the brain's dopamine neurons (an integral component of the reward system) hyperactive and increases sensitivity to the behavioral effects of THC during pre-adolescence. This may contribute to the increased risk of psychiatric disorders like schizophrenia and other forms of psychosis later in adolescence that previous research has linked to prenatal cannabis use, according to the study published today in journal Nature Neuroscience. The team of researchers, from UMSOM, the University of Cagliari (Italy) and the Hungarian Academy of Sciences (Hungary), found that exposure to THC in the womb increased susceptibility to THC in offspring on several behavioral tasks that mirrors the effects observed in many psychiatric diseases. These behavioral effects were caused, at least in part, by hyperactivity of dopamine neurons in a brain region called the ventral tegmental area (VTA), which regulates motivated behaviors. More importantly, the researchers were able to correct these behavioral problems and brain abnormalities by treating experimental animals with pregnenolone, an FDA-approved drug currently under investigation in clinical trials for cannabis use disorder, schizophrenia, autism, and bipolar disorder. ""This is an exciting finding that suggests a therapeutic approach for children born to mothers who used cannabis during pregnancy,"" said Joseph Cheer, PhD, a Professor of Anatomy & Neurobiology and Psychiatry at the University of Maryland School of Medicine. ""It also raises important questions that need to be addressed such as how does pregnenolone exert its effects and how can we improve its efficacy? Do these detrimental effects persist into adulthood, and if so, could they also be treated in a similar way?"" The researchers concluded that as physicians caution pregnant women against alcohol and cocaine intake because of their detrimental effects to the fetus, they should also, based on these new findings, advise them on the potential negative consequences of using cannabis specifically during pregnancy. "
Science Daily,New Approach to Slowing Nearsightedness in Children Shows Promise,Mind & Brain,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092236.htm,"   There is a world-wide epidemic of myopia, also known as nearsightedness. Since 1971, the incidence of nearsightedness in the US nearly doubled, to 42 percent. In Asia, up to 90 percent of teenagers and adults are nearsighted. Nearsightedness may not seem like a serious eye condition. Glasses and contact lenses can provide effective treatment. But high myopia, defined as -6 D or more, can lead to potentially blinding complications, such as glaucoma, retinal detachment and retinal degeneration. Myopia can't be stopped, but it can be slowed. There are two methods for slowing progression. One method uses 0.01% atropine eye drops, instilled in the eye every day. Atropine is a medication commonly used to dilate or widen the eye before an exam. How it slows progression is unclear (some evidence suggests atropine blocks muscarinic receptors in the retina). But research shows it is effective and safe. Another method is orthokeratology, which involves using rigid gas permeable contact lenses every night to reshape the cornea, the clear, front part of the eye. It's also unclear how contact lenses slow progression, but it is thought that reshaping the cornea changes the peripheral focus of the eye to reduce myopia progression. There are risks with overnight contact lens wear, such as corneal abrasions, ulcers or infections, and scaring that can lead to vision loss. Myopia progression can rebound with both methods, though less so with 0.01% atropine. Two treatments, each effective, each appear to work in a different way. What if they were combined? Would the combination have an additive or synergistic effect? To learn more, Nozomi Kinoshita, M.D., Ph.D., and colleagues at Jichi Medical University in Japan, randomized 80 children into two groups: one received both orthokeratology and atropine, while the second group received only orthokeratology. The children, aged 8 to 12 years old, exhibited a range of myopia, from low to high (from -1D to -6 D). They were treated for three months and then followed for two years. In children with higher myopia, combination treatment was 28 percent more effective compared with contact lenses alone. In children with lower myopia, combination treatment was 38 percent more effective. ""At present, using atropine together with orthokeratology can become a better treatment option to slowing myopia progression,"" Dr. Kinoshita said. ""We believe this combination will be an optimal treatment option because together, both therapies complement the weakness of each other."" "
Science Daily,Study Finds Topsoil Is Key Harbinger of Lead Exposure Risks for Children,Mind & Brain,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014181654.htm,"   The study, which focuses on New Orleans but could serve as a model for cities around the world, is the first to show how long-term changes in soil lead levels have a corresponding impact in lead blood levels in children. ""Lead dust is invisible and it's tragic that lead-contaminated outdoor areas are unwittingly provided for children as places to play,"" says lead study author Howard Mielke, a pharmacology research professor at Tulane University School of Medicine. ""Young children are extremely vulnerable to lead poisoning because of their normal crawling, hand-to-mouth, exploratory behavior."" Exposure to lead is often irreversible, particularly for children, and includes behavioral or learning problems, decreased IQ, hyperactivity, delayed growth, hearing problems, anemia, kidney disease and cancer. In rare cases, exposure can lead to seizures, coma, or death. In metropolitan New Orleans, children living in communities with more lead in the soil and higher blood lead levels have the lowest school performance scores. Lead was recently cited as a top risk factor for premature death in the United States, particularly from cardiovascular disease, and is responsible for 412,000 premature deaths each year. The research team began tracking the amount of lead in New Orleans soil in 2001, collecting about 5,500 samples in neighborhoods, along busy streets, close to houses and in open spaces including parks. The team from Mielke's Lead Lab collected another round of soil sampling 16 years later. Those samples showed a 44% decrease in the amount of soil lead in communities flooded during Hurricane Katrina in 2005 as well as soils in communities not affected by the levee failures and storm surge. Researchers then compared the soil lead with children's blood lead data maintained by the Louisiana Healthy Homes and Childhood Lead Poisoning Prevention Program from 2000-2005 and 2011-2016. Researchers found that lead in blood samples decreased by 64% from 2000-2005 to the 2011-2016 time period and that decreasing lead in topsoil played a key factor in the declining children's blood lead levels. Lead exposure is a critical environmental justice issue, according to researchers. The team found black children were three times more likely than white children to have higher blood lead levels, which could be explained by socioeconomic status and education, the type and age of housing and proximity to major roads and industry. ""While the metabolism of the city could theoretically affect all residents equally, in reality social formations produce inequitable outcomes in which vulnerable populations tend to bear greater burdens of contaminant exposure,"" Mielke says. Mielke says further study is needed to determine if demographic changes in New Orleans since 2001 contributed to the decline in children's blood lead levels, and if decreases are occurring equitably for all populations. This new study is co-authored by researchers from Australia, Colorado State University, and City University of New York. "
Science Daily,Dementia Spreads Via Connected Brain Networks,Mind & Brain,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111730.htm,"   ""Knowing how dementia spreads opens a window onto the biological mechanisms of the disease -- what parts of our cells or neural circuits are most vulnerable,"" said study lead author Jesse Brown, PhD, an assistant professor of neurology at the UCSF Memory and Aging Center and UCSF Weill Institute for Neurosciences. ""You can't really design a treatment until you know what you're treating."" FTD, the most common form of dementia in people under the age of 60, comprises a group of neurodegenerative conditions with diverse linguistic and behavioral symptoms. As in Alzheimer's disease, the diversity of FTD symptoms reflects significant differences in how the neurodegenerative disease spreads through patients' brains. This variability makes it difficult for scientists searching for cures to pin down the biological drivers of brain atrophy and for clinical trials to evaluate whether a novel treatment is making a difference in the progression of a patient's disease. Previous research by the study's senior author, William Seeley, MD, a professor of neurology and pathology at the Memory and Aging Center and Weill Institute, set off a sea change in dementia research by showing that patterns of brain atrophy in many forms of dementia map closely onto well-known brain networks -- groups of functionally related brain regions that work cooperatively via their synaptic connections, sometimes over long distances. In other words, Seeley's work proposed that neurodegenerative diseases don't spread evenly in all directions like a tumor, but can jump from one part of the brain to another along the anatomical circuits that wire these networks together. In their new study -- published October 14 in Neuron -- Brown, Seeley and colleagues provided further evidence supporting this idea by examining how well neural network maps based on brain scans in healthy individuals could predict the spread of brain atrophy in FTD patients over the course of a year. The researchers recruited 42 patients at the UCSF Memory and Aging Center with behavioral variant fronto-temporal dementia (bvFTD), a form of FTD that causes patients to exhibit inappropriate social behaviors, and 30 patients with semantic variant primary progressive aphasia (svPPA), a form of FTD that mainly impacts patients' language abilities. In their first visits to UCSF, each of these patients underwent a ""baseline"" MRI scan to assess the extent of existing brain degeneration and then had a follow-up scan about a year later to measure how their disease had progressed.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The researchers first estimated where the brain atrophy seen in each patient's baseline scans had begun, based on the hypothesis that brain degeneration begins in some particularly vulnerable location, then spreads out to anatomically connected brain regions. To do this, the researchers built standardized maps of the main functional partners of 175 different brain regions based on functional MRI (fMRI) scans of 75 healthy adults. They then identified which of these networks best matched the pattern of brain atrophy seen in a given FTD patient's baseline brain scans, and defined that network's central hub as the likely epicenter of the patient's degeneration. They then used the same standardized connectivity maps to predict where the patient's brain atrophy was most likely to have spread in the follow-up scans done one year later, and compared the accuracy of these predictions to others that didn't take functional network connectivity into account. They found that two particular connectivity measures significantly improved their predictions of a given brain region's chances of developing brain atrophy between the baseline and follow-up brain scans. One, called ""shortest path to the epicenter,"" captured the number of synaptic ""steps"" that region was from the estimated disease epicenter -- essentially the number of links in the neural chain connecting the two areas -- while the other, called ""nodal hazard,"" represented how many regions connected to a given region were already experiencing significant atrophy. ""It's like with an infectious disease, where your chances of becoming infected can be predicted by how many degrees of separation you have from 'Patient Zero' but also by how many people in your immediate social network are already sick,"" Brown said. The researchers showed that on average these two measures of network connectivity did better at predicting the spread of disease to a new brain region than its simple straight-line distance from a patient's existing atrophy. In many cases the disease completely bypassed brain areas that were adjacent but not anatomically connected to already-atrophied regions, instead jumping to more functionally linked regions.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Although this method shows great promise, the researchers emphasize that it is not yet ready for clinical use. They hope to improve the accuracy of their predictions by -- among other approaches -- using individualized network maps for each patient rather than using average connectivity maps, and by developing more specialized prediction models for particular subtypes of FTD. In addition to the biological insights the discovery provides about the mechanisms of spreading brain atrophy in FTD, which will inform ongoing efforts to develop treatments, the researchers also hope the findings will lead to improved metrics for evaluating therapies already entering clinical trials -- for instance by giving trial scientists early insights into whether the treatment is altering a predicted course of disease progression. Researchers could also use better predictions of how atrophy will spread through the brain to help prepare patients and their families for the symptoms they are likely to experience as their disease progresses. ""We are excited about this result because it represents an important first step toward a more precision medicine type of approach to predicting progression and measuring treatment effects in neurodegenerative disease,"" Seeley said. In the future, Brown said, scientists might be able to develop therapies that specifically target the likely next site of disease and perhaps prevent atrophy from spreading from one region to another. ""Just like epidemiologists rely on models of how infectious diseases spread to develop interventions targeted to key hubs or choke points,"" Brown said. ""Neurologists need to understand the underlying biological mechanisms of neurodegeneration to develop ways of slowing or halting the spread of the disease."" "
Science Daily,Reading the Past Like an Open Book: Researchers Use Text to Measure 200 Years of Happiness,Mind & Brain,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111713.htm,"   Using innovative new methods researchers at the University of Warwick, University of Glasgow Adam Smith Business School and The Alan Turing Institute in London have built a new index that uses data from books and newspaper to track levels of national happiness from 1820. Their research could help governments to make better decisions about policy priorities. Governments the world over are making increasing use of ""national happiness"" data derived from surveys to help them consider the impact of policy on national wellbeing. Unfortunately, data for most countries is only available from 2011 onwards, and for a select few from the mid 1970s. This makes it hard to establish long-run trends, or to say anything about the main historical causes of happiness. In order to tackle this problem, a team of researchers including Professor Thomas Hills (Warwick and The Alan Turing Institute), Professor Eugenio Proto (Glasgow), Professor Daniel Sgroi (Warwick), and Dr Chanuki Seresinhe (The Alan Turing Institute) took a key insight from psychology -- that more often than not what people say or write reveals much about their underlying happiness level -- and developed a method to apply it to online texts from millions of books and newspapers published over the past 200 years. The main source of language used for the analysis was the Google Books corpus, a collection of word frequency data for over 8 million books -- that's more than 6 per cent of all books ever published. The method uses psychological valence norms -- values of happiness that can be derived from text -- for thousands of words in di?erent languages to compute the relative proportion of positive and negative language for four di?erent nations (the USA, UK, Germany and Italy). The research team also controlled for the evolution of language, to take into account the fact that some words change their meaning over time. The new index was validated against existing survey-based measures and proven to be an accurate guide to the national mood. One theory as to why books and newspaper articles are such a good source of data is that editors prefer to publish pieces which match the mood of their readers. Studying the index, the researchers found that: Increases in national income do generate increases in national happiness but it takes a huge rise to have a noticeable effect at the national level An increase in longevity of one year had the same effect on happiness as a 4.3 per cent increase in GDP One less year of war had the equivalent effect on happiness of a 30 per cent rise in GDP In post-war UK the worst period for national happiness occurred around the appropriately named ""Winter of Discontent."" In post-war USA the lowest point of the index coincides with the Vietnam War and the evacuation of Saigon.Commenting on the findings, Professor Thomas Hills said: ""What's remarkable is that national subjective well-being is incredibly resilient to wars. Even temporary economic booms and busts have little long-term effect. We can see the American Civil War in our data, the revolutions of 48' across Europe, the roaring 20's and the Great Depression. But people quickly returned to their previous levels of subjective well-being after these events were over. Our national happiness is like an adjustable spanner that we open and close to calibrate our experiences against our recent past, with little lasting memory for the triumphs and tragedies of our age."" Professor Eugenio Proto added: ""Our index is an important first step in understanding people's satisfaction in the past. Looking at the Italian data, it is interesting to note a slow but constant decline in the years of fascism and a dramatic decline in the years after the last crisis."" Professor Daniel Sgroi said: 'Aspirations seem to matter a lot: after the end of rationing in the 1950s national happiness was very high as were expectations for the future, but unfortunately things did not pan out as people might have hoped and national happiness fell for many years until the low-point of the Winter of Discontent.' Dr Chanuki Seresinhe said: ""It was really important to ensure that the changing meaning of words over time was taken into account. For example, the word ""gay"" had a completely different meaning in the 1800s than it does today. We processed terabytes of word co-occurrence data from Google Books to understand how the meaning of words has changed over time, and we validate our findings using only words with the most stable historical meanings."" "
Science Daily,Another Reason to Get Cataract Surgery: It Can Make You 48% Safer on the Road,Mind & Brain,2019-10-12,-,https://www.sciencedaily.com/releases/2019/10/191012141221.htm,"   Cataracts are a normal consequence of aging. They happen gradually over years, as the clear lens inside the eye becomes cloudy. The effects of a developing cataract are sometimes hard to distinguish from other age-related vision changes. You may become more nearsighted; colors appear duller and glare from lights make it harder to see at night. By age 80, about half of us will have developed cataracts. Cataract surgery replaces the cloudy lens with an artificial lens. The surgery is low-risk, fast and effective. But not everyone has surgery right away. The decision is usually based on how much the cataract is interfering with daily life activities. Ophthalmologists typically operate on one eye at a time, starting with the eye with the denser cataract. If surgery is successful and vision improves substantially, sometimes surgery in the second eye is forgone or delayed. However, most people get significant benefit from having surgery on the second eye. Depth perception is improved, vision is crisper, making reading and driving easier. To better understand the true benefit of cataract surgery to patients' quality of life, Jonathon Ng, MD, and his colleagues at the University of Western Australia, tested the driving performance of 44 patients before they had cataract surgery. The driving simulator assessed a variety of variables: adjusted speed limits, traffic densities, uncontrolled intersections and pedestrian crossings. Patients were put through the driving simulator again after their first surgery and then again after their second eye surgery. After the first, near misses and crashes decreased by 35 percent; after the second surgery, the number fell to 48 percent. While visual acuity -- how well one sees the eye chart -- is an important method to assess a person's fitness to drive, it's an incomplete assessment, Dr. Ng said. Quality of vision is also an important indicator. Improved contrast sensitivity and better night vision improves drivers' safety on the road. ""In Australia and other countries, people may often wait months to receive government funded surgery after a cataract is diagnosed,"" said Dr. Ng. ""These results highlight the importance of timely cataract surgery in maintaining safety and continued mobility and independence in older adult drivers."" Some things to consider, when considering cataract surgery: Can you see to safety do your job and to drive? Do you have problems reading or watching TV? Is it difficult to cook, shop, climb stairs or take medications? Do vision problems affect your independence? Do bright lights make is harder to see? "
Science Daily,Slower Walkers Have Older Brains and Bodies at 45,Mind & Brain,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011112250.htm,"   Slower walkers were shown to have ""accelerated aging"" on a 19-measure scale devised by researchers, and their lungs, teeth and immune systems tended to be in worse shape than the people who walked faster. ""The thing that's really striking is that this is in 45-year-old people, not the geriatric patients who are usually assessed with such measures,"" said lead researcher Line J.H. Rasmussen, a post-doctoral researcher in the Duke University department of psychology & neuroscience. Equally striking, neurocognitive testing that these individuals took as children could predict who would become the slower walkers. At age 3, their scores on IQ, understanding language, frustration tolerance, motor skills and emotional control predicted their walking speed at age 45. ""Doctors know that slow walkers in their seventies and eighties tend to die sooner than fast walkers their same age,"" said senior author Terrie E. Moffitt, the Nannerl O. Keohane University Professor of Psychology at Duke University, and Professor of Social Development at King's College London. ""But this study covered the period from the preschool years to midlife, and found that a slow walk is a problem sign decades before old age."" The data come from a long-term study of nearly 1,000 people who were born during a single year in Dunedin, New Zealand. The 904 research participants in the current study have been tested, quizzed and measured their entire lives, mostly recently from April 2017 to April 2019 at age 45.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The study appears Oct. 11 in JAMA Network Open. MRI exams during their last assessment showed the slower walkers tended to have lower total brain volume, lower mean cortical thickness, less brain surface area and higher incidence of white matter ""hyperintensities,"" small lesions associated with small vessel disease of the brain. In short, their brains appeared somewhat older. Adding insult to injury perhaps, the slower walkers also looked older to a panel of eight screeners who assessed each participant's 'facial age' from a photograph. Gait speed has long been used as a measure of health and aging in geriatric patients, but what's new in this study is the relative youth of these study subjects and the ability to see how walking speed matches up with health measures the study has collected during their lives. ""It's a shame we don't have gait speed and brain imaging for them as children,"" Rasmussen said. (The MRI was invented when they were five, but was not given to children for many years after.) Some of the differences in health and cognition may be tied to lifestyle choices these individuals have made. But the study also suggests that there are already signs in early life of who would become the slowest walkers, Rasmussen said. ""We may have a chance here to see who's going to do better health-wise in later life."" This research was supported by grants the US National Institute on Aging (AG032282, AG049789, AG028716), the UK Medical Research Council (MR/P005918/1), the Jacobs Foundation, the New Zealand Health Research Council (16-604), the New Zealand Ministry of Business, Innovation and Employment, the Lundbeck Foundation (R288-2018-380), the US National Science Foundation (NSF DGE-1644868), the US National Institute of Child Health and Human Development (T32-HD007376). "
Science Daily,Overlap Allows Nanoparticles to Enhance Light-Based Detection,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015113318.htm,"   The labs of Rice chemists Christy Landes and Stephan Link demonstrated how to optimize a method that can sense small concentrations of molecules by amplifying the light they emit when their spectral frequencies overlap with those of nearby plasmonic nanoparticles. The surface plasmons, coherent electron waves that ripple across the surface of a metallic nanoparticle, act as antennas and enhance the molecules' emitted light up to 10 times when they're in the ""sweet spot"" near a particle. Their technique is the topic of a paper in a special edition of the Journal of Chemical Physics focused on emerging directions in plasmonics. The work at Rice could help researchers analyze the active surfaces of catalysts and other materials at the nanoscale, an important step in improving their efficiency. The discovery relies on the phenomenon of electrochemiluminescence (ECL), by which electricity drives chemical reactions that prompt molecules to emit light, said Thomas Heiderscheit, a Rice graduate student and the paper's lead author. It is often used to detect trace materials like heavy metals in water or the Zika virus in biological fluids. Previous studies inferred that spectral overlap of the nanoparticle and molecules would enhance the signal, but those studies could not account for the innate differences in nanoparticle sizes and shapes that could mask the effects. The Rice researchers had set out to minimize these other impacts to focus only on the role of spectral frequency overlap on signal enhancement. ""This study looks at what type of antenna is the best to use, because the properties of the nanoparticle dictate the spectrum and its overlap with the molecule,"" said Miranda Gallagher, a Rice postdoctoral research associate and co-author of the paper. ""Should it be round or should it have sharp edges? Should it be smaller or larger?"" In experiments, the researchers combined either gold nanospheres or sharp-tipped gold nanotriangles with a ruthenium-based dye molecule in a polymer shell that kept the molecules from migrating too far from the particles. ""That's essentially our electrode,"" Heiderscheit said. ""If we didn't have the polymer, the dye molecules would be free to move and we'd see light diffused across the sample."" With the molecules constrained by the polymer, they could clearly see molecules emitting near particles. They determined signal enhancement is controlled by a combination of size and frequency matching between the dye molecule and the nanospheres, and just frequency matching for nanotriangles. Single-molecule imaging is still a stretch for the nascent technique, Heiderscheit said. ""Essentially, we're imaging how active a surface is,"" he said. ""The Department of Energy (the main sponsor of the project) cares about this research because it could achieve super-resolution mapping of reactivity on a surface."" Super-resolution enables the capture of images below the diffraction limit of light. ""For instance, if you have nanoparticles in a battery system, you can use ECL to map where the reactions are most chemically active,"" Heiderscheit said. ""You're essentially determining what nanoparticles make a good catalyst, and we can use this tool to design better ones."" "
Science Daily,Analysis of Galileo's Jupiter Entry Probe Reveals Gaps in Heat Shield Modeling,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110659.htm,"   Researchers at the Universidade de Lisboa and the University of Illinois at Urbana-Champaign report their findings from new fluid radiative dynamics models using data transmitted from the of Galileo's 30-second entry. The paper, published in Physics of Fluids, from AIP Publishing, employs new computational techniques developed in the nearly 25 years since the mission. ""Early simulations for the probe design were conducted in the 1980s,"" said Mario Lino da Silva, an author on the paper. ""There are some things we can do in 2019, because we have the computational power, new devices, new theories and new data."" Galileo's probe entered Jupiter's gravity traveling 47.4 kilometers per second, making it one of the fastest human-made objects ever. The fireball caused by the descent warmed the carbon phenolic heat shield to temperatures hotter than the sun's surface. Data from the probe revealed the rim of the heat shield burned significantly more than even today's models would predict, measured by what is called the recession rate. ""The fireball is a kind of soup where a lot of things happen at the same time,"" he said. ""One problem with modeling is that there are many sources of uncertainty and only one observed parameter, the heat shield recession rate."" The group recalculated features of the hydrogen-helium mixture the probe passed through, such as viscosity, thermal conductivity and mass diffusion, and found the oft-cited Wilke/Blottner/Eucken transport model failed to accurately model interactions between hydrogen and helium molecules. They found the radiative heating properties of hydrogen molecules played a significant role in the additional heating the probe's heat shield experienced. ""The built-in heat shield engineering margins actually saved the spacecraft,"" Lino da Silva said. Lino da Silva hopes the work helps improve future spacecraft design, including upcoming projects to explore Neptune that will likely not reach their destinations until after he has retired. ""In a way, it's like building cathedrals or the pyramids,"" he said. ""You don't get to see the work when it's finished."" Lino da Silva next looks to validate some of the simulated findings by reproducing similar conditions in a shock-tube facility tailored for reproducing high-speed flows. "
Science Daily,Researchers Build a Soft Robot With Neurologic Capabilities,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110650.htm,"   Cunjiang Yu, Bill D. Cook Associate Professor of Mechanical Engineering at the University of Houston, said the work represents a significant step toward the development of prosthetics that could directly connect with the peripheral nerves in biological tissues, offering neurological function to artificial limbs, as well as toward advances in soft neurorobots capable of thinking and making judgments. Yu is corresponding author for a paper describing the work, published in Science Advances. He is also a principal investigator with the Texas Center for Superconductivity at the University of Houston. ""When human skin is touched, you feel it,"" Yu said to describe the human capabilities the new device can mimic. ""The feeling originates in your brain, through neural pathways from your skin to the brain."" The findings have implications for neuroprosthetics, as well as for neuromorphic computing, an emerging technology with the potential to allow high volume information processing using small amounts of energy through devices that mimic the electric behavior of neural networks. Inspired by Nature Inspired by nature, the researchers designed artificial synaptic transistors -- that is, transistors that function similarly to neurons -- which continue to work even after being stretched as much as 50%. While the resulting neurological function is less sophisticated than that exhibited by those of its living counterparts, they said it marks an important first step toward more powerful engineering systems in the future. The transistor, described by researchers as having stretching characteristics similar to those in a rubber band, exhibited functions similar to those of biological synapses, including excitatory postsynaptic potential, current, facilitation, and short-term memory and long-term memory. The soft neurorobot was equipped with a neurologically integrated tactile sensory skin, allowing it to sense the interaction with the external environment and respond accordingly. ""The neurorobot senses physical tapping and locomotes adaptively in a programmed manner through synapse memory encoded signals,"" the researchers wrote. "
Science Daily,Super Light Dampers for Low Tones,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103404.htm,"   Some of these properties are retained even if the atomic crystal structures are enlarged about 100,000,000 times and the crystals are replicated on a large scale. Physicists have been taking ad-vantage of this for several years now: If the original crystals scatter X-rays with very short wavelengths, the enlarged copies can scatter oscillations with long wavelengths in all directions. A very elegant way for vibration damping has thus been found. Enlarged crystal structures with such acoustic properties are called phononic crystals. Internally rotating crystal structures Andrea Bergamini and his team from Empa's Acoustics / Noise Reduction Department have now succeeded in integrating additional properties into the crystals that are not present in the originals. The researchers built small, rotating plates into the crystal structures, which are able to convert oscillations along the longitudinal axis into torsional movements. For the first time, an undesirable oscillation can not only be scattered in different spatial directions, but can also be converted into thermal energy. AAAA or ABAB arrangement In a next step, Bergamini and his research colleagues coupled several of the rotating disks in the crystal together. This can be done in two different ways: either all disks rotate in the same direction (isotactic arrangement) or they are alternately connected to each other with their rotational directions (syndiotactic arrangement). The effect differs dramatically: the syndiotactic ABAB arrangement of the direction of rotation creates a so-called frequency band gap. A wide range of oscillations is ""swallowed"" by the ro-tation mechanism and not passed through the crystal. On the other hand, the isotactic AAAA arrangement of the rotary motions generates new waves with similar frequencies, which are transmitted through the crystal. A mechanical component with certain geometry therefore determines whether the crystal is insulating or conducting. The team has now published the research results in the current issue of the journal Nature Communications. The ""Cryptography Window"" But how can such oscillation frequency band gaps be used? In the meantime, a first model has been developed in the laboratory showing a possible function of phononic crystals: Bergamini built a window from two Plexiglas plates in which rotating discs in syndiotactic arrangement are integrated. The size of the discs is tuned to the frequency of human speech. The idea: when certain frequencies are filtered out of speech, the spoken content becomes incomprehensible to the listener. The human brain can no longer assemble the acoustic information into a meaning. First tests in the acoustics laboratory show: the approach is very promising. You can clearly see the talking people and also hear who is talking in a muffled way. But not a single word can be understood clearly from the spoken text. Soundproofing elements that are a hundred times lighter? Bergamini and his colleagues expect that transparent, phononic crystals could be interesting for architects and interior designers. This physical trick makes it possible to produce rigid building materials with a stable shape that insulate sound very well and can be up to 100 times lighter than other phononic insulating materials which have the same effect. In mechanical engineering, aircraft construction and lightweight automotive construction, too, the filtering out of interfering frequencies with lightweight de-signer insulating materials could soon play a major role. "
Science Daily,Controlling the Charge State of Organic Molecule Quantum Dots in a 2D Nanoarray,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103353.htm,"   Molecular self-assembly on a metal results in a high-density, 2D, organic quantum-dot array with electric-field-controllable charge state, with the organic molecules used as 'nano-sized building blocks' in fabrication of functional nanomaterials. Achieved densities are an order of magnitude larger than conventional inorganic systems. The atomically-thin nanofilm consists of an ordered two-dimensional (2D) array of molecules which behave as 'zero dimensional' entities called quantum dots (QDs). This system has exciting implications for fields such as computer memory, light-emitting devices and quantum computing. The School of Physics and Astronomy study shows that a single-component, self-assembled 2D array of the organic (carbon-based) molecule dicyanoanthracene can be synthesised on a metal, such that the charge state of each molecule can be controlled individually via an applied electric field.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""This discovery would enable the fabrication of 2D arrays of individually addressable (switchable) quantum dots from the bottom-up, via self-assembly, says lead author Dhaneesh Kumar. ""We would be able to achieve densities tens of times larger than state-of-the-art, top-down synthesised inorganic systems."" QUANTUM DOTS: TINY, 'ZERO-DIMENSIONAL' POWERHOUSES Quantum dots are extremely small -- about one nanometre across (ie, a millionth of a millimetre). Because their size is similar to the wavelength of electrons, their electronic properties are radically different to conventional materials.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     In quantum dots, the motion of electrons is constrained by this extremely small scale, resulting in discrete electronic quantum energy levels. Effectively, they behave as 'zero-dimensional' (0D) objects, where the degree of occupancy (filled or empty) of their quantised electronic states determines the charge (in this study, neutral or negative) of the quantum dot. Ordered arrays of charge-controllable quantum dots can find application in computing memory as well as light-emitting devices (eg, low-energy TV or smartphone screens). Arrays of quantum dots are conventionally synthesised from inorganic materials via top-down fabrication approaches. However, using such 'top-down' approaches, it can be challenging to achieve arrays with large densities and high homogeneity (in terms of quantum-dot size and spacing). Because of their tunability and self-assembling capability, using organic (carbon-based) molecules as nano-sized building blocks can be particularly useful for the fabrication of functional nanomaterials, in particular well-defined scalable ensembles of quantum dots. THE STUDY The researchers synthesised a homogeneous, single-component, self-assembled 2D array of the organic molecule dicyanoanthracene (DCA) on a metal surface. The study was led by Monash University's Faculty of Science, with support by theory from the Monash Faculty of Engineering. This atomic-scale structural and electronic properties of this nanoscale array were studied experimentally via low-temperature scanning tunnelling microscopy (STM) and atomic force microscopy (AFM) (School of Physics and Astronomy, under Dr Agustin Schiffrin). Theoretical studies using density functional theory supported the experimental findings (Department of Material Science and Engineering, under A/Prof Nikhil Medhekar). The researchers found that the charge of individual DCA molecules in the self-assembled 2D array can be controlled (switched from neutral to negative and vice versa) by an applied electric field. This charge state electric-field-control is enabled by an effective tunneling barrier between molecule and surface (resulting from limited metal-adsorbate interactions) and a significant DCA electron affinity. Subtle, site-dependent variations of the molecular adsorption geometry were found to give rise to significant variations in the susceptibility for electric-field-induced charging. "
Science Daily,Quantum Paradox Experiment May Lead to More Accurate Clocks and Sensors,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092242.htm,"   University of Queensland physicist Dr Magdalena Zych said the international collaboration aimed to test Einstein's twin paradox using quantum particles in a 'superposition' state. ""The twin paradox is one of the most counterintuitive predictions of relativity theory,"" Dr Zych said. ""It says that time can pass at different speeds for people at different distances to an enormous mass or travelling with different velocities. ""For example, relative to a reference clock far from any massive object, a clock closer to a mass or moving at high speed will tick slower. ""This creates a 'twin paradox', where one of a pair of twins departs on a fast-speed journey while the other stays behind.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""When the twins reunite, the travelling twin would be much younger, as different amounts of time have passed for each of them. ""It's a mind-blowing effect -- featured in popular movies like Interstellar -- but it's also been verified by real world experiments, and is even taken into consideration in order for everyday GPS technology to work."" The team included researchers from the University of Ulm and Leibniz University Hannover and found how one could use advanced laser technology to realise a quantum version of the Einstein's twin paradox. In the quantum version, rather than twins there will be only one particle travelling in a quantum superposition. ""A quantum superposition means the particle is in two locations at the same time, in each of them with some probability, and yet this is different to placing the particle in one or the other location randomly,"" Dr Zych said.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""It's another way for an object to exist, only allowed by the laws of quantum physics. ""The idea is to put one particle in superposition on two trajectories with different speeds, and see if a different amount of time passes for each of them, as in the twin paradox. ""If our understanding of quantum theory and relativity is right, when the superposed trajectories meet, the quantum traveller will be in superposition of being older and younger than itself. ""This would leave an unmistakeable signature in the results of the experiment, and that's what we hope will be found when the experiment is realised in the future. ""It could lead to advanced technologies that will allow physicists to build more precise sensors and clocks -- potentially, a key part of future navigation systems, autonomous vehicles and earthquake early-warning networks."" The experiment itself will also answer some open questions in modern physics. ""A key example is, can time display quantum behaviour or is it fundamentally classical?"" Dr Zych said. ""This question is likely crucial for the 'holy grail' of theoretical physics: finding a joint theory of quantum and gravitational phenomena. ""We're looking forward to helping answer this question, and tackling many more."" "
Science Daily,Fire Blankets Can Protect Buildings from Wildfires,Matter & Energy,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015075319.htm,"   By rigorously testing different fabric materials in the laboratory and using them to shield structures that were exposed to fires of increasing magnitude, this research, published in Frontiers in Mechanical Engineering, confirms that existing blanket technology can protect structures from a short wildfire attack. For successful deployment against severe fires and in areas of high housing density, technological advancement of blanket materials and deployment methods, as well as multi-structure protection strategies, are needed. ""The whole-house fire blanket is a viable method of protection against fires at the wildland-urban interface,"" says lead study author Fumiaki Takahashi, a Professor at Case Western Reserve University, Cleveland, Ohio, USA, who teamed up with the NASA Glenn Research Center, U.S. Forest Service, New Jersey Forest Fire Service, and Cuyahoga Community College for this study. He continues, ""Current technology can protect an isolated structure against a relatively short wildfire attack and further technological developments are likely to enable this method to be applied to severe situations."" A burning need Wildfires in urban and suburban settings can have a devastating effect on communities and pose one of the greatest fire challenges of our time.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     People living and working in fire-risk areas contacted Professor Takahashi to find out if commercial products are available to help reduce the likelihood of structure ignition, which would reduce fire damage and improve public and firefighter safety. These pleas motivated the research and an initial investigation revealed that the concept of whole-structure fire blankets has been around for quite some time. ""I thought about a means to reduce wildland fire damage and found a U.S. patent 'conflagration-retardative curtain' i.e., a fire blanket, issued during World War Two. In addition, the U.S. Forest Service firefighters managed to save a historic forest cabin by wrapping it with their fire shelter materials,"" Takahashi reports. An old flame-retardant While there are anecdotal reports on the ability of fire blankets to protect buildings from fires, Takahashi's research highlighted a severe lack of scientific evidence to back up these claims. To rectify this, funded by a research grant from the U.S. Department of Homeland Security, the team conducted several experiments to test the ability of different blanket materials to shield structures against fires of increasing magnitude. ""The fire exposure tests determined how well the fire blankets protected various wooden structures, from a birdhouse in a burning room to a full-size shed in a real forest fire. We tested four types of fabric materials: aramid, fiberglass, amorphous silica, and pre-oxidized carbon, each with and without an aluminum surface. In addition, we conducted laboratory experiments under controlled heat exposure and measured the heat-insulation capabilities of these materials against direct flame contact or radiation heat."" A hot new industry    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The laboratory and real-fire assessments demonstrate that fire blankets could protect structures from a short exposure to a wildfire, but also highlight the technical limitations of their existing form. Further technological advancements are needed in the areas of material composition, deployment methods and multi-structure protection strategies. Takahashi explains, ""The fiberglass or amorphous silica fabrics laminated with aluminum foil performed best, due to high reflection/emission of radiation and good thermal insulation by the fabric. New technology is needed to enhance the fire blankets' heat-blocking capability for an extended period to prevent structure-to-structure ignition. In addition, it will be more effective If dozens or hundreds of homes are protected by such advanced fire blankets at the same time, particularly in high housing-density Wildland-Urban Interface communities."" He concludes by suggesting communities potentially affected by wildfires work together to turn the concept of whole-building fire blankets into a reality. ""Fire blanket protection will be significant to those living and fighting fires at the Wildland-Urban Interface and presents entrepreneurs and investors with business opportunities. The implication of the present findings is that the technical community, the general public, and the fire service must work together to take a step-by-step approach toward the successful application of this technology."" "
Science Daily,Unlocking the Biochemical Treasure Chest Within Microbes,Matter & Energy,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014122620.htm,"   Now, a team of microbiologists and genomicists led by the Department of Energy Joint Genome Institute (JGI) has invented a genetic engineering tool, called CRAGE, that could not only make studying these compounds much easier, but also fill significant gaps in our understanding of how microbes interact with their surroundings and evolve. Their work, a collaboration with Goethe University Frankfurt and DOE Environmental Molecular Sciences Laboratory (EMSL), is published in Nature Microbiology. Diving into microbiomes Secondary metabolites are thusly named because their activities and functions aren't essential for a microbe's survival, yet they may give the organism an advantage in the face of environmental pressures. Encoded by groups of genes called biosynthetic gene clusters (BGCs), the ability to produce these metabolites is easily passed back and forth among both closely and distantly related microbes through horizontal gene transfer. This rapid and widespread sharing allows microbes to adapt to changing conditions by quickly gaining or losing traits, and because the frequent swapping introduces mutations, horizontal gene transfer of BGCs drives the development of diverse compounds. Unfortunately, the fascinating world of secondary metabolism has traditionally been very hard to study because when microbes are brought into the lab, an artificial environment that presents little hardship or competition, they typically don't bother making these compounds. CRAGE -- short for chassis-independent recombinase-assisted genome engineering -- helps scientists get around this roadblock. ""These metabolites are like a language that microbes use to interact with their biomes, and when isolated, they go silent,"" said co-lead author Yasuo Yoshikuni, a scientist at JGI. ""We currently lack the technology to stimulate microbes into activating their BGCs and synthesizing the complete product -- a cellular process that involves many steps."" CRAGE is a highly efficient means of transplanting BGCs originating from one organism into many different potential production hosts simultaneously in order to identify microbial strains that are naturally capable of producing the secondary metabolite under laboratory conditions.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""CRAGE therefore allows us to access these compounds much more readily than before,"" said Helge Bode, co-lead author from Goethe University Frankfurt, Germany. ""In several cases, it has already enabled us to produce and characterize for the first time a compound of interest."" More broadly, by providing a technique to transfer microbial machinery from one species to another, CRAGE will enable scientists to go beyond theories and predictions and finally observe how compounds relegated to the category of ""biological dark matter"" actually work. ""This is a landmark development, because with CRAGE we can examine how different organisms can express one gene network differently, and thus how horizontally transferred capabilities can evolve. The previous tools to do this are much more limited,"" said co-author David Hoyt, a chemist at EMSL, which located at the Pacific Northwest National Laboratory. Hoyt and his colleagues Kerem Bingol and Nancy Washton helped characterize one of the previously unknown secondary metabolites produced when Yoshikuni's group tested CRAGE. Co-first author Jing Ke, a scientific engineering associate at JGI, added, ""Looking beyond secondary metabolites, CRAGE can be used to engineer microbes for the production of proteins, RNAs, and other molecules with a huge range of applications."" Next steps So far, the team has successfully transferred BGCs into 30 diverse bacterial strains, and expect that it should work in many others, though the technique will likely need to be adapted for some species. Further research and product development are currently underway, but the technique is now available to research teams who utilize JGI (a DOE Office of Science User Facility) through pilot programs.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Meanwhile, Yoshikuni -- who developed the precursor gene recombinant tool, RAGE, in 2013 -- and his JGI colleagues have begun applying CRAGE to their own projects, such as exploring unconventional bacterial hosts for biomanufacturing. ""Aside from a few very well-studied microbes, the so-called model organisms like E. coli, we don't know whether a strain will have the skills needed to perform all the steps of BGC activation,"" said Yoshikuni. ""Hopefully with CRAGE, we can start to shift that paradigm -- we can look into more wild species and find their properties that are more suitable for a production of products and medicines."" This work was supported by the DOE Office of Science, the DFG (German Research Foundation), and the LOEWE Center for Translational Biodiversity Genomics. "
Science Daily,How to Control Friction in Topological Insulators,Matter & Energy,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111735.htm,"   Thanks to their unique electrical properties, topological insulators promise many innovations in the electronics and computer industries, as well as in the development of quantum computers. The thin surface layer can conduct electricity almost without resistance, resulting in less heat than traditional materials. This makes them of particular interest for electronic components. Furthermore, in topological insulators, the electronic friction -- i.e. the electron-mediated conversion of electrical energy into heat -- can be reduced and controlled. Researchers of the University of Basel, the Swiss Nanoscience Institute (SNI) and the Istanbul Technical University have now been able to experimentally verify and demonstrate exactly how the transition from energy to heat through friction behaves -- a process known as dissipation. Measuring friction with a pendulum The team headed by Professor Ernst Meyer at the Department of Physics of the University of Basel investigated the effects of friction on the surface of a bismuth telluride topological insulator. The scientists used an atomic force microscope in pendulum mode. Here, the conductive microscope tip made of gold oscillates back and forth just above the two-dimensional surface of the topological insulator. When a voltage is applied to the microscope tip, the movement of the pendulum induces a small electrical current on the surface. In conventional materials, some of this electrical energy is converted into heat through friction. The result on the conductive surface of the topological insulator looks very different: the loss of energy through the conversion to heat is significantly reduced. ""Our measurements clearly show that at certain voltages there is virtually no heat generation caused by electronic friction,"" explains Dr. Dilek Yildiz, who carried out this work within the SNI PhD School. A novel mechanism The researchers were also able to observe for the first time a new quantum-mechanical dissipation mechanism that occurs only at certain voltages. Under these conditions, the electrons migrate from the tip through an intermediate state into the material -- similar to the tunneling effect in scanning tunneling microscopes. By regulating the voltage, the scientists were able to influence the dissipation. ""These measurements confirm the great potential of topological insulators, since electronic friction can be controlled in a targeted manner,"" adds Meyer. "
Science Daily,Scientists Pinpoint Cause of Harmful Dendrites and Whiskers in Lithium Batteries,Matter & Energy,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111723.htm,"   The team, led by Chongmin Wang at the Department of Energy's Pacific Northwest National Laboratory, has shown that the presence of certain compounds in the electrolyte -- the liquid material that makes a battery's critical chemistry possible -- prompts the growth of dendrites and whiskers. The team hopes the discovery will lead to new ways to prevent their growth by manipulating the battery's ingredients. The results were published online Oct. 14 in Nature Nanotechnology. Dendrites are tiny, rigid tree-like structures that can grow inside a lithium battery; their needle-like projections are called whiskers. Both cause tremendous harm; notably, they can pierce a structure known as the separator inside a battery, much like a weed can poke through a concrete patio or a paved road. They also increase unwanted reactions between the electrolyte and the lithium, speeding up battery failure. Dendrites and whiskers are holding back the widespread use of lithium metal batteries, which have higher energy density than their commonly used lithium-ion counterparts. The PNNL team found that the origin of whiskers in a lithium metal battery lies in a structure known as the ""SEI"" or solid-electrolyte interphase, a film where the solid lithium surface of the anode meets the liquid electrolyte. Further, the scientists pinpointed a culprit in the growth process: ethylene carbonate, an indispensable solvent added to electrolyte to enhance battery performance. It turns out that ethylene carbonate leaves the battery vulnerable to damage. Catching fast-moving action inside lithium batteries The team's findings include videos that show the step-by-step growth of a whisker inside a nanosized lithium metal battery specially designed for the study.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     A dendrite begins when lithium ions start to clump, or ""nucleate,"" on the surface of the anode, forming a particle that signifies the birth of a dendrite. The structure grows slowly as more and more lithium atoms glom on, growing the same way that a stalagmite grows from the floor of a cave. The team found that the energy dynamics on the surface of the SEI push more lithium ions into the slowly growing column. Then, suddenly, a whisker shoots forth. It wasn't easy for the team to capture the action. To do so, scientists integrated an atomic force microscope (AFM) and an environmental transmission electron microscope (ETEM), a highly prized instrument that allows scientists to study an operating battery under real conditions. The team used the AFM to measure the tiny force of the whisker as it grew. Much like a physician measures a patient's hand strength by asking the patient to push upward against the doctor's outstretched hands, the PNNL team measured the force of the growing whisker by pushing down on its tip with the cantilever of the AFM and measuring the force the dendrite exerted during its growth. The recipe for electrolyte  The team found that the level of ethylene carbonate directly correlates with dendrite and whisker growth. The more of the material the team put in the electrolyte, the more the whiskers grew. The scientists experimented with the electrolyte mix, changing ingredients in an effort to reduce dendrites. Some changes, such as the addition of cyclohexanone, prevented the growth of dendrites and whiskers. ""We don't want to simply suppress the growth of dendrites; we want to get to the root cause and eliminate them,"" said Wang, a corresponding author of the paper along with Wu Xu. ""We drew upon the expertise of our colleagues who have e xpertise in electrochemistry. My hope is that our findings will spur the community to look at this problem in new ways. Clearly, more research is needed."" Understanding what causes whiskers to start and grow will lead to new ideas for eliminating them or at least controlling them to minimize damage, added first author Yang He. He and the team tracked how whiskers respond to an obstacle, either buckling, yielding, kinking, or stopping. A greater understanding could help clear the path for the broad use of lithium metal batteries in electric cars, laptops, mobile phones, and other areas. "
Science Daily,Analysis of Galileo's Jupiter Entry Probe Reveals Gaps in Heat Shield Modeling,Space & Time,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110659.htm,"   Researchers at the Universidade de Lisboa and the University of Illinois at Urbana-Champaign report their findings from new fluid radiative dynamics models using data transmitted from the of Galileo's 30-second entry. The paper, published in Physics of Fluids, from AIP Publishing, employs new computational techniques developed in the nearly 25 years since the mission. ""Early simulations for the probe design were conducted in the 1980s,"" said Mario Lino da Silva, an author on the paper. ""There are some things we can do in 2019, because we have the computational power, new devices, new theories and new data."" Galileo's probe entered Jupiter's gravity traveling 47.4 kilometers per second, making it one of the fastest human-made objects ever. The fireball caused by the descent warmed the carbon phenolic heat shield to temperatures hotter than the sun's surface. Data from the probe revealed the rim of the heat shield burned significantly more than even today's models would predict, measured by what is called the recession rate. ""The fireball is a kind of soup where a lot of things happen at the same time,"" he said. ""One problem with modeling is that there are many sources of uncertainty and only one observed parameter, the heat shield recession rate."" The group recalculated features of the hydrogen-helium mixture the probe passed through, such as viscosity, thermal conductivity and mass diffusion, and found the oft-cited Wilke/Blottner/Eucken transport model failed to accurately model interactions between hydrogen and helium molecules. They found the radiative heating properties of hydrogen molecules played a significant role in the additional heating the probe's heat shield experienced. ""The built-in heat shield engineering margins actually saved the spacecraft,"" Lino da Silva said. Lino da Silva hopes the work helps improve future spacecraft design, including upcoming projects to explore Neptune that will likely not reach their destinations until after he has retired. ""In a way, it's like building cathedrals or the pyramids,"" he said. ""You don't get to see the work when it's finished."" Lino da Silva next looks to validate some of the simulated findings by reproducing similar conditions in a shock-tube facility tailored for reproducing high-speed flows. "
Science Daily,Astronomers Use Giant Galaxy Cluster as X-Ray Magnifying Lens,Space & Time,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111718.htm,"   While galaxy clusters have been used to magnify objects at optical wavelengths, this is the first time scientists have leveraged these massive gravitational giants to zoom in on extreme, distant, X-ray-emitting phenomena. What they detected appears to be a blue speck of an infant galaxy, about 1/10,000 the size of our Milky Way, in the midst of churning out its first stars -- supermassive, cosmically short-lived objects that emit high-energy X-rays, which the researchers detected in the form of a bright blue arc. ""It's this little blue smudge, meaning it's a very small galaxy that contains a lot of super-hot, very massive young stars that formed recently,"" says Matthew Bayliss, a research scientist in MIT's Kavli Institute for Astrophysics and Space Research. ""This galaxy is similar to the very first galaxies that formed in the universe ... the kind of which no one has ever seen in X-ray in the distant universe before."" Bayliss says the detection of this single, distant galaxy is proof that scientists can use galaxy clusters as natural X-ray magnifiers, to pick out extreme, highly energetic phenomena in the universe's early history. ""With this technique, we could, in the future, zoom in on a distant galaxy and age-date different parts of it -- to say, this part has stars that formed 200 million years ago, versus another part that formed 50 million years ago, and pick them apart in a way you cannot otherwise do,"" says Bayliss, who will be moving on to the University of Cincinnati as an assistant professor of physics.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     He and his co-authors, including Michael McDonald, assistant professor of physics at MIT, have published their results in the journal Nature Astronomy. A candle in the light Galaxy clusters are the most massive objects in the universe, composed of thousands of galaxies, all bound together by gravity as one enormous, powerful force. Galaxy clusters are so massive, and their gravitational pull is so strong, that they can distort the fabric of space-time, bending the universe and any surrounding light, much like an elephant would stretch and warp a trapeze net. Scientists have used galaxy clusters as cosmic magnifying glasses, with a technique known as gravitational lensing. The idea is that if scientists can approximate the mass of a galaxy cluster, they can estimate its gravitational effects on any surrounding light, as well as the angle at which a cluster may deflect that light. For instance, imagine if an observer, facing a galaxy cluster, were trying to detect an object, such as a single galaxy, behind that cluster. The light emitted by that object would travel straight toward the cluster, then bend around the cluster. It would continue traveling toward the observer, though at slightly different angles, appearing to the observer as mirrored images of the same object, which in the end can be combined as a single, ""magnified"" image.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Scientists have used galaxy clusters to magnify objects at optical wavelengths, but never in the X-ray band of the electromagnetic spectrum, mainly because galaxy clusters themselves emit an enormous amount of X-rays. Scientists have thought that any X-rays coming from a background source would be impossible to discern from the cluster's own glare. ""If you're trying to see an X-ray source behind a cluster, it's like trying to see a candle next to a really bright light,"" Bayliss says. ""So we knew this was a challenging measurement to make."" X-ray subtraction The researchers wondered: Could they subtract that bright light and see the candle behind it? In other words, could they remove the X-ray emissions coming from the galaxy cluster, to view the much fainter X-rays coming from an object, behind and magnified by the cluster? The team tested this idea with observations taken by NASA's Chandra X-ray Observatory, one of the world's most powerful X-ray space telescopes. They looked in particular at Chandra's measurements of the Phoenix cluster, a distant galaxy cluster located 5.7 billion light-years from Earth, which has been estimated to be about a quadrillion times as massive as the sun, with gravitational effects that should make it a powerful, natural magnifying lens. ""The idea is to take whatever your best X-ray telescope is -- in this case, Chandra -- and use a natural lens to magnify and effectively make Chandra bigger, so you can see more distant things,"" Bayliss says. He and his colleagues analyzed observations of the Phoenix cluster, taken continuously by Chandra for over a month. They also looked at images of the cluster taken by two optical and infrared telescopes -- the Hubble Space Telescope and the Magellan telescope in Chile. With all these various views, the team developed a model to characterize the cluster's optical effects, which allowed the researchers to precisely measure the X-ray emissions from the cluster itself, and subtract it from the data. They were left with two similar patterns of X-ray emissions around the cluster, which they determined were ""lensed,"" or gravitationally bent, by the cluster. When they traced the emissions backward in time, they found that they all originated from a single, distant source: a tiny dwarf galaxy from 9.4 billion years ago, when the universe itself was roughly 4.4 billion years old -- about a third of its current age. ""Previously, Chandra had seen only a handful of things at this distance,"" Bayliss says. ""In less than 10 percent of the time, we discovered this object, similarly far away. And gravitational lensing is what let us do it."" The combination of Chandra and the Phoenix cluster's natural lensing power enabled the team to see the tiny galaxy hiding behind the cluster, magnified about 60 times. At this resolution, they were able to zoom in to discern two distinct clumps within the galaxy, one producing many more X-rays than the other. As X-rays are typically produced during extreme, short-lived phenomena, the researchers believe that the first X-ray-rich clump signals a part of the dwarf galaxy that has very recently formed supermassive stars, while the quieter region is an older region that contains more mature stars. ""We're catching this galaxy at a very useful stage, where it's got these really young stars,"" Bayliss says. ""Every galaxy had to start out in this phase, but we don't see a lot of these kinds of galaxies in our own neighborhood. Now we can go back in time, look in the distant universe, find galaxies in this early phase of their life, and start to study how star formation is different there."" This research was funded, in part, by NASA, and by the Space Telescope Science Institute. "
Science Daily,Sharing Data for Improved Forest Protection and Monitoring,Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010125619.htm,"   Forest biomass is an essential indicator for monitoring the Earth's ecosystems and climate. It also provides critical input to greenhouse gas accounting, estimation of carbon losses and forest degradation, assessment of renewable energy potential, and for developing climate change mitigation policies. Although satellite remote sensing technology now allows researchers to produce extensive maps of aboveground biomass, these maps still require reliable, up-to-date, on-site data for calibration and validation. Collecting data in the field by measuring trees and documenting species is, however, a very labor intensive, expensive, and time-consuming exercise and it would therefore make sense to bring together the many extant data sets to provide real added value for a number of applications. In terms of policy applications, doing so can also lead to improved biomass products and better monitoring of forest resources, which could in turn lead to more effective forest protection measures. In a new paper published in the journal Scientific Data, 143 researchers involved in this type of data collection in the field, explored whether it was possible to build a network that openly shares their data on biomass for the benefit of different communities. They particularly wanted to see if they could bring together as much on-site data on biomass as possible to prepare for new satellite missions, such as the European Space Agency's BIOMASS mission, with a view to improving the accuracy of current remote sensing based products, and developing new synergies between remote sensing and ground-based ecosystem research communities. Their efforts have resulted in the establishment of the Forest Observation System (FOS) -- an international, collaborative initiative that aims to establish a global on-site forest aboveground biomass database to support Earth Observation and to encourage investment in relevant field-based measurements and research. ""Keeping in mind that this paper is a data descriptor and not a conventional paper with hypotheses, the whole idea behind this study is a new open database on biomass data. This is important for the following reasons: First, it represents a way to link the ecological/forestry and remote sensing communities. It also overcomes existing data sharing barriers, while promoting data sharing beyond small, siloed communities. Lastly, it provides recognition to the people working in the field, including those who collect the data, which is why there are 143 coauthors on this paper, as they are all contributors to the database,"" explains study lead author Dmitry Shchepashchenko, a researcher in the IIASA Ecosystems Services and Management Program. The researchers collected data from 1,645 permanent forest sample plots from 274 locations distributed around the globe. This data has now been made available for download via the FOS website. The initiative represents the first attempt at bringing this type of data together from different networks in a single location. The researchers point out that their work in this regard is ongoing and there are plans to continue adding more data sets and networks to the FOS. In addition to promoting data sharing, the system also promotes a new leading network on biomass data (through the FOS), which IIASA is leading and will continue to grow into the future. Apart from the obvious benefits that data sharing hold for the scientific community, the data are also essential for training various models at IIASA such as the BioGeoChemistry Management Model (BGC-MAN) and the Global Forest Model (G4M). Several on-going IIASA projects, as well as other ecological-, biophysical-, and economic models and projects outside of IIASA will also benefit, which means that providing access to the data can improve models and understanding of biomass more generally. ""A great deal of effort has gone into collecting forest data in the past, but people working in the field (ecologists and forestry scientists) hardly ever share the collected data, or if they do, they share it only within ecological networks. The data are valuable not only for ecology, but also for remote sensing calibration and validation, in other words, to train algorithms that create biomass maps, and for assessing the accuracy of the products along with inputs to a variety of models. This piece of work represents a real step forward in sharing a very valuable biomass data set,"" concludes IIASA researcher Linda See, who was also a study coauthor. "
Science Daily,Milky Way Raids Intergalactic 'Bank Accounts',Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010125615.htm,"   ""We expected to find the Milky Way's books balanced, with an equilibrium of gas inflow and outflow, but 10 years of Hubble ultraviolet data has shown there is more coming in than going out,"" said astronomer Andrew Fox of the Space Telescope Science Institute, Baltimore, Maryland, lead author of the study to be published in the Astrophysical Journal. Fox said that, for now, the source of the excess inflowing gas remains a mystery. One possible explanation is that new gas could be coming from the intergalactic medium. But Fox suspects the Milky Way is also raiding the gas ""bank accounts"" of its small satellite galaxies, using its considerably greater gravitational pull to siphon away their resources. Additionally, this survey, while galaxy-wide, looked only at cool gas, and hotter gas could play a role, too. The new study reports the best measurements yet for how fast gas flows in and out of the Milky Way. Prior to this study, astronomers knew that the galactic gas reserves are replenished by inflow and depleted by outflow, but they did not know the relative amounts of gas coming in compared to going out. The balance between these two processes is important because it regulates the formation of new generations of stars and planets. Astronomers accomplished this survey by collecting archival observations from Hubble's Cosmic Origins Spectrograph (COS), which was installed on the telescope by astronauts in 2009 during its last servicing mission. Researchers combed through the Hubble archives, analyzing 200 past ultraviolet observations of the diffuse halo that surrounds the disk of our galaxy. The decade's worth of detailed ultraviolet data provided an unprecedented look at gas flow across the galaxy and allowed for the first galaxy-wide inventory. The gas clouds of the galactic halo are only detectable in ultraviolet light, and Hubble is specialized to collect detailed data about the ultraviolet universe.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""The original Hubble COS observations were taken to study the universe far beyond our galaxy, but we went back to them and analyzed the Milky Way gas in the foreground. It's a credit to the Hubble archive that we can use the same observations to study both the near and the more distant universe. Hubble's resolution allows us to simultaneously study local and remote celestial objects,"" noted Rongmon Bordoloi of North Carolina State University in Raleigh, North Carolina, a co-author on the paper. Because the galaxy's gas clouds are invisible, Fox's team used light from background quasars to detect these clouds and their motion. Quasars, the cores of active galaxies powered by well-fed black holes, shine like brilliant beacons across billions of light-years. When the quasar's light reaches the Milky Way, it passes through the invisible clouds. The gas in the clouds absorbs certain frequencies of light, leaving telltale fingerprints in the quasar light. Fox singled out the fingerprint of silicon and used it to trace the gas around the Milky Way. Outflowing and inflowing gas clouds were distinguished by the Doppler shift of the light passing through them -- approaching clouds are bluer, and receding clouds are redder. Currently, the Milky Way is the only galaxy for which we have enough data to provide such a full accounting of gas inflow and outflow. ""Studying our own galaxy in detail provides the basis for understanding galaxies across the universe, and we have realized that our galaxy is more complicated than we imagined,"" said Philipp Richter of the University of Potsdam in Germany, another co-author on the study. Future studies will explore the source of the inflowing gas surplus, as well as whether other large galaxies behave similarly. Fox noted that there are now enough COS observations to conduct an audit of the Andromeda galaxy (M31), the closest large galaxy to the Milky Way. The Hubble Space Telescope is a project of international cooperation between ESA (the European Space Agency) and NASA. NASA's Goddard Space Flight Center in Greenbelt, Maryland, manages the telescope. The Space Telescope Science Institute (STScI) in Baltimore, Maryland, conducts Hubble science operations. STScI is operated for NASA by the Association of Universities for Research in Astronomy in Washington, D.C. "
Science Daily,The Milky Way Kidnapped Several Tiny Galaxies from Its Neighbor,Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010113203.htm,"   For example, more than 50 discovered satellite galaxies orbit our own galaxy, the Milky Way. The largest of these is the Large Magellanic Cloud, or LMC, a large dwarf galaxy that resembles a faint cloud in the Southern Hemisphere night sky. A team of astronomers led by scientists at the University of California, Riverside, has discovered that several of the small -- or ""dwarf"" -- galaxies orbiting the Milky Way were likely stolen from the LMC, including several ultrafaint dwarfs, but also relatively bright and well-known satellite galaxies, such as Carina and Fornax. The researchers made the discovery by using new data gathered by the Gaia space telescope on the motions of several nearby galaxies and contrasting this with state-of-the-art cosmological hydrodynamical simulations. The UC Riverside team used the positions in the sky and the predicted velocities of material, such as dark matter, accompanying the LMC, finding that at least four ultrafaint dwarfs and two classical dwarfs, Carina and Fornax, used to be satellites of the LMC. Through the ongoing merger process, however, the more massive Milky Way used its powerful gravitational field to tear apart the LMC and steal these satellites, the researchers report. ""These results are an important confirmation of our cosmological models, which predict that small dwarf galaxies in the universe should also be surrounded by a population of smaller fainter galaxy companions,"" said Laura Sales, an assistant professor of physics and astronomy, who led the research team. ""This is the first time that we are able to map the hierarchy of structure formation to such faint and ultrafaint dwarfs."" The findings have important implications for the total mass of the LMC and also on the formation of the Milky Way.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""If so many dwarfs came along with the LMC only recently, that means the properties of the Milky Way satellite population just 1 billion years ago were radically different, impacting our understanding of how the faintest galaxies form and evolve,"" Sales said. Study results appear in the November 2019 issue of the Monthly Notices of the Royal Astronomical Society. Dwarf galaxies are small galaxies that contain anywhere from a few thousand to a few billion stars. The researchers used computer simulations from the Feedback In Realistic Environments project to show the LMC and galaxies similar to it host numerous tiny dwarf galaxies, many of which contain no stars at all -- only dark matter, a type of matter scientists think constitutes the bulk of the universe's mass. ""The high number of tiny dwarf galaxies seems to suggest the dark matter content of the LMC is quite large, meaning the Milky Way is undergoing the most massive merger in its history, with the LMC, its partner, bringing in as much as one third of the mass in the Milky Way's dark matter halo -- the halo of invisible material that surrounds our galaxy,"" said Ethan Jahn, the first author of the paper and a graduate student in Sales' research group. Jahn explained that the number of tiny dwarf galaxies the LMC hosts may be higher than astronomers previously estimated, and that many of these tiny satellites have no stars.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""Small galaxies are hard to measure, and it's possible that some already-known ultrafaint dwarf galaxies are in fact associated with the LMC,"" he said. ""It's also possible that we will discover new ultrafaints that are associated with the LMC."" Dwarf galaxies can either be satellites of larger galaxies, or they can be ""isolated,"" existing on their own and independent of any larger object. The LMC used to be isolated, Jahn explained, but it was captured by the gravity of the Milky Way and is now its satellite. ""The LMC hosted at least seven satellite galaxies of its own, including the Small Magellanic Cloud in the Southern Sky, prior to them being captured by the Milky Way,"" he said. Next, the team will study how the satellites of LMC-sized galaxies form their stars and how that relates to how much dark matter mass they have. ""It will be interesting to see if they form differently than satellites of Milky Way-like galaxies,"" Jahn said. "
Science Daily,Light My Fire: How to Startup Fusion Devices Every Time,Space & Time,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010095809.htm,"   Physicists at the U.S. Department of Energy's (DOE) Princeton Plasma Physics Laboratory (PPPL), working with researchers at the Culham Centre for Fusion Energy (CCFE) in the United Kingdom, have constructed a simulation framework for developing and testing the plasma startup recipes for the National Spherical Torus Experiment-Upgrade (NSTX-U) at PPPL and the Mega Ampere Spherical Tokamak-Upgrade (MAST-U) at CCFE. ""This is a tool to help an operator design a successful startup recipe before sitting down in the driver seat at NSTX-U or MAST-U,"" said physicist Devon Battaglia, who leads the team of operators on the NSTX-U experiment and is lead author of a paper describing the model in the journal Nuclear Fusion. Fusing plasma particles Fusion fuses plasma particles to release massive amounts of energy. Scientists around the world are seeking to replicate the celestial process to produce a safe, clean, and virtually inexhaustible supply of power for generating electricity. The typical recipe for forming a plasma in magnetic fusion devices called tokamaks begins by applying voltage across a gas injected into a strong magnetic field. The gas becomes plasma within a few milliseconds and quickly heats up to millions of degrees. Creating the best recipe for a successful startup calls for finely tuning the gas pressure with a consistent evolution of the electric and magnetic fields, a delicate task that falls to the operator. The new simulation capability enables operators to quickly achieve that balance, significantly reducing the amount of time spent running experiments to find a recipe that works. Researchers derived and validated the models in the simulation framework against data collected from past experiments on the NSTX-U and its predecessor, and the predecessor of MAST-U. Battaglia worked closely with physicists at CCFE to develop the new model, making the paper a joint effort, and will travel there again for the scheduled startup of MAST-U. ""Plasma breakdown is a key milestone for MAST-U and Devon's work provides valuable insight into the best route to achieve startup,"" said physicist Andrew Thornton, lead operator at MAST-U and coauthor of the paper. ""Having Devon's expertise on site when we restart will be immensely valuable as he has performed similar experiments on NSTX-U that can guide efforts on MAST-U."" Providing new insights Development of the model provides new insights into the startup of spherical tokamaks such as NSTX-U and MAST-U, which are shaped like cored apples rather than the doughnut-like shape of more widely used conventional tokamaks. The process of putting together the simulation framework has also contributed to efforts to develop computational tools for the first operation of ITER, the international tokamak under construction in France to demonstrate the practicality of fusion energy. "
Science Daily,How Do the Strongest Magnets in the Universe Form?,Space & Time,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009131744.htm,"   Our Universe is threaded by magnetic fields. The Sun, for example, has an envelope in which convection continuously generates magnetic fields. ""Even though massive stars have no such envelopes, we still observe a strong, large-scale magnetic field at the surface of about ten percent of them,"" explains Dr Fabian Schneider from the Centre for Astronomy of Heidelberg University, who is the first author of the study in ""Nature."" Although such fields were already discovered in 1947, their origin has remained elusive so far. Over a decade ago, scientists suggested that strong magnetic fields are produced when two stars collide. ""But until now, we weren't able to test this hypothesis because we didn't have the necessary computational tools,"" says Dr Sebastian Ohlmann from the computing centre of the Max Planck Society in Garching near Munich. This time, the researchers used the AREPO code, a highly dynamic simulation code running on compute clusters of the Heidelberg Institute for Theoretical Studies (HITS), to explain the properties of Tau Scorpii (τ Sco), a magnetic star located 500 light years from Earth. Already in 2016, Fabian Schneider and Philipp Podsiadlowski from the University of Oxford realised that τ Sco is a so-called blue straggler. Blue stragglers are the product of merged stars. ""We assume that Tau Scorpii obtained its strong magnetic field during the merger process,"" explains Prof. Dr Philipp Podsiadlowski. Through its computer simulations of τ Sco, the German-British research team has now demonstrated that strong turbulence during the merger of two stars can create such a field. Stellar mergers are relatively frequent: Scientists assume that about ten percent of all massive stars in the Milky Way are the products of such processes. This is in good agreement with the occurrence rate of magnetic massive stars, according to Dr Schneider. Astronomers think that these very stars could form magnetars when they explode in supernovae. This may also happen to τ Sco when it explodes at the end of its life. The computer simulations suggest that the magnetic field generated would be sufficient to explain the exceptionally strong magnetic fields in magnetars. ""Magnetars are thought to have the strongest magnetic fields in the Universe -- up to one hundred million times stronger than the strongest magnetic field ever produced by humans,"" says Prof. Dr Friedrich Röpke from HITS. The research was funded by the Oxford Hintze Centre for Astrophysical Surveys and the Klaus Tschira Foundation (Heidelberg). "
Science Daily,Liquifying a Rocky Exoplanet,Space & Time,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009095830.htm,"   Rocky planets are built from the leftovers of the leftovers. ""Everything that doesn't make its way into the central star or a giant planet has the potential to end up forming a much smaller terrestrial planet,"" says Bower: ""We have reason to believe that processes occurring during the baby years of a planet's life are fundamental in determining its life path."" Therefore, Bower and a team of post-docs -- dominantly from within the PlanetS network -- were intrigued to uncover the observable nature of such a planet. Their study is now published in the journal Astronomy & Astrophysics. It shows that a molten Earth would actually be around 5% larger in radius than a solid Earth, and this is due to the difference in the behavior of molten versus solid materials at the extreme conditions of a planetary interior. ""In essence, a molten silicate occupies more volume than its equivalent solid, and this increases the size of the planet,"" Bower explains. A difference that CHEOPS can detect In the characterization of exoplanets outside our solar system and the search for potentially habitable worlds, researchers at the University of Bern are among the world leaders. Although detection of a rocky planet around a bright Sun-like star will remain beyond reach at least until the launch of the PLATO space mission in 2026, Earth-size planets around cooler and smaller stars such as the red dwarfs Trappist-1 or Proxima b are now set to take center stage. Interestingly, 5% difference in planetary radii can be measured with current and future observational facilities, notably the space telescope CHEOPS which was developed and assembled in Bern and will launch later this year. Indeed, the latest exoplanet data already provides an inkling that low mass molten planets, sustained by intense star-light, are present in the exoplanet catalogue. Some exoplanets could therefore be Earth-like in terms of similar building blocks, yet have different amounts of solid and molten rock to explain observed variations in planet size. ""They do not necessarily need to be made of exotic light materials to explain the data,"" says Bower. However, even a totally molten planet may not be able to explain the observation of the most extreme low density planets. But on this the research team also has a proposition: Molten planets early in their history can outgas large atmospheres of volatile species that were originally trapped inside the magma in the interior of the planet. This could explain an additional decrease in the observed planetary density. The James Webb Space Telescope (JWST) should be able to distinguish such an outgassed atmosphere on a planet around a cool red dwarf if it is dominated by either water or carbon dioxide. In addition to the consequences for observations, Bower, with his roots as an Earth Scientist, sees his study in a broader context: ""Clearly, we can never observe our own Earth in its history when it was also hot and molten. But interestingly, exoplanetary science is opening the door for observations of early Earth and early Venus analogues that could greatly impact our understanding of Earth and the Solar System planets. Thinking about Earth in the context of exoplanets, and vice-versa, offers new opportunities for understanding planets both within and beyond the Solar System."" "
Science Daily,Physicists Have Found a Way to 'Hear' Dark Matter,Space & Time,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009093942.htm,"   Dark matter is a mysterious substance that makes up 85% of the matter in the universe. Originally introduced to explain why the Strong Force (which holds together protons and neutrons) is the same backwards and forwards in time, the so called axion would provide a natural explanation for dark matter. Rather than discrete particles, axion dark matter would form a pervasive wave flowing throughout space. The axion is one of the best explanations for dark matter but has only recently been the focus of large scale experimental effort. Due to this renaissance there has been a rush to come up with new ideas for how to look for the axion in all the areas where it could be hiding. ""Finding the axion is a bit like tuning a radio: you have to tune your antenna until you pick up the right frequency. Rather than music, experimentalists would be rewarded with 'hearing' the dark matter that the Earth is travelling through. Despite being well motivated, axions have been experimentally neglected during the three decades since they were named by coauthor Frank Wilczek,"" says Dr. Alexander Millar, Postdoctor at the Department of Physics, Stockholm University, and author of the study. The key insight of the research team's new study is that inside a magnetic field axions would generate a small electric field that could be used to drive oscillations in the plasma. A plasma is a material where charged particles, such as electrons, can flow freely as a fluid. These oscillations amplify the signal, leading to a better ""axion radio."" Unlike traditional experiments based on resonant cavities, there is almost no limit on how large these plasmas can be, thus giving a larger signal. The difference is somewhat like the difference between a walkie talkie and a radio broadcast tower. ""Without the cold plasma, axions cannot efficiently convert into light. The plasma plays a dual role, both creating an environment which allows for efficient conversion, and providing a resonant plasmon to collect the energy of the converted dark matter,"" says Dr. Matthew Lawson, Postdoctor at the Department of Physics, Stockholm University, also author of the study. ""This is totally a new way to look for dark matter, and will help us search for one of the strongest dark matter candidates in areas that are just completely unexplored. Building a tuneable plasma would allow us to make much larger experiments than traditional techniques, giving much stronger signals at high frequencies,"" says Dr. Alexander Millar. To tune this ""axion radio"" the authors propose using something called a ""wire metamaterial,"" a system of wires thinner than hair that can be moved to change the characteristic frequency of the plasma. Inside a large, powerful magnet, similar to those used in Magnetic Resonance Imaging machines in hospitals, a wire metamaterial turns into a very sensitive axion radio. Searching for dark matter with plasmas will not remain just an interesting idea. In close collaboration with the researchers, an experimental group at Berkeley has been doing research and development on the concept with the intent of building such an experiment in the near future. ""Plasma haloscopes are one of the few ideas that could search for axions in this parameter space. The fact that the experimental community has latched onto this idea so quickly is very exciting and promising for building a full scale experiment,"" says Dr. Alexander Millar. "
Science Daily,Pressure Runs High at Edge of Solar System,Space & Time,2019-10-08,-,https://www.sciencedaily.com/releases/2019/10/191008115915.htm,"   Using observations of galactic cosmic rays -- a type of highly energetic particle -- from NASA's Voyager spacecraft scientists calculated the total pressure from particles in the outer region of the solar system, known as the heliosheath. At nearly 9 billion miles away, this region is hard to study. But the unique positioning of the Voyager spacecraft and the opportune timing of a solar event made measurements of the heliosheath possible. And the results are helping scientists understand how the Sun interacts with its surroundings. ""In adding up the pieces known from previous studies, we found our new value is still larger than what's been measured so far,"" said Jamie Rankin, lead author on the new study and astronomer at Princeton University in New Jersey. ""It says that there are some other parts to the pressure that aren't being considered right now that could contribute."" On Earth we have air pressure, created by air molecules drawn down by gravity. In space there's also a pressure created by particles like ions and electrons. These particles, heated and accelerated by the Sun create a giant balloon known as the heliosphere extending millions of miles out past Pluto. The edge of this region, where the Sun's influence is overcome by the pressures of particles from other stars and interstellar space, is where the Sun's magnetic influence ends. (Its gravitational influence extends much farther, so the solar system itself extends farther, as well.) In order to measure the pressure in the heliosheath, the scientists used the Voyager spacecraft, which have been travelling steadily out of the solar system since 1977. At the time of the observations, Voyager 1 was already outside of the heliosphere in interstellar space, while Voyager 2 still remained in the heliosheath. ""There was really unique timing for this event because we saw it right after Voyager 1 crossed into the local interstellar space,"" Rankin said. ""And while this is the first event that Voyager saw, there are more in the data that we can continue to look at to see how things in the heliosheath and interstellar space are changing over time."" The scientists used an event known as a global merged interaction region, which is caused by activity on the Sun. The Sun periodically flares up and releases enormous bursts of particles, like in coronal mass ejections. As a series of these events travel out into space, they can merge into a giant front, creating a wave of plasma pushed by magnetic fields. When one such wave reached the heliosheath in 2012, it was spotted by Voyager 2. The wave caused the number of galactic cosmic rays to temporarily decrease. Four months later, the scientists saw a similar decrease in observations from Voyager 1, just across the solar system's boundary in interstellar space. Knowing the distance between the spacecraft allowed them to calculate the pressure in the heliosheath as well as the speed of sound. In the heliosheath sound travels at around 300 kilometers per second -- a thousand times faster than it moves through air. The scientists noted that the change in galactic cosmic rays wasn't exactly identical at both spacecraft. At Voyager 2 inside the heliosheath, the number of cosmic rays decreased in all directions around the spacecraft. But at Voyager 1, outside the solar system, only the galactic cosmic rays that were traveling perpendicular to the magnetic field in the region decreased. This asymmetry suggests that something happens as the wave transmits across the solar system's boundary. ""Trying to understand why the change in the cosmic rays is different inside and outside of the heliosheath remains an open question,"" Rankin said. Studying the pressure and sound speeds in this region at the boundary of the solar system can help scientists understand how the Sun influences interstellar space. This not only informs us about our own solar system, but also about the dynamics around other stars and planetary systems. "
Science Daily,Researchers Build a Soft Robot With Neurologic Capabilities,Computers & Math,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015110650.htm,"   Cunjiang Yu, Bill D. Cook Associate Professor of Mechanical Engineering at the University of Houston, said the work represents a significant step toward the development of prosthetics that could directly connect with the peripheral nerves in biological tissues, offering neurological function to artificial limbs, as well as toward advances in soft neurorobots capable of thinking and making judgments. Yu is corresponding author for a paper describing the work, published in Science Advances. He is also a principal investigator with the Texas Center for Superconductivity at the University of Houston. ""When human skin is touched, you feel it,"" Yu said to describe the human capabilities the new device can mimic. ""The feeling originates in your brain, through neural pathways from your skin to the brain."" The findings have implications for neuroprosthetics, as well as for neuromorphic computing, an emerging technology with the potential to allow high volume information processing using small amounts of energy through devices that mimic the electric behavior of neural networks. Inspired by Nature Inspired by nature, the researchers designed artificial synaptic transistors -- that is, transistors that function similarly to neurons -- which continue to work even after being stretched as much as 50%. While the resulting neurological function is less sophisticated than that exhibited by those of its living counterparts, they said it marks an important first step toward more powerful engineering systems in the future. The transistor, described by researchers as having stretching characteristics similar to those in a rubber band, exhibited functions similar to those of biological synapses, including excitatory postsynaptic potential, current, facilitation, and short-term memory and long-term memory. The soft neurorobot was equipped with a neurologically integrated tactile sensory skin, allowing it to sense the interaction with the external environment and respond accordingly. ""The neurorobot senses physical tapping and locomotes adaptively in a programmed manner through synapse memory encoded signals,"" the researchers wrote. "
Science Daily,Diversity May Be Key to Reducing Errors in Quantum Computing,Computers & Math,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015103411.htm,"   Unlike conventional computers, the processing in quantum-based machines is noisy, which produces error rates dramatically higher than those of silicon-based computers. So quantum operations are repeated thousands of times to make the correct answer stands out statistically from all the wrong ones. But running the same operation over and over again on the same qubit set may just generate the same incorrect answers that can appear statistically to be the correct answer. The solution, according to researchers at the Georgia institute of Technology, is to repeat the operation on different qubit sets that have different error signatures - and therefore won't produce the same correlated errors. ""The idea here is to generate a diversity of errors so you are not seeing the same error again and again,"" said Moinuddin Qureshi, a professor in Georgia Tech's School of Electrical and Computer Engineering, who worked out the technique with his senior Ph.D. student, Swamit Tannu. ""Different qubits tend to have different error signatures. When you combine the results from diverse sets, the right answer appears even though each of them individually did not get the right answer,"" said Tannu. Tannu compares the technique, known as Ensemble of Diverse Mappings (EDM), to the game show Who Wants to be a Millionaire. Contestants who aren't sure of the answer to a multiple choice question can ask the studio audience for help. ""It's not necessary that the majority of the people in the audience know the right answer,"" Qureshi said. ""If even 20% know it, you can identify it. If the answers go equally in the four buckets from the people who don't know, the right answer will get 40% and you can select it even if only a relatively small number of people get it right."" Experiments with an existing Noisy Intermediate Scale Quantum (NISQ) computer showed that EDM improves the inference quality by 2.3 times compared to state-of-the-art mapping algorithms. By combining the output probability distributions of the diverse ensemble, EDM amplifies the correct answer by suppressing the incorrect ones.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The EDM technique, Tannu admits, is counterintuitive. Qubits can be ranked according to their error rate on specific types of problems, and the most logical course of action might be to use the set that's most accurate. But even the best qubits produce errors, and those errors are likely to be the same when the operation is done thousands of times. Choosing qubits with different error rates - and therefore different types of error - guards against that by ensuring that the one correct answer will rise above the diversity of errors. ""The goal of the research is to create several different versions of the program, each of which can make a mistake, but they will not make identical mistakes,"" Tannu explained. ""As long as they make diverse mistakes, when you average things out, the mistakes get canceled out and the right answer emerges."" Qureshi compares the EDM technique to team-building techniques promoted by human resource consultants. ""If you form a team of experts with identical backgrounds, all of them may have the same blind spot,"" he said, adding a human dimension. ""If you want to make a team resilient to blind spots, collect a group of people who have different blind spots. As a whole, the team will be guarded against specific blind spots."" Error rates in conventional silicon-based computers are practically negligible, about one in a thousand-trillion operations, but today's NISQ quantum computers produce an error in a mere 100 operations.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""These are really early-stage machines in which the devices have a lot of error,"" Qureshi said. ""That will likely improve over time, but because we are dependent on matter that has extremely low energy and lacks stability, we will never get the reliability we have come to expect with silicon. Quantum states are inherently about a single particle, but with silicon you are packing a lot of molecules together and averaging their activity. ""If the hardware is inherently unreliable, we have to write software to make the most of it,"" he said. ""We have to take the hardware characteristics into account to make these unique machines useful."" The notion of running a quantum operation thousands of times to get what's likely to be the right answer at first seems counterproductive. But quantum computing is so much faster than conventional computing that nobody would object to doing a few thousand duplicate runs. ""The objective with quantum computers is not to take a current program and run it faster,"" Qureshi said. ""Using quantum, we can solve problems that are virtually impossible to solve with even the fastest supercomputers. With several hundred qubits, which is beyond the current state of the art, we could solve problems that would take a thousand years with the fastest supercomputer."" Added Qureshi: ""You don't mind doing the computation a few thousand times to get an answer like that."" The quantum error mitigation scheme is scheduled to be presented on Oct. 14 at the 52nd Annual IEEE/ACM International Symposium on Microarchitecture. The work was supported by a gift from Microsoft. "
Science Daily,Quantum Paradox Experiment May Lead to More Accurate Clocks and Sensors,Computers & Math,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092242.htm,"   University of Queensland physicist Dr Magdalena Zych said the international collaboration aimed to test Einstein's twin paradox using quantum particles in a 'superposition' state. ""The twin paradox is one of the most counterintuitive predictions of relativity theory,"" Dr Zych said. ""It says that time can pass at different speeds for people at different distances to an enormous mass or travelling with different velocities. ""For example, relative to a reference clock far from any massive object, a clock closer to a mass or moving at high speed will tick slower. ""This creates a 'twin paradox', where one of a pair of twins departs on a fast-speed journey while the other stays behind.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""When the twins reunite, the travelling twin would be much younger, as different amounts of time have passed for each of them. ""It's a mind-blowing effect -- featured in popular movies like Interstellar -- but it's also been verified by real world experiments, and is even taken into consideration in order for everyday GPS technology to work."" The team included researchers from the University of Ulm and Leibniz University Hannover and found how one could use advanced laser technology to realise a quantum version of the Einstein's twin paradox. In the quantum version, rather than twins there will be only one particle travelling in a quantum superposition. ""A quantum superposition means the particle is in two locations at the same time, in each of them with some probability, and yet this is different to placing the particle in one or the other location randomly,"" Dr Zych said.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""It's another way for an object to exist, only allowed by the laws of quantum physics. ""The idea is to put one particle in superposition on two trajectories with different speeds, and see if a different amount of time passes for each of them, as in the twin paradox. ""If our understanding of quantum theory and relativity is right, when the superposed trajectories meet, the quantum traveller will be in superposition of being older and younger than itself. ""This would leave an unmistakeable signature in the results of the experiment, and that's what we hope will be found when the experiment is realised in the future. ""It could lead to advanced technologies that will allow physicists to build more precise sensors and clocks -- potentially, a key part of future navigation systems, autonomous vehicles and earthquake early-warning networks."" The experiment itself will also answer some open questions in modern physics. ""A key example is, can time display quantum behaviour or is it fundamentally classical?"" Dr Zych said. ""This question is likely crucial for the 'holy grail' of theoretical physics: finding a joint theory of quantum and gravitational phenomena. ""We're looking forward to helping answer this question, and tackling many more."" "
Science Daily,Scientists Reveal Mechanism of Electron Charge Exchange in Molecules,Computers & Math,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111714.htm,"   With this technique, the UCI scientists were able to observe electron distribution between atoms and molecules and uncover clues to the origins of ferroelectricity, the capacity of certain crystals to possess spontaneous electric polarization that can be switched by the application of an electric field. The research, which is highlighted in a study published today in Nature, also revealed the mechanism of charge transfer between two materials. ""This method is an advancement in electron microscopy -- from detecting atoms to imaging electrons -- that could help us engineer new materials with desired properties and functionalities for devices used in data storage, energy conversion and quantum computing,"" said team leader Xiaoqing Pan, UCI's Henry Samueli Endowed Chair in Engineering and a professor of both materials science & engineering and physics & astronomy. Employing a new aberration-corrected scanning transmission electron microscope with a fine electron probe measuring half an angstrom and a fast-direct electron detection camera, his group was able to acquire a 2D raster image of diffraction patterns from a region of interest in the sample. As obtained, the data sets are 4D, since they consist of 2D diffraction patterns from each probe location in a 2D scanning area. ""With our new microscope, we can routinely form an electron probe as small as 0.6 angstrom, and our high-speed camera with angular resolution can acquire 4D STEM images with 512 x 512 pixels at greater than 300 frames per second,"" Pan said. ""Using this technique, we can see the electron charge distribution between atoms in two different perovskite oxides, non-polar strontium titanate and ferroelectric bismuth ferrite."" Electron charge density in bulk materials can be measured by X-ray or electron diffraction techniques by assuming a perfectly defect-free structure within the beam-illuminated area. But, Pan said, there remains a challenge in resolving electron charge density in nanostructured materials consisting of interfaces and defects. ""In principle, local electric field and charge density can be determined by electron diffraction imaging using an aberration-corrected scanning transmission electron microscope with a sub-angstrom electron probe,"" he said. ""While penetrating through a specimen, the electron beam interacts with the internal electric field of material in its pathway, resulting in a change in its momentum reflected in the diffraction pattern. By measuring this change, the electric field in a local region of the specimen can be delineated, and the charge density can be derived."" Pan added that although this principle has been demonstrated in simulations, no experiment has been successful until now. ""The electron charge density maps obtained using the 4D STEM method match with theoretical results from the first-principle calculations,"" said lead author Wenpei Gao, a UCI postdoctoral researcher in materials science & engineering. ""The study of the ferroelectric/insulator interface between bismuth ferrite and strontium titanate using this technique directly shows how features of the bismuth compound's polar atomic structure leak across the interface, appearing in the normally non-polar strontium titanate. As a result, the interface hosts excess electrons confined to a small region less than 1 nanometer thick."" Pan said that this project gives materials scientists and engineers new tools for evaluating structures, defects and interfaces in functional materials and nanodevices. He noted that it may soon be possible to conduct high-throughput mapping of the charge density of materials and molecules to add to the database of properties aiding in the Materials Genome Initiative. ""As electron microscopy advances from imaging atoms to probing electrons, it will lead to new understanding and discovery in materials research,"" said co-author Ruqian Wu, UCI professor of physics & astronomy, who led the study's theoretical work. ""The ability to image the charge density distribution around atoms near interfaces, grain boundaries or other planar defects opens new fields for electron microscopy and materials science."" "
Science Daily,Hydrologic Simulation Models That Inform Policy Decisions Are Difficult to Interpret,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011165330.htm,"   Numerical models have become increasingly easy to employ with advances in computer technology and software with graphical user interface (GUI). While these technologies make the models more accessible, problems can arise if they are used by inexperienced modelers, says Juan Sebastian Acero Triana, a doctoral student in the Department of Agricultural and Biological Engineering at the University of Illinois. Acero Triana is lead author on a study that evaluates the accuracy of a commonly used numerical model in hydrology. Findings from the research show that even when the model appears to be properly calibrated, its results can be difficult to interpret correctly. The study, published in the Journal of Hydrology, provides recommendations for how to fine-tune the process and obtain more precise results. Model accuracy is important to ensure that policy decisions are based on realistic scenarios, says Maria Chu, a co-author of the study. Chu is an assistant professor of agricultural and biological engineering in the College of Agricultural, Consumer and Environmental Sciences and The Grainger College of Engineering at U of I. ""For example, you may want to estimate the impacts of future climate on the water availability over the next 100 years. If the model is not representing reality, you are going to draw the wrong conclusions. And wrong conclusions will lead to wrong policies, which can greatly affect communities that rely on the water supply,"" Chu says.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The study focuses on the Soil and Water Assessment model (SWAT), which simulates water circulation by incorporating data on land use, soil, topography, and climate. It is a popular model used to evaluate the impacts of climate and land management practices on water resources and contaminant movement. The researchers conducted a case study at the Fort Cobb Reservoir Experimental Watershed (FCREW) in Oklahoma to assess the model's accuracy. FCREW serves as a test site for the United States Department of Agriculture-Agricultural Research Service (USDA-ARS) and the United States Geological Survey (USGS); thus, detailed data are already available on stream flow, reservoir, groundwater, and topography. The study coupled the SWAT model with another model called MODFLOW, or the Modular Finite-difference Flow Model, which includes more detailed information on groundwater levels and fluxes. ""Our purpose was to determine if the SWAT model by itself can appropriately represent the hydrologic system,"" Acero Triana says. ""We discovered that is not the case. It cannot really represent the entire hydrologic system."" In fact, the SWAT model yielded 12 iterations of water movement that all appeared to be acceptable. However, when combined with MODFLOW it became clear that only some of these results properly accounted for groundwater flow. The researchers compared the 12 results from SWAT with 103 different groundwater iterations from MODFLOW in order to find a realistic representation of the water fluxes in the watershed.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Yielding several different results that all appear equally likely to be correct is called ""equifinality."" Careful calibration of the model can reduce equifinality, Acero Triana explains. Calibration must also be able to account for inherent limitations in the way the model is designed and how parameters are defined. In technical terms, it must account for model and constraint inadequacy. However, inexperienced modelers may not fully understand the intricacies of calibration. And because of the inherent constraints of both SWAT and MODFLOW, using metrics from just one model may not provide accurate results. The researchers recommend using a combination model called SWATmf, which integrates the SWAT and the MODFLOW processes. ""This paper presents a case study that provides general guidelines for how to use hydrological models,"" Acero Triana says. ""We show that to really represent a hydrologic system you need two domain models. You need to represent both the surface and the sub-surface processes that are taking place."" The differences in results may be small, but over time the effect could be significant, he concludes. "
Science Daily,'Electroadhesive' Stamp Picks Up and Puts Down Microscopic Structures,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011144619.htm,"   As circuit boards are packed with ever smaller components, however, robotic grippers' ability to manipulate these objects is approaching a limit. ""Electronics manufacturing requires handling and assembling small components in a size similar to or smaller than grains of flour,"" says Sanha Kim, a former MIT postdoc and research scientist who worked in the lab of mechanical engineering associate professor John Hart. ""So a special pick-and-place solution is needed, rather than simply miniaturizing [existing] robotic grippers and vacuum systems."" Now Kim, Hart, and others have developed a miniature ""electroadhesive"" stamp that can pick up and place down objects as small as 20 nanometers wide -- about 1,000 times finer than a human hair. The stamp is made from a sparse forest of ceramic-coated carbon nanotubes arranged like bristles on a tiny brush. When a small voltage is applied to the stamp, the carbon nanotubes become temporarily charged, forming prickles of electrical attraction that can attract a minute particle. By turning the voltage off, the stamp's ""stickiness"" goes away, enabling it to release the object onto a desired location. Hart says the stamping technique can be scaled up to a manufacturing setting to print micro- and nanoscale features, for instance to pack more elements onto ever smaller computer chips. The technique may also be used to pattern other small, intricate features, such as cells for artificial tissues. And, the team envisions macroscale, bioinspired electroadhesive surfaces, such as voltage-activated pads for grasping everyday objects and for gecko-like climbing robots.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Simply by controlling voltage, you can switch the surface from basically having zero adhesion to pulling on something so strongly, on a per unit area basis, that it can act somewhat like a gecko's foot,"" Hart says. The team has published its results today in the journal Science Advances. Like dry Scotch tape Existing mechanical grippers are unable to pick up objects smaller than about 50 to 100 microns, mainly because at smaller scales surface forces tend to win over gravity. You may see this when pouring flour from a spoon -- inevitably, some tiny particles stick to the spoon's surface, rather than letting gravity drag them off. ""The dominance of surface forces over gravity forces becomes a problem when trying to precisely place smaller things -- which is the foundational process by which electronics are assembled into integrated systems,"" Hart says.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     He and his colleagues noted that electroadhesion, the process of adhering materials via an applied voltage, has been used in some industrial settings to pick and place large objects, such as fabrics, textiles, and whole silicon wafers. But this same electroadhesion had never been applied to objects at the microscopic level, because a new material design for controlling electroadhesion at smaller scales was needed. Hart's group has previously worked with carbon nanotubes (CNTs) -- atoms of carbon linked in a lattice pattern and rolled into microscopic tubes. CNTs are known for their exceptional mechanical, electrical, and chemical properties, and they have been widely studied as dry adhesives. ""Previous work on CNT-based dry adhesives focused on maximizing the contact area of the nanotubes to essentially create a dry Scotch tape,"" Hart says. ""We took the opposite approach, and said, 'let's design a nanotube surface to minimize the contact area, but use electrostatics to turn on adhesion when we need it.'"" A sticky on/off switch The team found that if they coated CNTs with a thin dielectric material such as aluminum oxide, when they applied a voltage to the nanotubes, the ceramic layer became polarized, meaning its positive and negative charges became temporarily separated. For instance, the positive charges of the tips of the nanotubes induced an opposite polarization in any nearby conducting material, such as a microscopic electronic element. As a result, the nanotube-based stamp adhered to the element, picking it up like tiny, electrostatic fingers. When the researchers turned the voltage off, the nanotubes and the element depolarized, and the ""stickiness"" went away, allowing the stamp to detach and place the object onto a given surface. The team explored various formulations of stamp designs, altering the density of carbon nanotubes grown on the stamp, as well as the thickness of the ceramic layer that they used to coat each nanotube. They found that the thinner the ceramic layer and the more sparsely spaced the carbon nanotubes were, the greater the stamp's on/off ratio, meaning the greater the stamp's stickiness was when the voltage was on, versus when it was off. In their experiments, the team used the stamp to pick up and place down films of nanowires, each about 1,000 times thinner than a human hair. They also used the technique to pick and place intricate patterns of polymer and metal microparticles, as well as micro-LEDs. Hart says the electroadhesive printing technology could be scaled up to manufacture circuit boards and systems of miniature electronic chips, as well as displays with microscale LED pixels. ""With ever-advancing capabilities of semiconductor devices, an important need and opportunity is to integrate smaller and more diverse components, such as microprocessors, sensors, and optical devices,"" Hart says. ""Often, these are necessarily made separately but must be integrated together to create next-generation electronic systems. Our technology possibly bridges the gap necessary for scalable, cost-effective assembly of these systems."" This research was supported in part by the Toyota Research Insititute, the National Science Foundation, and the MIT-Skoltech Next Generation Program. "
Science Daily,New Soft Actuators Could Make Soft Robots Less Bulky,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011142000.htm,"   As a proof of concept, engineers used these new actuators to build a soft, battery-powered robot that can walk untethered on flat surfaces and move objects. They also built a soft gripper that can grasp and pick up small objects. The team, led by UC San Diego mechanical and aerospace engineering professor Shengqiang Cai, published the work Oct. 11 in Science Advances. A problem with most soft actuators is that they come with bulky setups. That's because their movements are controlled by pumping either air or fluids through chambers inside. So building robots with these types of actuators would require tethering them to pumps, large power sources and other specialized equipment. In the current study, UC San Diego engineers created soft actuators that are controlled with electricity. ""This feature makes our tubular actuators compatible with most low-cost, commercially available electronic devices and batteries,"" Cai said. The actuators are made from a type of material used for artificial muscles in robots, called liquid crystal elastomers. They are composed of liquid crystal molecules embedded in a stretchy polymer network. What's special about these materials is they change shape, move and contract in response to stimuli such as heat or electricity -- similar to how muscles contract in response to signals from nerve cells.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     To construct each actuator, engineers sandwiched three heating wires between two thin films of liquid crystal elastomer. The material is then rolled into a tube, pre-stretched and exposed to UV light. Each heating wire can be controlled independently to make the tube bend in six different directions. When an electric current is passed through one or two of the wires, it heats up part of the tube and makes it bend in the direction of those wires. When a current is sent through all three wires, the entire tube contracts, shortening in length. When the electricity is turned off, the tube slowly cools down and returns to its original shape. ""Using an externally applied electrical potential makes it easy to program the position of each tubular actuator,"" said first author Qiguang He, a mechanical and aerospace engineering Ph.D. student at the UC San Diego Jacobs School of Engineering. Combining multiple actuators together enabled engineers to build different types of soft robots. They built an untethered, walking robot using four actuators as legs. This robot is powered by a small lithium/polymer battery on board. They also built a soft gripper using three actuators as fingers. Each robot has an on-board microcontroller in which engineers programmed a sequence of electrically controlled motions for the actuators. This allows the robots to move independently. The team is now working on making soft actuators that can move faster. The current actuators take about 30 seconds to fully bend and contract, and up to four minutes to return to their original shapes. That's because the material takes a bit of time to fully heat up and cool down. The ultimate goal is to make actuators that can contract and relax as quickly as human muscles, He said. This work was supported by the Office of Naval Research (grant N00014-17-1-2062) and the National Science Foundation (grant CMMI-1554212). This work was performed in part at the San Diego Nanotechnology Infrastructure (SDNI) at UC San Diego, a member of the National Nanotechnology Coordinate Infrastructure, which is supported by the National Science Foundation (grant ECCS-1542148). "
Science Daily,Combination of Techniques Could Improve Security for IoT Devices,Computers & Math,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010164838.htm,"   ""By 2020, more than 20 billion IoT devices will be in operation, and these devices can leave people vulnerable to security breaches that can put their personal data at risk or worse, affect their safety,"" said Beulah Samuel, a student in the Penn State World Campus information sciences and technology program. ""Yet no strategy exists to identify when and where a network security attack on these devices is taking place and what such an attack even looks like."" The team applied a combination of approaches often used in traditional network security management to an IoT network simulated by the University of New South Wales Canberra. Specifically, they showed how statistical data, machine learning and other data analysis methods could be applied to assure the security of IoT systems across their lifecycle. They then used intrusion detection and a visualization tool, to determine whether or not an attack had already occurred or was in progress within that network. The researchers describe their approach and findings in a paper to be presented today (Oct. 10) at the 2019 IEEE Ubiquitous Computing, Electronics and Mobile Communication Conference. The team received the ""Best Paper"" award for their work. One of the data analysis techniques the team applied was the open-source freely available R statistical suite, which they used to characterize the IoT systems in use on the Canberra network. In addition, they used machine learning solutions to search for patterns in the data that were not apparent using R. ""One of the challenges in maintaining security for IoT networks is simply identifying all the devices that are operating on the network,"" said John Haller, a student in the Penn State World Campus information sciences and technology program. ""Statistical programs, like R, can characterize and identify the user agents."" The researchers used the widely available Splunk intrusion detection tool, which comprises software for searching, monitoring and analyzing network traffic, via a Web-style interface.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Splunk is an analytical tool that is often used in traditional network traffic monitoring, but had only seen limited application to IoT traffic, until now,"" said Melanie Seekins. Using these tools, and others, the team identified three IP addresses that were actively trying to break into the Canberra network's devices. ""We observed three IP addresses attempting to attach to the IoT devices multiple times over a period of time using different protocols,"" said Andrew Brandon. ""This clearly indicates a Distributed Denial of Service attack, which aims to disrupt and/or render devices unavailable to the owners."" As the basis for their approach, the researchers compared it to a common framework used to help manage risk, the National Institute of Standards and Technology (NIST) Risk Management Framework (RMF). ""The NIST RMF was not created for IoT systems, but it provides a framework that organizations can use to tailor, test, and monitor implemented security controls. This lends credibility to our approach,"" said Brandon.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Ultimately, Seekins said, the ability to analyze IoT data using the team's approach may enable security professionals to identify and manage controls to mitigate risk and analyze incidents as they occur. ""Knowing what has taken place in an actual attack helps us write scripts and monitors to look for those patterns,"" she said. ""These predictive patterns and the use of machine learning and artificial intelligence can help us anticipate and prepare for major attacks using IoT devices."" The team hopes their approach will contribute to the creation of a standard protocol for IoT network security. ""There is no standardization for IoT security,"" said Seekins. ""Each manufacturer or vendor creates their own idea of what security looks like, and this can become proprietary and may or may not work with other devices. Our strategy is a good first step toward alleviating this problem."" "
Science Daily,Controlling Superconducting Regions Within an Exotic Metal,Computers & Math,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011074717.htm,"   EPFL's Laboratory of Quantum Materials (QMAT), headed by Philip Moll, has been working on a specific group of unconventional superconductors known as heavy fermion materials. The QMAT scientists, as part of a broad international collaboration between EPFL, the Max Planck Institute for Chemical Physics of Solids, the Los Alamos National Laboratory and Cornell University, made a surprising discovery about one of these materials, CeIrIn5. CeIrIn5 is a metal that superconducts at a very low temperature, only 0.4°C above absolute zero (around -273°C). The QMAT scientists, together with Katja C. Nowack from Cornell University, have now shown that this material could be produced with superconducting regions coexisting alongside regions in a normal metallic state. Better still, they produced a model that allows researchers to design complex conducting patterns and, by varying the temperature, to distribute them within the material in a highly controlled way. Their research has just been published in Science. To achieve this feat, the scientists sliced very thin layers of CeIrIn5 -- only around a thousandth of a millimeter thick -- that they joined to a sapphire substrate. When cooled, the material contracts significantly whereas the sapphire contracts very little. The resulting interaction puts stress on the material, as if it were being pulled in all directions, thus slightly distorting the atomic bonds in the slice. As the superconductivity in CeIrIn5 is unusually sensitive to the material's exact atomic configuration, engineering a distortion pattern is all it takes to achieve a complex pattern of superconductivity. This new approach allows researchers to ""draw"" superconducting circuitry on a single crystal bar, a step that paves the way for new quantum technologies. This discovery represents a major step forward in controlling superconductivity in heavy fermion materials. But that's not the end of the story. Following on from this project, a post-doc researcher has just begun exploring possible technological applications. ""We could, for example, change the regions of superconductivity by modifying the material's distortion using a microactuator,"" says Moll. ""The ability to isolate and connect superconducting regions on a chip could also create a kind of switch for future quantum technologies, a little like the transistors used in today's computing."" "
Science Daily,"That New Yarn? Wearable, Washable Textile Devices Are Possible With MXene-Coated Yarns",Computers & Math,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010161548.htm,"   Two groups of researchers at Drexel University -- one, who is leading the development of industrial functional fabric production techniques, and the other, a pioneer in the study and application of one of the strongest, most electrically conductive super materials in use today -- believe they have a solution. They've improved a basic element of textiles: yarn. By adding technical capabilities to the fibers that give textiles their character, fit and feel, the team has shown that it can knit new functionality into fabrics without limiting their wearability. In a paper recently published in the journal Advanced Functional Materials, the researchers, led by Yury Gogotsi, PhD, Distinguished University and Bach professor in Drexel's College of Engineering, and Genevieve Dion, an associate professor in Westphal College of Media Arts & Design and director of Drexel's Center for Functional Fabrics, showed that they can create a highly conductive, durable yarn by coating standard cellulose-based yarns with a type of conductive two-dimensional material called MXene. Hitting snags ""Current wearables utilize conventional batteries, which are bulky and uncomfortable, and can impose design limitations to the final product,"" they write. ""Therefore, the development of flexible, electrochemically and electromechanically active yarns, which can be engineered and knitted into full fabrics provide new and practical insights for the scalable production of textile-based devices."" The team reported that its conductive yarn packs more conductive material into the fibers and can be knitted by a standard industrial knitting machine to produce a textile with top-notch electrical performance capabilities. This combination of ability and durability stands apart from the rest of the functional fabric field today.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Most attempts to turn textiles into wearable technology use stiff metallic fibers that alter the texture and physical behavior of the fabric. Other attempts to make conductive textiles using silver nanoparticles and graphene and other carbon materials raise environmental concerns and come up short on performance requirements. And the coating methods that are successfully able to apply enough material to a textile substrate to make it highly conductive also tend to make the yarns and fabrics too brittle to withstand normal wear and tear. ""Some of the biggest challenges in our field are developing innovative functional yarns at scale that are robust enough to be integrated into the textile manufacturing process and withstand washing,"" Dion said. ""We believe that demonstrating the manufacturability of any new conductive yarn during experimental stages is crucial. High electrical conductivity and electrochemical performance are important, but so are conductive yarns that can be produced by a simple and scalable process with suitable mechanical properties for textile integration. All must be taken into consideration for the successful development of the next-generation devices that can be worn like everyday garments."" The winning combination Dion has been a pioneer in the field of wearable technology, by drawing on her background on fashion and industrial design to produce new processes for creating fabrics with new technological capabilities. Her work has been recognized by the Department of Defense, which included Drexel, and Dion, in its Advanced Functional Fabrics of America effort to make the country a leader in the field. She teamed with Gogotsi, who is a leading researcher in the area of two-dimensional conductive materials, to approach the challenge of making a conductive yarn that would hold up to knitting, wearing and washing.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Gogotsi's group was part of the Drexel team that discovered highly conductive two-dimensional materials, called MXenes, in 2011 and have been exploring their exceptional properties and applications for them ever since. His group has shown that it can synthesize MXenes that mix with water to create inks and spray coatings without any additives or surfactants -- a revelation that made them a natural candidate for making conductive yarn that could be used in functional fabrics. ""Researchers have explored adding graphene and carbon nanotube coatings to yarn, our group has also looked at a number of carbon coatings in the past,"" Gogotsi said. ""But achieving the level of conductivity that we demonstrate with MXenes has not been possible until now. It is approaching the conductivity of silver nanowire-coated yarns, but the use of silver in the textile industry is severely limited due to its dissolution and harmful effect on the environment. Moreover, MXenes could be used to add electrical energy storage capability, sensing, electromagnetic interference shielding and many other useful properties to textiles."" In its basic form, titanium carbide MXene looks like a black powder. But it is actually composed of flakes that are just a few atoms thick, which can be produced at various sizes. Larger flakes mean more surface area and greater conductivity, so the team found that it was possible to boost the performance of the yarn by infiltrating the individual fibers with smaller flakes and then coating the yarn itself with a layer of larger-flake MXene. Putting it to the test The team created the conductive yarns from three common, cellulose-based yarns: cotton, bamboo and linen. They applied the MXene material via dip-coating, which is a standard dyeing method, before testing them by knitting full fabrics on an industrial knitting machine -- the kind used to make most of the sweaters and scarves you'll see this fall. Each type of yarn was knit into three different fabric swatches using three different stitch patterns -- single jersey, half gauge and interlock -- to ensure that they are durable enough to hold up in any textile from a tightly knit sweater to a loose-knit scarf. ""The ability to knit MXene-coated cellulose-based yarns with different stitch patterns allowed us to control the fabric properties, such as porosity and thickness for various applications,"" the researchers write. To put the new threads to the test in a technological application, the team knitted some touch-sensitive textiles -- the sort that are being explored by Levi's and Yves Saint Laurent as part of Google's Project Jacquard. Not only did the MXene-based conductive yarns hold up against the wear and tear of the industrial knitting machines, but the fabrics produced survived a battery of tests to prove its durability. Tugging, twisting, bending and -- most importantly -- washing, did not diminish the touch-sensing abilities of the yarn, the team reported -- even after dozens of trips through the spin cycle. Pushing forward But the researchers suggest that the ultimate advantage of using MXene-coated conductive yarns to produce these special textiles is that all of the functionality can be seamlessly integrated into the textiles. So instead of having to add an external battery to power the wearable device, or wirelessly connect it to your smartphone, these energy storage devices and antennas would be made of fabric as well -- an integration that, though literally seamed, is a much smoother way to incorporate the technology. ""Electrically conducting yarns are quintessential for wearable applications because they can be engineered to perform specific functions in a wide array of technologies,"" they write. Using conductive yarns also means that a wider variety of technological customization and innovations are possible via the knitting process. For example, ""the performance of the knitted pressure sensor can be further improved in the future by changing the yarn type, stitch pattern, active material loading and the dielectric layer to result in higher capacitance changes,"" according to the authors. Dion's team at the Center for Functional Fabrics is already putting this development to the test in a number of projects, including a collaboration with textile manufacturer Apex Mills -- one of the leading producers of material for car seats and interiors. And Gogotsi suggests the next step for this work will be tuning the coating process to add just the right amount of conductive MXene material to the yarn for specific uses. ""With this MXene yarn, so many applications are possible,"" Gogotsi said. ""You can think about making car seats with it so the car knows the size and weight of the passenger to optimize safety settings; textile pressure sensors could be in sports apparel to monitor performance, or woven into carpets to help connected houses discern how many people are home -- your imagination is the limit."" "
Science Daily,The Makeup of Mariculture: Global Trends in Seafood Farming,Earth & Climate,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015113320.htm,"   The process of farming seafood in the ocean, known as mariculture, is a growing trend yet little is known about the trajectories of its development. That's why a team of Florida State University researchers set out to shed some light on the industry. FSU postdoctoral researcher Rebecca Gentry, doctoral student Elizabeth Bess Ruff and Assistant Professor of Geography Sarah Lester examined more than 50 years of data from 1950 to 2016 from more than 100 countries around the world. Their study, published in Nature Sustainability, outlined several consistent patterns of mariculture taking place globally. ""Aquaculture is an increasingly important component of global food production,"" Gentry said. ""Therefore, understanding patterns of development has important implications for managing our changing global food systems and ensuring economic development, food security and environmental sustainability."" Gentry and her team examined different development trajectories of mariculture production overall and that of specific groups of species, such as fish and crustaceans. They found that countries with relatively stable production farmed a greater diversity of species than countries with other development trajectories. For example, stable countries produced 15.2 species on average, compared to 6.5 for countries who have experienced a crash in production. Lester pointed out that this result suggests that increasing the diversity of mariculture crops could support more robust and resilient seafood production. Additionally, researchers found the type of species grown had a positive connection with a country's development trajectory. Specifically, countries that initially farmed molluscs, such as oysters or mussels, were more likely to have stable production than countries that started with farming fish. Researchers also found that governance and economic indicators were related to trajectories of mariculture production. For instance, low production countries tended to have lower annual gross domestic product (GDP) scores, lower governmental regulatory quality and lower levels of internet connectivity. Further, the team demonstrated that many countries had stabilized their mariculture production at a level far below their potential productivity. ""This indicates that governance, regulatory or economic changes could unlock further opportunities for growth,"" Gentry said. ""Environmental regulations are important for preventing significant environmental decline, local over-development and unsustainable farm practices. However, for those countries currently failing to meet their mariculture potential, policies to encourage thoughtful growth may be worth considering."" The study is just one part of a larger National Science Foundation-funded project led by Lester that is examining the socioeconomic and ecological drivers of mariculture development. ""This type of multidisciplinary research is essential for better understanding the current effects and future potential of marine aquaculture -- which will be all the more important as the global human population continues to increase and we reach the sustainable limits of other types of food production,"" Lester said. "
Science Daily,Achieving a Safe and Just Future for the Ocean Economy,Earth & Climate,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092238.htm,"   In a climate of environmental change and financial uncertainty, much attention has been given to the growth of the ""Blue Economy"" -- a term which refers to the sustainable use of ocean and marine resources for economic growth, jobs, and improved livelihoods. Ocean resources are viewed as lucrative areas for increased investment, including in fisheries, aquaculture, bio-prospecting, renewable energy, oil and gas, and other businesses. Ensuring that socially equitable and sustainable development occurs should be the mandate of governments and industry, maintain an international group of researchers, led by UBC's Nathan Bennett and Rashid Sumaila. ""Coastal countries and small island developing states have the most at stake when it comes to increased economic activities in local waters,"" said Nathan Bennett, research faculty member in UBC's Institute for the Oceans and Fisheries and lead author on the paper. ""It is important that this not be like a Gold Rush scenario, where unbridled ocean development produces substantial harms for both the marine environment and the wellbeing of the populations who dependent on it. In this paper, we provide solutions to proactively address the potential harms produced by ocean development."" The five recommendations in the paper focus on managing for sustainability, benefit sharing, and creating inclusive decision-making processes at local, national and international levels:1. Establish a global coordinating body and develop international guidelines;    2. Ensure national policies and institutions safeguard sustainability; 3. Promote equitable sharing of benefits and minimization of harms; 4. Employ inclusive governance and decision-making processes; and 5. Engage with insights from interdisciplinary ocean science.""There are currently no set of guidelines, or even an obvious international coordinating body, which focuses on the Blue Economy,"" said Dr. Rashid Sumaila, senior author, professor at UBC's Institute for the Oceans and Director of the OceanCanada Partnership. ""Nothing exists in many nations either. This lack of coordination can lead to situations like we are already seeing in the global fishing industry, where harmful subsidies are leading to overfishing, human rights abuses are occurring, and local access to fish stocks and food security are being undermined."" ""The blue economy is already growing. But, we have an opportunity and responsibility to shape future growth so that it is sustainable and equitable,"" said Bennett. ""Including civil society, such as small-scale fishers, women and Indigenous people, in the decision-making and management processes will help to ensure that benefits are shared."" "
Science Daily,Fire Blankets Can Protect Buildings from Wildfires,Earth & Climate,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015075319.htm,"   By rigorously testing different fabric materials in the laboratory and using them to shield structures that were exposed to fires of increasing magnitude, this research, published in Frontiers in Mechanical Engineering, confirms that existing blanket technology can protect structures from a short wildfire attack. For successful deployment against severe fires and in areas of high housing density, technological advancement of blanket materials and deployment methods, as well as multi-structure protection strategies, are needed. ""The whole-house fire blanket is a viable method of protection against fires at the wildland-urban interface,"" says lead study author Fumiaki Takahashi, a Professor at Case Western Reserve University, Cleveland, Ohio, USA, who teamed up with the NASA Glenn Research Center, U.S. Forest Service, New Jersey Forest Fire Service, and Cuyahoga Community College for this study. He continues, ""Current technology can protect an isolated structure against a relatively short wildfire attack and further technological developments are likely to enable this method to be applied to severe situations."" A burning need Wildfires in urban and suburban settings can have a devastating effect on communities and pose one of the greatest fire challenges of our time.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     People living and working in fire-risk areas contacted Professor Takahashi to find out if commercial products are available to help reduce the likelihood of structure ignition, which would reduce fire damage and improve public and firefighter safety. These pleas motivated the research and an initial investigation revealed that the concept of whole-structure fire blankets has been around for quite some time. ""I thought about a means to reduce wildland fire damage and found a U.S. patent 'conflagration-retardative curtain' i.e., a fire blanket, issued during World War Two. In addition, the U.S. Forest Service firefighters managed to save a historic forest cabin by wrapping it with their fire shelter materials,"" Takahashi reports. An old flame-retardant While there are anecdotal reports on the ability of fire blankets to protect buildings from fires, Takahashi's research highlighted a severe lack of scientific evidence to back up these claims. To rectify this, funded by a research grant from the U.S. Department of Homeland Security, the team conducted several experiments to test the ability of different blanket materials to shield structures against fires of increasing magnitude. ""The fire exposure tests determined how well the fire blankets protected various wooden structures, from a birdhouse in a burning room to a full-size shed in a real forest fire. We tested four types of fabric materials: aramid, fiberglass, amorphous silica, and pre-oxidized carbon, each with and without an aluminum surface. In addition, we conducted laboratory experiments under controlled heat exposure and measured the heat-insulation capabilities of these materials against direct flame contact or radiation heat."" A hot new industry    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The laboratory and real-fire assessments demonstrate that fire blankets could protect structures from a short exposure to a wildfire, but also highlight the technical limitations of their existing form. Further technological advancements are needed in the areas of material composition, deployment methods and multi-structure protection strategies. Takahashi explains, ""The fiberglass or amorphous silica fabrics laminated with aluminum foil performed best, due to high reflection/emission of radiation and good thermal insulation by the fabric. New technology is needed to enhance the fire blankets' heat-blocking capability for an extended period to prevent structure-to-structure ignition. In addition, it will be more effective If dozens or hundreds of homes are protected by such advanced fire blankets at the same time, particularly in high housing-density Wildland-Urban Interface communities."" He concludes by suggesting communities potentially affected by wildfires work together to turn the concept of whole-building fire blankets into a reality. ""Fire blanket protection will be significant to those living and fighting fires at the Wildland-Urban Interface and presents entrepreneurs and investors with business opportunities. The implication of the present findings is that the technical community, the general public, and the fire service must work together to take a step-by-step approach toward the successful application of this technology."" "
Science Daily,Study Finds Topsoil Is Key Harbinger of Lead Exposure Risks for Children,Earth & Climate,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014181654.htm,"   The study, which focuses on New Orleans but could serve as a model for cities around the world, is the first to show how long-term changes in soil lead levels have a corresponding impact in lead blood levels in children. ""Lead dust is invisible and it's tragic that lead-contaminated outdoor areas are unwittingly provided for children as places to play,"" says lead study author Howard Mielke, a pharmacology research professor at Tulane University School of Medicine. ""Young children are extremely vulnerable to lead poisoning because of their normal crawling, hand-to-mouth, exploratory behavior."" Exposure to lead is often irreversible, particularly for children, and includes behavioral or learning problems, decreased IQ, hyperactivity, delayed growth, hearing problems, anemia, kidney disease and cancer. In rare cases, exposure can lead to seizures, coma, or death. In metropolitan New Orleans, children living in communities with more lead in the soil and higher blood lead levels have the lowest school performance scores. Lead was recently cited as a top risk factor for premature death in the United States, particularly from cardiovascular disease, and is responsible for 412,000 premature deaths each year. The research team began tracking the amount of lead in New Orleans soil in 2001, collecting about 5,500 samples in neighborhoods, along busy streets, close to houses and in open spaces including parks. The team from Mielke's Lead Lab collected another round of soil sampling 16 years later. Those samples showed a 44% decrease in the amount of soil lead in communities flooded during Hurricane Katrina in 2005 as well as soils in communities not affected by the levee failures and storm surge. Researchers then compared the soil lead with children's blood lead data maintained by the Louisiana Healthy Homes and Childhood Lead Poisoning Prevention Program from 2000-2005 and 2011-2016. Researchers found that lead in blood samples decreased by 64% from 2000-2005 to the 2011-2016 time period and that decreasing lead in topsoil played a key factor in the declining children's blood lead levels. Lead exposure is a critical environmental justice issue, according to researchers. The team found black children were three times more likely than white children to have higher blood lead levels, which could be explained by socioeconomic status and education, the type and age of housing and proximity to major roads and industry. ""While the metabolism of the city could theoretically affect all residents equally, in reality social formations produce inequitable outcomes in which vulnerable populations tend to bear greater burdens of contaminant exposure,"" Mielke says. Mielke says further study is needed to determine if demographic changes in New Orleans since 2001 contributed to the decline in children's blood lead levels, and if decreases are occurring equitably for all populations. This new study is co-authored by researchers from Australia, Colorado State University, and City University of New York. "
Science Daily,Lakes Worldwide Are Experiencing More Severe Algal Blooms,Earth & Climate,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111746.htm,"   Reports of harmful algal blooms -- like the ones that shut down Toledo's water supply in 2014 or led to states of emergency being declared in Florida in 2016 and 2018 -- are growing. These aquatic phenomena are harmful either because of the intensity of their growth, or because they include populations of toxin-producing phytoplankton. But before this research effort, it was unclear whether the problem was truly getting worse on a global scale. Likewise, the degree to which human activity -- including agriculture, urban development, and climate change -- was contributing to this problem was uncertain. ""Toxic algal blooms affect drinking water supplies, agriculture, fishing, recreation, and tourism,"" explained lead author Ho. ""Studies indicate that just in the United States, freshwater blooms result in the loss of $4 billion each year."" Despite this, studies on freshwater algal blooms have either focused on individual lakes or specific regions, or the period examined was comparatively short. No long-term global studies of freshwater blooms had been undertaken until now. Ho, Michalak, and Pahlevan used 30 years of data from NASA and the U.S. Geological Survey's Landsat 5 near-Earth satellite, which monitored the planet's surface between 1984 and 2013 at 30 meter resolution, to reveal long-term trends in summer algal blooms in 71 large lakes in 33 countries on six continents. To do so, they created a partnership with Google Earth Engine to process and analyze more than 72 billion data points. ""We found that the peak intensity of summertime algal blooms increased in more than two-thirds of lakes but decreased in a statistically significant way in only six of the lakes,"" Michalak explained. ""This means that algal blooms really are getting more widespread and more intense, and it's not just that we are paying more attention to them now than we were decades ago."" Although the trend towards more-intense blooms was clear, the reasons for this increase seemed to vary from lake to lake, with no consistent patterns among the lakes where blooms have gotten worse when considering factors such as fertilizer use, rainfall, or temperature. One clear finding, however, is that among the lakes that improved at any point over the 30-year period, only those that experienced the least warming were able to sustain improvements in bloom conditions. This suggests that climate change is likely already hampering lake recovery in some areas. ""This finding illustrates how important it is to identify the factors that make some lakes more susceptible to climate change,"" Michalak said. ""We need to develop water management strategies that better reflect the ways that local hydrological conditions are affected by a changing climate."" This research was supported by the U.S. National Science Foundation, the Natural Sciences and Engineering Research Council of Canada, a Google Earth Engine Research Award, a NASA ROSES grant, and by a USGS Landsat Science Team Award. "
Science Daily,Evolutionary History of Oaks,Earth & Climate,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111726.htm,"   Fundamental questions about relationships between organisms and the genes that drive ecological diversification underlie the secrets of biodiversity. Understanding the past of this ecologically, economically and culturally important group provides a baseline of knowledge that will allow scientists to address additional questions about oaks and other trees, as well as help with conservation efforts. ""This paper demonstrates that oaks have repeatedly and globally diversified in response to ecological opportunity"" says Hipp. ""The changes in the global landscape have given us the gift of the oak diversity we observe today."" Patchwork of Histories  The new paper, to be published in New Phytologist, is available free through an Early View online for one month beginning October 14. The study provides the most detailed account to date of the evolutionary history of the world's oaks. Investigating which parts of the oak genome distinguish species from one another, researchers at The Morton Arboretum, in collaboration with 17 institutions around the world, discovered that each gene or stretch of DNA in the genome has the potential to record multiple histories; each section bears the history of speciation of one oak lineage, but it may record the history of hybridization for a different lineage. In other words, there is no one region of the genome that defines oaks: it is the patchwork of histories embedded in the genome that characterize the history of oak evolution. In addition, this research shows that different oak lineages have repeatedly diversified in the same area. Red oaks, white oaks, ring-cupped oaks, turkey and cork oaks, and three of the other oak sections arose rapidly and segregated to either the Americas or Eurasia. All of these lineages can be found in part of their range with at least one other lineage. As oaks migrated, species interbred, hybridized and diversified opportunistically in response to changes in the landscape. The highest rates of species diversification have been in response to migrations into new territory. Over and over, oaks have taken advantage of ecological opportunity to produce the diversity we see today, providing humans with ships, homes, wine barrels, furniture and acorns to eat, and providing food and homes for countless insects, mammals, birds and fungi. ""For the first time, this paper demonstrates that the history of different [oak] lineages is driven by different sets of genes,"" said co-author Dr. Antoine Kremer from the French National Institute for Agricultural Research. ""The story of oak evolution is especially fascinating due to the ecological and morphological convergence in different oak lineages that cohabit on the same continent."" The importance of oaks  Oaks support the planet's ecosystem like very few other tree species do. As both stately trees and dry-land shrubs, oaks are fundamental to the health of forests, providing critical food, habitat and shelter for animals, birds and insects, and have the highest amount of biomass compared to any other tree species in the forest, working harder to clean the air than many other tree species. Today, oaks need the help of people. Around the world, oaks are under threat, due to pests, diseases and loss of habitat. If oaks are lost, it will upset the delicate balance of forest ecosystems and leave humans without their benefits. Researchers and conservationists at The Morton Arboretum are committed to ensuring oaks thrive. Learn more about what The Morton Arboretum is doing to conserve oaks globally. "
Science Daily,Hydrologic Simulation Models That Inform Policy Decisions Are Difficult to Interpret,Earth & Climate,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011165330.htm,"   Numerical models have become increasingly easy to employ with advances in computer technology and software with graphical user interface (GUI). While these technologies make the models more accessible, problems can arise if they are used by inexperienced modelers, says Juan Sebastian Acero Triana, a doctoral student in the Department of Agricultural and Biological Engineering at the University of Illinois. Acero Triana is lead author on a study that evaluates the accuracy of a commonly used numerical model in hydrology. Findings from the research show that even when the model appears to be properly calibrated, its results can be difficult to interpret correctly. The study, published in the Journal of Hydrology, provides recommendations for how to fine-tune the process and obtain more precise results. Model accuracy is important to ensure that policy decisions are based on realistic scenarios, says Maria Chu, a co-author of the study. Chu is an assistant professor of agricultural and biological engineering in the College of Agricultural, Consumer and Environmental Sciences and The Grainger College of Engineering at U of I. ""For example, you may want to estimate the impacts of future climate on the water availability over the next 100 years. If the model is not representing reality, you are going to draw the wrong conclusions. And wrong conclusions will lead to wrong policies, which can greatly affect communities that rely on the water supply,"" Chu says.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The study focuses on the Soil and Water Assessment model (SWAT), which simulates water circulation by incorporating data on land use, soil, topography, and climate. It is a popular model used to evaluate the impacts of climate and land management practices on water resources and contaminant movement. The researchers conducted a case study at the Fort Cobb Reservoir Experimental Watershed (FCREW) in Oklahoma to assess the model's accuracy. FCREW serves as a test site for the United States Department of Agriculture-Agricultural Research Service (USDA-ARS) and the United States Geological Survey (USGS); thus, detailed data are already available on stream flow, reservoir, groundwater, and topography. The study coupled the SWAT model with another model called MODFLOW, or the Modular Finite-difference Flow Model, which includes more detailed information on groundwater levels and fluxes. ""Our purpose was to determine if the SWAT model by itself can appropriately represent the hydrologic system,"" Acero Triana says. ""We discovered that is not the case. It cannot really represent the entire hydrologic system."" In fact, the SWAT model yielded 12 iterations of water movement that all appeared to be acceptable. However, when combined with MODFLOW it became clear that only some of these results properly accounted for groundwater flow. The researchers compared the 12 results from SWAT with 103 different groundwater iterations from MODFLOW in order to find a realistic representation of the water fluxes in the watershed.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Yielding several different results that all appear equally likely to be correct is called ""equifinality."" Careful calibration of the model can reduce equifinality, Acero Triana explains. Calibration must also be able to account for inherent limitations in the way the model is designed and how parameters are defined. In technical terms, it must account for model and constraint inadequacy. However, inexperienced modelers may not fully understand the intricacies of calibration. And because of the inherent constraints of both SWAT and MODFLOW, using metrics from just one model may not provide accurate results. The researchers recommend using a combination model called SWATmf, which integrates the SWAT and the MODFLOW processes. ""This paper presents a case study that provides general guidelines for how to use hydrological models,"" Acero Triana says. ""We show that to really represent a hydrologic system you need two domain models. You need to represent both the surface and the sub-surface processes that are taking place."" The differences in results may be small, but over time the effect could be significant, he concludes. "
Science Daily,How Preprocessing Methods Affect the Conversion Efficiency of Biomass Energy Production,Earth & Climate,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011165320.htm,"   Grift is co-author on a new study, published in Bioresource Technology Reports, that takes a look at the bioconversion efficiency of two products often used as biomass for energy production, miscanthus giganteus and sugarcane bagasse. ""Our goal was to determine how much energy it takes to prepare these materials. It's a comprehensive look at various preprocessing methods and their relationship to conversion efficiency,"" he explains. The two materials were chosen because of their importance for energy production. Miscanthus is typically grown as an ornamental crop, but it has a high amount of biomass and grows easily with very little nitrogen use. Sugarcane bagasse is the byproduct left over after sugarcane is crushed to extract the juice for sugar. The study was done in collaboration with chemists from University of California at Berkeley. Grift says the interdisciplinary approach makes the research unique, because it considers the whole energy balance. The U of I researchers studied the energy expenditure of harvesting and preprocessing materials, while the Berkeley chemists focused on converting the biomass to glucose, which is used to make ethanol. The researchers defined the percentage of inherent heating value (PIHV), which measures the amount of energy going into and out of the production process. ""It tells you that you have a certain amount of biomass, which contains a certain amount of energy. How much energy do you spend on processing? You don't want to spend more than 5% of the total energy value,"" Grift says.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The researchers subjected the two materials to nine different preprocessing methods, either separately or as a blend. Preprocessing is done for various reasons, Grift explains. After the crop is harvested, it needs to be transported to a processing plant, and to make transportation efficient, the material first undergoes a process called comminution, in which is it chopped or cut into smaller pieces, and then it is compressed. Grift explains that harvesting and compression do not add much to the energy equation. The main source of energy expenditure is comminution, or size reduction. That brings the energy expenditure to 5%. ""Smaller particle sizes make compression easier,"" he says. ""It's also better for energy production, because it provides a larger surface area for enzymes to attach to in the conversion process. But comminution takes a certain amount of energy, so there is a tradeoff."" The preprocessing methods included chopped and cut stems, pelletization, comminution, and various levels of compression. Of the nine treatment groups, five included miscanthus, three included sugarcane bagasse, and one included a blend of the two products. The processed materials were all subjected to the same chemical processes to release the glucose. The researchers also evaluated the effects of particle size, compression level, and blending on biomass conversion efficiency. The results showed that comminution had a positive effect on the efficiency of miscanthus but not sugarcane bagasse, while the opposite was the case for pelletization. The researchers also found that a 50/50 blend of the two materials had higher conversion efficiency than sugarcane bagasse, but there was no significant difference compared to miscanthus alone. The results can be used to help make biomass energy production more efficient, Grift says. ""The differences are not huge. But if you want to do something on a larger scale it's actually quite important to figure these things out,"" he explains. Grift emphasizes that the results are preliminary and should be examined in further studies. Continued research is needed to substantiate the findings and to broaden the knowledge base to other products and other preprocessing methods. "
Science Daily,New Tool Enables Nova Scotia Lobster Fishery to Address Impacts of Climate Change,Earth & Climate,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011155319.htm,"   ""Climate change has socio-economic impacts on coastal communities and the seafood market, but integrating that information into planning and decision-making has been a challenge,"" said Vincent Saba, a fishery biologist at the Northeast Fisheries Science Center and a co-author of the study. ""Ocean warming is leading to an accelerated redistribution of marine species. Knowing how animals will shift distribution, and what to do about shifts across management borders both regional and international, will be critical to planning on how to adapt to those changes."" American lobster is Canada's most valuable fishery, contributing 44 percent of the total commercial value of all fisheries in Atlantic Canada in 2016. Lobster landings have been trending upward in recent decades, and many small rural communities in Atlantic Canada rely heavily on lobster for their economic well-being. Changing climate could have a significant impact on the fishery and on those communities. Researchers from Fisheries and Oceans Canada at the Bedford Institute of Oceanography in Halifax, Nova Scotia and at NOAA's Northeast Fisheries Science Center collaborated on the study. Their findings, published in Frontiers in Marine Science, indicate that overall projected changes in offshore lobster habitat for the region as a whole are positive, but that changes in resource management need to be considered to promote the long-term sustainability of the fishery in Nova Scotia. Ocean temperatures have been warming in the Gulf of Maine and along the Northeast Continental Shelf during the past few decades, causing many species to shift their distribution to the northeast. When ocean temperatures are above the preferred range for lobsters, it can reduce their survival, growth, and reproduction. The potential effects of marine heat waves, like that observed in the Gulf of Maine lobster population in 2012, can also be significant on the Scotian Shelf, a region with a relatively high proportion of species at the edge of their thermal range. Coastal Infrastructure and Lobster Vulnerability Researchers generated two climate change vulnerability indices, one for coastal communities and one for lobster in Nova Scotia. Two ocean models, a regional ocean model with high resolution in the Scotian Shelf and Gulf of Maine region and a global climate model, provided projections of ocean bottom temperatures over multiple decades. The coastal infrastructure vulnerability index puts a numerical value on each lobster management area to indicate relative vulnerability to the effects of climate change. Factors included economic dependence on the fishery, community population size, diversity of the fishery revenue, status of harbor infrastructure, total replacement cost of each harbor, increased relative sea level and flooding, impacts of wind and wave climate, and sea ice. The vulnerability index also puts a numerical value on the vulnerability of offshore lobster habitat to ocean warming and changes in zooplankton, a primary prey, as well as anticipated changes in fishery productivity across management borders. Study authors suggest the new assessment tool could prepare a region for changes in potential catch through adjustments in licensing and quotas, or adapting to a decrease in productivity by encouraging and assisting fishermen to diversify their targeted species, where they fish, or to seek non-fisheries-related income. It could also support planning for projected increases in catch through investing in upgrades to coastal community infrastructure. ""Our analysis is a first step in considering this information in local fishery management decisions and longer-term economic development strategies,' said Saba, who is located at NOAA's Geophysical Fluid Dynamics Laboratory at Princeton University. ""This tool anticipates change and could be incorporated into assessment models and help fishermen and resource managers with long-term planning."" "
Science Daily,Fast-Acting German Insecticide Lost in the Aftermath of WWII,Earth & Climate,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011131907.htm,"   ""We set out to study the growth of crystals in a little-known insecticide and uncovered its surprising history, including the impact of World War II on the choice of DDT -- and not DFDT -- as a primary insecticide in the 20th century,"" said Bart Kahr, professor of chemistry at New York University and one of the study's senior authors. Discovering solid forms of DFDT Kahr and fellow NYU chemistry professor Michael Ward study the growth of crystals, which two years ago led them to discover a new crystal form of the notorious insecticide DDT. DDT is known for its detrimental effect on the environment and wildlife. But the new form developed by Kahr and Ward was found to be more effective against insects -- and in smaller amounts, potentially minimizing its environmental impact. In continuing to explore the crystal structure of insecticides, the research team began studying fluorinated forms of DDT, swapping out chlorine atoms for fluorine. They prepared two solid forms of the compound -- a monofluoro and a difluoro analog -- and tested them on fruit flies and mosquitoes, including mosquito species that carry malaria, yellow fever, Dengue, and Zika. The solid forms of fluorinated DDT killed insects more quickly than did DDT; the difluoro analog, known as DFDT, killed mosquitoes two to four times faster. ""Speed thwarts the development of resistance,"" said Ward, a senior author on the study. ""Insecticide crystals kill mosquitoes when they are absorbed through the pads of their feet. Effective compounds kill insects quickly, possibly before they are able to reproduce."" The researchers also made a detailed analysis of the relative activities of the solid-state forms of fluorinated DDT, noting that less thermodynamically stable forms -- in which the crystals liberate molecules more easily -- were more effective at quickly killing insects.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The forgotten history of DFDT In addition to their chemical analyses, the researchers sought to determine if their creation had a precedent. In doing so, they uncovered a rich and unsettling backstory for DFDT. Through historical documents, they learned that DFDT was created as an insecticide by German scientists during World War II and was used by the German military for insect control in the Soviet Union and North Africa, in parallel with the use of DDT by American armed forces in Europe and the South Pacific. In the post-war chaos, however, DFDT manufacturing came to an abrupt end. Allied military officials who interviewed Third Reich scientists dismissed the Germans' claims that DFDT was faster and less toxic to mammals than DDT, calling their studies ""meager"" and ""inadequate"" in military intelligence reports. In his 1948 Nobel Prize address for the discovery of the insect-killing capability of DDT, Paul Müller noted that DFDT should be the insecticide of the future, given that it works more quickly than does DDT. Despite this, DFDT has largely been forgotten and was unknown to contemporary entomologists with whom the NYU researchers consulted. ""We were surprised to discover that at the outset DDT had a competitor which lost the race because of geopolitical and economic circumstances, not to mention its connection to the German military, and not necessarily because of scientific considerations. A faster, less persistent insecticide, as is DFDT, might have changed the course of the 20th century; it forces us to imagine counterfactual science histories,"" said Kahr.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The need for new insecticides Mosquito-borne diseases such as malaria -- which kills a child every two minutes -- are major public health concerns, resulting in 200 million illnesses annually. Newer diseases like Zika may pose growing threats to health in the face of a changing climate. Mosquitoes are increasingly resistant and are failing to respond to the pyrethroid insecticides built into bed nets. Public health officials are concerned and have reconsidered the use of DDT -- which has been banned for decades in much of the world with the exception of selective use for malaria control -- but its controversial history and environmental impact encourage the need for new insecticides. ""While more research is needed to better understand the safety and environmental impact of DFDT, we, along with the World Health Organization, recognize the urgent need for new, fast insecticides. Not only are fast-acting insecticides critical for fighting the development of resistance, but less insecticide can be used, potentially reducing its environmental impact,"" said Ward. In addition to Ward and Kahr, the study authors are Xiaolong Zhu, Chunhua T. Hu, Jingxiang Yang, and Mengdi Qi of NYU's Department of Chemistry, as well as Leo A. Joyce of Arrowhead Pharma. This work was supported by the NYU Materials Research Science and Engineering Center (MRSEC) program of the National Science Foundation (award number DMR-1420073). The NYU X-ray facility is supported partially by the NSF (award number CRIF/CHE-0840277). "
Science Daily,Achieving a Safe and Just Future for the Ocean Economy,Science & Society,2019-10-15,-,https://www.sciencedaily.com/releases/2019/10/191015092238.htm,"   In a climate of environmental change and financial uncertainty, much attention has been given to the growth of the ""Blue Economy"" -- a term which refers to the sustainable use of ocean and marine resources for economic growth, jobs, and improved livelihoods. Ocean resources are viewed as lucrative areas for increased investment, including in fisheries, aquaculture, bio-prospecting, renewable energy, oil and gas, and other businesses. Ensuring that socially equitable and sustainable development occurs should be the mandate of governments and industry, maintain an international group of researchers, led by UBC's Nathan Bennett and Rashid Sumaila. ""Coastal countries and small island developing states have the most at stake when it comes to increased economic activities in local waters,"" said Nathan Bennett, research faculty member in UBC's Institute for the Oceans and Fisheries and lead author on the paper. ""It is important that this not be like a Gold Rush scenario, where unbridled ocean development produces substantial harms for both the marine environment and the wellbeing of the populations who dependent on it. In this paper, we provide solutions to proactively address the potential harms produced by ocean development."" The five recommendations in the paper focus on managing for sustainability, benefit sharing, and creating inclusive decision-making processes at local, national and international levels:1. Establish a global coordinating body and develop international guidelines;    2. Ensure national policies and institutions safeguard sustainability; 3. Promote equitable sharing of benefits and minimization of harms; 4. Employ inclusive governance and decision-making processes; and 5. Engage with insights from interdisciplinary ocean science.""There are currently no set of guidelines, or even an obvious international coordinating body, which focuses on the Blue Economy,"" said Dr. Rashid Sumaila, senior author, professor at UBC's Institute for the Oceans and Director of the OceanCanada Partnership. ""Nothing exists in many nations either. This lack of coordination can lead to situations like we are already seeing in the global fishing industry, where harmful subsidies are leading to overfishing, human rights abuses are occurring, and local access to fish stocks and food security are being undermined."" ""The blue economy is already growing. But, we have an opportunity and responsibility to shape future growth so that it is sustainable and equitable,"" said Bennett. ""Including civil society, such as small-scale fishers, women and Indigenous people, in the decision-making and management processes will help to ensure that benefits are shared."" "
Science Daily,Study Finds Topsoil Is Key Harbinger of Lead Exposure Risks for Children,Science & Society,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014181654.htm,"   The study, which focuses on New Orleans but could serve as a model for cities around the world, is the first to show how long-term changes in soil lead levels have a corresponding impact in lead blood levels in children. ""Lead dust is invisible and it's tragic that lead-contaminated outdoor areas are unwittingly provided for children as places to play,"" says lead study author Howard Mielke, a pharmacology research professor at Tulane University School of Medicine. ""Young children are extremely vulnerable to lead poisoning because of their normal crawling, hand-to-mouth, exploratory behavior."" Exposure to lead is often irreversible, particularly for children, and includes behavioral or learning problems, decreased IQ, hyperactivity, delayed growth, hearing problems, anemia, kidney disease and cancer. In rare cases, exposure can lead to seizures, coma, or death. In metropolitan New Orleans, children living in communities with more lead in the soil and higher blood lead levels have the lowest school performance scores. Lead was recently cited as a top risk factor for premature death in the United States, particularly from cardiovascular disease, and is responsible for 412,000 premature deaths each year. The research team began tracking the amount of lead in New Orleans soil in 2001, collecting about 5,500 samples in neighborhoods, along busy streets, close to houses and in open spaces including parks. The team from Mielke's Lead Lab collected another round of soil sampling 16 years later. Those samples showed a 44% decrease in the amount of soil lead in communities flooded during Hurricane Katrina in 2005 as well as soils in communities not affected by the levee failures and storm surge. Researchers then compared the soil lead with children's blood lead data maintained by the Louisiana Healthy Homes and Childhood Lead Poisoning Prevention Program from 2000-2005 and 2011-2016. Researchers found that lead in blood samples decreased by 64% from 2000-2005 to the 2011-2016 time period and that decreasing lead in topsoil played a key factor in the declining children's blood lead levels. Lead exposure is a critical environmental justice issue, according to researchers. The team found black children were three times more likely than white children to have higher blood lead levels, which could be explained by socioeconomic status and education, the type and age of housing and proximity to major roads and industry. ""While the metabolism of the city could theoretically affect all residents equally, in reality social formations produce inequitable outcomes in which vulnerable populations tend to bear greater burdens of contaminant exposure,"" Mielke says. Mielke says further study is needed to determine if demographic changes in New Orleans since 2001 contributed to the decline in children's blood lead levels, and if decreases are occurring equitably for all populations. This new study is co-authored by researchers from Australia, Colorado State University, and City University of New York. "
Science Daily,Reading the Past Like an Open Book: Researchers Use Text to Measure 200 Years of Happiness,Science & Society,2019-10-14,-,https://www.sciencedaily.com/releases/2019/10/191014111713.htm,"   Using innovative new methods researchers at the University of Warwick, University of Glasgow Adam Smith Business School and The Alan Turing Institute in London have built a new index that uses data from books and newspaper to track levels of national happiness from 1820. Their research could help governments to make better decisions about policy priorities. Governments the world over are making increasing use of ""national happiness"" data derived from surveys to help them consider the impact of policy on national wellbeing. Unfortunately, data for most countries is only available from 2011 onwards, and for a select few from the mid 1970s. This makes it hard to establish long-run trends, or to say anything about the main historical causes of happiness. In order to tackle this problem, a team of researchers including Professor Thomas Hills (Warwick and The Alan Turing Institute), Professor Eugenio Proto (Glasgow), Professor Daniel Sgroi (Warwick), and Dr Chanuki Seresinhe (The Alan Turing Institute) took a key insight from psychology -- that more often than not what people say or write reveals much about their underlying happiness level -- and developed a method to apply it to online texts from millions of books and newspapers published over the past 200 years. The main source of language used for the analysis was the Google Books corpus, a collection of word frequency data for over 8 million books -- that's more than 6 per cent of all books ever published. The method uses psychological valence norms -- values of happiness that can be derived from text -- for thousands of words in di?erent languages to compute the relative proportion of positive and negative language for four di?erent nations (the USA, UK, Germany and Italy). The research team also controlled for the evolution of language, to take into account the fact that some words change their meaning over time. The new index was validated against existing survey-based measures and proven to be an accurate guide to the national mood. One theory as to why books and newspaper articles are such a good source of data is that editors prefer to publish pieces which match the mood of their readers. Studying the index, the researchers found that: Increases in national income do generate increases in national happiness but it takes a huge rise to have a noticeable effect at the national level An increase in longevity of one year had the same effect on happiness as a 4.3 per cent increase in GDP One less year of war had the equivalent effect on happiness of a 30 per cent rise in GDP In post-war UK the worst period for national happiness occurred around the appropriately named ""Winter of Discontent."" In post-war USA the lowest point of the index coincides with the Vietnam War and the evacuation of Saigon.Commenting on the findings, Professor Thomas Hills said: ""What's remarkable is that national subjective well-being is incredibly resilient to wars. Even temporary economic booms and busts have little long-term effect. We can see the American Civil War in our data, the revolutions of 48' across Europe, the roaring 20's and the Great Depression. But people quickly returned to their previous levels of subjective well-being after these events were over. Our national happiness is like an adjustable spanner that we open and close to calibrate our experiences against our recent past, with little lasting memory for the triumphs and tragedies of our age."" Professor Eugenio Proto added: ""Our index is an important first step in understanding people's satisfaction in the past. Looking at the Italian data, it is interesting to note a slow but constant decline in the years of fascism and a dramatic decline in the years after the last crisis."" Professor Daniel Sgroi said: 'Aspirations seem to matter a lot: after the end of rationing in the 1950s national happiness was very high as were expectations for the future, but unfortunately things did not pan out as people might have hoped and national happiness fell for many years until the low-point of the Winter of Discontent.' Dr Chanuki Seresinhe said: ""It was really important to ensure that the changing meaning of words over time was taken into account. For example, the word ""gay"" had a completely different meaning in the 1800s than it does today. We processed terabytes of word co-occurrence data from Google Books to understand how the meaning of words has changed over time, and we validate our findings using only words with the most stable historical meanings."" "
Science Daily,Another Reason to Get Cataract Surgery: It Can Make You 48% Safer on the Road,Science & Society,2019-10-12,-,https://www.sciencedaily.com/releases/2019/10/191012141221.htm,"   Cataracts are a normal consequence of aging. They happen gradually over years, as the clear lens inside the eye becomes cloudy. The effects of a developing cataract are sometimes hard to distinguish from other age-related vision changes. You may become more nearsighted; colors appear duller and glare from lights make it harder to see at night. By age 80, about half of us will have developed cataracts. Cataract surgery replaces the cloudy lens with an artificial lens. The surgery is low-risk, fast and effective. But not everyone has surgery right away. The decision is usually based on how much the cataract is interfering with daily life activities. Ophthalmologists typically operate on one eye at a time, starting with the eye with the denser cataract. If surgery is successful and vision improves substantially, sometimes surgery in the second eye is forgone or delayed. However, most people get significant benefit from having surgery on the second eye. Depth perception is improved, vision is crisper, making reading and driving easier. To better understand the true benefit of cataract surgery to patients' quality of life, Jonathon Ng, MD, and his colleagues at the University of Western Australia, tested the driving performance of 44 patients before they had cataract surgery. The driving simulator assessed a variety of variables: adjusted speed limits, traffic densities, uncontrolled intersections and pedestrian crossings. Patients were put through the driving simulator again after their first surgery and then again after their second eye surgery. After the first, near misses and crashes decreased by 35 percent; after the second surgery, the number fell to 48 percent. While visual acuity -- how well one sees the eye chart -- is an important method to assess a person's fitness to drive, it's an incomplete assessment, Dr. Ng said. Quality of vision is also an important indicator. Improved contrast sensitivity and better night vision improves drivers' safety on the road. ""In Australia and other countries, people may often wait months to receive government funded surgery after a cataract is diagnosed,"" said Dr. Ng. ""These results highlight the importance of timely cataract surgery in maintaining safety and continued mobility and independence in older adult drivers."" Some things to consider, when considering cataract surgery: Can you see to safety do your job and to drive? Do you have problems reading or watching TV? Is it difficult to cook, shop, climb stairs or take medications? Do vision problems affect your independence? Do bright lights make is harder to see? "
Science Daily,Hydrologic Simulation Models That Inform Policy Decisions Are Difficult to Interpret,Science & Society,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011165330.htm,"   Numerical models have become increasingly easy to employ with advances in computer technology and software with graphical user interface (GUI). While these technologies make the models more accessible, problems can arise if they are used by inexperienced modelers, says Juan Sebastian Acero Triana, a doctoral student in the Department of Agricultural and Biological Engineering at the University of Illinois. Acero Triana is lead author on a study that evaluates the accuracy of a commonly used numerical model in hydrology. Findings from the research show that even when the model appears to be properly calibrated, its results can be difficult to interpret correctly. The study, published in the Journal of Hydrology, provides recommendations for how to fine-tune the process and obtain more precise results. Model accuracy is important to ensure that policy decisions are based on realistic scenarios, says Maria Chu, a co-author of the study. Chu is an assistant professor of agricultural and biological engineering in the College of Agricultural, Consumer and Environmental Sciences and The Grainger College of Engineering at U of I. ""For example, you may want to estimate the impacts of future climate on the water availability over the next 100 years. If the model is not representing reality, you are going to draw the wrong conclusions. And wrong conclusions will lead to wrong policies, which can greatly affect communities that rely on the water supply,"" Chu says.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     The study focuses on the Soil and Water Assessment model (SWAT), which simulates water circulation by incorporating data on land use, soil, topography, and climate. It is a popular model used to evaluate the impacts of climate and land management practices on water resources and contaminant movement. The researchers conducted a case study at the Fort Cobb Reservoir Experimental Watershed (FCREW) in Oklahoma to assess the model's accuracy. FCREW serves as a test site for the United States Department of Agriculture-Agricultural Research Service (USDA-ARS) and the United States Geological Survey (USGS); thus, detailed data are already available on stream flow, reservoir, groundwater, and topography. The study coupled the SWAT model with another model called MODFLOW, or the Modular Finite-difference Flow Model, which includes more detailed information on groundwater levels and fluxes. ""Our purpose was to determine if the SWAT model by itself can appropriately represent the hydrologic system,"" Acero Triana says. ""We discovered that is not the case. It cannot really represent the entire hydrologic system."" In fact, the SWAT model yielded 12 iterations of water movement that all appeared to be acceptable. However, when combined with MODFLOW it became clear that only some of these results properly accounted for groundwater flow. The researchers compared the 12 results from SWAT with 103 different groundwater iterations from MODFLOW in order to find a realistic representation of the water fluxes in the watershed.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Yielding several different results that all appear equally likely to be correct is called ""equifinality."" Careful calibration of the model can reduce equifinality, Acero Triana explains. Calibration must also be able to account for inherent limitations in the way the model is designed and how parameters are defined. In technical terms, it must account for model and constraint inadequacy. However, inexperienced modelers may not fully understand the intricacies of calibration. And because of the inherent constraints of both SWAT and MODFLOW, using metrics from just one model may not provide accurate results. The researchers recommend using a combination model called SWATmf, which integrates the SWAT and the MODFLOW processes. ""This paper presents a case study that provides general guidelines for how to use hydrological models,"" Acero Triana says. ""We show that to really represent a hydrologic system you need two domain models. You need to represent both the surface and the sub-surface processes that are taking place."" The differences in results may be small, but over time the effect could be significant, he concludes. "
Science Daily,CO2 Emissions Cause Lost Labor Productivity,Science & Society,2019-10-11,-,https://www.sciencedaily.com/releases/2019/10/191011131856.htm,"   Climate change may also be making outdoor labour more dangerous, according to a new study published in Scientific Reports. It was led by Yann Chavaillaz, a former postdoctoral researcher at Concordia and the Ouranos Institute, and Damon Matthews, professor and Concordia Research Chair in Climate Science and Sustainability in the Department of Geography, Planning and Environment. The researchers examine how extreme high temperatures caused by CO2 emissions could lead to losses in labour productivity. Using calculations based on widely used guidelines regarding rest time recommendations per hour of labour and heat exposure, the authors found that every trillion tonnes of CO2 emitted could cause global GDP losses of about half a percent. They add that we may already be seeing economic losses of as much as two per cent of global GDP as a result of what we have already emitted. They identify agriculture, mining and quarrying, manufacturing and construction as the economic sectors most vulnerable to heat exposure. These sectors account for 73 per cent of low-income countries' output, according to the authors. Developing countries are hardest hit ""The thresholds of heat exposure leading to labour productivity loss are likely to be exceeded sooner and more extensively in developing countries in warmer parts of the world,"" says Matthews.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""These countries are also more vulnerable because a higher fraction of their work force is employed in these sectors and because they have less ability to implement infrastructural changes that deal with a changing climate."" The research suggests that lower-income countries will experience much stronger economic impacts than higher-income countries. Worst hit are tropical areas of the globe such as Southeast Asia, north-central Africa and northern South America. ""The labour productivity loss computed for low- and lower-middle-income countries is approximately nine times higher than the one of high-income countries,"" reads the report. (The authors are also careful to point out that health recommendations are not obligatory and are often not seriously or consistently applied at real-world work sites. Their estimates of productivity loss is based on the strict adherence to health guidelines regarding labour in extreme heat.) From emissions to impacts Matthews and his co-authors based their calculations of historical and future increases of heat exposure using simulations from eight separate Earth Systems Models. While many academic studies have estimated socioeconomic impacts of climate change, he says this paper is novel because it predicts future impacts as a direct function of CO2 emissions.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""The relationship between emissions and impact is pretty linear, so we are able to say that this additional quantity of CO2 emissions will lead to this additional increase in impact,"" he explains. ""The impact scales pretty well with the total amount of emissions we produce."" Cost of business The authors write that their research linking CO2 emissions to loss of labour productivity from heat exposure can help countries adopt mitigating measures. But Matthews says it may also help people change their thinking about the overall consequences of a relentlessly warming planet. ""We can see that every additional ton of CO2 emission that we produce will have this additional impact, and we can quantify that increase,"" he says. ""So this study can help us point to specific countries that are experiencing a quantifiable share of the economic damages that result from the emissions we produce."" "
Science Daily,"Beyond the 'Replication Crisis,' Does Research Face an 'Inference Crisis'?",Science & Society,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010161540.htm,"   Using an example from their own field of memory research, they designed a test for the accuracy of theoretical conclusions made by researchers. The study was spearheaded by associate professor Jeffrey Starns, professor Caren Rotello, and doctoral student Andrea Cataldo, who has now completed her Ph.D. They shared authorship with 27 teams or individual cognitive psychology researchers who volunteered to submit their expert research conclusions for data sets sent to them by the UMass researchers. ""Our results reveal substantial variability in experts' judgments on the very same data,"" the authors state, suggesting a serious inference problem. Details are newly released in the journal Advancing Methods and Practices in Psychological Science. Starns says that objectively testing whether scientists can make valid theoretical inferences by analyzing data is just as important as making sure they are working with replicable data patterns. ""We want to ensure that we are doing good science. If we want people to be able to trust our conclusions, then we have an obligation to earn that trust by showing that we can make the right conclusions in a public test."" For this work, the researchers first conducted an online study testing recognition memory for words, ""a very standard task"" in which people decide whether or not they saw a word on a previous list. The researchers manipulated memory strength by presenting items once, twice, or three times and they manipulated bias -- the overall willingness to say things are remembered -- by instructing participants to be extra careful to avoid certain types of errors, such as failing to identify a previously studied item. Starns and colleagues were interested in one tricky interpretation problem that arises in many recognition studies, that is, the need to correct for differences in bias when comparing memory performance across populations or conditions. Unfortunately, this situation can arise if memory for the population of interest if equal to, better than, or worse than controls. Recognition researchers use a number of analysis tools to distinguish these possibilities, some of which have been around since the 1950's.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     To determine if researchers can use these tools to accurately distinguish memory and bias, the UMass researchers created seven two-condition data sets and sent them to contributors without labels, asking them to indicate whether or not the conditions were from the same or different levels of the memory strength or response bias manipulations. Rotello explains, ""These are the same sort of data they'd be confronted with in an experiment in their own labs, but in this case we knew the answers. We asked, 'did we vary memory strength, response bias, both or neither?'"" The volunteer cognitive psychology researchers could use any analyses they thought were appropriate, Starns adds, and ""some applied multiple techniques, or very complex, cutting-edge techniques. We wanted to see if they could make accurate inferences and whether they could accurately gauge uncertainty. Could they say, 'I think there's a 20 percent chance that you only manipulated memory in this experiment,' for example."" Starns, Rotello and Cataldo were mainly interested in the reported probability that memory strength was manipulated between the two conditions. What they found was ""enormous variability between researchers in what they inferred from the same sets of data,"" Starns says. ""For most data sets, the answers ranged from 0 to 100 percent across the 27 responders,"" he adds, ""that was the most shocking."" Rotello reports that about one-third of responders ""seemed to be doing OK,"" one-third did a bit better than pure guessing, and one-third ""made misleading conclusions."" She adds, ""Our jaws dropped when we saw that. How is it that researchers who have used these tools for years could come to completely different conclusions about what's going on?"" Starns notes, ""Some people made a lot more incorrect calls than they should have. Some incorrect conclusions are unavoidable with noisy data, but they made those incorrect inferences with way too much confidence. But some groups did as well as can be expected. That was somewhat encouraging."" In the end, the UMass Amherst researchers ""had a big reveal party"" and gave participants the option of removing their responses or removing their names from the paper, but none did. Rotello comments, ""I am so impressed that they were willing to put everything on the line, even though the results were not that good in some cases."" She and colleagues note that this shows a strong commitment to improving research quality among their peers. Rotello adds, ""The message here is not that memory researchers are bad, but that this general tool can assess the quality of our inferences in any field. It requires teamwork and openness. It's tremendously brave what these scientists did, to be publicly wrong. I'm sure it was humbling for many, but if we're not willing to be wrong we're not good scientists."" Further, ""We'd be stunned if the inference problems that we observed are unique. We assume that other disciplines and research areas are at risk for this problem."" "
Science Daily,Scientists 'Must Be Allowed to Cry' About Destruction of Nature,Science & Society,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010142124.htm,"   In a letter published in the journal Science, three leading researchers say it is ""dangerously misguided"" to assume scientists are dispassionate observers. They say many scientists experience ""strong grief responses"" to the current ecological crisis, and there are profound risks to ignoring this emotional trauma. Tim Gordon, lead author of the letter and a marine biologist from the University of Exeter, said ""We're documenting the destruction of the world's most beautiful and valuable ecosystems, and it's impossible to remain emotionally detached. ""When you spend your life studying places like the Great Barrier Reef or the Arctic ice caps, and then watch them bleach into rubble fields or melt into the sea, it hits you really hard."" Co-writer Professor Andy Radford, of the University of Bristol, added: ""The emotional burden of this kind of research should not be underestimated.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Grief, when unaddressed, can cloud judgment, inhibit creativity and engender a sense that there is no way forward."" The letter calls on academic institutions to support environmental scientists, allowing them to address their ecological grief professionally and emerge stronger from traumatic experiences to discover new insights about the natural world. The authors fear that environmental scientists tend to respond to degradation of the natural world by ignoring, suppressing or denying the resulting painful emotions while at work. But they propose that much can be learned from professions where distressing events are common, such as healthcare, emergency services and the military. In these fields, well-defined strategies exist for employees to anticipate and manage their emotional distress, including training, debriefing, support and counselling after disturbing events.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Dr Steve Simpson of the University of Exeter, also a co-writer of the letter, said: ""Instead of ignoring or suppressing our grief, environmental scientists should be acknowledging, accepting and working through it. ""In doing so, we can use grief to strengthen our resolve and find ways to understand and protect ecosystems that still have a chance of survival in our rapidly changing world."" The letter ends by suggesting that better psychological support for environmental scientists might improve their ability to think creatively about the future. Gordon said: ""If we're serious about finding any sort of future for our natural ecosystems, we need to avoid getting trapped in cycles of grief. ""We need to allow ourselves to cry -- and then see beyond our tears."" "
Science Daily,New Study Analyzes FEMA-Funded Home Buyout Program,Science & Society,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010113205.htm,"   A research team led by scientists at the University of Miami (UM) Rosenstiel School of Marine and Atmospheric Science found that FEMA-funded voluntary buyouts of flood-prone properties have been more likely to take place in counties with higher population and income. However, the buyouts themselves were concentrated in neighborhoods with lower income and greater social vulnerability. The researchers hope their analysis can provide insights to help revise this program in the future. ""In recent decades, communities throughout the United States have been building experiences with retreat from flood-prone areas, largely through voluntary buyouts of properties after disasters,"" said the study's lead author Katharine Mach, an associate professor of marine ecosystems and society at the UM Rosenstiel School. ""While a proven method of reducing risk, buyouts to date have also illustrated the challenges with locally driven managed retreat and the potential benefits of experimentation with different retreat policies in the future."" In the U.S., over 40,000 homes in flood-prone areas have been purchased and returned to open space in 49 states and three U.S. territories under the FEMA program since its inception in the late 1990s. The federally funded program is typically administered through local government agencies. Mach and colleagues analyzed data on the more than 40,000 buyouts alongside data on flood risk, socioeconomics and demographics to reveal that local governments in counties with higher population and income were more likely to administer buyouts of flood-prone properties. However, at the zip-code level, the bought-out properties themselves were concentrated in areas of lower population and income. As a first national-level analysis of all FEMA-funded buyouts implemented to date, the authors' findings suggest insights on what is working and what is not. The results can support revision of buyout practices, including their potential deployment at larger scales in the future in combination with different types of retreat and climate adaptation policies. ""Retreat in some places will become unavoidable under intensifying climate change, but there are open questions about where, when, and how retreat under climate change will occur,"" said Caroline Kraan, a graduate student at the UM Abess Center for Ecosystem Science and Policy and a coauthor of the study. ""Our analysis offers lessons from buyouts to date that can inform and support strategies of managed retreat in response to climate change."" The analysis also showed that the top three states for most flood damage -- Florida, Louisiana and Mississippi -- ranked 23, 18, and 21 respectively in deployment of buyouts, pointing to the fact that not all flood-damaged areas are currently utilizing the FEMA program for flood risk management. ""These findings suggest that buyouts are resource-intensive to administer, which might make them less feasible in areas that need them the most,"" said study coauthor A.R. Siders, an assistant professor in the Disaster Research Center at the University of Delaware. ""They also suggest that something in the FEMA program favors using buyouts in lower-income neighborhoods, which may have social justice implications."" The authors believe that the analysis can help inform a larger dialogue to better understand how managed retreat is being used now and into the future as governments increasingly look for ways to protect people and property from rising sea levels and more frequent flooding and other natural hazards as a result of climate change. ""The buyout program represents billions of dollars in government spending, yet until now, we didn't have a broad understanding of who benefitted from those investments,"" said Miyuki Hino, a postdoctoral researcher at Stanford University and coauthor of the study. ""Now, we can see not only who has benefitted, but also what types of places and communities may be falling through the cracks."" This study is part of a broader project on managed retreat by the research team. "
Science Daily,"Bad Behavior Between Moms Driven by Stereotypes, Judgment",Science & Society,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009132326.htm,"   The results, published in the Journal of Family Communication, are concerning to Kelly Odenweller, lead author and assistant teaching professor of communication studies at Iowa State University. She says support networks are critical and negative experiences with other mothers may be detrimental to a mother's overall well-being. ""It's not unusual for moms to have low self-esteem or feel they're not living up to the standards of what it means to be a mom,"" Odenweller said. ""If other moms treat them poorly, even when they're trying to do a good job, they may feel they can't turn to other people in their community for support. It can be very isolating and all that self-doubt can lead to anxiety and depression, which can negatively affect the entire family."" The study builds on previous research, in which Odenweller identified seven different stereotypes of stay-at-home and working mothers (see below). She and co-authors at West Virginia and Chapman universities surveyed more than 500 mothers to learn more about their attitudes, emotions and even harmful behaviors toward mothers who fit one of the seven stereotypes. According to the results, ideal and lazy mothers drew the most contempt from both working and stay-at-home mothers. The overworked stay-at-home mom also was near the top of the list. Odenweller says survey participants expressed negative feelings and admitted they would treat a lazy or ideal mother poorly, by excluding her, arguing with or verbally attacking her. Not all of the responses were negative. All mothers felt pity for overworked working mothers and were more willing to offer them help. Working mothers did express admiration for ideal moms who appear to have it all together. Odenweller says this response only came from working mothers and she suspects they see these ideal moms as a champion for their cause.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Working moms juggle a lot and want more support for all mothers with careers. For them, it may be more of a social statement that women can be great at their careers and being moms,"" Odenweller said. Stereotypes may not be reality The positive and negative responses varied depending on how mothers categorized themselves and the stereotypes they applied to other mothers. Odenweller says this was one of the more interesting findings because the way a mother treated another was based on her own perception of the other mother. For example, a working mom may feel envy or contempt toward an ideal, stay-at-home mom, but that mom may see herself differently. ""In some cases,"" she said, ""these are mothers who embody what our culture believes is a good mom and yet among mothers, they are treating each other very negatively."" Odenweller says many of the stereotypes have developed from societal ideals applied to mothers. TV, movies and other types of media perpetuate these standards of what makes a good mom. This all adds to the pressures on mothers.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     What's a mom to do? While mothers cannot control how they are judged, they can control the impression they make on other mothers. Odenweller says one way to do that is to establish common ground and shared interests. When you first meet another mother, it may be tempting to boast about the things you do for your kids or share pictures, but Odenweller recommends avoiding that temptation until you've built a relationship. ""Mothers should think of other mothers as an ally, not someone to compare themselves to,"" she said. ""Try to avoid coming across like the best mom. Talk about things you have in common, things you both enjoy as mothers and do not feel like it's necessary to be better than her."" Christine Rittenour, Megan Dillow, Aaron Metzger and Scott Myers, West Virginia University; and Keith Weber, Chapman University; all contributed to the research. Stereotypes of mothers The following apply to both stay-at-home and working mothers, with the exception of lazy Overworked: Wants to do it all, but is overextended and it shows  Home, family-oriented: Prioritizes children, partner's needs and responsibilities at home  Ideal: Juggling several responsibilities, but gets it done and doesn't appear stressed  Hardworking, balanced: Not an ideal mom, but ambitious, dedicated  Non-traditional: Modern, liberal progressive -- makes choices that are good for herself and family, whether at home or work  Traditional: Embodies the roles expected of a woman, believes her main purpose is to raise children and maintain the household  Lazy: Not nurturing, attentive or hardworking -- applies only to stay-at-home moms "
Science Daily,Graphene Turns 15 on Track to Deliver on Its Promises,Business & Industry,2019-10-04,-,https://www.sciencedaily.com/releases/2019/10/191004105625.htm,"   In a world dominated by the immediacy of social media and digital technologies, it is hard to take a step back and think about how long materials take to develop. The silicon transistor, at the heart of all our beloved gadgets, was engineered in 1958. However, scientists had known of silicon for over 120 years -- it was discovered in 1824. Although expecting broad market penetration for graphene today would not be realistic, the truth is that one can already find graphene-enabled products on the market. A number of these commercial applications have been enabled by the Graphene Flagship, a project funded by the European Commission that kicked off in 2013. Bringing together nearly 150 partners from 23 countries, it created the perfect breeding ground for innovation, which could not emerge without an intricate web of collaborations between academics, researchers, and industries. The Graphene Flagship also acted as inspiration for many programmes on graphene and related layered materials in many other countries. The Graphene Flagship expects short-term applications in the materials sector, with graphene-enabled inks, composites, and coatings, for applications ranging from food packaging to textiles and sports goods. In the mid-term, graphene could be crucial for the energy sector, and market analyses agree on a high potential for graphene-enabled batteries and supercapacitors. With the first graphene-enabled solar farm to be installed in Crete next year, the Graphene Flagship will showcase how graphene can enable more sustainable energy generation, in line with Europe's commitment to renewable energies. A host of applications for graphene are expected to hit the market 10 to 15 years from now. These are related to (opto)electronics, where graphene can deliver performances orders of magnitude higher than current technologies. The developments in this area could trigger the next-generation of (opto)electronic devices, bringing the 'more-than-Moore' devices to reality. To secure its most valuable strength -- bridging the gap between basic and applied research -- the Graphene Flagship has also announced the creation of the first graphene foundry. With a budget of almost €20 million over four years, this experimental pilot line will pave the way towards commercially competitive graphene products, such as transceivers, photodetectors, and sensors. The Graphene Flagship foundry will also develop a process design kit: a set of 'instructions' to support product tape-out and guarantee that the finalised designs are high-quality and consistent. The foundry will be accessible by academia and industry stakeholders worldwide. Kari Hjelt, Graphene Flagship Head of Innovation stated: ""We are now seeing the first wave of graphene-enabled products on the market. The commercialisation activities of graphene are moving from materials development towards components and system level integration. In the future we will see a growing number of high-value add products for various application domains."" Thomas Reiss, Graphene Flagship Work Package Leader for Industrialisation, adds: ""Key factors facilitating the further commercialisation of graphene comprise establishing innovation ecosystems and providing holistic innovation support. This includes elaborating innovation roadmaps and creating trust and confidence in graphene among industry by trusted validation and standardization services."" Andrea C. Ferrari, Graphene Flagship Science and Technology Officer and Chair of its Management Panel, concludes: ""Graphene and related materials are progressing towards commercialization at the expected pace. The Graphene Flagship is not about hype, but about concrete and tangible results and progress. The Flagship Foundry will strengthen the EU position as world leader and pioneer in graphene technology and facilitate incorporation of graphene devices in various industries."" "
Science Daily,How Much Are You Polluting Your Office Air Just by Existing?,Business & Industry,2019-10-03,-,https://www.sciencedaily.com/releases/2019/10/191003162541.htm,"   To find out, a team of engineers at Purdue University has been conducting one of the largest studies of its kind in the office spaces of a building rigged with thousands of sensors. The goal is to identify all types of indoor air contaminants and recommend ways to control them through how a building is designed and operated. ""If we want to provide better air quality for office workers to improve their productivity, it is important to first understand what's in the air and what factors influence the emissions and removal of pollutants,"" said Brandon Boor, an assistant professor of civil engineering with a courtesy appointment in environmental and ecological engineering. The data is showing that people and ventilation systems greatly impact the chemistry of indoor air -- possibly more than anything else in an office space. The researchers will present their initial findings at the 2019 American Association for Aerosol Research Conference in Portland, Oregon, Oct. 14-18. ""The chemistry of indoor air is dynamic. It changes throughout the day based on outdoor conditions, how the ventilation system operates and occupancy patterns in the office,"" Boor said. The building, called the Living Labs at Purdue's Ray W. Herrick Laboratories, uses an array of sensors to precisely monitor four open-plan office spaces and to track the flow of indoor and outdoor air through the ventilation system. The team developed a new technique to track occupancy by embedding temperature sensors in each desk chair.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Through use of the Living Labs, Boor's team has begun to identify previously unknown behaviors of chemicals called volatile organic compounds, such as how they are transformed in ventilation systems and removed by filters. ""We wanted to shed light on the behind-the-scenes role ventilation systems have on the air we breathe,"" Boor said. Boor teamed up with researchers at RJ Lee Group to deploy a highly sensitive ""nose"" -- an instrument that scientists call a proton transfer reaction time-of-flight mass spectrometer. The instrument, typically used for measuring outdoor air quality, helped ""sniff"" out compounds in human breath, such as isoprene, in real time. Boor's team found that isoprene and many other volatile compounds linger in the office even after people have left the room. A greater number of people in a room also means more emissions of these compounds. A YouTube video is available at https://youtu.be/mi1xi31QAfQ. ""Our preliminary results suggest that people are the dominant source of volatile organic compounds in a modern office environment,"" Boor said. ""We found levels of many compounds to be 10 to 20 times higher indoors than outdoors. If an office space is not properly ventilated, these volatile compounds may adversely affect worker health and productivity."" The team also revealed that a pollutant entering from outside, ozone, disappears inside. This is because ozone interacts with other indoor compounds and the vast surfaces of a furnished office. The researchers found that ozone and compounds released from peeling an orange, called monoterpenes, mix to form new, super-tiny particles as small as one-billionth of a meter. The newly formed particles could be toxic because they are small enough to get into the deepest regions of a person's lungs. The effects of volatile compounds released in an office might not just be restricted to indoors. The researchers believe that chemicals emitted from self-care products such as deodorant, makeup, and hair spray may elevate levels outdoors as they are vented outside by the ventilation system. This work is funded in part by the National Science Foundation Environmental Engineering Program, the Alfred P. Sloan Foundation Chemistry of Indoor Environments Program and the Purdue Research Foundation. "
Science Daily,No Evidence That Power Posing Works,Business & Industry,2019-10-01,-,https://www.sciencedaily.com/releases/2019/10/191001110824.htm,"   The concept of power posing -- think of a Wonder Woman stance -- gained popularity after a 2010 study reported that people who adopted an expansive physical pose decreased cortisol levels (an indicator of stress), increased testosterone levels and felt more powerful and willing to take risks. However, Marcus Credé, an associate professor of psychology at Iowa State, says there is not a single study to support the claims that power posing works. Not long after the original study was published it drew criticism because the results could not be replicated. In 2018, the researchers responded to critics by presenting an updated analysis of their own research and other studies on power posing to support their claims. In a new commentary, published by the open-access journal Meta-Psychology, Credé reviewed every study on power posing as well as the analysis the researchers provided and found a significant flaw. Nearly all of the studies he reviewed were poorly designed and failed to compare power poses to normal poses. Instead, they only compared power poses to contractive ones, such as slouching. Credé says not having a neutral pose for comparison can skew the results. That's because any difference between a power pose and a contractive pose could occur because a contractive pose makes you feel worse, rather than an expansive pose making you feel better. The lack of oversight is troubling, Credé said, knowing that dozens of researchers have worked on this issue and never identified the problem. What he finds even more concerning is the number of people who have bought into the concept. A TED Talk on power posing has been viewed more than 70 million times and a book on power posing was a New York Times bestseller. ""There has literally never been a study that compared a power pose to a normal pose and found any positive effect for a power pose,"" Credé said. ""I find this pretty stunning because of the multimillion-dollar industry that has been built up around power posing. It is not dissimilar to a drug being sold to the public without a single study ever having been able to show that the drug works better than placebo or doing nothing."" Feelings of power diminished when compared to neutral pose Only four of the nearly 40 studies that exist on power posing were designed in such a way as to shed light on the benefits, Credé said. One of those studies compared the effect of slouched, neutral and power poses on feelings of dominance. According to the findings, feelings of dominance were highest in the neutral position and the power pose was associated with diminished feelings of power. Similarly, three other studies examined the three poses to determine the effect on mood. All reported significant differences in mood for different poses, but Credé says the results are driven by the negative effect of the slouching posture. ""The only conclusion researchers should draw from the existing literature on postural feedback is that contractive poses such as slouching should be avoided, which is hardly novel,"" he said. ""I recall my elementary school teachers yelling at us about slouching and not what has been sold here."" "
Science Daily,Impostor Syndrome Is More Common Than You Think; Study Finds Best Way to Cope With It,Business & Industry,2019-09-24,-,https://www.sciencedaily.com/releases/2019/09/190924080016.htm,"   Findings of the study, co-authored by Brigham Young University professors Jeff Bednar, Bryan Stewart, and James Oldroyd, revealed that 20 percent of the college students in their sample suffered from very strong feelings of impostorism. The researchers conducted interviews with students in an elite academic program to understand the various coping mechanisms students used to escape these feelings, but one particular method stood out above the rest: seeking social support from those outside their academic program. The findings of their interview study suggest that if students ""reached in"" to other students within their major, they felt worse more often than they felt better. However, if the student ""reached out"" to family, friends outside their major, or even professors, perceptions of impostorism were reduced. ""Those outside the social group seem to be able to help students see the big picture and recalibrate their reference groups,"" said Bednar, a BYU management professor and co-author on the study. ""After reaching outside their social group for support, students are able to understand themselves more holistically rather than being so focused on what they felt they lacked in just one area."" Along with seeking social support, the study also uncovered negative ways students coped with impostorism. Some students tried to get their mind off schoolwork through escapes such as video games but ended up spending more time gaming than studying. Other students tried to hide how they really felt around their classmates, pretending they were confident and excited about their performance when deep down they questioned if they actually belonged. In a second study, the researchers surveyed 213 students to confirm what was revealed in their interview study about seeking social support: reaching out to individuals outside the major proved to be more effective than reaching in to individuals within the major. Surprisingly, the study also reveals that perceptions of impostorism lack a significant relationship with performance. This means that individuals who suffer with the impostor syndrome are still capable of doing their jobs well, they just don't believe in themselves. Researchers also explain that social-related factors impact impostorism more than an individual's actual ability or competence. ""The root of impostorism is thinking that people don't see you as you really are,"" said Stewart, an accounting professor at BYU and co-author on the study. ""We think people like us for something that isn't real and that they won't like us if they find out who we really are."" Outside the classroom, researchers believe that implications from this study can and should be applied in the workplace as well. ""It's important to create cultures where people talk about failure and mistakes,"" Bednar said. ""When we create those cultures, someone who is feeling strong feelings of impostorism will be more likely to get the help they need within the organization."" "
Science Daily,The Future of 'Extremely' Energy-Efficient Circuits,Business & Industry,2019-09-18,-,https://www.sciencedaily.com/releases/2019/09/190918105637.htm,"   To answer this demand, a team of researchers from Japan and the United States have developed a framework to reduce energy consumption while improving efficiency. They published their results on July 19 in Scientific Reports, a Nature journal. ""The significant amount of energy consumption has become a critical problem in modern society,"" said Olivia Chen, corresponding author of the paper and assistant professor in the Institute of Advanced Sciences at Yokohama National University. ""There is an urgent requirement for extremely energy-efficient computing technologies."" The research team used a digital logic process called Adiabatic Quantum-Flux-Parametron (AQFP). The idea behind the logic is that direct current should be replaced with alternating current. The alternating current acts as both the clock signal and the power supply -- as the current switches directions, it signals the next time phase for computing. The logic, according to Chen, could improve conventional communication technologies with currently available fabrication processes.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""However, there lacks a systematic, automatic synthesis framework to translate from high-level logic description to Adiabatic Quantum-Flux-Parametron circuit netlist structures,"" Chen said, referring to the individual processors within the circuit. ""In this paper, we mitigate that gap by presenting an automatic flow. We also demonstrate that AQFP can achieve a reduction in energy use by several orders of magnitude compared to traditional technologies."" The researchers proposed a top-down framework for computing decisions that can also analyze its own performance. To do this, they used logic synthesis, a process by which they direct the passage of information through logic gates within the processing unit. Logic gates can take in a little bit of information and output a yes or no answer. The answer can trigger other gates to respond and move the process forward, or stop it completely. With this basis, the researchers developed a computation logic that takes the high-level understanding of processing and how much energy a system uses and dissipates and describes it as an optimized map for each gate within the circuit model. From this, Chen and the research team can balance the estimation of power needed to process through the system and the energy that the system dissipates. According to Chen, this approach also compensates for the cooling energy needed for superconducting technologies and reduces the energy dissipation by two orders of magnitude. ""These results demonstrate the potential of AQFP technology and applications for large-scale, high-performance and energy-efficient computations,"" Chen said. Ultimately, the researchers plan to develop a fully automated framework to generate the most efficient AQFP circuit layout. ""The synthesis results of AQFP circuits are highly promising in terms of energy-efficient and high-performance computing,"" Chen said. ""With the future advancing and maturity of AQFP fabrication technology, we anticipate broader applications ranging from space applications and large-scale computing facilities such as data centers."" "
Science Daily,Virtual Reality Training Could Improve Employee Safety,Business & Industry,2019-09-16,-,https://www.sciencedaily.com/releases/2019/09/190916212516.htm,"   The Human Factors Research Group at the University of Nottingham, developed an immersive VR system to stimulate participants' perception of temperature, and senses of smell, sight and hearing to explore how they behaved during two health and safety training scenarios: an emergency evacuation in the event of a fire and a fuel leak. In one scenario, participants had to evacuate from a virtual fire in an office, seeing and hearing using a VR headset but could also feel heat from three 2kW heaters, and could smell smoke from a scent diffuser, creating a multisensory virtual environment. This group was compared against another group who were observed in this scenario using only audio-visual elements of VR. Observing real life behaviours Previous research on human behaviour during real-world fire incidents has shown that a lack of understanding of the spread and movement of fire often means that occupants are unprepared and misjudge appropriate actions. Immersive health and safety training enables employers to train people about hazards and hazardous environments without putting anyone at risk. The Nottingham research, funded by the Institution of Occupational Safety and Health (IOSH), found contrasts between the groups in the way participants reacted to the scenario. Those in the multi-sensory group had a greater sense of urgency, reflecting a real-life scenario, and were more likely to avoid the virtual fires. Evidence from the audio-visual participants suggested that they were treating the experience more like a game and behaviours were less consistent with those expected in a real world situation.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     Dr Glyn Lawson, Associate Professor in the Faculty of Engineering, University of Nottingham, said: ""Health and safety training can fail to motivate and engage employees and can lack relevance to real-life contexts. Our research, which has been funded by the Institution of Occupational Safety and Health, suggests that virtual environments can help address these issues, by increasing trainees' engagement and willingness to participate in further training. There are also business benefits associated with the use of virtual environment training, such as the ability to deliver training at or near the workplace and at a time that is convenient to the employee."" Virtual Reality vs. PowerPoint A further test was done, as part of the study, to measure the effectiveness of VR training versus traditional PowerPoint training. Participants took questionnaires, testing their knowledge on either fire safety or safe vehicle disassembly procedure, before and after training as well as one week later. While those trained via PowerPoint appeared to have gained more knowledge when tested directly after training, there was a significantly larger decrease in knowledge scores when participants were retested one week later. In comparison, the VR group's long term retention was better and reported higher levels of engagement; attitude to occupational safety and health; and willingness to undertake training in the future. The research suggests that the increased cognitive engagement of learning in the virtual environment creates more established and comprehensive mental models which can improve recall, and implies that testing an employee's knowledge immediately following health and safety training may not be an effective means of gaging long-term knowledge of health and safety.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Applications to the work place Mary Ogungbeje, Research Manager at IOSH, said: ""The wheels are turning so that virtual and smart learning is increasingly engrained in the workplace and everyday life. ""Technology is continuously advancing and in many cases becoming more affordable, so this study gives us a taste of what's to come. By improving training strategies with the use of technology and stimulated sensory experiences, we are heading in a direction where the workforce will not just enjoy a more immersive and interesting training course but participate in an effective learning experience, so they are better prepared and equipped to stay safe, healthy and well at work."" The researchers conducted meetings, discussions, and visits with partners including Rolls-Royce, for expert advice around fire safety and safe handling of hazardous chemicals. The University of Nottingham's Health and Safety advisors also contributed to help the researchers better understand how the training may be implemented in industry. The study aims to produce evidence-based guidance for the development and use of virtual environments in engaging and effective training using cost-effective and accessible solutions. The full study features in a report, titled 'Immersive virtual worlds: Multisensory virtual environments for health and safety training', to be released at the IOSH's annual conference on Tuesday 17 September. Further information: https://www.iosh.com/multisensoryVE "
Science Daily,A Little Kindness Goes a Long Way for Worker Performance and Health,Business & Industry,2019-09-10,-,https://www.sciencedaily.com/releases/2019/09/190910154708.htm,"   ""An ultimate solution to improve worker performance and health could be big pay raises or reduced workloads, but when those solutions aren't feasible, we found that even small offerings can make a big difference,"" said Bu Zhong, associate professor of journalism at Penn State. According to Zhong, bus drivers are vulnerable to specific health problems due in large part to their stressful working environment, which often includes irregular shift schedules, unpredictable traffic conditions and random meal times. In addition, the sedentary nature of driving and continuous whole-body vibration contributes to fatigue, musculoskeletal problems such as lower-back pain, cardiovascular diseases and gastrointestinal issues. Zhong and his colleagues conducted an experiment with 86 Shenzen bus drivers. During the experiment, on-duty bus drivers were given, in addition to their typical box lunch which includes no fruit, a serving of fresh fruit -- either an apple or a banana -- for three weeks. The cost of the fruit was 73 cents per meal. The team distributed surveys to the bus drivers at three time intervals -- one week before the experiment began, once in the middle of the three-week-long experiment and one week following the end of the experiment. The findings appear today in the International Journal of Occupational Safety and Ergonomics. The researchers assessed depression with a personal health questionnaire that is recommended by the U.S. Centers for Disease Control and Prevention. The scale consisted of eight items, asking the participants to rate, for example, how often during the past two weeks they felt down, depressed or hopeless, and had trouble falling or staying asleep. ""Bus drivers reported significantly decreased depression levels one week after the experiments ended compared to one week before it began,"" said Zhong. The team measured self-efficacy -- perceived confidence and ability to implement the necessary actions and tasks so as to achieve specific goals -- using the 10-item General Self-Efficacy Scale. Items on this scale included, ""I can always manage to solve difficult problems if I try hard enough"" and ""I can usually handle whatever comes my way."" ""We found that self-efficacy was significantly higher in the middle of the experiment week than in the week after the experiment ended,"" said Zhong. Zhong concluded that while eating an extra apple at lunchtime may seem trivial, its impact can be large. ""This research suggests that employees can be sensitive to any improvement at the workplace,"" he said. ""Before an ultimate solution is possible, some small steps can make a difference -- one apple at a time."" "
Science Daily,Good at Math? It Means Little If You're Not Confident,Business & Industry,2019-09-09,-,https://www.sciencedaily.com/releases/2019/09/190909154211.htm,"   In two studies, researchers found that the key to success in personal finances and dealing with a complex disease was a match between a person's math abilities and how comfortable and assured he or she felt using those skills. A lack of numeric confidence can essentially wipe out most of the advantage a person with good math skills may have, said Ellen Peters, who completed the work as a professor of psychology at The Ohio State University. And the effects were not small. Take, for example, people who scored 100 percent on a math test used in this research, published today (Sept. 9, 2019) in the journal PNAS. Having high confidence in their math ability compared to low confidence was the equivalent of having $94,000 more in annual income. ""If you have low numeric confidence, all the math skills in the world appear not to help much,"" said Peters, who is now at the University of Oregon.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""We think that those who lack confidence don't persist with the numbers when the going gets tough or tedious. As a result, their skills aren't used."" In both studies, participants took a test that measured their objective math skills. They also completed a questionnaire that measured how confident and self-assured they felt using numbers. Those who scored high in numeric confidence reported feeling comfortable in their abilities with numbers and believed they were good with fractions and probabilities. These people are likely to enjoy doing number-related tasks more, Peters said. Most importantly, they are more likely to persist when a math-related task is tedious or difficult, she said. In the first study, the researchers investigated self-reported financial outcomes among 4,572 Americans who participated in the Understanding America Study, run by the University of Southern California.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     Participants reported various financial outcomes, such as credit card debt, investments and whether they had payday loans. Results showed that the interaction between participants' objective math scores and their numeric confidence predicted how well they were doing financially -- as the above example from the study revealed. The second study involved 91 patients at Ohio State's Wexner Medical Center who were being treated for lupus. Lupus has no cure, but medical interventions and lifestyle changes can help control it. However, it takes good math skills to navigate the disease, such as understanding the risks and benefits of drugs, using correct drug doses and making good health insurance and provider choices, Peters said. But just as importantly, because it is a chronic disease, patients must persist using these math skills over a lifetime in order to adhere to multiple timed medications, navigate frequent treatment changes and adopt healthy behaviors. Results showed that the interaction between patients' objective math skills and their numeric confidence was related to how their physicians rated disease activity, such as new rashes or seizures. Those who scored high in skill and confidence showed less disease activity than those who had the skills but not the confidence. But the worst outcomes came to those who thought they were great at math, but actually were not. Among those who had high confidence, patients who were good at math had only a 7 percent chance of having bad disease activity -- compared to 44 percent who had low math skills. ""If you have low ability and high confidence, you may end up making mistakes that you don't recognize. You're not asking for help because you think you don't need it, so you end up in worse shape,"" Peters said. So how many people are mismatched between their abilities and skills? In these two studies, 18 to 20 percent had good math skills and low confidence. Another 12 to 13 percent had bad math skills combined with high confidence. ""Almost a third of our population is mismatched in ways that could harm them,"" Peters said. The results suggest that efforts to improve people's math skills or their numeric confidence alone may not be helpful. ""More of one is not necessarily better if you don't increase both,"" she said. The best advice for everyone should be to understand their skills, Peters said. For some, that may mean to ""be open to the possibility that you're good at math,"" she said. For others, it may mean asking for and accepting help as needed. "
Science Daily,"E-Cigarettes, Tobacco and Cannabis Products Are Littering High Schools",Education & Learning,2019-10-10,-,https://www.sciencedaily.com/releases/2019/10/191010135706.htm,"   The study of a dozen Bay Area high schools uncovered hundreds of waste items littering the parking lots and sidewalks in and around the schools. In addition to reflecting the widespread use of these products among teens, the researchers say these items are an environmental hazard due to the heavy metals, plastics, nicotine, lithium-ion batteries and other toxins these products can contain. The paper publishes Oct. 10, 2019 in Morbidity and Mortality Weekly Report (MMWR), a scientific journal of the federal Centers for Disease Control and Prevention. ""We're in the midst of an epidemic of teen e-cigarette use that is causing substantial toxic waste contamination at some high schools from teens discarding these products on the ground,"" said first author Jeremiah Mock, PhD, a health anthropologist and associate professor in UCSF's School of Nursing's Institute for Health and Aging. He also is a member of UCSF's Center for Tobacco Control Research and Education. ""But e-cigarettes are not the only problem,"" Mock said. ""Our research also shows that little cigars, cigarillos and menthol cigarettes are popular at schools with large proportions of lower-income Latinx and African American families. These toxic products are contaminating school environments and surrounding areas, going down storm drains and contaminating the bay."" The United States is experiencing an epidemic of vaping, especially by youth using flavored products. Often, the vaping takes place in the bathrooms, classrooms and parking lots at their schools.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     From 2017 to 2018, e-cigarette use among U.S. high schools increased by 78 percent -- from 12 percent of high school students to 21 percent -- and by 48 percent among middle school students, according to the CDC. Communities in the Bay Area have been at the forefront of this epidemic: for example, in Marin County between 2016 and 2018, e-cigarette use among 11th graders rose 155 percent, from 11 percent to 28 percent, reports the MMWR study. JUUL, the dominant e-cigarette maker, is headquartered in San Francisco. The Trump administration in September announced plans to ban flavored e-cigarette products, which account for a significant portion of the market. A recent analysis of the National Youth Tobacco Survey showed that among high school students who currently used e-cigarettes, the percentage who used flavored e-cigarettes increased from 65.1% in 2014 to 67.8% in 2018. San Francisco and other cities and towns throughout the Bay Area are banning retail sales of flavored e-cigarettes and other flavored tobacco products including, menthol cigarettes; various commercial retailers are also ending e-cigarette sales. San Francisco has banned retail and internet sales of all e-cigarettes. The new garbology study of environmental contamination was conducted at 12 public high schools in San Francisco, Contra Costa, Alameda and Marin counties. The schools, which are not identified by name, were stratified by the percentages of students from low-income families. Researchers and student interns collected waste items from the student parking lots and exterior perimeter areas on a single day at each school between July 2018 and April 2019.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     ""Our study novelly detected the under-addressed problem of flavored little cigars and cigarillos in low-income youth populations,"" said co-author Yogi Hale Hendlin, PhD, a public health scientist and environmental philosopher with appointments at Erasmus University Rotterdam and UCSF's Environmental Health Initiative. ""Youth e-cigarette use -- as epidemic as it is -- seems to be lopsided towards higher-income student populations, with combustible tobacco product waste found in higher concentrations in lower-income schools,"" he said. Altogether, nearly 900 waste items were collected. The researchers found: Approximately 19 percent of the product waste was from e-cigarettes -- nearly all of these were JUUL or JUUL-compatible pods and pod caps. Far more JUUL products were found at middle- and upper-income area schools. 73 out of 74 pod caps were from flavored pods other than tobacco flavors, including ""Cool Mint,"" ""Cool Cucumber,"" and ""Classic Menthol."" At four high schools where the student populations are predominantly lower-income African American and Latinx, 71 little cigar or cigarillo plastic packages and mouthpieces were found; no such items were found at upper-income area schools. Across all the schools, 620 cigarette butts were collected, with 65 percent of identifiable butts being from recently smoked cigarettes. ""Action is needed to reduce youth tobacco smoking and cannabis access and use, and to eliminate environmental contamination from these products,"" said Mock, who has started a crowdfunding site to engage parents and communities in addressing the problem. ""Schools can engage students in garbology projects to clean up their schools and to raise awareness about the health and environmental hazards of these products."" "
Science Daily,Finding Upends Theory About the Cerebellum's Role in Reading and Dyslexia,Education & Learning,2019-10-09,-,https://www.sciencedaily.com/releases/2019/10/191009131759.htm,"   The cerebellum, a brain structure traditionally considered to be involved in motor function, has been implicated in the reading disability, developmental dyslexia, however, this ""cerebellar deficit hypothesis"" has always been controversial. The new research shows that the cerebellum is not engaged during reading in typical readers and does not differ in children who have dyslexia. That is the finding of a new study involving children with and without dyslexia published October 9, 2019, in the journal Human Brain Mapping. It is well established that dyslexia, a common learning disability, involves a weakness in understanding the mapping of sounds in spoken words to their written counterparts, a process that requires phonological awareness. It is also well known that this kind of processing relies on brain regions in the left cortex. However, it has been argued by some that the difficulties in phonological processing that lead to impaired reading originate in the cerebellum, a structure outside (and below the back) of the cortex. ""Prior imaging research on reading in dyslexia had not found much support for this theory called the cerebellar deficit hypothesis of dyslexia, but these studies tended to focus on the cortex,"" says the study's first author, Sikoya Ashburn, a Georgetown PhD candidate in neuroscience. ""Therefore, we tackled the question by specifically examining the cerebellum in more detail. We found no signs of cerebellar involvement during reading in skilled readers nor differences in children with reading disability."" The researchers used functional magnetic resonance imaging to look for brain activation during reading. They also tested for functional connections between the cerebellum and the cortex during reading.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Functional connectivity occurs when two brain regions behave similarly over time; they operate in sync,"" says Ashburn. ""However, brain regions in the cortex known to partake in the reading process were not communicating with the cerebellum in children with or without dyslexia while the brain was processing words."" The results revealed that when reading was not considered in the analysis -- that is when just examining the communications between brain regions at rest -- the cerebellum was communicating with the cortex more strongly in the children with dyslexia. ""These differences are consistent with the widely distributed neurobiological alterations that are associated with dyslexia, but not all of them are likely to be causal to the reading difficulties,"" Ashburn explains. ""The evidence for the cerebellar deficit theory was never particularly strong, yet people have jumped on the idea and even developed treatment approaches targeting the cerebellum,"" says senior author and neuroscientist Guinevere Eden, D. Phil, professor in the Department of Pediatrics at Georgetown University Medical Center and director for its Center for the Study of Learning. ""Standing on a wobble board -- one exercise promoted for improving dyslexia that isn't supported by the evidence -- is not going to improve a child's reading skills. Such treatments are a waste money and take away from other treatment approaches that entail structured intervention for reading difficulties, involving the learning of phonologic and orthographic processing."" In the long run, these researchers believe the findings can be used to refine models of dyslexia and to assist parents of struggling readers to make informed decisions about which treatment programs to pursue. More information about dyslexia can be found at the International Dyslexia Association or at Understood.org. This work was supported in part by grants from the Eunice Kennedy Shriver National Institute of Child Health and Human Development (P50 HD040095, R01 HD081078), and the National Center for Advancing Translational Sciences of the National Institutes of Health (TL1 TR001431). "
Science Daily,Screening Kindergarten Readiness,Education & Learning,2019-10-08,-,https://www.sciencedaily.com/releases/2019/10/191008155708.htm,"   Now, University of Missouri College of Education researchers have found that a readiness test can predict kindergarteners' success in school after 18 months. Melissa Stormont, a professor of special education, says identifying students early in the academic year who may need additional support can allow teachers and parents more time to build essential academic and social behavioral skills. ""Kindergarteners come to school from varying backgrounds and have different abilities,"" Stormont said. ""This is a critical time to assess student academic and social readiness, so that teachers can provide support as early as possible before issues worsen and become harder to change. This screening tool is a simple first step that can help children in the long run."" The researchers distributed the screening tool to 19 teachers in six elementary schools. Early in the school year, those teachers used the screener to rate 350 students. The MU researchers then compared the students' scores from the screener to their performances on a math and reading achievement test and to teacher ratings of their social and emotional skills 18 months later. Children who rated poor in academic readiness were nine to 10 times more likely to have low reading scores at the end of first grade. In addition, children who rated poor in behavior readiness were six times more likely to be rated as having displayed disruptive behavior and poor social skills by their first-grade teachers. ""Using this tool could help teachers in developing lessons and interventions to help their students who are having difficulties,"" Stormont said. ""This study highlights the need to support children more when they transition to kindergarten and these positive results definitely merit further study."" Stormont recommends that parents support children entering kindergarten by talking with their child about social behavior expectations in kindergarten and have them practice doing things like taking turns and following directions. In addition, parents and their children can meet with teachers to discuss what those expectations are. Parents also can explore summer programs before school starts that can help acclimate children to the classroom and learn routines. The study results also support efforts to help children with reading and math, as initial poor academic readiness predicted problems 18 months later. "
Science Daily,Children's Language Skills May Be Harmed by Social Hardship,Education & Learning,2019-10-08,-,https://www.sciencedaily.com/releases/2019/10/191008094254.htm,"   Researchers say the findings highlight the need for policies to address the social factors that can hamper speech, language and communication (SLC) development. Failing to do so means children might not fully develop the language skills that are critical for emotional development, wellbeing and educational and employment opportunities. A team from the University of Edinburgh and NHS Lothian looked at more than 26,000 records of children who had a routine health review between 27 and 30 months between April 2013 and April 2016. It showed that pre-school children living in the most economically deprived neighbourhoods were three times more likely to have SLC concern than those brought up in better-off areas. It is believed growing up in neighbourhoods with low income and unemployment -- which experience problems with education, health, access to services, crime and housing -- can increase the risk of setbacks. Researchers also discovered that each week a child spent in the womb from 23 to 36 weeks was associated with an 8.8 per cent decrease in the likelihood of them having an SLC concern reported at 27 months. The study used birth data from children born in the Lothians but experts say similar results might be expected across the UK. Professor of Neonatal Medicine at the University of Edinburgh's MRC Centre for Reproductive Health, James Boardman, said: ""Growing up in a disadvantaged neighbourhood where there is poverty and reduced access to services is closely associated with problems with pre-school language development. These results suggest that policies designed to lessen deprivation could reduce language and communication difficulties among pre-school children."" "
Science Daily,Analysis of US Labor Data Suggests 'Reskilling' Workers for a 'Feeling Economy',Education & Learning,2019-10-07,-,https://www.sciencedaily.com/releases/2019/10/191007153437.htm,"   The first wave of AI already has replaced humans for physical repetitive tasks like inspecting equipment, manufacturing goods, repairing things and crunching numbers. That shift started way back with the Industrial Revolution and gave rise to our current Thinking Economy, where employment and wages are more tied to workers' abilities to process, analyze and interpret information to make decisions and solve problems. But be prepared, because AI is already starting to take over those thinking tasks, Rust says. ""It means that if humans want jobs, they better get good at feeling,"" Rust says. ""Things like interpersonal relationships and emotional intelligence will be much more important."" Even though people skills have always been important, what the researchers conclude is that the value of these skills will soon be of unprecedented importance. Rust and Maryland Smith finance professor Vojislav Maksimovic, along with Professor Ming-Hui Huang of National Taiwan University, have been studying this shift. They sifted through U.S. Department of Labor data about work tasks associated with jobs and the people who perform those jobs, covering millions of workers throughout the U.S. economy. They coded the things people report doing in their day-to-day jobs as physical tasks, thinking tasks or feeling tasks and compared the breakdown for each job in 2006 and 2016. Their results reveal a profound shift across the board toward feeling tasks, a big indication that the move to a Feeling Economy is already under way. Their paper, ""The Feeling Economy: Managing in the Next Generation of Artificial Intelligence,"" appears in the most recent issue of the peer-reviewed California Management Review that examines how AI will change business. ""This is something that is going to hit people before they know it,"" says Rust. ""It's already happening. We're already seeing the shift in feeling as being more important, not only in terms of employment growth, but in terms of compensation growth. There is greater compensation growth in feeling than there is in thinking. This is really across the board -- you name a job and we can show a shift from thinking to feeling."" Take the job of a financial analyst, for example. Think that sounds pretty quantitative and thinking-oriented? No so, says Rust. The research reveals it has become much more feeling oriented in the last 10 years. ""People are using more AI-powered tools that can do a lot more of their analytical work for them, and what's left in their job is to hold people's hands and to reassure them about things like stock market dips,"" he says. Going forward, those ""feeling"" skills become even more critical.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""What we're expecting is 'people-people' will be the ones who will be the big successes,"" says Rust. ""This is different from how it is right now and how people assume it's going to be in the future."" Since AI can do more of the thinking tasks, firms need to recruit people who can perform well in feeling tasks and jobs, say the researchers. People management, working with others, emotional intelligence and negotiation skills are already in strong demand and will continue to be top skills for the future. As the Feeling Economy emerges, the nature of all jobs will change, so companies and individuals should prepare, says Rust. Organizations need to manage differently, with more emphasis on feeling, empathy and emotional intelligence. The companies that take advantage of this trend will be the most successful, he says. There will be new opportunities for feeling-oriented companies and products. This also creates opportunities to pull ahead in the global market, says Rust. Individual workers can safeguard their jobs by enhancing their feeling and empathetic skills and gravitating toward jobs that emphasize those tasks. The most successful workers will be those who can manage relationships in an empathetic and emotionally intelligent way. Managerial jobs need to be more people-oriented and feeling-conscious. This may give the edge to women for their emotional intelligence, say the researchers. The ""people"" person becomes much more valuable than the anti-social tech geek. Rust says there are also big implications for education at all levels, where more emphasis is needed on developing emotional intelligence. ""You certainly don't need to worry about things like multiplication tables,"" he says. ""You can do that on a machine, and everybody's cell phone will do that for them. That kind of skill is just useless."" Rust says we better get used to the idea of AI doing more. He thinks AI will eventually even take over most of the emotional tasks of relating to people. And as AI gets more sophisticated, there's no going back, he says. ""The genie is out of the bottle."" "
Science Daily,"Urban, Home Gardens Could Help Curb Food Insecurity, Health Problems",Education & Learning,2019-10-07,-,https://www.sciencedaily.com/releases/2019/10/191007180035.htm,"   Researchers from the University of California at San Francisco partnered with Valley Verde, a community-based urban garden organization in Santa Clara County, California, to better understand participants' perceptions of the health benefits and acceptability of urban home gardening programs. Interest in such programs has been on the rise, and this is a critical next step before beginning large-scale trials of how effective they are. ""This home-based model can play a vital role in urban agriculture and has the potential to directly impact health by tying the garden to the household,"" said lead author Kartika Palar, PhD, Department of Medicine, University of California San Francisco, San Francisco, CA, USA. She added that home and community gardens are complementary approaches to urban agriculture, together promoting a more resilient local food system. Researchers followed 32 participants -- mostly Hispanic/Latinos and women -- in Valley Verde's gardening program for one year. The program serves a predominantly low-income and immigrant population, providing them with the knowledge, skills and tools needed to grow their own organic vegetable gardens. Valley Verde staff provided 10 monthly workshops for each participant focused on organic gardening skills building as well as nutritional education, like strategies to increase vegetable, fruit and whole-grain intake; healthy shopping strategies; and culturally preferred healthy recipes. Participants were interviewed before, during and after the program to track what they learned and how they were implementing it. Nearly every participant indicated they ate more vegetables and fruits because of the program, citing increased affordability, accessibility, freshness, flavor and convenience of the garden produce. ""We had some delicious meals with lots of peas because the winter peas were doing really well, and then we could just draw on that when you're out of options,"" a 47-year-old female participant said in the study, describing how the garden helped during times of the month when money ran low. ""[Fruits and vegetables] are a more steady supply. Yeah, it isn't like, 'Oh guess what? In this pay period we can actually afford some salad.' Now we just go and just harvest it and just have it all the time."" ""I value more the things that I cook, and the things that I get from my garden, over the things I buy,"" a 34-year-old male participant said in the study. ""There's a big difference....I feel good that I grew it and I am eating something that I grew. So for me, it's priceless."" Participants also frequently described having less stress, as well as a rise in exercise and drop in sedentary behavior both for adults and children. Tending the garden led to more physical activity because of the need to water, weed, harvest and plant at regular intervals. The study suggests an urban gardening model that integrates home gardening with culturally appropriate nutrition and gardening education has the potential to improve a range of health behaviors that are critical to preventing and managing chronic disease, especially among low-income, urban Hispanic/Latino households. ""Urban agriculture is an important community resource that may contribute not only to nutrition and health, but also to urban development and social connection,"" said Dr. Sheri Weiser, MD, the senior author of the study. She added that combining urban home gardening with nutrition education is an innovative strategy to help to reduce the burden of preventable diseases, such as diabetes, in low-income populations with limited access to healthy food. "
Science Daily,Why the Language-Ready Brain Is So Complex,Education & Learning,2019-10-03,-,https://www.sciencedaily.com/releases/2019/10/191003141149.htm,"   In the classical view, there are two major language areas in the left half of our brain. Broca's area (in the frontal lobe) is responsible for the production of language (speaking and writing), while Wernicke's area (in the temporal lobe) supports the comprehension of language (listening and reading). A large fibre tract (the arcuate fasciculus) connects these two 'perisylvian' areas (around the Sylvian fissure, the split which divides the two lobes). ""The classical view is largely wrong,"" says Hagoort. Language is infinitely more complex than speaking or understanding single words, which is what the classical model was based on. While words are among the elementary 'building blocks' of language, we also need 'operations' to combine words into structured sentences, such as 'the editor of the newspaper loved the article'. To understand and interpret such an utterance, knowing the speech sounds (or letters) and meaning of the individual words is not enough. For instance, we also need information about the context (who is the speaker?), the intonation (is the tone cynical?), and knowledge of the world (what does an editor do?). Multiple language areas In recent years neuroanatomists discovered that Broca's and Wernicke's regions actually contain multiple neuroanatomical areas. Also, newly discovered language areas extend beyond the classical areas, even into the parietal lobe, with more connections between these areas than previously thought. Moreover, the traditional areas are involved in language comprehension as well as production. Scientist also learned that other regions of the brain are more important for language than once thought, including the right hemisphere and the cerebellum. Interestingly, language areas also turn out to be somewhat variable. For instance, in people who are born blind, language can spread to the occipital lobe (or visual brain). Our brains process language with astonishing speed and 'immediacy', in a dynamic network of interacting brain areas. All the relevant information becomes available immediately, as we start combining the meanings of individual words, unifying the different sources of information. To speed up this process, our brain actively predicts what is coming next (for instance, we might expect 'newspaper' to follow 'the editor of the ...'). As most utterances are part of a conversation, some information is usually already shared between the speaker and the listener. Speakers make sure that they mark 'new information', using the order of the words or pitch to focus the listener's attention (after hearing that readers of the newspaper did not like the article, one could say 'the EDITOR of the newspaper loved the article'). Only when relevant 'new' information is unexpected or ungrammatical, people's brains are shown to react. Listeners likely process 'old' information in a 'good-enough' manner, ignoring some of the details, explains Hagoort, which is why they do not seem to notice unexpected 'old' information. To make matters even more complex, language is often indirect. To know what a speaker really means, listeners need to infer a speaker's intention. For instance, 'It is hot here' could well be intended as a request to open the window, rather than a statement about the temperature. Neuroimaging studies show that such 'pragmatic' inferences depend on brain areas that are involved in 'Theory of Mind', or thinking about other people's beliefs, emotions and desires. Language is a ""complex biocultural hybrid,"" concludes Hagoort. But what is the essence of human language? Is it syntax, to be found in Broca's area? Hagoort challenges this old notion: ""Accounting for the full picture of human language skills is not helped by a distinction between essential and nonessential aspects of speech and language."" Instead, the neuroscientist argues for a multiple brain-network view of language, in which some operations might well be shared with other cognitive domains, such as music and arithmetic. Language being the multi-layered system that it is, no wonder that the language-ready brain is so enormously complex,"" says Hagoort. "
Science Daily,Anticipating Performance Can Hinder Memory,Education & Learning,2019-10-03,-,https://www.sciencedaily.com/releases/2019/10/191003105427.htm,"   The study's findings also suggest that the presence of an audience may be an important factor that contributes to this pre-performance memory deficit. ""Performance anticipation could weaken memory because people tend to focus on the details of their upcoming presentation instead of paying attention to information that occurs before their performance,"" says lead author Noah Forrin, a postdoctoral fellow in Psychology at Waterloo. ""People who experience performance anxiety may be particularly likely to experience this phenomenon."" Building on what previous research called the next-in-line effect, Forrin and his co-authors explored how different ways of preparing for a presentation impact the pre-performance memory deficit. They experimented with a variety of techniques that enhance memory, including the production effect, which is the simple yet powerful idea that we can remember something best if we say it aloud. One of the study's co-authors, Psychology professor Colin MacLeod, coined the term production effect from previous research which identified that reading aloud involves at least three distinct processes that help to encode memory: articulation, audition, and self-reference. Research by Forrin and MacLeod has demonstrated that reading aloud is better for memory than reading silently, writing, or hearing another person speak aloud. In the new study, however, the findings suggest that the production effect has a downside: When people anticipate reading out loud, they may have worse memory for information that they encounter before reading aloud. The researchers conducted four experiments with 400 undergraduate students and found that students have worse memory for words that they read silently when they anticipate having to read upcoming words aloud (compared to when they anticipate having to read upcoming words silently). ""Our results show that performance anticipation may be detrimental to effective memory encoding strategies,"" said Forrin. ""Students frequently have upcoming performances -- whether for class presentations or the expectation of class participation."" ""We are currently examining whether the anticipation of those future performances reduces students' learning and memory in the classroom."" One strategy to avoid pre-performance memory deficits, says Forrin, is ""try to get your performance over with by being the first student in class (or employee in a meeting) to present. After that, you can focus on others' presentations without anticipating your own."" "
Science Daily,Neuroimaging Reveals Hidden Communication Between Brain Layers During Reading,Education & Learning,2019-10-01,-,https://www.sciencedaily.com/releases/2019/10/191001102222.htm,"   A research team led by Daniel Sharoh from the Donders Center for Cognitive Neuroimaging at Radboud University Nijmegen, Kirsten Weber (Radboud University, MPI), David Norris (Radboud University, MPI), and Peter Hagoort (Radboud University, MPI) wanted to investigate the brain's reading network at a more fine-grained level. They used the 7 Tesla MRI at the Erwin L Hahn Institute in Essen for laminar functional Magnetic Resonance Imaging (lfMRI), to measure brain activation at different depths or 'layers' of the brain-typically right next to each other and smaller than a millimetre. Measuring at this level is important, as the layers can be related to the direction of the signals. Deep layers are associated with top-down information, whereas middle layers are associated with bottom-up information. Only laminar fMRI is sensitive enough to detect the deeper layers of the brain. With this new technique, would the investigators be able to find a top-down flow of information to the deeper layers of the brain for word reading? To answer this question, the researchers created pseudowords such as ""rorf"" and ""bofgieneer,"" to be compared with real words such as ""zalm"" (salmon) and ""batterij"" (battery). Pseudowords are 'possible' words that happen not to exist; they are pronounceable and therefore 'readable'. Twenty-two native Dutch speakers were asked to silently read the words and pseudowords as their brains were being scanned. The participants also viewed 'unreadable' sequences of invented 'false font' characters that resembled existing letters. The task was to only press a button when the items were real words. By comparing the brain activation for 'readable' items (words and pseudowords) and 'unreadable' items (false font), the investigators could isolate the 'reading area' of the brain. This area is also known as the 'visual word form area' (VWFA) and is situated in the temporal lobe (the left occipitotemporal cortex). As a next step, the researchers compared words directly to pseudowords, to further explore the VWFA. Bottom-up sensory information is needed for both types of items, to recognise the strings as letters. But would top-down information from language areas be visible as well, needed to distinguish words from pseudowords? The researchers found stronger activation for words than pseudo-words in the deep layers of the VWFA. This activation was caused by top-down projections from higher language areas of the brain (the left Middle Temporal Gyrus (lMTG) and left posterior middle temporal gyrus (lpMTG)). These are well-known language areas involved in retrieving words and their meaning. In contrast, the researchers found decreased activation in the middle layer of the reading area, indicating that the deep layer 'suppresses' activation of the middle layer during word reading. A conventional fMRI would have missed this nuance, as only laminar fMRI is sensitive to layer-specific activation. ""This is a breakthrough finding for all imaging research,"" says Peter Hagoort, director at the Max Planck Institute for Psycholinguistics in Nijmegen and co-author of the study. ""For the first time we have established different activation profiles at different layers of cortex, and figured out how this pattern is related to top-down influence in cortical processing. This has far-reaching consequences for cognitive neuroimaging, expanding our knowledge of brain networks."" "
Science Daily,Teenagers Less Likely to Respond to Mothers With Controlling Tone of Voice,Education & Learning,2019-09-26,-,https://www.sciencedaily.com/releases/2019/09/190926202307.htm,"   Speaking to a son or daughter in a pressurising tone is also accompanied by a range of negative emotions and less feelings of closeness, a new study has discovered. The experimental study involving over 1000 adolescents aged 14-15 is the first to examine how subjects respond to the tone of voice when receiving instructions from their mothers, even when the specific words that are used are exactly the same. Lead author of the study Dr Netta Weinstein, from Cardiff University, said: ""If parents want conversations with their teens to have the most benefit, it's important to remember to use supportive tones of voice. It's easy for parents to forget, especially if they are feeling stressed, tired, or pressured themselves."" The study showed that subjects were much more likely to engage with instructions that conveyed a sense of encouragement and support for self-expression and choice. The results, whilst of obvious interest to parents, could also be of relevance to schoolteachers whose use of more motivational language could impact the learning and well-being of students in their classrooms.    advertisement     										googletag.cmd.push(function() {  											deployads.push(function() { deployads.gpt.display(""adslot-mobile-middle-rectangle"") });  										}); 									     ""Adolescents likely feel more cared about and happier, and as a result they try harder at school, when parents and teachers speak in supportive rather than pressuring tones of voice,"" Dr Weinstein continued. The new study, published today in the journal Developmental Psychology, involved 486 males and 514 females, aged 14-15. In the experiment each of the subjects was randomly assigned to groups that would hear identical messages delivered by mothers of adolescents in either a controlling, autonomy-supportive, or neutral tone of voice. Expressions of control impose pressure and attempt to coerce or push listeners to action. In contrast, those that express 'autonomy support' convey a sense of encouragement and support for listeners' sense of choice and opportunity for self-expression. Each of the mothers delivered 30 sentences that centred around school work, and included instructions such as: ""It's time now to go to school,"" ""you will read this book tonight,"" and ""you will do well on this assignment."" After the delivery of the messages, each student undertook a survey and answered questions about how they would feel if their own mother had spoken to them in that particular way.    advertisement     									googletag.cmd.push(function() {  										deployads.push(function() { deployads.gpt.display(""adslot-mobile-bottom-rectangle"") });  									}); 								     The findings showed that the tone of voice used by mothers can impact significantly on teenagers' emotional, relational, and behavioural intention responses. Across most outcomes, adolescents who listened to mothers making motivational statements in a controlling tone of voice responded in undesirable ways. In contrast, autonomy-supportive tones elicited positive reactions from listeners as compared to listening to mothers who used a neutral tone of voice to deliver their motivational sentences. Co-author of the study Professor Silke Paulmann, of the University of Essex, added: ""These results nicely illustrate how powerful our voice is and that choosing the right tone to communicate is crucial in all of our conversations."" The researchers now intend to take their work a step further by investigating how tone of voice can impact physiological responses, such as heart rates or skin conductance responses, and how long lasting these effects may be. The study was funded by the Leverhulme Trust and involved researchers from Ghent University and the University of Essex. "
