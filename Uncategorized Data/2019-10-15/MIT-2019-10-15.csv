Source,Heading,Category,Date,Time,URL,Text
MIT News,Machine learning you can dance to,Computer Science,2019-09-18,-,http://news.mit.edu/2019/machine-learning-you-can-dance-to-samply-0918,"  Rhythmic flashes from a computer screen illuminate a dark room as sounds fill the air. The snare drum sample comes out crisp and clean by itself, but turns muddy in the mix, no matter how the levels are set. Welcome to the world of modern music-making — and its discontents. Today’s digital music producers face a common dilemma: how to mesh samples that may sound great on their own but do not necessarily fit into a song like they originally imagined. One solution is to find and audit dozens of different samples, a tedious process that can take time to finesse. “There’s a lot of manual searching to get the right musical result, which can be distracting and time-consuming,” says Justin Swaney, a PhD student in the MIT Department of Chemical Engineering, a music producer, and co-creator of a new tool that uses machine learning to help producers find just the perfect sound. Called Samply, Swaney’s visual sample-library explorer combines music and machine learning into a new technology for producers. The top winner at the MIT Stephen A. Schwarzman College of Computing Machine Learning Across Disciplines Challenge at the Hello World celebration last winter, the tool uses a convolutional neural network to analyze audio waveforms. “Samply organizes samples based on their sonic characteristics,” explains Swaney. “The result is an interactive plot where similar sounds are closer together and different sounds are farther apart. Samply allows multiple sample libraries to be visualized simultaneously, shortening the lag between imagining a sound in your head and finding it.” For Swaney, the development of Samply drew on both his research expertise and personal life. Before coming to MIT, he had produced albums with indie musicians including Eric Schirtzinger, a drummer and co-creator of the tool. The two recorded drums in a basement and tried to improvise with cheap hardware and hacks — like hanging rugs from the ceiling to dampen reverberation. “The constraints made us get creative,” says Schirtzinger, who is now a computer science major at the University of Wisconsin at Madison. That creativity was further honed after Swaney completed 6.862 (Applied Machine Learning). He saw an opportunity to rekindle his music production hobby by applying what he had learned from the project-based course, devising a way to automate the search for the right samples when producing a new song. “I figured the computer could listen to samples much faster than I could,” he says. Beyond the clever use of machine learning, the real magic of Samply is that conceptually, it is founded on a deep understanding of what it takes to make music. “We aren’t just AI enthusiasts applying machine learning to music,” says Schirtzinger. “We are musicians who want better tools for making music.” It turns out that at MIT, they aren’t the only ones with a song in their hearts. While presenting Samply at the Schwarzman College of Computing exposition last winter, dozens of faculty, staff, and students gathered around Swaney’s poster and live demonstration to exchange ideas. Some had years of experience producing music with professional software, while others simply appreciated the visualizations and sounds in the demo. Spurred by the interest in Samply at the exposition, Swaney and Shirtzinger are in the process of turning their project into a startup company. As a first step, the two reached out to the Technology Licensing Office (TLO) for advice, which referred them to the Venture Mentoring Service (VMS). Samply joined VMS in April and was paired with two MIT-affiliated mentors and entrepreneurs, Stephen Bayle and John Stempeck. After pitching Samply to his mentors, Swaney received sage advice on a crafting a business plan and sales strategy, and then began making connections with others interested in music technology as a business. Samply has since been accepted into the ELEVATE accelerator, sponsored by the local digital marketing firm HubSpot, and Swaney is applying for seed funding through the MIT Sandbox Innovation Fund. “Starting a company as a student can be daunting, but the MIT community gives us confidence,” he says. “If we can’t do it at MIT, then where can we?” In fact, the time and attention he has spent on Samply has had an “almost paradoxical” benefit to his academic life as a graduate student. “I was spending all of my time in the lab,” he says. “When I took a step back to make Samply, I could see the forest from the trees in my research.” Swaney found that focusing on his love of music served as an “emotional outlet,” helping to mitigate intellectual burnout. Although Samply may have taken him away from the lab bench, it has also ended up informing his research. The original idea of visualizing samples, he says, stemmed from “my work on single-cell analysis.” Applying the method to the tool clarified his thinking in the biological realm, leading to a new method to produce better clustering, or a way to better sort, recognize, and visualize groups of cells. “It was a bit like a musical theme and variation, but with my research,” Swaney says. As for Samply, there will be a free beta version of the app launching in September, and a Kickstarter campaign is due in the coming year to fuel future developments. “We want to get Samply into the hands of more producers and content creators so that we can establish a feedback loop that guides our priorities,” he says. “Our technology may also have applications in live music performance, instrumentation, and in film and videography. We are excited to explore those possibilities.” "
MIT News,Notation system allows scientists to communicate polymers more easily,Computer Science,2019-09-18,-,http://news.mit.edu/2019/bigsmiles-notation-system-allows-scientists-communicate-polymers-more-easily-0918,"  Having a compact, yet robust, structurally-based identifier or representation system for molecular structures is a key enabling factor for efficient sharing and dissemination of results within the research community. Such systems also lay down the essential foundations for machine learning and other data-driven research. While substantial advances have been made for small molecules, the polymer community has struggled in coming up with an efficient representation system. For small molecules, the basic premise is that each distinct chemical species corresponds to a well-defined chemical structure. This does not hold for polymers. Polymers are intrinsically stochastic molecules that are often ensembles with a distribution of chemical structures. This difficulty limits the applicability of all deterministic representations developed for small molecules. In a paper published Sept. 12 in ACS Central Science, researchers at MIT, Duke University, and Northwestern University report a new representation system that is capable of handling the stochastic nature of polymers, called BigSMILES. “BigSMILES addresses a significant challenge in the digital representation of polymers,” explains Connor Coley PhD ’19, co-author of the paper. “Polymers are almost always ensembles of multiple chemical structures, generated through stochastic processes, so we can't use the same strategies for writing down their structures as for small molecules.” Co-authors are Coley; associate professor of chemical engineering Bradley D. Olsen at MIT; Warren K. Lewis Professor of Chemical Engineering Klavs F. Jensen at MIT; assistant professor of chemistry Julia A. Kalow at Northwestern University; associate professor of chemistry Jeremiah A. Johnson at MIT; William T. Miller Professor of Chemistry Stephen L. Craig at Duke University; graduate student Eliot Woods at Northwestern University; graduate student Zi Wang at Duke University; graduate student Wencong Wang at MIT; graduate student Haley K. Beech at MIT; visiting researcher Hidenobu Mochigase at MIT; and graduate student Tzyy-Shyang Lin at MIT. There are several line notations to communicate molecular structure, with simplified molecular-input line-entry system (SMILES) being the most popular. SMILES is generally considered the most human-readable variant, with by far the widest software support. In practice, SMILES provides a simple set of representations that are suitable as labels for chemical data and as a memory-compact identifier for data exchange between researchers. As a text-based system, SMILES is also a natural fit to many text-based machine learning algorithms. These characteristics have made SMILES a perfect tool for translating chemistry knowledge into a machine-friendly form, and it has been successfully applied for small molecule property prediction and computer-aided synthesis planning. Polymers, however, have resisted description by this and other structural languages. This is because most structural languages such as SMILES have been designed to describe molecules or chemical fragments that are well-defined atomistic graphs. Since polymers are stochastic molecules, they do not have unique SMILES representations. This lack of a unified naming or identifier convention for polymer materials is one of the major hurdles slowing down the development of the polymer informatics field. While pioneering efforts on polymer informatics, such as the Polymer Genome Project, have demonstrated the usefulness of SMILES extensions in polymer informatics, the fast development of new chemistry and the rapid development of materials informatics and data-driven research make the need for a universally applicable naming convention for polymers important. “Machine learning presents an enormous opportunity to accelerate chemical development and discovery,” says Lin He, acting deputy division director for the National Science Foundation (NSF) Division of Chemistry. “This expanded tool to label structures, specifically devised to address the unique challenges inherent to polymers, greatly enhances the searchability of chemical structural data, and brings us one step closer to harnessing the data revolution.” The researchers have created a new structurally-based construct as an addition to the highly successful SMILES representation that can treat the random nature of polymer materials. Since polymers are high molar mass molecules, this construct is named BigSMILES. In BigSMILES, polymeric fragments are represented by a list of repeating units enclosed by curly brackets. The chemical structures of the repeating units are encoded using normal SMILES syntax, but with additional bonding descriptors that specify how different repeating units are connected to form polymers. This simple design of syntax would enable the encoding of macromolecules over a wide range of different chemistries, including homopolymer, random copolymers and block copolymers, and a variety of molecular connectivity, ranging from linear polymers to ring polymers to even branched polymers. As in SMILES, BigSMILES representations are compact, self-contained text strings. “Standardizing the digital representation of polymeric structures with BigSMILES will encourage the sharing and aggregation of polymer data, improving model quality over time and reinforcing the benefits of its use,” says Jason Clark, the materials lead in Open Innovation for Renewable Chemicals and Materials at Braskem, who was not associated with the research. “BigSMILES is a significant contribution to the field in that it addresses the need for a flexible system to represent complex polymer structures digitally.”   Clark adds, “The challenges faced by the plastics industry in the context of the circular economy begins with the source of raw materials and continues all the way through end-of-life management. Addressing these challenges requires the innovative design of polymer-based materials, which has traditionally suffered from lengthy development cycles. Advances in artificial intelligence and machine learning have shown promise to accelerate the development cycle for applications utilizing metal alloys and small organic molecules, motivating the plastics industry to seek a parallel approach.” BigSMILES digital representations facilitate the evaluation of structure-performance relationships by application of data science methods, he says, ultimately accelerating the convergence to the polymer structures or compositions that will help enable the circular economy. “A multitude of complicated polymer structures can be constructed through the composition of three new basic operators and original SMILES symbols,” says Olsen, “Entire fields of chemistry, materials science, and engineering, including polymer science, biomaterials, materials chemistry, and much of biochemistry, are based upon macromolecules which have stochastic structures. This can basically be thought of as a new language for how to write the structure of large molecules.” “One of the things I’m excited about is how the data entry might eventually be tied directly to the synthetic methods used to make a particular polymer,” says Craig, “Because of that, there is an opportunity to actually capture and process more information about the molecules than is typically available from standard characterizations. If this can be done, it will enable all sorts of discoveries.” This work was funded by the NSF through the Center for the Chemistry of Molecularly Optimized Networks, an NSF Center for Chemical Innovation. "
MIT News,Lincoln Laboratory's new artificial intelligence supercomputer is the most powerful at a university,Computer Science,2019-09-27,-,http://news.mit.edu/2019/lincoln-laboratory-ai-supercomputer-tx-gaia-0927,"  The new TX-GAIA (Green AI Accelerator) computing system at the Lincoln Laboratory Supercomputing Center (LLSC) has been ranked as the most powerful artificial intelligence supercomputer at any university in the world. The ranking comes from TOP500, which publishes a list of the top supercomputers in various categories biannually. The system, which was built by Hewlett Packard Enterprise, combines traditional high-performance computing hardware — nearly 900 Intel processors — with hardware optimized for AI applications — 900 Nvidia graphics processing unit (GPU) accelerators. ""We are thrilled by the opportunity to enable researchers across Lincoln and MIT to achieve incredible scientific and engineering breakthroughs,"" says Jeremy Kepner, a Lincoln Laboratory fellow who heads the LLSC. ""TX-GAIA will play a large role in supporting AI, physical simulation, and data analysis across all laboratory missions."" TOP500 rankings are based on a LINPACK Benchmark, which is a measure of a system's floating-point computing power, or how fast a computer solves a dense system of linear equations. TX-GAIA's TOP500 benchmark performance is 3.9 quadrillion floating-point operations per second, or petaflops (though since the ranking was announced in June 2019, Hewlett Packard Enterprise has updated the system's benchmark to 4.725 petaflops). The June TOP500 benchmark performance places the system No. 1 in the Northeast, No. 20 in the United States, and No. 51 in the world for supercomputing power. The system's peak performance is more than 6 petaflops. But more notably, TX-GAIA has a peak performance of 100 AI petaflops, which makes it No. 1 for AI flops at any university in the world. An AI flop is a measure of how fast a computer can perform deep neural network (DNN) operations. DNNs are a class of AI algorithms that learn to recognize patterns in huge amounts of data. This ability has given rise to ""AI miracles,"" as Kepner puts it, in speech recognition and computer vision; the technology is what allows Amazon's Alexa to understand questions and self-driving cars to recognize objects in their surroundings. The more complex these DNNs grow, the longer it takes for them to process the massive datasets they learn from. TX-GAIA's Nvidia GPU accelerators are specially designed for performing these DNN operations quickly. TX-GAIA is housed in a new modular data center, called an EcoPOD, at the LLSC’s green, hydroelectrically powered site in Holyoke, Massachusetts. It joins the ranks of other powerful systems at the LLSC, such as the TX-E1, which supports collaborations with the MIT campus and other institutions, and TX-Green, which is currently ranked 490th on the TOP500 list. Kepner says that the system's integration into the LLSC will be completely transparent to users when it comes online this fall. ""The only thing users should see is that many of their computations will be dramatically faster,"" he says. Among its AI applications, TX-GAIA will be tapped for training machine learning algorithms, including those that use DNNs. It will more quickly crunch through terabytes of data — for example, hundreds of thousands of images or years' worth of speech samples — to teach these algorithms to figure out solutions on their own. The system's compute power will also expedite simulations and data analysis. These capabilities will support projects across the laboratory's R&D areas, such as improving weather forecasting, accelerating medical data analysis, building autonomous systems, designing synthetic DNA, and developing new materials and devices. TX-GAIA, which is also ranked the No. 1 system in the U.S. Department of Defense, will also support the recently announced MIT-Air Force AI Accelerator. The partnership will combine the expertise and resources of MIT, including those at the LLSC, and the U.S. Air Force to conduct fundamental research directed at enabling rapid prototyping, scaling, and application of AI algorithms and systems. "
MIT News,Using math to blend musical notes seamlessly,Computer Science,2019-09-27,-,http://news.mit.edu/2019/math-portamento-music-0927,"  In music, “portamento” is a term that’s been used for hundreds of years, referring to the effect of gliding a note at one pitch into a note of a lower or higher pitch. But only instruments that can continuously vary in pitch — such as the human voice, string instruments, and trombones — can pull off the effect. Now an MIT student has invented a novel algorithm that produces a portamento effect between any two audio signals in real-time. In experiments, the algorithm seamlessly merged various audio clips, such as a piano note gliding into a human voice, and one song blending into another. His paper describing the algorithm won the “best student paper” award at the recent International Conference on Digital Audio Effects. The algorithm relies on “optimal transport,” a geometry-based framework that determines the most efficient ways to move objects — or data points — between multiple origin and destination configurations. Formulated in the 1700s, the framework has been applied to supply chains, fluid dynamics, image alignment, 3-D modeling, computer graphics, and more.        In work that originated in a class project, Trevor Henderson, now a graduate student in computer science, applied optimal transport to interpolating audio signals — or blending one signal into another. The algorithm first breaks the audio signals into brief segments. Then, it finds the optimal way to move the pitches in  each segment to pitches in the other signal, to produce the smooth glide of the portamento effect. The algorithm also includes specialized techniques to maintain the fidelity of the audio signal as it transitions. “Optimal transport is used here to determine how to map pitches in one sound to the pitches in the other,” says Henderson, a classically trained organist who performs electronic music and has been a DJ on WMBR 88.1, MIT’s radio station. “If it’s transforming one chord into a chord with a different harmony, or with more notes, for instance, the notes will split from the first chord and find a position to seamlessly glide to in the other chord.” According to Henderson, this is one of the first techniques to apply optimal transport to transforming audio signals. He has already used the algorithm to build equipment that seamlessly transitions between songs on his radio show. DJs could also use the equipment to transition between tracks during live performances. Other musicians might use it to blend instruments and voice on stage or in the studio. Henderson’s co-author on the paper is Justin Solomon, an X-Consortium Career Development Assistant Professor in the Department of Electrical Engineering and Computer Science. Solomon — who also plays cello and piano — leads the Geometric Data Processing Group in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and is a member of the Center for Computational Engineering. Henderson took Solomon’s class, 6.838 (Shape Analysis), which tasks students with applying geometric tools like optimal transport to real-world applications. Student projects usually focus on 3-D shapes from virtual reality or computer graphics. So Henderson’s project came as a surprise to Solomon. “Trevor saw an abstract connection between geometry and moving frequencies around in audio signals to create a portamento effect,” Solomon says. “He was in and out of my office all semester with DJ equipment. It wasn’t what I expected to see, but it was pretty entertaining.” For Henderson, it wasn’t too much of a stretch. “When I see a new idea, I ask, ‘Is this applicable to music?’” he says. “So, when we talked about optimal transport, I wondered what would happen if I connected it to audio spectra.” A good way to think of optimal transport, Henderson says, is finding “a lazy way to build a sand castle.” In that analogy, the framework is used to calculate the way to move each grain of sand from its position in a shapeless pile into a corresponding position in a sand castle, using as little work as possible. In computer graphics, for instance, optimal transport can be used to transform or morph shapes by finding the optimal movement from each point on one shape into the other. Applying this theory to audio clips involves some additional ideas from signal processing. Musical instruments produce sound through vibrations of components, depending on the instrument. Violins use strings, brass instruments use air inside hollow bodies, and humans use vocal cords. These vibrations can be captured as audio signals, where the frequency and amplitude (peak height) represent different pitches.  Conventionally, the transition between two audio signals is done with a fade, where one signal is reduced in volume while the other rises. Henderson’s algorithm, on the other hand, smoothly slides frequency segments from one clip into another, with no fading of volume. To do so, the algorithm splits any two audio clips into windows of about 50 milliseconds. Then, it runs a Fourier transform, which turns each window into its frequency components. The frequency components within a window are lumped together into individual synthesized “notes.” Optimal transport then maps how the notes in one signal’s window will move to the notes in the other. Then, an “interpolation parameter” takes over. That’s basically a value that determines where each note will be on the path from its starting pitch in one signal to its ending pitch in the other. Manually changing the parameter value will sweep the pitches between the two positions, producing the portamento effect. That single parameter can also be programmed into and controlled by, say, a crossfader, a slider component on a DJ’s mixing board that smoothly fades between songs. As the crossfader slides, the interpolation parameter changes to produce the effect. Behind the scenes are two innovations that ensure a distortion-free signal. First, Henderson used a novel application of a signal-processing technique, called “frequency reassignment,” that lumps the frequency bins together to form single notes that can easily transition between signals. Second, he invented a way to synthesize new phases for each audio signal while stitching together the 50-millisecond windows, so neighboring windows don’t interfere with each other. Next, Henderson wants to experiment with feeding the output of the effect back into its input. This, he thinks, could automatically create another classic music effect, “legato,” which is a smooth transition between distinct notes. Unlike a portamento — which plays all notes between a start and end note — a legato seamlessly transitions between two distinct notes, without capturing any notes in between. "
MIT News,Photovoltaic-powered sensors for the “internet of things”,Computer Science,2019-09-27,-,http://news.mit.edu/2019/photovoltaic-rfid-sensors-iot-0927,"  By 2025, experts estimate the number of “internet of things” devices — including sensors that gather real-time data about infrastructure and the environment — could rise to 75 billion worldwide. As it stands, however, those sensors require batteries that must be replaced frequently, which can be problematic for long-term monitoring.   MIT researchers have designed photovoltaic-powered sensors that could potentially transmit data for years before they need to be replaced. To do so, they mounted thin-film perovskite cells — known for their potential low cost, flexibility, and relative ease of fabrication — as energy-harvesters on inexpensive radio-frequency identification (RFID) tags. The cells could power the sensors in both bright sunlight and dimmer indoor conditions. Moreover, the team found the solar power actually gives the sensors a major power boost that enables greater data-transmission distances and the ability to integrate multiple sensors onto a single RFID tag. “In the future, there could be billions of sensors all around us. With that scale, you’ll need a lot of batteries that you’ll have to recharge constantly. But what if you could self-power them using the ambient light? You could deploy them and forget them for months or years at a time,” says Sai Nithin Kantareddy, a PhD student in the MIT Auto-ID Laboratory. “This work is basically building enhanced RFID tags using energy harvesters for a range of applications.” In a pair of papers published in the journals Advanced Functional Materials and IEEE Sensors, MIT Auto-ID Laboratory and MIT Photovoltaics Research Laboratory researchers describe using the sensors to continuously monitor indoor and outdoor temperatures over several days. The sensors transmitted data continuously at distances five times greater than traditional RFID tags — with no batteries required. Longer data-transmission ranges mean, among other things, that one reader can be used to collect data from multiple sensors simultaneously. Depending on certain factors in their environment, such as moisture and heat, the sensors can be left inside or outside for months or, potentially, years at a time before they degrade enough to require replacement. That can be valuable for any application requiring long-term sensing, indoors and outdoors, including tracking cargo in supply chains, monitoring soil, and monitoring the energy used by equipment in buildings and homes. Joining Kantareddy on the papers are: Department of Mechanical Engineering (MechE) postdoc Ian Mathews, researcher Shijing Sun, chemical engineering student Mariya Layurova, researcher Janak Thapa, researcher Ian Marius Peters, and Georgia Tech Professor Juan-Pablo Correa-Baena, who are all members of the Photovoltaics Research Laboratory; Rahul Bhattacharyya, a researcher in the AutoID Lab; Tonio Buonassisi, a professor in MechE; and Sanjay E. Sarma, the Fred Fort Flowers and Daniel Fort Flowers Professor of Mechanical Engineering. Combining two low-cost technologies  In recent attempts to create self-powered sensors, other researchers have used solar cells as energy sources for internet of things (IoT) devices. But those are basically shrunken-down versions of traditional solar cells — not perovskite. The traditional cells can be efficient, long-lasting, and powerful under certain conditions “but are really infeasible for ubiquitous IoT sensors,” Kantareddy says. Traditional solar cells, for instance, are bulky and expensive to manufacture, plus they are inflexible and cannot be made transparent, which can be useful for temperature-monitoring sensors placed on windows and car windshields. They’re also really only designed to efficiently harvest energy from powerful sunlight, not low indoor light. Perovskite cells, on the other hand, can be printed using easy roll-to-roll manufacturing techniques for a few cents each; made thin, flexible, and transparent; and tuned to harvest energy from any kind of indoor and outdoor lighting. The idea, then, was combining a low-cost power source with low-cost RFID tags, which are battery-free stickers used to monitor billions of products worldwide. The stickers are equipped with tiny, ultra-high-frequency antennas that each cost around three to five cents to make. RFID tags rely on a communication technique called “backscatter,” that transmits data by reflecting modulated wireless signals off the tag and back to a reader. A wireless device called a reader — basically similar to a Wi-Fi router — pings the tag, which powers up and backscatters a unique signal containing information about the product it’s stuck to. Traditionally, the tags harvest a little of the radio-frequency energy sent by the reader to power up a little chip inside that stores data, and uses the remaining energy to modulate the returning signal. But that amounts to only a few microwatts of power, which limits their communication range to less than a meter. The researchers’ sensor consists of an RFID tag built on a plastic substrate. Directly connected to an integrated circuit on the tag is an array of perovskite solar cells. As with traditional systems, a reader sweeps the room, and each tag responds. But instead of using energy from the reader, it draws harvested energy from the perovskite cell to power up its circuit and send data by backscattering RF signals. Efficiency at scale The key innovations are in the customized cells. They’re fabricated in layers, with perovskite material sandwiched between an electrode, cathode, and special electron-transport layer materials. This achieved about 10 percent efficiency, which is fairly high for still-experimental perovskite cells. This layering structure also enabled the researchers to tune each cell for its optimal “bandgap,” which is an electron-moving property that dictates a cell’s performance in different lighting conditions. They then combined the cells into modules of four cells. In the Advanced Functional Materials paper, the modules generated 4.3 volts of electricity under one sun illumination, which is a standard measurement for how much voltage solar cells produce under sunlight. That’s enough to power up a circuit — about 1.5 volts — and send data around 5 meters every few seconds. The modules had similar performances in indoor lighting. The IEEE Sensors paper primarily demonstrated wide‐bandgap perovskite cells for indoor applications that achieved between 18.5 percent and 21. 4 percent efficiencies under indoor fluorescent lighting, depending on how much voltage they generate. Essentially, about 45 minutes of any light source will power the sensors indoors and outdoors for about three hours.   The RFID circuit was prototyped to only monitor temperature. Next, the researchers aim to scale up and add more environmental-monitoring sensors to the mix, such as humidity, pressure, vibration, and pollution. Deployed at scale, the sensors could especially aid in long-term data-collection indoors to help build, say, algorithms that help make smart buildings more energy efficient. “The perovskite materials we use have incredible potential as effective indoor-light harvesters. Our next step is to integrate these same technologies using printed electronics methods, potentially enabling extremely low-cost manufacturing of wireless sensors,"" Mathews says. "
MIT News,How cities can leverage citizen data while protecting privacy,Computer Science,2019-09-25,-,http://news.mit.edu/2019/how-cities-citizen-data-privacy-0925,"  India is on a path with dual — and potentially conflicting — goals related to the use of citizen data. To improve the efficiency their municipal services, many Indian cities have started enabling government-service requests, which involves collecting and sharing citizen data with government officials and, potentially, the public. But there’s also a national push to protect citizen privacy, potentially restricting data usage. Cities are now beginning to question how much citizen data, if any, they can use to track government operations. In a new study, MIT researchers find that there is, in fact, a way for Indian cities to preserve citizen privacy while using their data to improve efficiency. The researchers obtained and analyzed data from more than 380,000 government service requests by citizens across 112 cities in one Indian state for an entire year. They used the dataset to measure each city government’s efficiency based on how quickly they completed each service request. Based on field research in three of these cities, they also identified the citizen data that’s necessary, useful (but not critical), or unnecessary for improving efficiency when delivering the requested service. In doing so, they identified “model” cities that performed very well in both categories, meaning they maximized privacy and efficiency. Cities worldwide could use similar methodologies to evaluate their own government services, the researchers say. The study was presented at this past weekend’s Technology Policy Research Conference. “How do municipal governments collect citizen data to try to be transparent and efficient, and, at the same time, protect privacy? How do you find a balance?” says co-author Karen Sollins, a researcher in the Computer Science and Artificial Intelligence Laboratory (CSAIL), a principal investigator for the Internet Policy Research Initiative (IPRI), and a member of the Privacy, Innovation and e-Governance using Quantitative Systems (PIEQS) group. “We show there are opportunities to improve privacy and efficiency simultaneously, instead of saying you get one or the other, but not both.” Joining Sollins on the paper are: first author Nikita Kodali, a graduate student in the Department of Electrical Engineering and Computer Science; and Chintan Vaishnav, a senior lecturer in the MIT Sloan School of Management, a principal investigator for IPRI, and a member PIEQS. Intersections of privacy and efficiency In recent years, India’s eGovernment Foundation has aimed to significantly improve the transparency, accountability, and efficiency of operations in its many municipal governments. The foundation aims to move all of these governments from paper-based systems to fully digitized systems with citizen interfaces to request and interact with government service departments. In 2017, however, India’s Supreme Court ruled that its citizens have a constitutional right to data privacy and have a say in whether or not their personal data could be used by governments and the private sector. That could potentially limit the information that towns and cities could use to track the performance of their services. Around that time, the researchers had started studying privacy and efficiency issues surrounding the eGovernment Foundation’s digitization efforts. That led to a report that determined which types of citizen data could be used to track government service operations. Building on that work, the researchers were provided 383,959 anonymized citizen-government transactions from digitized modules from 112 local governments in an Indian state for all of 2018. The modules focused on three areas: new water tap tax assessment; new property tax assessment; and public grievances about sanitation, stray animals, infrastructure, schools, and other issues. Citizens send requests to those modules via mobile or web apps by entering various types of personal and property information, and then monitor the progress of the requests. The request and related data pass through various officials that each complete an individual subtask, known as a service level agreement, within a designated time limit. Then, the request passes on to another official, and so on. But much of that citizen information is also visible to the public. The software captured each step of each request, moving from initiation to completion, with time stamps, for each municipal government. The researchers then could rank each task within a town or city, or in aggregation across each town or city on two metrics: a government efficiency index and an information privacy index. The government efficiency index primarily measures a service’s timeliness, compared to the predetermined service level agreement. If a service is completed before its timeframe, it’s more efficient; if it’s completed after, it’s less efficient. The information privacy index measures how responsible is a government in collecting, using, and disclosing citizen data that may be privacy sensitive, such as personally identifiable information. The more the city collects and shares inessential data, the lower its privacy rating. Phone numbers and home addresses, for instance, aren’t needed for many of the services or grievances, yet are collected — and publicly disclosed — by many of the modules. In fact, the researchers found that some modules historically collected detailed personal and property information across dozens of data fields, yet the governments only needed about half of those fields to get the job done. Model behavior By analyzing the two indices, they found eight “model” municipal governments that performed in the top 25 percent for all services in both the efficiency and privacy indices. In short, they used only the essential data — and passed that essential data through fewer officials — to complete a service in a timely manner. The researchers now plan to study how the model cities are able to get services done so quickly. They also hope to study why some cities performed so poorly, in the bottom 25 percent, for any given service. “First, we’re showing India that this is what your best cities look like and what other cities should become,” Vaishnav says. “Then we want to look at why a city becomes a model city.” Similar studies can be conducted in places where similar citizen and government data are available and which have equivalents to India’s service level agreements — which serve as a baseline for measuring efficiency. That information isn’t common worldwide yet, but could be in the near future, especially in cities like Boston and Cambridge, Vaishnav says. “We gather a large amount of data and there’s an urge to do something with the data to improve governments and engage citizens better,” he says. “That may soon be a requirement in democracies around the globe.” Next, the researchers want to create an innovation-based matrix, which will determine which citizen data can and cannot be made public to private parties to help develop new technologies. They’re also working on a model that provides information on a city’s government efficiency and information privacy scores in real time, as citizen requests are being processed. "
MIT News,Computing and artificial intelligence: Humanistic perspectives from MIT ,Computer Science,2019-09-24,-,http://news.mit.edu/2019/computing-and-ai-humanistic-perspectives-0924,"  The MIT Stephen A. Schwarzman College of Computing (SCC) will reorient the Institute to bring the power of computing and artificial intelligence to all fields at MIT, and to allow the future of computing and AI to be shaped by all MIT disciplines. To support ongoing planning for the new college, Dean Melissa Nobles invited faculty from all 14 of MIT’s humanistic disciplines in the School of Humanities, Arts, and Social Sciences to respond to two questions:    1) What domain knowledge, perspectives, and methods from your field should be integrated into the new MIT Schwarzman College of Computing, and why?  2) What are some of the meaningful opportunities that advanced computing makes possible in your field?   As Nobles says in her foreword to the series, “Together, the following responses to these two questions offer something of a guidebook to the myriad, productive ways that technical, humanistic, and scientific fields can join forces at MIT, and elsewhere, to further human and planetary well-being.”  The following excerpts highlight faculty responses, with links to full commentaries. The excerpts are sequenced by fields in the following order: the humanities, arts, and social sciences.  Foreword by Melissa Nobles, professor of political science and the Kenan Sahin Dean of the MIT School of Humanities, Arts, and Social Sciences  “The advent of artificial intelligence presents our species with an historic opportunity — disguised as an existential challenge: Can we stay human in the age of AI?  In fact, can we grow in humanity, can we shape a more humane, more just, and sustainable world? With a sense of promise and urgency, we are embarked at MIT on an accelerated effort to more fully integrate the technical and humanistic forms of discovery in our curriculum and research, and in our habits of mind and action.” Read more >> Comparative Media Studies: William Uricchio, professor of comparative media studies “Given our research and practice focus, the CMS perspective can be key for understanding the implications of computation for knowledge and representation, as well as computation’s relationship to the critical process of how knowledge works in culture — the way it is formed, shared, and validated.” Recommended action: “Bring media and computer scholars together to explore issues that require both areas of expertise: text-generating algorithms (that force us to ask what it means to be human); the nature of computational gatekeepers (that compels us to reflect on implicit cultural priorities); and personalized filters and texts (that require us to consider the shape of our own biases).” Read more >> Global Languages: Emma J. Teng, the T.T. and Wei Fong Chao Professor of Asian Civilizations “Language and culture learning are gateways to international experiences and an important means to develop cross-cultural understanding and sensitivity. Such understanding is essential to addressing the social and ethical implications of the expanding array of technology affecting everyday life across the globe.” Recommended action: “We aim to create a 21st-century language center to provide a convening space for cross-cultural communication, collaboration, action research, and global classrooms. We also plan to keep the intimate size and human experience of MIT’s language classes, which only increase in value as technology saturates the world.” Read more >> History: Jeffrey Ravel, professor of history and head of MIT History  “Emerging innovations in computational methods will continue to improve our access to the past and the tools through which we interpret evidence. But the field of history will continue to be served by older methods of scholarship as well; critical thinking by human beings is fundamental to our endeavors in the humanities.” Recommended action: “Call on the nuanced debates in which historians engage about causality to provide a useful frame of reference for considering the issues that will inevitably emerge from new computing technologies. This methodology of the history field is a powerful way to help imagine our way out of today’s existential threats.” Read more >> Linguistics: Faculty of MIT Linguistics “Perhaps the most obvious opportunities for computational and linguistics research concern the interrelation between specific hypotheses about the formal properties of language and their computational implementation in the form of systems that learn, parse, and produce human language.” Recommended action: “Critically, transformative new tools have come from researchers at institutions where linguists work side-by-side with computational researchers who are able to translate back and forth between computational properties of linguistic grammars and of other systems.” Read more >> Literature: Shankar Raman, with Mary C. Fuller, professors of literature “In the age of AI, we could invent new tools for reading. Making the expert reading skills we teach MIT students even partially available to readers outside the academy would widen access to our materials in profound ways.” Recommended action: At least three priorities of current literary engagement with the digital should be integrated into the SCC’s research and curriculum: democratization of knowledge; new modes of and possibilities for knowledge production; and critical analysis of the social conditions governing what can be known and who can know it.” Read more >> Philosophy: Alex Byrne, professor of philosophy and head of MIT Philosophy; and Tamar Schapiro, associate professor of philosophy “Computing and AI pose many ethical problems related to: privacy (e.g., data systems design), discrimination (e.g., bias in machine learning), policing (e.g., surveillance), democracy (e.g., the Facebook-Cambridge Analytica data scandal), remote warfare, intellectual property, political regulation, and corporate responsibility.” Recommended action: “The SCC presents an opportunity for MIT to be an intellectual leader in the ethics of technology. The ethics lab we propose could turn this opportunity into reality.” Read more >> Science, Technology, and Society: Eden Medina and Dwaipayan Banerjee, associate professors of science, technology, and society “A more global view of computing would demonstrate a broader range of possibilities than one centered on the American experience, while also illuminating how computer systems can reflect and respond to different needs and systems. Such experiences can prove generative for thinking about the future of computing writ large.” Recommended action: “Adopt a global approach to the research and teaching in the SCC, an approach that views the U.S. experience as one among many.” Read more >> Women's and Gender Studies: Ruth Perry, the Ann Friedlaender Professor of Literature; with Sally Haslanger, the Ford Professor of Philosophy, and Elizabeth Wood, professor of history “The SCC presents MIT with a unique opportunity to take a leadership role in addressing some of most pressing challenges that have emerged from the role computing technologies play in our society — including how these technologies are reinforcing social inequalities.” Recommended action: “Ensure that women’s voices are heard and that coursework and research is designed with a keen awareness of the difference that gender makes. This is the single-most powerful way that MIT can address the inequities in the computing fields.” Read more >> Writing: Tom Levenson, professor of science writing  “Computation and its applications in fields that directly affect society cannot be an unexamined good. Professional science and technology writers are a crucial resource for the mission of new college of computing, and they need to be embedded within its research apparatus.” Recommended action: “Intertwine writing and the ideas in coursework to provide conceptual depth that purely technical mastery cannot offer.” Read more >> Music: Eran Egozy, professor of the practice in music technology “Creating tomorrow’s music systems responsibly will require a truly multidisciplinary education, one that covers everything from scientific models and engineering challenges to artistic practice and societal implications. The new music technology will be accompanied by difficult questions. Who owns the output of generative music algorithms that are trained on human compositions? How do we ensure that music, an art form intrinsic to all humans, does not become controlled by only a few?” Recommended action: Through the SCC, our responsibility will be not only to develop the new technologies of music creation, distribution, and interaction, but also to study their cultural implications and define the parameters of a harmonious outcome for all.” Read more >> Theater Arts: Sara Brown, assistant professor of theater arts and MIT Theater Arts director of design “As a subject, AI problematizes what is means to be human. There are an unending series of questions posed by the presence of an intelligent machine. The theater, as a synthetic art form that values and exploits liveness, is an ideal place to explore the complex and layered problems posed by AI and advanced computing.” Recommended action: “There are myriad opportunities for advanced computing to be integrated into theater, both as a tool and as a subject of exploration. As a tool, advanced computing can be used to develop performance systems that respond directly to a live performer in real time, or to integrate virtual reality as a previsualization tool for designers.” Read more >> Anthropology: Heather Paxson, the William R. Kenan, Jr. Professor of Anthropology “The methods used in anthropology — a field that systematically studies human cultural beliefs and practices — are uniquely suited to studying the effects of automation and digital technologies in social life. For anthropologists, ‘Can artificial intelligence be ethical?’ is an empirical, not a hypothetical, question. Ethical for what? To whom? Under what circumstances?” Recommended action: “Incorporate anthropological thinking into the new college to prepare students to live and work effectively and responsibly in a world of technological, demographic, and cultural exchanges. We envision an ethnography lab that will provide digital and computing tools tailored to anthropological research and projects.” Read more >> Economics: Nancy L. Rose, the Charles P. Kindleberger Professor of Applied Economics and head of the Department of Economics; and David Autor, the Ford Professor of Economics and co-director of the MIT Task Force on the Work of the Future “The intellectual affinity between economics and computer science traces back almost a century, to the founding of game theory in 1928. Today, the practical synergies between economics and computer science are flourishing. We outline some of the many opportunities for the two disciplines to engage more deeply through the new SCC.” Recommended action: “Research that engages the tools and expertise of economics on matters of fairness, expertise, and cognitive biases in machine-supported and machine-delegated decision-making; and on market design, industrial organization, and the future of work. Scholarship at the intersection of data science, econometrics, and causal inference. Cultivate depth in network science, algorithmic game theory and mechanism design, and online learning. Develop tools for rapid, cost-effective, and ongoing education and retraining for workers.” Read more >> Political Science: Faculty of the Department of Political Science “The advance of computation gives rise to a number of conceptual and normative questions that are political, rather than ethical in character. Political science and theory have a significant role in addressing such questions as: How do major players in the technology sector seek to legitimate their authority to make decisions that affect us all? And where should that authority actually reside in a democratic polity?” Recommended action: “Incorporate the research and perspectives of political science in SCC research and education to help ensure that computational research is socially aware, especially with issues involving governing institutions, the relations between nations, and human rights.” Read more >> Series prepared by SHASS Communications Series Editor and Designer: Emily Hiestand Series Co-Editor: Kathryn O’Neill "
MIT News,What a little more computing power can do,Computer Science,2019-09-16,-,http://news.mit.edu/2019/what-extra-computing-power-can-do-0916,"  Neural networks have given researchers a powerful tool for looking into the future and making predictions. But one drawback is their insatiable need for data and computing power (""compute"") to process all that information. At MIT, demand for compute is estimated to be five times greater than what the Institute can offer. To help ease the crunch, industry has stepped in. An $11.6 million supercomputer recently donated by IBM comes online this fall, and in the past year, both IBM and Google have provided cloud credits to MIT Quest for Intelligence for distribution across campus. Four projects made possible by IBM and Google cloud donations are highlighted below. Smaller, faster, smarter neural networks To recognize a cat in a picture, a deep learning model may need to see millions of photos before its artificial neurons “learn” to identify a cat. The process is computationally intensive and carries a steep environmental cost, as new research attempting to measure artificial intelligence's (AI’s) carbon footprint has highlighted.  But there may be a more efficient way. New MIT research shows that models only a fraction of the size are needed. “When you train a big network there’s a small one that could have done everything,” says Jonathan Frankle, a graduate student in MIT’s Department of Electrical Engineering and Computer Science (EECS). With study co-author and EECS Professor Michael Carbin, Frankle estimates that a neural network could get by with on-tenth the number of connections if the right subnetwork is found at the outset. Normally, neural networks are trimmed after the training process, with irrelevant connections removed then. Why not train the small model to begin with, Frankle wondered? Experimenting with a two-neuron network on his laptop, Frankle got encouraging results and moved to larger image-datasets like MNIST and CIFAR-10, borrowing GPUs where he could. Finally, through IBM Cloud, he secured enough compute power to train a real ResNet model. “Everything I’d done previously was toy experiments,” he says. “I was finally able to run dozens of different settings to make sure I could make the claims in our paper.” Frankle spoke from Facebook’s offices, where he worked for the summer to explore ideas raised by his Lottery Ticket Hypothesis paper, one of two picked for a best paper award at this year’s International Conference on Learning Representations. Potential applications for the work go beyond image classification, Frankle says, and include reinforcement learning and natural language processing models. Already, researchers at Facebook AI Research, Princeton University, and Uber have published follow-on studies.  “What I love about neural networks is we haven’t even laid the foundation yet,” says Frankle, who recently shifted from studying cryptography and tech policy to AI. “We really don’t understand how it learns, where it’s good and where it fails. This is physics 1,000 years before Newton.” Distinguishing fact from fake news Networking platforms like Facebook and Twitter have made it easier than ever to find quality news. But too often, real news is drowned out by misleading or outright false information posted online. Confusion over a recent video of U.S. House Speaker Nancy Pelosi doctored to make her sound drunk is just the latest example of the threat misinformation and fake news pose to democracy.  “You can put just about anything up on the internet now, and some people will believe it,” says Moin Nadeem, a senior and EECS major at MIT. If technology helped create the problem, it can also help fix it. That was Nadeem’s reason for picking a superUROP project focused on building an automated system to fight fake and misleading news. Working in the lab of James Glass, a researcher at MIT’s Computer Science and Artificial Intelligence Laboratory, and supervised by Mitra Mohtarami, Nadeem helped train a language model to fact-check claims by searching through Wikipedia and three types of news sources rated by journalists as high-quality, mixed-quality or low-quality. To verify a claim, the model measures how closely the sources agree, with higher agreement scores indicating the claim is likely true. A high disagreement score for a claim like, “ISIS infiltrates the United States,” is a strong indicator of fake news. One drawback of this method, he says, is that the model doesn’t identify the independent truth so much as describe what most people think is true. With the help of Google Cloud Platform, Nadeem ran experiments and built an interactive website that lets users instantly assess the accuracy of a claim. He and his co-authors presented their results at the North American Association of Computational Linguistics (NAACL) conference in June and are continuing to expand on the work. “The saying used to be that seeing is believing,” says Nadeem, in this video about his work. “But we’re entering a world where that isn’t true. If people can’t trust their eyes and ears it becomes a question of what can we trust?” Visualizing a warming climate From rising seas to increased droughts, the effects of climate change are already being felt. A few decades from now, the world will be a warmer, drier, and more unpredictable place. Brandon Leshchinskiy, a graduate student in MIT’s Department of Aeronautics and Astronautics (AeroAstro), is experimenting with generative adversarial networks, or GANs, to imagine what Earth will look like then.  GANs produce hyper-realistic imagery by pitting one neural network against another. The first network learns the underlying structure of a set of images and tries to reproduce them, while the second decides which images look implausible and tells the first network to try again. Inspired by researchers who used GANs to visualize sea-level rise projections from street-view images, Leshchinskiy wanted to see if satellite imagery could similarly personalize climate projections. With his advisor, AeroAstro Professor Dava Newman, Leshchinskiy is currently using free IBM Cloud credits to train a pair of GANs on images of the eastern U.S. coastline with their corresponding elevation points. The goal is to visualize how sea-level rise projections for 2050 will redraw the coastline. If the project works, Leshinskiy hopes to use other NASA datasets to imagine future ocean acidification and changes in phytoplankton abundance.  “We’re past the point of mitigation,” he says. “Visualizing what the world will look like three decades from now can help us adapt to climate change.” Identifying athletes from a few gestures A few moves on the field or court are enough for a computer vision model to identify individual athletes. That’s according to preliminary research by a team led by Katherine Gallagher, a researcher at MIT Quest for Intelligence. The team trained computer vision models on video recordings of tennis matches and soccer and basketball games and found that the models could recognize individual players in just a few frames from key points on their body providing a rough outline of their skeleton.  The team used a Google Cloud API to process the video data, and compared their models' performance against models trained on Google Cloud's AI platform. “This pose information is so distinctive that our models can identify players with accuracy almost as good as models provided with much more information, like hair color and clothing,” she says.  Their results are relevant for automated player identification in sports analytics systems, and they could provide a basis for further research on inferring player fatigue to anticipate when players should be swapped out. Automated pose detection could also help athletes refine their technique by allowing them to isolate the precise moves associated with a golfer’s expert drive or a tennis player’s winning swing. "
MIT News,Using machine learning to estimate risk of cardiovascular death ,Computer Science,2019-09-12,-,http://news.mit.edu/2019/using-machine-learning-estimate-risk-cardiovascular-death-0912,"  Humans are inherently risk-averse: We spend our days calculating routes and routines, taking precautionary measures to avoid disease, danger, and despair.  Still, our measures for controlling the inner workings of our biology can be a little more unruly.  With that in mind, a team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with a new system for better predicting health outcomes: a machine learning model that can estimate, from the electrical activity of their heart, a patient’s risk of cardiovascular death.  The system, called “RiskCardio,” focuses on patients who have survived an acute coronary syndrome (ACS), which refers to a range of conditions where there’s a reduction or blockage of blood to the heart. Using just the first 15 minutes of a patient's raw electrocardiogram (ECG) signal, the tool produces a score that places patients into different risk categories.  RiskCardio’s high-risk patients — patients in the top quartile — were nearly seven times more likely to die of cardiovascular death when compared to the low-risk group in the bottom quartile. By comparison, patients identified as high risk by the most common existing risk metrics were only three times more likely to suffer an adverse event compared to their low-risk counterparts.  ""We're looking at the data problem of how we can incorporate very long time series into risk scores, and the clinical problem of how we can help doctors identify patients at high risk after an acute coronary event,” says Divya Shanmugam, lead author on a new paper about RiskCardio. “The intersection of machine learning and healthcare is replete with combinations like this — a compelling computer science problem with potential real-world impact.”  Risky business  Previous machine learning models have attempted to get a handle on risk by either making use of external patient information like age or weight, or using knowledge and expertise specific to the system — more broadly known as domain-specific knowledge — to help their model select different features.  RiskCardio, however, uses just the patients’ raw ECG signal, with no additional information. Say a patient checks into the hospital following an ACS. After intake, a physician would first estimate the risk of cardiovascular death or heart attack using medical data and lengthy tests, and then choose a course of treatment.  RiskCardio aims to improve that first step of estimating risk. To do this, the system separates a patient’s signal into sets of consecutive beats, with the idea that variability between adjacent beats is telling of downstream risk. The system was trained using data from a study of past patients. To get the model up and running, the team first separated each patient's signal into a collection of adjacent heart beats. They then assigned a label — i.e., whether or not the patient died of cardiovascular death — to each set of adjacent heartbeats. The researchers trained the model to classify each pair of adjacent heartbeats to its patient outcome: Heartbeats from patients who died were labeled “risky,” while heartbeats from patients who survived were labeled “normal.”  Given a new patient, the team created a risk score by averaging the patient prediction from each set of adjacent heartbeats. Within the first 15 minutes of a patient experiencing an ACS, there was enough information to estimate whether or not they would suffer from cardiovascular death within 30, 60, 90, or 365 days.  Still, calculating a risk score from just the ECG signal is no simple task. The signals are very long, and as the number of inputs to a model increase, it becomes harder to learn the relationship between those inputs.  The team tested the model by producing risk scores for a set of patients. Then, they measured how much more likely a patient would suffer from cardiovascular death as a high-risk patient when compared to a set of low-risk patients. They found that in roughly 1,250 post-ACS patients, 28 would die of cardiovascular death within a year. Using the proposed risk score, 19 of those 28 patients were classified as high-risk.  In the future, the team hopes to make the dataset more inclusive to account for different ages, ethnicities, and genders. They also plan to examine medical scenarios where there’s a lot of poorly labeled or unlabeled data, and evaluate how their system processes and handles that information to account for more ambiguous cases.  “Machine learning is particularly good at identifying patterns, which is deeply relevant to assessing patient risk,'' says Shanmugam. “Risk scores are useful for communicating patient state, which is valuable in making efficient care decisions.”  Shanmugam presented the paper at the Machine Learning for Healthcare Conference alongside PhD student Davis Blalock and MIT Professor John Guttag. "
MIT News,Detecting patients’ pain levels via their brain signals,Computer Science,2019-09-12,-,http://news.mit.edu/2019/detecting-pain-levels-brain-signals-0912,"  Researchers from MIT and elsewhere have developed a system that measures a patient’s pain level by analyzing brain activity from a portable neuroimaging device. The system could help doctors diagnose and treat pain in unconscious and noncommunicative patients, which could reduce the risk of chronic pain that can occur after surgery. Pain management is a surprisingly challenging, complex balancing act. Overtreating pain, for example, runs the risk of addicting patients to pain medication. Undertreating pain, on the other hand, may lead to long-term chronic pain and other complications. Today, doctors generally gauge pain levels according to their patients’ own reports of how they’re feeling. But what about patients who can’t communicate how they’re feeling effectively — or at all — such as children, elderly patients with dementia, or those undergoing surgery? In a paper presented at the International Conference on Affective Computing and Intelligent Interaction, the researchers describe a method to quantify pain in patients. To do so, they leverage an emerging neuroimaging technique called functional near infrared spectroscopy (fNIRS), in which sensors placed around the head measure oxygenated hemoglobin concentrations that indicate neuron activity. For their work, the researchers use only a few fNIRS sensors on a patient’s forehead to measure activity in the prefrontal cortex, which plays a major role in pain processing. Using the measured brain signals, the researchers developed personalized machine-learning models to detect patterns of oxygenated hemoglobin levels associated with pain responses. When the sensors are in place, the models can detect whether a patient is experiencing pain with around 87 percent accuracy. “The way we measure pain hasn’t changed over the years,” says Daniel Lopez-Martinez, a PhD student in the Harvard-MIT Program in Health Sciences and Technology and a researcher at the MIT Media Lab. “If we don’t have metrics for how much pain someone experiences, treating pain and running clinical trials becomes challenging. The motivation is to quantify pain in an objective manner that doesn’t require the cooperation of the patient, such as when a patient is unconscious during surgery.” Traditionally, surgery patients receive anesthesia and medication based on their age, weight, previous diseases, and other factors. If they don’t move and their heart rate remains stable, they’re considered fine. But the brain may still be processing pain signals while they’re unconscious, which can lead to increased postoperative pain and long-term chronic pain. The researchers’ system could provide surgeons with real-time information about an unconscious patient’s pain levels, so they can adjust anesthesia and medication dosages accordingly to stop those pain signals. Joining Lopez-Martinez on the paper are: Ke Peng of Harvard Medical School, Boston Children’s Hospital, and the CHUM Research Centre in Montreal; Arielle Lee and David Borsook, both of Harvard Medical School, Boston Children’s Hospital, and Massachusetts General Hospital; and Rosalind Picard, a professor of media arts and sciences and director of affective computing research in the Media Lab. Focusing on the forehead In their work, the researchers adapted the fNIRS system and developed new machine-learning techniques to make the system more accurate and practical for clinical use. To use fNIRS, sensors are traditionally placed all around a patient’s head. Different wavelengths of near-infrared light shine through the skull and into the brain. Oxygenated and deoxygenated hemoglobin absorb the wavelengths differently, altering their signals slightly. When the infrared signals reflect back to the sensors, signal-processing techniques use the altered signals to calculate how much of each hemoglobin type is present in different regions of the brain. When a patient is hurt, regions of the brain associated with pain will see a sharp rise in oxygenated hemoglobin and decreases in deoxygenated hemoglobin, and these changes can be detected through fNIRS monitoring. But traditional fNIRS systems place sensors all around the patient’s head. This can take a long time to set up, and it can be difficult for patients who must lie down. It also isn’t really feasible for patients undergoing surgery. Therefore, the researchers adapted the fNIRS system to specifically measure signals only from the prefrontal cortex. While pain processing involves outputs of information from multiple regions of the brain, studies have shown the prefrontal cortex integrates all that information. This means they need to place sensors only over the forehead.  Another problem with traditional fNIRS systems is they capture some signals from the skull and skin that contribute to noise. To fix that, the researchers installed additional sensors  to capture and filter out those signals. Personalized pain modeling On the machine-learning side, the researchers trained and tested a model on a labeled pain-processing dataset they collected from 43 male participants. (Next they plan to collect a lot more data from diverse patient populations, including female patients — both during surgery and while conscious, and at a range of pain intensities — in order to better evaluate the accuracy of the system.) Each participant wore the researchers’ fNIRS device and was randomly exposed to an innocuous sensation and then about a dozen shocks to their thumb at two different pain intensities, measured on a scale of 1-10: a low level (about a 3/10) or high level (about 7/10). Those two intensities were determined with pretests: The participants self-reported the low level as being only strongly aware of the shock without pain, and the high level as the maximum pain they could tolerate.  In training, the model extracted dozens of features from the signals related to how much oxygenated and deoxygenated hemoglobin was present, as well as how quickly the oxygenated hemoglobin levels rose. Those two metrics — quantity and speed — give a clearer picture of a patient’s experience of pain at the different intensities. Importantly, the model also automatically generates “personalized” submodels that extract high-resolution features from individual patient subpopulations. Traditionally, in machine learning, one model learns classifications — “pain” or “no pain” — based on average responses of the entire patient population. But that generalized approach can reduce accuracy, especially with diverse patient populations. The researchers’ model instead trains on the entire population but simultaneously identifies shared characteristics among subpopulations within the larger dataset. For example, pain responses to the two intensities may differ between young and old patients, or depending on gender. This generates learned submodels that break off and learn, in parallel, patterns of their patient subpopulations. At the same time, however, they’re all still sharing information and learning patterns shared across the entire population. In short, they’re simultaneously leveraging fine-grained personalized information and population-level information to train better. The personalized models and a traditional model were evaluated in classifying pain or no-pain in a random hold-out set of participant brain signals from the dataset, where the self-reported pain scores were known for each participant. The personalized models outperformed the traditional model by about 20 percent, reaching about 87 percent accuracy. “Because we are able to detect pain with this high accuracy, using only a few sensors on the forehead, we have a solid basis for bringing this technology to a real-world clinical setting,” Lopez-Martinez says. "
MIT News,Letter regarding update on the MIT Schwarzman College of Computing,Computer Science,2019-09-11,-,http://news.mit.edu/2019/update-schwarzman-college-computing-0911,"  The following letter was sent to MIT faculty members on Sept. 11 by Provost Martin A. Schmidt. Dear Colleagues, I am writing to share with you a status update from Dean Dan Huttenlocher on the formation of the MIT Stephen A. Schwarzman College of Computing. This is the first of what I expect to be regular updates over the next few months as the initial set of academic units and structure of the college are being determined. As I have mentioned in the past, the college stands on three legs: advancing computer science, building strong connections between computer science and all other disciplines at MIT, and being intentional in contemplating the societal implications of computing. Dean Huttenlocher is working to advance each of these legs as we build the college, and his note below provides an update on where we stand. Out of necessity, the plans for each leg are moving at different paces, but it is important for me to stress that each one of these legs is vital to the college’s overall success.   Sincerely, Martin A. Schmidt   MIT Schwarzman College of Computing  September 11, 2019 Update Dan Huttenlocher, Dean I would first like to thank the MIT community—particularly those who have given so generously of their time and ideas—for their continued support in the creation and establishment of the MIT Schwarzman College of Computing (SCC). While much remains to be done, I am excited to share this update on progress made over the summer, including: Establishment of an advisory group, building on the College of Computing Task Force. Initial administrative leadership appointments and planned additional appointments over the coming year. Process for creating an organizational plan for the college. Organizational plan for the Department of Electrical Engineering and Computer Science (EECS). Summary of the college’s mission and scope. 1. Advisory Group. I am delighted that members of last year’s College of Computing Task Force Steering Committee have agreed to serve this year as an advisory group for the formation of the SCC, as they have a unique understanding of the opportunities and challenges from their deep involvement in the working groups. The advisory group members are Eran Ben-Joseph, Anantha Chandrakasan, Rick Danheiser, Srinivas Devadas, Benoit Forget, William Freeman, Melissa Nobles, Asu Ozdaglar, Nelson Repenning, Nicholas Roy, Julie Shah, and Troy Van Voorhis. I have also recently met with undergraduate and graduate student groups and will be working with them on avenues for student input, which are likely to largely be through academic units, as those are the main loci of educational programs. 2. Leadership Appointments. I am also pleased to announce initial leadership appointments for the SCC. These take effect immediately and will report directly to me. Daniela Rus will serve as deputy dean of research and continue as director of CSAIL; Asu Ozdaglar will serve as deputy dean of academics and continue as head of EECS; David Kaiser and Julie Shah will serve as co-heads of social and ethical responsibilities of computing; Eileen Ng will serve as assistant dean of administration; moving from the School of Engineering; and Terri Park will serve as director of communications, moving from the Innovation Initiative. Please join me in congratulating them and thanking them for taking on these important new roles. Over the coming year, we expect to fill other SCC leadership positions, including an assistant or associate dean of equity and inclusion, assistant dean for development, director of a new Center for Advanced Study of Computing, and leadership of a new teaching collaborative for both disciplinary and interdisciplinary computing classes, once the structure of the college and these additional roles becomes more fully defined. We further expect that Institute for Data, Systems, and Society (IDSS), Center for Computational Engineering (CCE), Computer Science and Artificial Intelligence Lab (CSAIL), Laboratory for Information and Decision Systems (LIDS), the Quest for Intelligence, and perhaps other units, will likely become part of the SCC as planning proceeds, and that their directors will be asked to remain in their roles. 3. College Planning Process. Based on last spring’s College of Computing Task Force Working Group reports, a strawman organizational plan for the college is being developed through an iterative process. The school deans and the aforementioned advisory group are reviewing an early draft. A revised strawman will then be circulated to the school councils, and then to the full MIT faculty for feedback and final revision. We expect to begin sharing the strawman more broadly in the coming months through forums to solicit feedback from the MIT community. We do not foresee much in the way of curriculum or subject changes this school year, as these are defined by the faculty of academic units through deliberative processes, although there will likely be some pilots. 4. EECS Plan. An organizational plan for EECS, developed over the summer based on the task force working group reports, is now being implemented. This plan also was developed iteratively, first involving the co-chairs of the Organizational Structure and Faculty Appointments working groups, followed by the Engineering Council and the EECS faculty. The plan calls for the department to form three overlapping academic units, as suggested by the Organizational Structure Working Group. These units are termed faculties: a faculty of electrical engineering (EE), a faculty of computer science (CS), and a faculty of artificial intelligence and decision making (AI+D). Each faculty will have a head, and will be overseen by the head of EECS. The faculties will be the main locus of faculty recruiting, mentoring, and promotion for the department. The department will report jointly to the School of Engineering and the Schwarzman College of Computing, and will remain responsible for Course 6. A search committee for the heads of the three faculties has just been formed. No specific curricular changes are foreseen at this time, as those will take time and will be led by the faculties and the department. 5. Mission and Scope. The planning efforts for the Schwarzman College of Computing are driven by the following mission and scope of activities. Widespread advances in computing—from hardware to software to algorithms to artificial intelligence—have improved people’s lives in myriad ways, with numerous promising opportunities on the horizon. Yet at the same time, we face critical and growing challenges regarding the social and ethical implications and responsibilities of computing, particularly with the increasing applicability of artificial intelligence. Moreover, despite the unprecedented growth of computer science, artificial intelligence, and related academic program areas, substantial unmet demand for expertise in computing remains, as does the constant need to keep up with rapidly changing academic content. The mission of the MIT Stephen A. Schwarzman College of Computing (SCC) is to address the challenges and opportunities of the computing age by transforming the capabilities of academia in three key areas: Computing fields: Support the rapid growth and evolution of computer science and computational areas of allied fields such as electrical engineering, as reflected notably in the rise of artificial intelligence; Computing across disciplines: Facilitate productive research and teaching collaborations between computing and other fields, rather than place one field in service of another; Social and ethical aspects of computing: Lead the development of and changes in academic research and education, and effectively inform practice and policy in industry and government. These three areas are integral to the SCC mission. Moreover, they are not independent but rather should inform and amplify one another. Based on the College of Computing Task Force Working Group reports and follow-on planning over the summer, the SCC is expected to: Have academic units that are more flexible and interconnected than conventional departments and schools while simultaneously reinforcing MIT’s strength in computing fields such as CS and large parts of EE, which generally have traditional departments at other top institutions. The SCC is expected to have multiple types of academic structures to meet this variety of needs. Involve faculty from a broad range of departments and schools who are engaged in computing education and research through a variety of programs and affiliations including: (i) the Common Ground, a teaching collaborative for both disciplinary and interdisciplinary computing classes, (ii) centers that offer graduate programs in computing, (iii) computing research labs and centers, and (iv) additional scholarly activities in computing, including workshops and other convenings. Lead in the social and ethical aspects of computing with: (i) education that helps develop “habits of mind and action” for those who create and deploy computing technologies, (ii) research that brings technological, social science, and humanistic approaches together, and (iii) impact on government and corporate policy as well as the development of a better understanding among the general public. Lead in the rapid evolution of computing fields, both in research and education, currently exemplified by the rise of areas such as AI and machine learning (ML). Deliver outstanding undergraduate and graduate education in: (i) CS and EE, (ii) evolving areas of computing such as AI/ML and others, (iii) computing across the disciplines, and (iv) social and ethical aspects of computing. Improve equity and inclusion in computing at MIT with the aim of helping address diversity in computing with regard to gender, race, and range of backgrounds and experience. Focus on increasing the diversity of top faculty candidates, with new programs for faculty, postdocs, and PhD students, as well as improvements to existing programs. Broaden participation in computing classes and academic programs at all levels, including improvements to the climate and the development of more effective connections with other, more diverse, disciplines. The SCC will include both existing and new academic units and programs. Bringing existing activities together will help facilitate coordination and alignment of computing education and research as well as provide new opportunities for improvement, such as bringing more coordination to the academic programs that mix computing and other disciplines and to the teaching of social and ethical issues in computing. Creating new programs and units will help address areas that are not well covered by existing ones and also help in fostering new connections. "
MIT News,Computing in Earth science: a non-linear path,Computer Science,2019-09-11,-,http://news.mit.edu/2019/computing-earth-science-non-linear-path-sonia-reilly-0911,"  Machine learning is undeniably a tool that most disciplines like to have in their toolbox. However, scientists are still investigating the limits and barriers to incorporating machine learning into their research. Junior Sonia Reilly spent her summer opening up the machine learning black box to better understand how information flows through neural networks as part of the Undergraduate Research Opportunities Program (UROP). Her project, which investigates how machine learning works with the intention of improving its application to the observation of natural phenomena, was overseen by Sai Ravela in the Department of Earth, Atmospheric and Planetary Sciences (EAPS). As a major in Course 18C (Mathematics with Computer Science), Reilly is uniquely equipped to help investigate these connections. “In recent years, deep learning has become an immensely popular tool in all kinds of research fields, but the mathematics of how and why it is so effective is still very poorly understood,” says Reilly. “Having that knowledge will enable the design of better-performing learning machines.” To do that, she looks more closely at how the algorithms evolve to produce their final most-probable conclusions, with the end goal of providing insights on information flow, bottlenecks, and maximizing gain from neural networks. “We don’t want to be drowning in big data. On the contrary, we want to transform big data into perhaps what we might call smart data,” Ravela says of how machine learning must proceed. “The end goal is always a sensing agent that gathers data from our environment, but one that is knowledge-driven and does just enough work to gather just enough information for meaningful inferences.” For Ravela, who leads the Earth Signals and Systems Group (ESSG), better-performing learning machines means more robust early predictions of potential disasters. His group’s research lies largely in how the Earth works as a system, primarily focusing on climate and natural hazards. They observe natural phenomena to produce effective predictive models for dynamic natural processes, such as hurricanes, clouds, volcanoes, earthquakes, glaciers, and wildlife conservation strategies, as well as making advances in engineering and learning itself. “In all these projects, it’s impossible to gather dense data in space and time. We show that actively mining the environment through a systems analytic approach is promising,” he says. Ravela recently delivered his group’s latest work — including Reilly’s contributions — to the Association of Computing Machinery’s special interest group on knowledge discovery and data mining (SIGKDD 2019) in early August. He teaches an “infinite course” with a duology of classes taught in spring and fall semesters that provides an overview of machine learning foundations for natural systems science, which anyone can follow along with online. According to Ravela, if Reilly is to succeed at advancing the mathematical basis for computational learning models, she will be one of the “early pioneers of learning that can be explained,” an achievement that can provide a promising career path. That is ideal for Reilly’s goals of obtaining a PhD in mathematics after graduating from MIT and remaining a contributor to research that can positively impact the world. She’s starting with cramming as much research as she can manage into her schedule over her final two undergraduate years at MIT, including her experience this summer. Although this was Reilly’s first UROP experience, it is her second time undertaking a research project that blends mathematics, computer science, and Earth science. Previously, at the Johns Hopkins University Applied Physics Laboratory, Reilly helped develop signal processing techniques and software that would improve the retrieval of useful climate change information from low-quality satellite data. “I’ve always wanted to be part of an interdisciplinary research environment where I could use my knowledge of math to contribute to the work of scientists and engineers,” Reilly says of working within EAPS. “It’s encouraging to see that type of environment and get a taste of what it would be like to work in one.” Ravela explains that the ESSG is fond of the mutually beneficial inclusion of UROP students. “For me, UROPs are better than grad student and postdocs if, and only if, one can create the right-sized questions for them to run with. But then they run the fastest and are the most clever of all.” He says he feels the UROP program is invaluable and could be beneficial for all students to incorporate, as it offers a chance to learn about other fields and interdisciplinary research, as well as how to incorporate what they learn into tangible results. For Reilly, research builds on her foundation obtained from taking classes at MIT, which are a controlled and predictable environment, she says, “but research is nowhere near so linear.” She has relied on her foundation of mathematics and computer science from her courses during her UROP experience while having to learn how to connect and apply them to new fields and to consider topics often outside an undergraduate education. “It often feels like every step I take requires me to learn about an entirely new field of mathematics, and it’s difficult to know where to start. I definitely feel lost sometimes, but I’m also learning an incredible amount.” "
MIT News,Faster video recognition for the smartphone era,Computer Science,2019-10-11,-,http://news.mit.edu/2019/faster-video-recognition-smartphone-era-1011,"  A branch of machine learning called deep learning has helped computers surpass humans at well-defined visual tasks like reading medical scans, but as the technology expands into interpreting videos and real-world events, the models are getting larger and more computationally intensive.  By one estimate, training a video-recognition model can take up to 50 times more data and eight times more processing power than training an image-classification model. That’s a problem as demand for processing power to train deep learning models continues to rise exponentially and concerns about AI’s massive carbon footprint grow. Running large video-recognition models on low-power mobile devices, where many AI applications are heading, also remains a challenge.  Song Han, an assistant professor at MIT’s Department of Electrical Engineering and Computer Science (EECS), is tackling the problem by designing more efficient deep learning models. In a paper at the International Conference on Computer Vision, Han, MIT graduate student Ji Lin and MIT-IBM Watson AI Lab researcher Chuang Gan, outline a method for shrinking video-recognition models to speed up training and improve runtime performance on smartphones and other mobile devices. Their method makes it possible to shrink the model to one-sixth the size by reducing the 150 million parameters in a state-of-the-art model to 25 million parameters.         “Our goal is to make AI accessible to anyone with a low-power device,” says Han. “To do that, we need to design efficient AI models that use less energy and can run smoothly on edge devices, where so much of AI is moving.”  The falling cost of cameras and video-editing software and the rise of new video-streaming platforms has flooded the internet with new content. Each hour, 30,000 hours of new video are uploaded to YouTube alone. Tools to catalog that content more efficiently would help viewers and advertisers locate videos faster, the researchers say. Such tools would also help institutions like hospitals and nursing homes to run AI applications locally, rather than in the cloud, to keep sensitive data private and secure.  Underlying image and video-recognition models are neural networks, which are loosely modeled on how the brain processes information. Whether it’s a digital photo or sequence of video images, neural nets look for patterns in the pixels and build an increasingly abstract representation of what they see. With enough examples, neural nets “learn” to recognize people, objects, and how they relate.  Top video-recognition models currently use three-dimensional convolutions to encode the passage of time in a sequence of images, which creates bigger, more computationally-intensive models. To reduce the calculations involved, Han and his colleagues designed an operation they call a temporal shift module which shifts the feature maps of a selected video frame to its neighboring frames. By mingling spatial representations of the past, present, and future, the model gets a sense of time passing without explicitly representing it. The result: a model that outperformed its peers at recognizing actions in the Something-Something video dataset, earning first place in version 1 and version 2, in recent public rankings. An online version of the shift module is also nimble enough to read movements in real-time. In a recent demo, Lin, a PhD student in EECS, showed how a single-board computer rigged to a video camera could instantly classify hand gestures with the amount of energy to power a bike light.  Normally it would take about two days to train such a powerful model on a machine with just one graphics processor. But the researchers managed to borrow time on the U.S. Department of Energy’s Summit supercomputer, currently ranked the fastest on Earth. With Summit’s extra firepower, the researchers showed that with 1,536 graphics processors the model could be trained in just 14 minutes, near its theoretical limit. That’s up to three times faster than 3-D state-of-the-art models, they say. Dario Gil, director of IBM Research, highlighted the work in his recent opening remarks at AI Research Week hosted by the MIT-IBM Watson AI Lab. “Compute requirements for large AI training jobs is doubling every 3.5 months,” he said later. “Our ability to continue pushing the limits of the technology will depend on strategies like this that match hyper-efficient algorithms with powerful machines.”  "
MIT News,Learning about China by learning its language,Computer Science,2019-10-11,-,http://news.mit.edu/2019/learning-about-china-learning-language-max-allen-1011,"  Among MIT students who didn’t grow up speaking Chinese, few are able to discuss “machine learning models” in passable Mandarin. But that is just what computer science and engineering senior Max Allen is able to do, and this ability comes as a result of academic work, stints abroad, an internship, and also just having the passion to learn Chinese. With China a growing economic powerhouse and leader in STEM, it is no wonder that more and more students are attracted to studying Chinese. Nationally, enrollments in Chinese classes are up, as they are at MIT. But for Max Allen, his interest was first piqued by a teacher’s visit to his eighth-grade class. Intrigued by the sound of the language and structure of the writing system, Allen started taking Chinese classes in high school. To him, learning the language was akin to a big puzzle whose solution is slowly revealed. And since Allen has always been fond of puzzles, he wanted to pursue this. After only two years of high-school language study, Allen spent his 11th-grade year living with a host family in Beijing and attending school through a program called School Year Abroad. Allen returned to the United States able to converse in Mandarin, and also more adept at fitting in culturally. He found that living with a family gives you a level of familiarity with people that is hard to achieve otherwise. Chinese has gradually occupied a greater and greater area of interest for Allen. Upon entering MIT, he decided to pursue a major in computer science and engineering (Course 6-3). After discovering that he could take Chinese to fulfill his humanities concentration requirements, Allen took Chinese V and VI, building on the work he did in high school. Even among MIT students who are known for high academic achievement, Chinese Lecturer Tong Chen noted that Allen stood out for his effort and seriousness. The more classes he took, and the more time he invested, the more Allen began to consider how Chinese might be part of his future academic and career paths. In spring 2018, Allen took “Business Chinese” as an elective concentration subject. Business Chinese helped Allen understand social dynamics and subtleties of social relations in a business setting in China, including how these express themselves in language. As Panpan Gao, the instructor of Business Chinese, explains, the pedagogical approach of the class emphasizes case studies: “Through case studies of multinational companies and introductions to crucial business issues in China, we try to help students better understand Chinese business culture and trends, and expand their language skills so that they can communicate effectively and professionally with Chinese speakers in the workplace.” The class really got Allen thinking about whether he might want to pursue jobs that would employ his knowledge of Chinese. Allen put his Chinese skills to good use the following summer. He took an engineering internship with Airbnb — on a team with a special focus on mitigating financial fraud coming from China. The team was mostly made up of Chinese nationals, and team members generally discussed work matters in Mandarin. To do business in China, the team would need to understand how to market the product to Chinese customers; how to build a secure platform; and how to build payment applications that are in line with expectations of Chinese consumer. This experience gave Allen a hands-on taste of the complexities of functioning in a Chinese business context. After the internship, Allen realized that to take his Chinese to the next level, he would need to put aside other academic pursuits for a period and spend more time studying the language in an immersive Chinese-speaking setting. He spent academic year 2018-2019 abroad studying Chinese: the fall in Taipei at the International Chinese Language Program of National Taiwan University, and the spring in Beijing at the Inter-University Program for Chinese Language Studies at Tsinghua University. Both programs are top Chinese language centers in the world and are intensive instructional programs with hours of work a day devoted to learning Mandarin. He particularly appreciated the intensive focus on conversation. While abroad, Allen found that when he ventured to out-of-the-way spots, he encountered curiosity from strangers who were less accustomed to seeing tourists. But when he demonstrated he could speak Chinese, people warmed up. “Speaking their native language helps to establish trust and rapport, which is important when they see you as just another outsider. But once a certain level of trust is established, people become more comfortable talking about meaningful things. And that's where the time investment of learning the language really pays off.” Now back at MIT for his senior year, Allen is considering how his multiple interests in computer science, international business, Chinese language, and cross-cultural communication skills might combine into a career path. The answer will take some time to untangle, but Allen is always up for the challenge of a big puzzle, and will remain open to the possibilities as he heads toward graduation. "
MIT News,Meet the 2019-20 MLK Visiting Professors and Scholars,Computer Science,2019-10-08,-,http://news.mit.edu/2019/dr-martin-luther-king-jr-visiting-professors-and-scholars-program-1008,"  Founded in 1990, the Martin Luther King Jr. (MLK) Visiting Professors and Scholars Program honors the life and legacy of Martin Luther King by increasing the presence of, and recognizing the contributions of, underrepresented minority scholars at MIT. MLK Visiting Professors and Scholars enhance their scholarship through intellectual engagement with the MIT community and enrich the cultural, academic, and professional experience of students. The program hosts between four and eight scholars each year with financial and institutional support from the Office of the Provost and oversight from the Institute Community and Equity Office. Six scholars are visiting MIT this academic year as part of the program. Kasso Okoudjou is returning for a second year as an MLK Visiting Professor in the Department of Mathematics. Originally from Benin, he moved to the United States in 1998 and earned a PhD in mathematics from Georgia Tech. Okoudjou joins MIT from the University of Maryland College Park, where he is a professor. His research interests include applied and pure harmonic analysis, especially time-frequency and time-scale analysis; frame theory; and analysis and differential equations on fractals. He is interested in broadening the participation of underrepresented minorities in (undergraduate) research in the mathematical sciences. Matthew Schumaker joins MIT for another year in the Music and Theater Arts Section within the School of Humanities, Arts, and Social Sciences. Schumaker received his doctorate in music composition from the University of California at Berkeley. At MIT, he teaches a new course, 21M.380 (Composing for Solo Instrument and Live Electronics), a hands-on music technology composition seminar combining instrumental writing with real-time computer music. Additionally, The Radius Ensemble in Cambridge, Massachusetts has commissioned Schumaker to write a new piece of music that seeks to translate into music the vibrant, curved gestures and slashed markings in the abstract landscapes of celebrated Ethiopian-born painter Julie Mehretu. Jamie Macbeth is visiting from Smith College, where he is an assistant professor in computer science. He received his PhD in computer science from University of California at Los Angeles. Although this is his first year as an MLK Visiting Scholar, he is not new to MIT, since he has been a visiting scientist since 2017. He is hosted by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). Macbeth’s research is focused on building and studying intelligent computing systems that demonstrate a human-like capability for in-depth understanding and production of natural language, and thus can achieve richer interactions with human users. He is especially keen on building systems that decompose the meaning of language into complex conceptual structures that reflect humans’ embodied cognition, memory, imagery and knowledge about social situations. Ben McDonald has been a postdoc in the Department of Chemistry since 2018 and is now an MLK Visiting Scholar. McDonald received his PhD in synthetic organic chemistry from Northwestern University. His research focused on the total synthesis of flavonolignan natural products and the development of reverse-polarity carbon-carbon bond forming reactions. As a member of the department’s Chemistry Alliance for Inclusion and Diversity, he is focused on advancing diversity, equity and inclusion efforts. One of the initiatives he seeks to establish is a summer research program, which recruits talented future scientists from underrepresented backgrounds. Tina Opie is an associate professor in the Management Division at Babson College. Opie obtained her PhD in management (with a concentration in organizational behavior) from New York University’s Stern School of Business. As an MLK Visiting Scholar in MIT Sloan School of Management, along with access to MIT’s Behavioral Research Lab, she is conducting research to develop the construct of Shared Sisterhood. “Shared Sisterhood examines how high-quality relationships (e.g., relationships characterized by trust, emotional vulnerability) between black, white, and Latinx women at work facilitate workplace inclusion and equity.” Though her work has a specific focus, people of all genders and racioethnic backgrounds can be “sisters” and can contribute to fostering a more inclusive work environment. Opie established Opie Consulting Group, a diversity-and-inclusion consultancy that incorporates Shared Sisterhood in creating inclusive workplaces.   Rhonda Williams, an MLK Visiting Professor hosted by the Department of History, joins MIT from Vanderbilt University, where she was recently appointed the John L. Seigenthaler Chair in American History. She is the founder of the Social Justice Institute at Case Western Reserve University. Her essay titled “Black Women Who Educate for Justice and Put Their Time, Lives, and Spirits on the Line"" was recently published in ""Black Women And Social Justice Education: Legacies and Lessons"" (2019, SUNY Press). On Oct. 25, Williams will deliver a social justice-related performance-lecture called “The Things That Divide Us: Meditations” at MIT. In spring 2020, she will facilitate a social justice workshop for students, faculty and staff. For more information about our scholars and the program, visit mlkscholars.mit.edu. "
MIT News,Funding for sustainable concrete cemented for five more years,Computer Science,2019-10-04,-,http://news.mit.edu/2019/renewed-funding-sustainable-concrete-1004,"  The MIT Concrete Sustainability Hub (CSHub), an interdisciplinary team of researchers dedicated to concrete and infrastructure science, engineering, and economics, has renewed its relationship with its industry partners for another five years. Founded in 2009, CSHub has spent a decade over two five-year phases collaborating with the Portland Cement Association (PCA) and the Ready Mixed Concrete Research & Education Foundation (RMC) to achieve durable and sustainable buildings and infrastructure in ever-more-demanding environments. Over its next five-year phase, CSHub will receive $10 million of additional funding from its partners to continue its research efforts. “Taking CSHub’s work to the next level will not only help us achieve our goal of making concrete more sustainable, but will also continue to strengthen our communities by providing designers, owners, and policymakers with the best information and tools available to make the best choices for their construction projects,” says Julia Garbini, the executive director of RMC. According to Michael Ireland, PCA president and CEO, CSHub’s past research has also allowed the industry to investigate the unique properties of concrete and cement. “For 10 years and counting, the MIT CSHub has helped the cement and concrete industry to identify and study the myriad benefits of its products,” he says. Concrete, the world’s most-used building material, is made by mixing cement with abundant aggregate materials like sand and gravel. The result is an extremely strong and stiff material that can be produced nearly anywhere from readily available ingredients using relatively inexperienced labor. Concrete also offers numerous properties such as durability, formability, and thermal mass that can reduce energy consumption. “On a per-unit-weight basis, concrete is a low environmental impact material,” says Jeremy Gregory, CSHub’s executive director. “It’s essential to our built environment due to its durability, strength, and affordability. As a consequence, it’s the most-used building material in the world and hence, there is a significant opportunity to look at how we balance both its role in sustainable development and lower its environmental impact.” To do this, CSHub has taken a bottom-up approach, studying concrete from its nanoscale to its application in pavements and buildings, all the way to its role in urban environments and broader economic systems. “Classical concrete science and structural engineering often use top-down approaches,” says CSHub Faculty Director and MIT Professor Franz-Josef Ulm. “You identify weaknesses at a large scale, go to a smaller scale, make a change, and then observe the response. It is different when you go from the bottom-up — you have all of the possibilities in front of you.” Over the past decade, CSHub researchers have used this bottom-up approach to develop tools that measure the costs, environmental impacts, and hazard resilience of infrastructure and construction projects. In 2018, they developed the Break-Even Mitigation Percentage dashboard to provide developers with data on the costs of hazard mitigation. The dashboard shows the return on investment for hazard-resistant construction. In some communities, researchers found that that return can come as early as two years. Their investigation into the life cycle of buildings has also led to the creation of the Building Attribute to Impact Algorithm (BAIA), which informs designers of which aspects of a building will have the strongest impact on its life cycle cost and environmental impact. Researchers have applied these same life cycle perspectives to pavements. A case study conducted with North Carolina’s Department of Transportation highlighted actions that could reduce spending on pavements by tens of millions of dollars while meeting or exceeding performance and emissions targets. Recent CSHub materials science research has also informed the discovery of novel solutions to longstanding durability issues in concrete. In particular, researchers identified new explanations for two major causes of damage in concrete — freeze-thaw cycles and alkali-silica reaction. “Whenever you touch old problems, there are perceptions that they are very difficult to change,” says Ulm. “However, here we applied a bottom-up approach to an old problem and found solutions that have not been looked at before.” In the next phase of collaboration, CSHub will expand its scope to investigate concrete’s role in solving economic, environmental, and social challenges. “We have done a lot of work in the past two phases on the technical aspects of concrete,” says Gregory. “What we are trying to do in this next phase is to conduct research that will engage the broader public by leveraging crowdsourced data, artificial intelligence, and the latest tools of data science.” One Phase III project is already in development. Using their past work on pavements, CSHub researchers have created Carbin, an app that uses a smartphone to record pavement quality from within a moving vehicle. Through crowdsourcing, the app has recorded data on over 130,000 miles of roads across the world. The data will eventually support decisions on infrastructure maintenance at a far lower cost than that of traditional technologies, like laser scanning. “With the CSHub now entering its third phase, we are excited about the opportunities this close industry-academia collaboration brings to MIT, the concrete industry, and society at large,” says Markus Buehler, Jerry McAfee Professor in Engineering and MIT Department of Civil and Environmental Engineering head. “Applying cutting-edge fundamental research to problems in industry has the potential for large-scale impact.” "
MIT News,Strong mentorship through great decision-making,Computer Science,2019-10-03,-,http://news.mit.edu/2019/strong-mentorship-through-great-decision-making-c2c-1003,"  Faculty mentors Gabriella Carolini, Paula Hammond, and David Trumper are known for guiding students through the trenches of graduate school — one decision at a time.   Students encounter various obstacles in graduate school, many of which are unexpected. Selecting a research project may become an all-consuming task. Starting a family in graduate school may be both the best and the most-daunting decision. In addition to helping graduate students make choices, caring faculty mentors demonstrate support for the decisions graduate students make on their own and affirm that no obstacle is insurmountable. Through such acts of validation, these professors help to cultivate graduate students as productive and confident researchers. For this reason, among others, Carolini, Hammond, and Trumper have been honored as Committed to Caring (C2C). Gabriella Carolini: making extraordinary the norm One student extols Gabriella Carolini as “the single most-defining influence in my MIT experience to date.” This sentiment is far from an outlier. Associate professor in the Department of Urban Studies and Planning (DUSP), Carolini’s research focuses on the planning, implementation, and administration of infrastructure systems in vulnerable urban and peri-urban communities. Her work, largely based in sub-Saharan Africa and Latin America, examines how financing, project partnering practices, and project evaluations impact distributive justice in urban development, particularly with regard to water and sanitation services as well as community health. Carolini builds community in the DUSP International Development Group by sharing her own experiences with her students, and in doing so “fosters a friendly and inclusive work environment” (a C2C mentoring guidepost). One nominator remarks that Carolini’s “candor in sharing her experiences as a tenure-track female academic” has helped inform the student’s own career decisions. Another nomination describes Carolini as honest about the ups and downs of academia, including the tenure process, the management of research projects and publications, and family-work life balances. When an international graduate student and his wife were expecting a baby, Carolini — pregnant herself at the time — went out of her way to demonstrate her concern and personally provide support for them. Her actions made them feel like they “had a community to rely on at MIT.” This was especially appreciated in light of the current political climate, in which many international students have felt destabilized and socially isolated. In considering general obstacles for her students, Carolini explains that “choice” is perhaps their toughest hurdle. “Making a decision about what specific questions or issues to commit to is a perennially difficult challenge our students face. They are talented and able — so we understand why. But we all still have to choose to move forward.” Faculty members, Carolini says, need to help students find and commit to their research and professional practice aspirations. It helps when faculty members demonstrate excitement about students’ work and help them to develop a plan towards achievement. To Carolini, this means recognizing both the strengths and weaknesses of a project, and addressing the latter “without losing sight of the value of their work.” Paula Hammond: individual and departmental advancement Paula Hammond excels at actively listening to her students, helping her students move successfully through their programs, and improving departmental systems to encourage diversity and inclusion. Hammond is head of the Department of Chemical Engineering and the David H. Koch (1962) Professor in Engineering at MIT. The Hammond Research Group at the MIT Koch Institute for Integrative Cancer Research focuses on the self-assembly of polymeric nanomaterials, including the use of electrostatics and other complementary interactions to generate multifunctional materials with highly controlled architecture. Her work has a number of electro-optical, electro-mechanical, and biological applications. In cancer research, for example, the Hammond lab works on the generation of polymer-based films and nanoparticles for drug delivery. Whether students are struggling with qualifying exams or unproductive data, Hammond assures her students that their obstacles can be overcome. “I let them know that there is a path to a PhD here and we’re going to find it.” This sets students at ease, and alleviates much of the stress. Hammond says that all these challenges are “part of the journey, and everyone experiences them — we just need to get out on the other side of it.” Hammond provides channels for students to express their difficulties (a C2C mentoring guidepost). She says that during a recent departmental retreat, “we learned that there is a gendered experience for women in our department.” As department head, Hammond has set out to learn how this climate can shift into “one in which women feel equally recognized and equally able as soon as they walk in the door.” According to C2C nominators, Hammond makes every effort to empower students from diverse backgrounds. This is perhaps best illustrated by her ongoing commitment to the MIT Summer Research Program (MSRP). By welcoming MSRP interns into her lab, Hammond gives her own graduate students valuable experiences mentoring potential future labmates. “I feel a responsibility as a woman, and as an underrepresented minority to be visible to others,” Hammond says. “I want to say, ‘There are people who look like you and have similar backgrounds to you doing this work.’” David Trumper: champion of balance David Trumper is a reliable guide for his students, who say that he invariably “encourages us to do what we are passionate about and supports us in any way he can.” As professor of mechanical engineering, Trumper’s research investigates the design of precision mechatronic systems, magnetic levitation for nanometer-scale motion control, and novel actuation and sensing devices. As director of the Precision Motion Control Laboratory, Trumper works with his group to conduct research in the design of electromechanical systems for precise positioning applications, such as semiconductor photolithography, high-speed machine tools, and scanned probe microscopy. According to his students, Trumper takes the time to sit down and listen, not just as a professor or advisor, but also as a friend. One nominator wrote, “we talked about the loss of my dad, about the presidential election, about life in general, and about life at MIT. It was the most encouraging and helpful experience that I have ever had with an MIT professor.” In addition to promoting a healthy work/life balance (a C2C mentoring guidepost), Trumper’s students say he constantly stresses balance of every kind, for example “between creative thinking and precise detailing, between analysis and learning from prototyping, and between going fast and slowing down.” Trumper demonstrates to his students that balance is important, and ultimately more effective for everyone. He encourages his students to make time for creative efforts and physical activities. Trumper leads by example, spending time photographing, hiking, and rock climbing. “I also encourage my students to be more broadly educated by reading books that have nothing to do with their technical field,” he relays. “Reading for pleasure should be a lifelong habit.” Along with balance comes perspective, and Trumper is always offering a positive outlook. One student recalls that after making a careless calculation, his experiment failed. Trumper “did not criticize me once about the mistake, and instead, he simply said: ‘Now you will never forget this, which is great’.” More on Committed to Caring  The Committed to Caring (C2C) program is an initiative of the Office of Graduate Education and contributes to its mission of making graduate education at MIT “empowering, exciting, holistic, and transformative.” C2C invites graduate students from across MIT’s campus to nominate professors whom they believe to be outstanding mentors. Selection criteria for the honor include the scope and reach of advisor impact on the experience of graduate students, excellence in scholarship, and demonstrated commitment to diversity and inclusion. By recognizing the human element of graduate education, C2C seeks to encourage excellent advising and mentorship across MIT’s campus.  "
MIT News,"Professor Emeritus Woodie Flowers, innovator in design and engineering education, dies at 75",Robotics,2019-10-14,-,http://news.mit.edu/2019/Professor-Emeritus-Woodie-Flowers-dies-75-1014,"  Woodie Flowers SM ’68, MEng ’71, PhD ’73, the Pappalardo Professor Emeritus of Mechanical Engineering, passed away on Oct. 11 at the age of 75. Flowers’ passion for design and his infectious kindness have impacted countless engineering students across the world. Flowers was instrumental in shaping MIT’s hands-on approach to engineering design education, first developing teaching methods and learning opportunities that culminated in a design competition for class 2.70, now called 2.007 (Design and Manufacturing I). This annual MIT event, which has now been held for nearly five decades, has impacted generations of students and has been emulated at universities around the world. Flowers expanded this concept to high school and elementary school students, working to help found the world-wide FIRST Robotics Competition, which has introduced millions of children to science and engineering. Born in 1943, Flowers was reared in Jena, Louisiana. He became interested in mechanical engineering and design at a young age thanks in large part to his mother and his father, who was a welder with a penchant for tinkering and building. Growing up, Flowers also expressed a love of nature and traveling. When he wasn’t working on cars or building rockets as a teenager, he was camping with his family in Louisiana or collecting butterflies. This interest in nature led to an award-winning science fair project on the impact the environment has on Lepidoptera. Flowers’ passion for both building and nature also helped him earn the rank of Eagle Scout. Flowers received his bachelor’s degree in engineering from Louisiana Tech University in 1966. After graduating, he spent a summer as an engineering trainee for the Humble Oil Company before enrolling in MIT for graduate school. He received his master’s of science in mechanical engineering in 1968 and an engineer’s degree in 1971. Two years later he earned his doctoral degree under the supervision of the late Professor Robert Mann. For his thesis, Flowers designed a “man-interactive simulator system” for the development of prosthetics for above-knee amputees. He would continue to design above-knee prosthetics throughout his career. As Flower’s academic career progressed, his wife Margaret acted as a partner in everything he did. Early in their marriage, when Flowers was just starting out at MIT, she worked to support their family financially. Later in life, she left her own career to partner with Flowers on his work in FIRST. After earning his PhD, Flowers joined MIT’s faculty as assistant professor of mechanical engineering. Within his first year he was teaching what was then known as 2.70, now called 2.007. Under Flowers’ leadership, the class evolved into a hands-on experience for undergraduate students, culminating in a final robot competition. “From the beginning 2.007/2.70 was about building a device to accomplish a task,” Flowers explained in a 2015 video. Students were given an assortment of materials to design and build their devices. In the 1970s these materials included tongue depressors and rubber bands, but over the years the competition has gone on to include 3D-printed parts and computer chips. Despite the increased sophistication, according to Flowers the core of the course remained unchanged. “Some of the stuff that stayed the same is the wonderful way you compete like crazy but help each other out,” he said. Flowers would coin the phrase “gracious professionalism” to describe this idea of being kind and respecting and valuing others, even in the heat of competition. PBS highlighted Flowers’ innovative educational approach to class 2.007 in a 1981 documentary “Discover: The World of Science.” The network continued to cover the 2.007 robotics competition throughout the 1980s, and nearly a decade later, Flowers hosted the popular PBS series “Scientific American Frontiers,” from 1990-1993. One of the program’s objectives was to get people interested in science and engineering. He was awarded a regional Emmy Award for his work on the series. At the same time, Flowers helped develop a new program to inspire young people that built upon the competition he developed for 2.007. He collaborated with Dean Kamen, founder of FIRST (For Inspiration and Recognition of Science and Technology), to develop a robotics competition for high school students. In 1992, the inaugural FIRST Robotics Competition was held, giving high school students from around the world an opportunity to design and build their own robots. Over the past three decades, FIRST robotics has grown into a global movement serving 660,000 students from over 100 countries each year. It provides scholarship opportunities totaling over $80 million available to FIRST high school students. Flowers’ mantra of “gracious professionalism” remains at FIRST’s core. In 1996, William P. Murphy, Jr. founded the annual Woodie Flowers Award within FIRST to celebrate communication in engineering and design. The award “recognizes an individual who has done an outstanding job of motivation through communication while also challenging the students to be clear and succinct in recognizing the value of communication.” While working on FIRST, Flowers continued to have impact on mechanical engineering education, its future directions, and engineers’ professional role in society, in addition to envisioning how the use of digital resources could enhance residential learning. At MIT, he served as head for the systems and design division in the Department of Mechanical Engineering in the early 1990s and was named Pappalardo Professor of Mechanical Engineering in 1994.  While Flowers retired in 2007, he remained an active member of the MIT community as professor emeritus up until his death. Flowers mentored countless engineering students during his 35 years on the MIT faculty. He served as undergraduate and master’s thesis advisor for Megan Smith ’86, SM ’88, former chief technology officer of the United States, as well as doctoral advisor to David Wallace SM ’91, PhD ’95, Ely Sachs ’76, SM ’76, PhD ’83, and Alexander Slocum ’82, SM ’83, PhD ’85, all of whom are professors of mechanical engineering at MIT. Flowers has had a lasting impact on the generations of mechanical engineering students he taught. From encouraging students to embrace ambiguity to pulling out a massive dictionary in the middle of class to help students find a precise word to articulate their point, his role in shaping students’ lives went far beyond the tenants of design and engineering. Many of his students who have gone on to be educators themselves have implemented his educational ethos in their own classrooms and labs.   Throughout his career, Flowers received numerous awards and accolades for his vast contributions to engineering education. The American Society of Mechanical Engineers honored him with both the Ruth and Joel Spira Outstanding Design Educator Award and the Edwin F. Church Medal. Flowers received the J.P. Den Hartog Distinguished Educator Award, was a MacVicar Fellow, and was elected to the National Academy of Engineering. He also served as a distinguished partner and a member of the President's Council at Olin College of Engineering. Flowers is survived by his beloved wife Margaret Flowers of Weston, Massachusetts, his sister, Kay Wells of St. Augustine, Florida, his niece Catherine Calabria, also of St. Augustine, his nephew, David Morrison of Arlington, Virginia, as well as generations of grateful and adoring students. Memorial donations to FIRST and memories of Flowers may be delivered via this website or mailed to FIRST c/o Director Dia Stolnitz, 200 Bedford Street, Manchester, New Hampshire, 03101. This article will be updated with information about memorial services as it becomes available. "
MIT News,Robots help patients manage chronic illness at home,Robotics,2019-10-10,-,http://news.mit.edu/2019/catalia-health-mabu-robot-1011,"  The Mabu robot, with its small yellow body and friendly expression, serves, literally, as the face of the care management startup Catalia Health. The most innovative part of the company’s solution, however, lies behind Mabu’s large blue eyes. Catalia Health’s software incorporates expertise in psychology, artificial intelligence, and medical treatment plans to help patients manage their chronic conditions. The result is a sophisticated robot companion that uses daily conversations to give patients tips, medication reminders, and information on their condition while relaying relevant data to care providers. The information exchange can also take place on patients’ mobile phones. “Ultimately, what we’re building are care management programs to help patients in particular disease states,” says Catalia Health founder and CEO Cory Kidd SM ’03, PhD ’08. “A lot of that is getting information back to the people providing care. We’re helping them scale up their efforts to interact with every patient more frequently.” Heart failure patients first brought Mabu into their homes about a year and a half ago as part of a partnership with the health care provider Kaiser Permanente, who pays for the service. Since then, Catalia Health has also partnered with health care systems and pharmaceutical companies to help patients dealing with conditions including rheumatoid arthritis and kidney cancer. Treatment plans for chronic diseases can be challenging for patients to manage consistently, and many people don’t follow them as prescribed. Kidd says Mabu’s daily conversations help not only patients, but also human care givers as they make treatment decisions using data collected by their robot counterpart. Robotics for change Kidd was a student and faculty member at Georgia Tech before coming to MIT for his master’s degree in 2001. His work focused on addressing problems in health care caused by an aging population and an increase in the number of people managing chronic diseases. “The way we deliver health care doesn’t scale to the needs we have, so I was looking for technologies that might help with that,” Kidd says. Many studies have found that communicating with someone in person, as opposed to over the phone or online, makes that person appear more trustworthy, engaging, and likeable. At MIT, Kidd conducted studies aimed at understanding if those findings translated to robots. “What I found was when we used an interactive robot that you could look in the eye and share the same physical space with, you got the same psychological effects as face-to-face interaction,” Kidd says. As part of his PhD in the Media Lab’s Media Arts and Sciences program, Kidd tested that finding in a randomized, controlled trial with patients in a diabetes and weight management program at the Boston University Medical Center. A portion of the patients were given a robotic weight-loss coach to take home, while another group used a computer running the same software. The tabletop robot conducted regular check ups and offered tips on maintaining a healthy diet and lifestyle. Patients who received the robot were much more likely to stick with the weight loss program. Upon finishing his PhD in 2007, Kidd immediately sought to apply his research by starting the company Intuitive Automata to help people manage their diabetes using robot coaches. Even as he pursued the idea, though, Kidd says he knew it was too early to be introducing such sophisticated technology to a health care industry that, at the time, was still adjusting to electronic health records. Intuitive Automata ultimately wasn’t a major commercial success, but it did help Kidd understand the health care sector at a much deeper level as he worked to sell the diabetes and weight management programs to providers, pharmaceutical companies, insurers, and patients. “I was able to build a big network across the industry and understand how these people think about challenges in health care,” Kidd says. “It let me see how different entities think about how they fit in the health care ecosystem.” Since then, Kidd has watched the costs associated with robotics and computing plummet. Many people have also enthusiastically adopted computer assistance like Amazon’s Alexa and Apple’s Siri. Finally, Kidd says members of the health care industry have developed an appreciation for technology’s potential to complement traditional methods of care. “The common ways [care is delivered] on the provider side is by bringing patients to the doctor’s office or hospital,” Kidd explains. “Then on the pharma side, it’s call center-based. In the middle of these is the home visitation model. They’re all very human powered. If you want to help twice as many patients, you hire twice as many people. There’s no way around that.” In the summer of 2014, he founded Catalia Health to help patients with chronic conditions at scale. “It’s very exciting because I’ve seen how well this can work with patients,” Kidd says of the company’s potential. “The biggest challenge with the early studies was that, in the end, the patients didn’t want to give the robots back. From my perspective, that’s one of the things that shows this really does work.” Mabu makes friends Catalia Health uses artificial intelligence to help Mabu learn about each patient through daily conversations, which vary in length depending on the patient’s answers. “A lot of conversations start off with ‘How are you feeling?’ similar to what a doctor or nurse might ask,” Kidd explains. “From there, it might go off in many directions. There are a few things doctors or nurses would ask if they could talk to these patients every day.” For example, Mabu would ask heart failure patients how they are feeling, if they have shortness of breath, and about their weight. “Based on patients’ answers, Mabu might say ‘You might want to call your doctor,’ or ‘I’ll send them this information,’ or ‘Let’s check in tomorrow,’” Kidd says. Last year, Catalia Health announced a collaboration with the American Heart Association that has allowed Mabu to deliver the association’s guidelines for patients living with heart failure. “A patient might say ‘I’m feeling terrible today’ and Mabu might ask ‘Is it one of these symptoms a lot of people with your condition deal with?’ We’re trying to get down to whether it’s the disease or the drug. When that happens, we do two things: Mabu has a lot of information about problems a patient might be dealing with, so she’s able to give quick feedback. Simultaneously, she’s sending that information to a clinician — a doctor, nurse, or pharmacists — whoever’s providing care.” In addition to health care providers, Catalia also partners with pharmaceutical companies. In each case, patients pay nothing out of pocket for their robot companions. Although the data Catalia Health sends pharmaceutical companies is completely anonymized, it can help them follow their treatment’s effects on patients in real time and better understand the patient experience. Details about many of Catalia Health’s partnerships have not been disclosed, but the company did announce a collaboration with Pfizer last month to test the impact of Mabu on patient treatment plans. Over the next year, Kidd hopes to add to the company’s list of partnerships and help patients dealing a wider swath of diseases. Regardless of how fast Catalia Health scales, he says the service it provides will not diminish as Mabu brings its trademark attentiveness and growing knowledge base to every conversation. “In a clinical setting, if we talk about a doctor with good bedside manner, we don’t mean that he or she has more clinical knowledge than the next person, we simply mean they’re better at connecting with patients,” Kidd says. “I’ve looked at the psychology behind that — what does it mean to be able to do that? — and turned that into the algorithms we use to help create conversations with patients.” "
MIT News,An interdisciplinary approach to accelerating human-machine collaboration,Robotics,2019-10-02,-,http://news.mit.edu/2019/humatics-david-mindell-machine-1002,"  David Mindell has spent his career defying traditional distinctions between disciplines. His work has explored the ways humans interact with machines, drive innovation, and maintain societal well-being as technology transforms our economy. And, Mindell says, he couldn’t have done it anywhere but MIT. He joined MIT’s faculty 23 years ago after completing his PhD in the Program in Science, Technology, and Society, and he currently holds a dual appointment in engineering and humanities as the Frances and David Dibner Professor of the History of Engineering and Manufacturing in the School of Humanities, Arts, and Social Sciences and professor of aeronautics and astronautics. Mindell’s experience combining fields of study has shaped his ideas about the relationship between humans and machines. Those ideas are what led him to found Humatics — a startup named from the merger of “human” and “robotics.” Humatics is trying to change the way humans work alongside machines, by enabling location tracking and navigation indoors, underground, and in other areas where technologies like GPS are limited. It accomplishes this by using radio frequencies to track things at the millimeter scale — unlocking what Mindell calls microlocation technology. The company’s solution is already being used in places like shipping ports and factories, where humans work alongside cranes, industrial tools, automated guided vehicles (AGVs), and other machines. These businesses often lack consistent location data for their machines and are forced to adopt inflexible routes for their mobile robots. “One of the holy grails is to have humans and robots share the same space and collaborate, and we’re enabling mobile robots to work in human environments safely and on a large scale,” Mindell says. “Safety is a critical first form of collaboration, but beyond that, we’re just beginning to learn how to work [in settings] where robots and people are exquisitely aware of where they are.” A company decades in the making MIT has a long history of transcending research fields to improve our understanding of the world. Take, for example, Norbert Wiener, who served on MIT’s faculty in the Department of Mathematics between 1919 and his death in 1964. Wiener is credited with formalizing the field of cybernetics, which is an approach to understanding feedback systems he defined as “the scientific study of control and communication in the animal and the machine."" Cybernetics can be applied to mechanical, biological, cognitive, and social systems, among others, and it sparked a frenzy of interdisciplinary study and scientific collaboration. In 2002, Mindell wrote a book exploring the history of cybernetics before Wiener and its emergence at the intersection of a range of disciplines during World War II. It is one of several books Mindell has written that deal with interdisciplinary responses to complex problems, particularly in extreme environments like lunar landings and the deep sea. The interdisciplinary perspective Mindell forged at MIT has helped him identify the limitations of technology that prevent machines and humans from working together seamlessly. One particular shortcoming that Mindell has thought about for years is the lack of precise location data in places like warehouses, subway systems, and shipping ports. “In five years, we’ll look back at 2019 and say, ‘I can’t believe we didn’t know where anything was,’” Mindell says. “We’ve got so much data floating around, but the link between the actual physical world we all inhabit and move around in and the digital world that’s exploding is really still very poor.” In 2014, Mindell partnered with Humatics co-founder Gary Cohen, who has worked as an intellectual property strategist for biotech companies in the Kendall Square area, to solve the problem. In the beginning of 2015, Mindell collaborated with Lincoln Laboratory alumnus and radar expert Greg Charvat; the two built a prototype navigation system and started the company two weeks later. Charvat became Humatics’ CTO and first employee. “It was clear there was about to be this huge flowering of robotics and autonomous systems and AI, and I thought the things we learned in extreme environments, notably under sea and in aviation, had an enormous amount of application to industrial environments,” Mindell says. “The company is about bringing insights from years of experience with remote and autonomous systems in extreme environments into transit, logistics, e-commerce, and manufacturing.” Bringing microlocation to industry Factories, ports, and other locations where GPS data is unworkable or insufficient adopt a variety of solutions to meet their tracking and navigation needs. But each workaround has its drawbacks. RFID and Bluetooth technologies, for instance, can track assets but have short ranges and are expensive to deploy across large areas. Cameras and sensing methods like LIDAR can be used to help machines see their environment, but they struggle with things like rain and different lighting conditions. Floor tape embedded with wires or magnets is also often used to guide machines through fixed routes, but it isn’t well-suited for today’s increasingly dynamic warehouses and production lines. Humatics has focused on making the capabilities of its microlocation location system as easy to leverage as possible. The location and tracking data it collects can be integrated into whatever warehouse management system or “internet of things” (IoT) platforms customers are already using. Its radio frequency beacons have a range of up to 500 meters and, when installed as part of a constellation, can pinpoint three dimensional locations to within 2 centimeters, creating a virtual grid of the surrounding environment. The beacons can be combined with an onboard navigation hub that helps mobile robots move around dynamic environments. Humatics’ system also gathers location data from multiple points at once, monitoring the speed of a forklift, helping a crane operator place a shipping crate, and guiding a robot around obstacles simultaneously. The data Humatics collects don’t just help customers improve their processes; they can also transform the way workers and machines share space and work together. Indeed, with a new chip just emerging from its labs, Mindell says Humatics is moving industries such as manufacturing and logistics into “the world of ubiquitous, millimeter-accurate positioning.” It’s all possible because of the company’s holistic approach to the age-old problem of human-machine interaction. “Humatics is an example of what can happen when we think about technology in a unique, broader context,” Mindell says. “It’s an example of what MIT can accomplish when it pays serious attention to these two ways [from humanities and engineering] of looking at the world.” "
MIT News,This flat structure morphs into shape of a human face when temperature changes,Robotics,2019-09-30,-,http://news.mit.edu/2019/mesh-structure-shape-temperature-changes-0930,"  Researchers at MIT and elsewhere have designed 3-D printed mesh-like structures that morph from flat layers into predetermined shapes, in response to changes in ambient temperature. The new structures can transform into configurations that are more complex than what other shape-shifting materials and structures can achieve. As a demonstration, the researchers printed a flat mesh that, when exposed to a certain temperature difference, deforms into the shape of a human face. They also designed a mesh embedded with conductive liquid metal, that curves into a dome to form an active antenna, the resonance frequency of which changes as it deforms. The team’s new design method can be used to determine the specific pattern of flat mesh structures to print, given the material’s properties, in order to make the structure transform into a desired shape. The researchers say that down the road, their technique may be used to design deployable structures, such as tents or coverings that automatically unfurl and inflate in response to changes in temperature or other ambient conditions. Such complex, shape-shifting structures could also be of use as stents or scaffolds for artificial tissue, or as deformable lenses in telescopes. Wim van Rees, assistant professor of mechanical engineering at MIT, also sees applications in soft robotics. “I’d like to see this incorporated in, for example, a robotic jellyfish that changes shape to swim as we put it in water,” says van Rees. “If you could use this as an actuator, like an artificial muscle, the actuator could be any arbitrary shape that transforms into another arbitrary shape. Then you’re entering an entirely new design space in soft robotics.” Van Rees and his colleagues are publishing their results this week in the Proceedings of the National Academy of Sciences. His co-authors are J. William Boley of Boston University; Ryan Truby, Arda Kotikian, Jennifer Lewis, and L. Mahadevan of Harvard University; Charles Lissandrello of Draper Laboratory; and Mark Horenstein of Boston University. Gift wrap’s limit Two years ago, van Rees came up with a theoretical design for how to transform a thin flat sheet into a complex shape such as a human face. Until then, researchers in the field of 4-D materials — materials designed to deform over time — had developed ways for certain materials to change, or morph, but only into relatively simple structures. “My goal was to start with a complex 3-D shape that we want to achieve, like a human face, and then ask, ‘How do we program a material so it gets there?’” van Rees says. “That’s a problem of inverse design.” He came up with a formula to compute the expansion and contraction that regions of a bilayer material sheet would have to achieve in order to reach a desired shape, and developed a code to simulate this in a theoretical material. He then put the formula to work, and visualized how the method could transform a flat, continuous disc into a complex human face. But he and his collaborators quickly found that the method wouldn’t apply to most physical materials, at least if they were trying to work with continuous sheets. While van Rees used a continuous sheet for his simulations, it was of an idealized material, with no physical constraints on the amount of expansion and contraction it could achieve. Most materials, in contrast, have very limited growth capabilities. This limitation has profound consequences on a property known as double curvature, meaning a surface that can curve simultaneously in two perpendicular directions — an effect that is described in an almost 200-year-old theorem by Carl Friedrich Gauss called the Theorema Egregium, Latin for “Remarkable Theorem.” If you’ve ever tried to gift wrap a soccer ball, you’ve experienced this concept in practice: To transform paper, which has no curvature at all, to the shape of a ball, which has positive double curvature, you have to crease and crumple the paper at the sides and bottom to completely wrap the ball. In other words, for the paper sheet to adapt to a shape with double curvature, it would have to stretch or contract, or both, in the necessary places to wrap a ball uniformly. To impart double curvature to a shape-shifting sheet, the researchers switched the basis of the structure from a continuous sheet to a lattice, or mesh. The idea was twofold: first, a temperature-induced bending of the lattice’s ribs would result in much larger expansions and contractions of the mesh nodes, than could be achieved in a continuous sheet. Second, the voids in the lattice can easily accommodate large changes in surface area when the ribs are designed to grow at different rates across the sheet. The researchers also designed each individual rib of the lattice to bend by a predetermined degree in order to create the shape of, say, a nose rather than an eye-socket. For each rib, they incorporated four skinnier ribs, arranging two to line up atop the other two. All four miniribs were made from carefully selected variations of the same base material, to calibrate the required different responses to temperature. When the four miniribs were bonded together in the printing process to form one larger rib, the rib as a whole could curve due to the difference in temperature response between the materials of the smaller ribs: If one material is more responsive to temperature, it may prefer to elongate. But because it is bonded to a less responsive rib, which resists the elongation, the whole rib will curve instead. The researchers can play with the arrangement of the four ribs to “preprogram” whether the rib as a whole curves up to form part of a nose, or dips down as part of an eye socket. Shapes unlocked To fabricate a lattice that changes into the shape of a human face, the researchers started with a 3-D image of a face — to be specific, the face of Gauss, whose principles of geometry underly much of the team’s approach. From this image, they created a map of the distances a flat surface would require to rise up or dip down to conform to the shape of the face. Van Rees then devised an algorithm to translate these distances into a lattice with a specific pattern of ribs, and ratios of miniribs within each rib. The team printed the lattice from PDMS, a common rubbery material which naturally expands when exposed to an increase in temperature. They adjusted the material’s temperature responsiveness by infusing one solution of it with glass fibers, making it physically stiffer and more resistant to a change in temperature. After printing lattice patterns of the material, they cured the lattice in a 250-degree-Celsius oven, then took it out and placed it in a saltwater bath, where it cooled to room temperature and morphed into the shape of a human face.   Courtesy of the researchers The team also printed a latticed disc made from ribs embedded with a liquid metal ink — an antenna of sorts, that changed its resonant frequency as the lattice transformed into a dome. Van Rees and his colleagues are currently investigating ways to apply the design of complex shape-shifting to stiffer materials, for sturdier applications, such as temperature-responsive tents and self-propelling fins and wings. This research was supported, in part, by the National Science Foundation, and Draper Laboratory. "
MIT News,An immersive experience in industry ,Robotics,2019-09-19,-,http://news.mit.edu/2019/immersive-engineering-experience-industry-0919,"  This summer, four mechanical engineering graduate students had the opportunity to gain hands-on experience working in industry. Through the recently launched Industry Immersion Project Program (I2P), students were paired with a company and tasked with tackling a short-term project. Projects in this inaugural year for the program came from a diverse range of industries, including manufacturing, robotics, and aerospace engineering. A flagship program of the MechE Alliance, the I2P Program matches students with a company and project that best fits within their own academic experience at MIT. Projects are designed to be short term, lasting three to six months. Building upon programs such as the Master of Engineering in Advanced Manufacturing and Design and Leaders for Global Operations, which foster collaborations between students and the manufacturing industry, the I2P Program offers graduate students real-world experiences across industries. “For some students, this could be their first experience working in industry before graduating,” says Brian W. Anthony, program faculty director of the I2P Program. “Having that industry experience arms them with knowledge to help make career choices, may inform their further research, and provides skills they will utilize throughout their careers — whether they end up working in academia or industry.” Throughout the course of the projects, students are supported by both a supervisor at the company they’re working for and an academic supervisor from MIT’s mechanical engineering faculty. They also produce a report of their experience and receive academic credit for their industry projects and are enrolled in the class 2.992 (Professional Industry Immersion Project). “It’s been great hearing just how rich the experience has been from the students who participated this summer,” adds Theresa Werth, program manager for the MechE Alliance. “Not only have they spent the summer working on a project that’s relevant to their own research or thesis, they have honed some of the softer skills of professional development.” The four students participating in this year’s I2P Program have shared highlights and takeaways from their experiences: Sara Nagelberg — 3M A PhD candidate working with Associate Professor Mathias Kolle in the Bio-Inspired Photonic Engineering research group, Sara Nagelberg studies optical engineering. Through the I2P Program, this summer she worked at 3M on a project that seeks to automate surface finish analysis in manufacturing by understanding visual perception. While much of manufacturing involves automation, automating quality inspection for the surface finish on appliances or cars offers some technical challenges. The project Nagelberg worked on at 3M hopes to define what makes a surface ""good,"" then develop algorithms so that a computer can determine whether a surface finish is good quality or flawed. “The long-term goal of the project is to automate surface-quality inspection,” Nagelberg explains. She and her team identified parameters that could be used to judge the visual appearance of surfaces — things like color, glossiness, shape, and texture. “By working on this project, I learned about a variety of instruments and metrics that can be used to quantify visual surface finish parameters,” she adds. In addition to gaining experience on an interdisciplinary team at 3M, Nagelberg learned about computer vision, machine learning, and how to relate human perception to measurable parameters. Katie Hahm — Amazon Robotics This summer was one of transition for Katie Hahm. Having graduated with her master’s degree in June, Hahm is now a PhD candidate working in the Device Realization Lab with program director Anthony. As a master’s student, Hahm previously worked with Professor Harry Asada on designing robotic limbs to help manufacturing workers maintain positions for extended periods of time. Through the I2P Program, Hahm worked on a project at Amazon Robotics to improve efficiencies in the robotic process. “Working on this project was a great academic experience,” says Hahm. “I gained insights into the many facets and complexities of robotics.” Hahm also received a ground truth in what it’s like to work at a company like Amazon. She visited a local fulfillment center to gain a deeper understanding of their operations and visited Seattle to attend a company conference. At the conference, she and her fellow interns met with company leadership and teams from other Amazon sectors. One of the biggest takeaways from her experience at Amazon, according to Hahm, was how to approach research projects moving forward. “I learned not only valuable information from working with other professionals, but also the skills and approaches to asking more effective questions for research-oriented work,” she adds. Sai Nithin Reddy Kantareddy — Amazon Robotics A junior PhD candidate, much of Sai Nithin Reddy Kantareddy’s work involves using radio frequency identification (RFID) tags to sense activity and gather data about the surrounding environment. These RFID tags can then be used to connect objects to the internet of things. “Going into this summer, I knew I wanted to work on something related to sensors because of my research interest in environmental sensing,” explains Kantareddy. Through the I2P Program, Kantareddy was assigned to a project about material identification and sensing in robotics at Amazon Robotics. “Material identification for robotic applications really aligns with my own research interests,” he adds. While at Amazon Robotics, he gained hands-on experience working with sensors, cameras, and robots. He also built machine learning models on experimental data. While his background isn’t in robotics research, Kantareddy quickly learned about how robots are designed and what some of the challenges are in field implementation and warehouse automation. In addition to this in-depth technical knowledge, he also gained firsthand experience working in a team setting. “I enjoyed being part of a very resourceful and talented R&D team,” he recalls.  “I hope to take back these real-world insights and technical learnings and put them to practice in my PhD work.” Abhishek Patkar — Systems Technology Inc. A sophomore master’s student, Abhishek Patkar works in the flight controls group the Active Adaptive Control Laboratory, led by in Senior Research Scientist Anuradha Annaswamy. Working at Systems Technology Inc. (STI) was a natural fit. Much of STI’s work focuses on aerospace engineering. For his internship, Patkar was matched with Aditya Kotikalpudi, a senior research engineer at STI and the principal investigator for NASA’s project entitled Performance Adaptive Aeroelastic Wing. “I primarily worked on system identification and model parameter update for an aeroelastic vehicle,” says Patkar. While his internship was based in Los Angeles, California, Patkar had the opportunity to visit the University of Minnesota and witness the actual process of flight testing. He worked with the real data taken from these flight tests. Patkar also used STI software to identify aeroelastic mode shapes and obtain transfer function estimates from control surfaces to measured quantities like center body pitch rate.  “Through this internship, I was able to learn a lot about aircraft dynamics, aeroelasticity, and the process of performing system identification on an aircraft,” Patkar adds. He expects to use this knowledge back in the flight controls group in the Active Adaptive Control Laboratory. "
MIT News,Engineers develop multimaterial fiber “ink” for 3-D-printed devices,Electronics,2019-09-11,-,http://news.mit.edu/2019/fiber-3-d-print-electronics-0912,"  A new method developed by MIT researchers uses standard 3-D printers to produce functioning devices with the electronics already embedded inside. The devices are made of fibers containing multiple interconnected materials, which can light up, sense their surroundings, store energy, or perform other actions. The new 3-D printing method is described in the journal Nature Communication, in a paper by MIT doctoral student Gabriel Loke, professors John Joannopoulos and Yoel Fink, and four others at MIT and elsewhere. The system makes use of conventional 3-D printers outfitted with a special nozzle and a new kind of filament to replace the usual single-material polymer filament, which typically gets fully melted before it’s extruded from the printer’s nozzle. The researchers’ new filament has a complex internal structure made up of different materials arranged in a precise configuration, and is surrounded by polymer cladding on the outside. In the new printer, the nozzle operates at a lower temperature and pulls the filament through faster conventional printers do, so that only its outer layer gets partially molten. The interior stays cool and solid, with its embedded electronic functions unaffected. In this way, the surface is melted just enough to make it adhere solidly to adjacent filaments during the printing process, to produce a sturdy 3-D structure. The internal components in the filament include metal wires that serve as conductors, semiconductors that can be used to control active functions, and polymer insulators to prevent wires from contacting each other. As a demonstration, the team printed a wing for a model airplane, using filaments that contained both light-emitting and light-detecting electronics. These components could potentially reveal the formation of any microscopic cracks that might develop. While the filaments used in the model wing contained eight different materials, Loke says that in principle they could contain even more. Until this work, he says, “a printer capable of depositing metals, semiconductors, and polymers in a single platform still did not exist, because printing each of these materials requires different hardware and techniques.” This method is up to three times faster than any other current approach to fabricating 3-D devices, Loke says, and as with all 3-D printers, offers much more flexibility regarding the kinds of forms that can be produced than typical manufacturing methods do. “Unique to 3-D printing, this approach is able to construct devices of any freeform shapes, which are not achievable by any other methods thus far,” he says. The method makes use of thermally drawn fibers that contain a variety of different materials embedded within them, a process that Fink and his collaborators have been perfecting for two decades. They have created an array of fibers that have electronic components within them, making the fibers able to carry out a variety of functions. For example, for communications applications, flashing lights can transmit data that is then picked up by other fibers containing light sensors. This approach has for the first time produced fibers, and fabrics woven from them, that have these functions built in. Now, this new process makes this whole family of fibers available as the raw material for producing functional 3-D devices that can sense, communicate, or store energy, among other actions. To make the fibers themselves, the different materials are initially assembled into a larger-scale version called a preform, which is then heated and drawn in a furnace to produce a very narrow fiber that contains all those materials, in their same exact relative positions but greatly reduced in size. The method could potentially be developed further to produce a variety of different kinds of devices, especially for applications where the ability to precisely customize each device is essential. One such area is for biomedical devices, where matching the device to the patient’s own body can be important, says Fink, who is a professor of materials science as well as of electrical engineering and computer science and the CEO of the nonprofit Advanced Functional Fabrics of America, and associate director of the Research Laboratory of Electronics. For example, prosthetic limbs might someday be printed using this method, not only matching the precise dimensions and contours of the patient’s limb, but with all the electronics to monitor and control the limb embedded in place. Over the years, the group has developed a wide array of fibers containing different materials and functionalities. Loke says virtually all of these can be adapted for the new 3-D-printing technique, making it possible to print objects with a wide variety of different combinations of materials and functions. The device makes use of a standard type of 3-D printer known as a fused deposition modeling (FDM) printer, which is already found in many labs, offices, and even homes. One application that may be possible in the future would be to print materials for biomedical implants that would provide a scaffolding for the growth of new cells to replace a damaged organ, and include within it sensors to monitor the progress of that growth. The new method could also be useful for prototyping of devices — already a major application for 3-D printing, but in this case the prototypes would have actual functionality, rather than being static models. The research team included MIT graduate student Rodger Yuan; former MIT graduate student Michael Rein, who now works at AFFOA; postdoc Tural Khudiyev, and undergraduate student Yash Jain at Stony Brook University in New York. The work was partly supported by the National Science Foundation, the U.S. Army Research Laboratory and the U.S. Army Research Office through the Institute for Soldier Nanotechnologies. "
MIT News,Study reveals how mucus tames microbes,Research,2019-10-14,-,http://news.mit.edu/2019/how-mucus-tames-microbes-1014,"  More than 200 square meters of our bodies — including the digestive tract, lungs, and urinary tract — are lined with mucus. In recent years, scientists have found some evidence that mucus is not just a physical barrier that traps bacteria and viruses, but it can also disarm pathogens and prevent them from causing infections.  A new study from MIT reveals that glycans — branched sugar molecules found in mucus — are responsible for most of this microbe-taming. There are hundreds of different glycans in mucus, and the MIT team discovered that these molecules can prevent bacteria from communicating with each other and forming infectious biofilms, effectively rendering them harmless. “What we have in mucus is a therapeutic gold mine,” says Katharina Ribbeck, the Mark Hyman, Jr. Career Development Professor of Biological Engineering at MIT. “These glycans have biological functions that are very broad and sophisticated. They have the ability to regulate how microbes behave and really tune their identity.” In this study, which appears today in Nature Microbiology, the researchers focused on glycans’ interactions with Pseudomonas aeruginosa, an opportunistic pathogen that can cause infections in cystic fibrosis patients and people with compromised immune systems. Work now underway in Ribbeck’s lab has shown that glycans can regulate the behavior of other microbes as well. The lead author of the Nature Microbiology paper is MIT graduate student Kelsey Wheeler. Powerful defenders The average person produces several liters of mucus every day, and until recently this mucus was thought to function primarily as a lubricant and a physical barrier. However, Ribbeck and others have shown that mucus can actually interfere with bacterial behavior, preventing microbes from attaching to surfaces and communicating with one another. In the new study, Ribbeck wanted to test whether glycans were involved in mucus’ ability to control the behavior of microbes. These sugar molecules, a type of oligosaccharide, attach to proteins called mucins, the gel-forming building blocks of mucus, to form a bottlebrush-like structure. Mucus-associated glycans have been little studied, but Ribbeck thought they might play a major role in the microbe-disarming activity she had previously seen from mucus. To explore that possibility, she isolated glycans and exposed them to Pseudomonas aeruginosa. Upon exposure to mucin glycans, the bacteria underwent broad shifts in behavior that rendered them less harmful to the host. For example, they no longer produced toxins, attached to or killed host cells, or expressed genes essential for bacterial communication. This microbe-disarming activity had powerful consequences on the ability of this bacterium to establish infections. Ribbeck has shown that treatment of Pseudomonas-infected burn wounds with mucins and mucin glycans reduces bacterial proliferation, indicating the therapeutic potential of these virulence-neutralizing agents. “We’ve seen that intact mucins have regulatory effects and can cause behavioral switches in a whole range of pathogens, but now we can pinpoint the molecular mechanism and the entities that are responsible for this, which are the glycans,” Ribbeck says. In these experiments, the researchers used collections of hundreds of glycans, but they now plan to study the effects of individual glycans, which may interact specifically with different pathways or different microbes. “This is an important paper, as it shows that bacterial biofilm formation is inhibited by normal mucus, and especially its glycans. [Ribbeck] has now once more shown that normal mucus has beneficial effects on bacteria and that mucus is more complex than mostly appreciated,” says Gunnar Hansson, a professor of medical biochemistry at the University of Gothenburg, who was not involved in the study. Bacterial interactions Pseudomonas aeruginosa is just one of many opportunistic pathogens that healthy mucus keeps in check. Ribbeck is now studying the role of glycans in regulating other pathogens, including Streptococcus and the fungus Candida albicans, and she is also working on identifying receptors on microbe cell surfaces that interact with glycans. Her work on Streptococcus has shown that glycans can block horizontal gene transfer, a process that microbes often use to spread genes for drug resistance. Ribbeck and other researchers are now interested in using what they have learned about mucins and glycans to develop artificial mucus, which could offer a new way to treat diseases stemming from lost or defective mucus. Harnessing the powers of mucus could also lead to new ways to treat antibiotic-resistant infections, because it offers a complementary strategy to traditional antibiotics, Ribbeck says. “What we find here is that nature has evolved the ability to disarm difficult microbes, instead of killing them. This would not only help limit selective pressure for developing resistance, because they are not under pressure to find ways to survive, but it should also help create and maintain a diverse microbiome,” she says. Ribbeck suspects that glycans in mucus also play a key role in determining the composition of the microbiome — the trillions of bacterial cells that live inside the human body. Many of these microbes are beneficial to their human hosts, and glycans may be providing them with nutrients they need, or otherwise helping them to flourish, she says. In this way, mucus-associated glycans are similar to the many oligosaccharides found in human milk, which also contains a wide array of sugars that can regulate microbe behavior. “This is a theme that is likely at play in many systems where the goal is to shape and manipulate communities inside the body, not just in humans but throughout the animal kingdom,” Ribbeck says. The research was funded by the National Institute of Biomedical Imaging and Bioengineering, the National Institutes of Health, the National Science Foundation, the National Institute of Environmental Health Sciences, and the MIT Deshpande Center for Technological Innovation. "
MIT News,Engineers put Leonardo da Vinci’s bridge design to the test,Research,2019-10-09,-,http://news.mit.edu/2019/leonardo-da-vinci-bridge-test-1010,"  In 1502 A.D., Sultan Bayezid II sent out the Renaissance equivalent of a government RFP (request for proposals), seeking a design for a bridge to connect Istanbul with its neighbor city Galata. Leonardo da Vinci, already a well-known artist and inventor, came up with a novel bridge design that he described in a letter to the Sultan and sketched in a small drawing in his notebook. He didn’t get the job. But 500 years after his death, the design for what would have been the world’s longest bridge span of its time intrigued researchers at MIT, who wondered how thought-through Leonardo’s concept was and whether it really would have worked. Spoiler alert: Leonardo knew what he was doing. To study the question, recent graduate student Karly Bast MEng ’19, working with professor of architecture and of civil and environmental engineering John Ochsendorf and undergraduate Michelle Xie, tackled the problem by analyzing the available documents, the possible materials and construction methods that were available at the time, and the geological conditions at the proposed site, which was a river estuary called the Golden Horn. Ultimately, the team built a detailed scale model to test the structure’s ability to stand and support weight, and even to withstand settlement of its foundations. The results of the study were presented in Barcelona this week at the conference of the International Association for Shell and Spatial Structures. They will also be featured in a talk at Draper in Cambridge, Massachusetts, later this month and in an episode of the PBS program NOVA, set to air on Nov. 13. A flattened arch In Leonardo’s time, most masonry bridge supports were made in the form of conventional semicircular arches, which would have required 10 or more piers along the span to support such a long bridge. Leonardo’s bridge concept was dramatically different — a flattened arch that would be tall enough to allow a sailboat to pass underneath with its mast in place, as illustrated in his sketch, but that would cross the wide span with a single enormous arch. The bridge would have been about 280 meters long (though Leonardo himself was using a different measurement system, since the metric system was still a few centuries off), making it the longest span in the world at that time, had it been built. “It’s incredibly ambitious,” Bast says. “It was about 10 times longer than typical bridges of that time.” The design also featured an unusual way of stabilizing the span against lateral motions — something that has resulted in the collapse of many bridges over the centuries. To combat that, Leonardo proposed abutments that splayed outward on either side, like a standing subway rider widening her stance to balance in a swaying car. In his notebooks and letter to the Sultan, Leonardo provided no details about the materials that would be used or the method of construction. Bast and the team analyzed the materials available at the time and concluded that the bridge could only have been made of stone, because wood or brick could not have carried the loads of such a long span. And they concluded that, as in classical masonry bridges such as those built by the Romans, the bridge would stand on its own under the force of gravity, without any fasteners or mortar to hold the stone together. To prove that, they had to build a model and demonstrate its stability. That required figuring out how to slice up the complex shape into individual blocks that could be assembled into the final structure. While the full-scale bridge would have been made up of thousands of stone blocks, they decided on a design with 126 blocks for their model, which was built at a scale of 1 to 500 (making it about 32 inches long). Then the individual blocks were made on a 3D printer, taking about six hours per block to produce. “It was time-consuming, but 3D printing allowed us to accurately recreate this very complex geometry,” Bast says. Testing the design’s feasibility This is not the first attempt to reproduce Leonardo’s basic bridge design in physical form. Others, including a pedestrian bridge in Norway, have been inspired by his design, but in that case modern materials — steel and concrete — were used, so that construction provided no information about the practicality of Leonardo’s engineering. “That was not a test to see if his design would work with the technology from his time,” Bast says. But because of the nature of gravity-supported masonry, the faithful scale model, albeit made of a different material, would provide such a test. “It’s all held together by compression only,” she says. “We wanted to really show that the forces are all being transferred within the structure,” which is key to ensuring that the bridge would stand solidly and not topple. As with actual masonry arch bridge construction, the “stones” were supported by a scaffolding structure as they were assembled, and only after they were all in place could the scaffolding be removed to allow the structure to support itself. Then it came time to insert the final piece in the structure, the keystone at the very top of the arch. “When we put it in, we had to squeeze it in. That was the critical moment when we first put the bridge together. I had a lot of doubts” as to whether it would all work, Bast recalls. But “when I put the keystone in, I thought, ‘this is going to work.’ And after that, we took the scaffolding out, and it stood up.” “It’s the power of geometry” that makes it work, she says. “This is a strong concept. It was well thought out.” Score another victory for Leonardo. “Was this sketch just freehanded, something he did in 50 seconds, or is it something he really sat down and thought deeply about? It’s difficult to know” from the available historical material, she says. But proving the effectiveness of the design suggests that Leonardo really did work it out carefully and thoughtfully, she says. “He knew how the physical world works.” He also apparently understood that the region was prone to earthquakes, and incorporated features such as the spread footings that would provide extra stability. To test the structure’s resilience, Bast and Xie built the bridge on two movable platforms and then moved one away from the other to simulate the foundation movements that might result from weak soil. The bridge showed resilience to the horizontal movement, only deforming slightly until being stretched to the point of complete collapse. The design may not have practical implications for modern bridge designers, Bast says, since today’s materials and methods provide many more options for lighter, stronger designs. But the proof of the feasibility of this design sheds more light on what ambitious construction projects might have been possible using only the materials and methods of the early Renaissance. And it once again underscores the brilliance of one of the world’s most prolific inventors. It also demonstrates, Bast says, that “you don’t necessarily need fancy technology to come up with the best ideas.” "
MIT News,New method visualizes groups of neurons as they compute,Research,2019-10-09,-,http://news.mit.edu/2019/flourescent-visualize-neuron-activity-1009,"  Using a fluorescent probe that lights up when brain cells are electrically active, MIT and Boston University researchers have shown that they can image the activity of many neurons at once, in the brains of mice. This technique, which can be performed using a simple light microscope, could allow neuroscientists to visualize the activity of circuits within the brain and link them to specific behaviors, says Edward Boyden, the Y. Eva Tan Professor in Neurotechnology and a professor of biological engineering and of brain and cognitive sciences at MIT. “If you want to study a behavior, or a disease, you need to image the activity of populations of neurons because they work together in a network,” says Boyden, who is also a member of MIT’s McGovern Institute for Brain Research, Media Lab, and Koch Institute for Integrative Cancer Research. Using this voltage-sensing molecule, the researchers showed that they could record electrical activity from many more neurons than has been possible with any existing, fully genetically encoded, fluorescent voltage probe. Boyden and Xue Han, an associate professor of biomedical engineering at Boston University, are the senior authors of the study, which appears in the Oct. 9 online edition of Nature. The lead authors of the paper are MIT postdoc Kiryl Piatkevich, BU graduate student Seth Bensussen, and BU research scientist Hua-an Tseng. Seeing connections Neurons compute using rapid electrical impulses, which underlie our thoughts, behavior, and perception of the world. Traditional methods for measuring this electrical activity require inserting an electrode into the brain, a process that is labor-intensive and usually allows researchers to record from only one neuron at a time. Multielectrode arrays allow the monitoring of electrical activity from many neurons at once, but they don’t sample densely enough to get all the neurons within a given volume.  Calcium imaging does allow such dense sampling, but it measures calcium, an indirect and slow measure of neural electrical activity. In 2018, Boyden’s team developed an alternative way to monitor electrical activity by labeling neurons with a fluorescent probe. Using a technique known as directed protein evolution, his group engineered a molecule called Archon1 that can be genetically inserted into neurons, where it becomes embedded in the cell membrane. When a neuron’s electrical activity increases, the molecule becomes brighter, and this fluorescence can be seen with a standard light microscope. In the 2018 paper, Boyden and his colleagues showed that they could use the molecule to image electrical activity in the brains of transparent worms and zebrafish embryos, and also in mouse brain slices. In the new study, they wanted to try to use it in living, awake mice as they engaged in a specific behavior. To do that, the researchers had to modify the probe so that it would go to a subregion of the neuron membrane. They found that when the molecule inserts itself throughout the entire cell membrane, the resulting images are blurry because the axons and dendrites that extend from neurons also fluoresce. To overcome that, the researchers attached a small peptide that guides the probe specifically to membranes of the cell bodies of neurons. They called this modified protein SomArchon. “With SomArchon, you can see each cell as a distinct sphere,” Boyden says. “Rather than having one cell’s light blurring all its neighbors, each cell can speak by itself loudly and clearly, uncontaminated by its neighbors.” The researchers used this probe to image activity in a part of the brain called the striatum, which is involved in planning movement, as mice ran on a ball. They were able to monitor activity in several neurons simultaneously and correlate each one’s activity with the mice’s movement. Some neurons’ activity went up when the mice were running, some went down, and others showed no significant change. “Over the years, my lab has tried many different versions of voltage sensors, and none of them have worked in living mammalian brains until this one,” Han says. Using this fluorescent probe, the researchers were able to obtain measurements similar to those recorded by an electrical probe, which can pick up activity on a very rapid timescale. This makes the measurements more informative than existing techniques such as imaging calcium, which neuroscientists often use as a proxy for electrical activity. “We want to record electrical activity on a millisecond timescale,” Han says. “The timescale and activity patterns that we get from calcium imaging are very different. We really don’t know exactly how these calcium changes are related to electrical dynamics.” With the new voltage sensor, it is also possible to measure very small fluctuations in activity that occur even when a neuron is not firing a spike. This could help neuroscientists study how small fluctuations impact a neuron’s overall behavior, which has previously been very difficult in living brains, Han says. The study “introduces a new and powerful genetic tool” for imaging voltage in the brains of awake mice, says Adam Cohen, a professor of chemistry, chemical biology, and physics at Harvard University. “Previously, researchers had to impale neurons with fine glass capillaries to make electrical recordings, and it was only possible to record from one or two cells at a time. The Boyden team recorded from about 10 cells at a time. That’s a lot of cells,” says Cohen, who was not involved in the research. “These tools open new possibilities to study the statistical structure of neural activity. But a mouse brain contains about 75 million neurons, so we still have a long way to go.” Mapping circuits The researchers also showed that this imaging technique can be combined with optogenetics — a technique developed by the Boyden lab and collaborators that allows researchers to turn neurons on and off with light by engineering them to express light-sensitive proteins. In this case, the researchers activated certain neurons with light and then measured the resulting electrical activity in these neurons. This imaging technology could also be combined with expansion microscopy, a technique that Boyden’s lab developed to expand brain tissue before imaging it, make it easier to see the anatomical connections between neurons in high resolution. “One of my dream experiments is to image all the activity in a brain, and then use expansion microscopy to find the wiring between those neurons,” Boyden says. “Then can we predict how neural computations emerge from the wiring.” Such wiring diagrams could allow researchers to pinpoint circuit abnormalities that underlie brain disorders, and may also help researchers to design artificial intelligence that more closely mimics the human brain, Boyden says. The MIT portion of the research was funded by Edward and Kay Poitras, the National Institutes of Health, including a Director’s Pioneer Award, Charles Hieken, John Doerr, the National Science Foundation, the HHMI-Simons Faculty Scholars Program, the Human Frontier Science Program, and the U.S. Army Research Office. "
MIT News,Using machine learning to hunt down cybercriminals ,Research,2019-10-08,-,http://news.mit.edu/2019/using-machine-learning-hunt-down-cybercriminals-1009,"   Hijacking IP addresses is an increasingly popular form of cyber-attack. This is done for a range of reasons, from sending spam and malware to stealing Bitcoin. It’s estimated that in 2017 alone, routing incidents such as IP hijacks affected more than 10 percent of all the world’s routing domains. There have been major incidents at Amazon and Google and even in nation-states — a study last year suggested that a Chinese telecom company used the approach to gather intelligence on western countries by rerouting their internet traffic through China. Existing efforts to detect IP hijacks tend to look at specific cases when they’re already in process. But what if we could predict these incidents in advance by tracing things back to the hijackers themselves?   That’s the idea behind a new machine-learning system developed by researchers at MIT and the University of California at San Diego (UCSD). By illuminating some of the common qualities of what they call “serial hijackers,” the team trained their system to be able to identify roughly 800 suspicious networks — and found that some of them had been hijacking IP addresses for years.  “Network operators normally have to handle such incidents reactively and on a case-by-case basis, making it easy for cybercriminals to continue to thrive,” says lead author Cecilia Testart, a graduate student at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) who will present the paper at the ACM Internet Measurement Conference in Amsterdam on Oct. 23. “This is a key first step in being able to shed light on serial hijackers’ behavior and proactively defend against their attacks.” The paper is a collaboration between CSAIL and the Center for Applied Internet Data Analysis at UCSD’s Supercomputer Center. The paper was written by Testart and David Clark, an MIT senior research scientist, alongside MIT postdoc Philipp Richter and data scientist Alistair King as well as research scientist Alberto Dainotti of UCSD. The nature of nearby networks IP hijackers exploit a key shortcoming in the Border Gateway Protocol (BGP), a routing mechanism that essentially allows different parts of the internet to talk to each other. Through BGP, networks exchange routing information so that data packets find their way to the correct destination.  In a BGP hijack, a malicious actor convinces nearby networks that the best path to reach a specific IP address is through their network. That’s unfortunately not very hard to do, since BGP itself doesn’t have any security procedures for validating that a message is actually coming from the place it says it’s coming from. “It’s like a game of Telephone, where you know who your nearest neighbor is, but you don’t know the neighbors five or 10 nodes away,” says Testart. In 1998 the U.S. Senate's first-ever cybersecurity hearing featured a team of hackers who claimed that they could use IP hijacking to take down the Internet in under 30 minutes. Dainotti says that, more than 20 years later, the lack of deployment of security mechanisms in BGP is still a serious concern. To better pinpoint serial attacks, the group first pulled data from several years’ worth of network operator mailing lists, as well as historical BGP data taken every five minutes from the global routing table. From that, they observed particular qualities of malicious actors and then trained a machine-learning model to automatically identify such behaviors. The system flagged networks that had several key characteristics, particularly with respect to the nature of the specific blocks of IP addresses they use: Volatile changes in activity: Hijackers’ address blocks seem to disappear much faster than those of legitimate networks. The average duration of a flagged network’s prefix was under 50 days, compared to almost two years for legitimate networks. Multiple address blocks: Serial hijackers tend to advertise many more blocks of IP addresses, also known as “network prefixes.” IP addresses in multiple countries: Most networks don’t have foreign IP addresses. In contrast, for the networks that serial hijackers advertised that they had, they were much more likely to be registered in different countries and continents. Identifying false positives Testart said that one challenge in developing the system was that events that look like IP hijacks can often be the result of human error, or otherwise legitimate. For example, a network operator might use BGP to defend against distributed denial-of-service attacks in which there’s huge amounts of traffic going to their network. Modifying the route is a legitimate way to shut down the attack, but it looks virtually identical to an actual hijack. Because of this issue, the team often had to manually jump in to identify false positives, which accounted for roughly 20 percent of the cases identified by their classifier. Moving forward, the researchers are hopeful that future iterations will require minimal human supervision and could eventually be deployed in production environments. “The authors' results show that past behaviors are clearly not being used to limit bad behaviors and prevent subsequent attacks,” says David Plonka, a senior research scientist at Akamai Technologies who was not involved in the work. “One implication of this work is that network operators can take a step back and examine global Internet routing across years, rather than just myopically focusing on individual incidents.” As people increasingly rely on the Internet for critical transactions, Testart says that she expects IP hijacking’s potential for damage to only get worse. But she is also hopeful that it could be made more difficult by new security measures. In particular, large backbone networks such as AT&T have recently announced the adoption of resource public key infrastructure (RPKI), a mechanism that uses cryptographic certificates to ensure that a network announces only its legitimate IP addresses.  “This project could nicely complement the existing best solutions to prevent such abuse that include filtering, antispoofing, coordination via contact databases, and sharing routing policies so that other networks can validate it,” says Plonka. “It remains to be seen whether misbehaving networks will continue to be able to game their way to a good reputation. But this work is a great way to either validate or redirect the network operator community's efforts to put an end to these present dangers.” The project was supported, in part, by the MIT Internet Policy Research Initiative, the William and Flora Hewlett Foundation, the National Science Foundation, the Department of Homeland Security, and the Air Force Research Laboratory. "
MIT News,Alzheimer’s plaque emerges early and deep in the brain,Research,2019-10-08,-,http://news.mit.edu/2019/study-pinpoints-early-alzheimers-plaque-emergence-1008,"  Long before symptoms like memory loss even emerge, the underlying pathology of Alzheimer’s disease, such as an accumulation of amyloid protein plaques, is well underway in the brain. A longtime goal of the field has been to understand where it starts so that future interventions could begin there. A new study by MIT neuroscientists at The Picower Institute for Learning and Memory could help those efforts by pinpointing the regions with the earliest emergence of amyloid in the brain of a prominent mouse model of the disease. Notably, the study also shows that the degree of amyloid accumulation in one of those same regions of the human brain correlates strongly with the progression of the disease.        “Alzheimer’s is a neurodegenerative disease, so in the end you can see a lot of neuron loss,” says Wen-Chin “Brian” Huang, co-lead author of the study and a postdoc in the lab of co-senior author Li-Huei Tsai, Picower Professor of Neuroscience and director of the Picower Institute. “At that point, it would be hard to cure the symptoms. It’s really critical to understand what circuits and regions show neuronal dysfunction early in the disease. This will, in turn, facilitate the development of effective therapeutics.” In addition to Huang, the study’s co-lead authors are Rebecca Canter, a former member of the Tsai lab, and Heejin Choi, a former member of the lab of co-senior author Kwanghun Chung, associate professor of chemical engineering and a member of the Picower Institute and the MIT Institute for Medical Engineering and Science. Tracking plaques Many research groups have made progress in recent years by tracing amyloid’s path in the brain using technologies such as positron emission tomography, and by looking at brains post-mortem, but the new study in Communications Biology adds substantial new evidence from the 5XFAD mouse model because it presents an unbiased look at the entire brain as early as one month of age. The study reveals that amyloid begins its terrible march in deep brain regions such as the mammillary body, the lateral septum, and the subiculum before making its way along specific brain circuits that ultimately lead it to the hippocampus, a key region for memory, and the cortex, a key region for cognition. The team used SWITCH, a technology developed by Chung, to label amyloid plaques and to clarify the whole brains of 5XFAD mice so that they could be imaged in fine detail at different ages. The team was consistently able to see that plaques first emerged in the deep brain structures and then tracked along circuits, such as the Papez memory circuit, to spread throughout the brain by six-12 months (a mouse’s lifespan is up to three years). The findings help to cement an understanding that has been harder to obtain from human brains, Huang says, because post-mortem dissection cannot easily account for how the disease developed over time and PET scans don’t offer the kind of resolution the new study provides from the mice. Key validations Importantly, the team directly validated a key prediction of their mouse findings in human tissue: If the mammillary body is indeed a very early place that amyloid plaques emerge, then the density of those plaques should increase in proportion with how far advanced the disease is. Sure enough, when the team used SWITCH to examine the mammillary bodies of post-mortem human brains at different stages of the disease, they saw exactly that relationship: The later the stage, the more densely plaque-packed the mammillary body was. “This suggests that human brain alterations in Alzheimer’s disease look similar to what we observe in mouse,” the authors wrote. “Thus we propose that amyloid-beta deposits start in susceptible subcortical structures and spread to increasingly complex memory and cognitive networks with age.” The team also performed experiments to determine whether the accumulation of plaques they observed were of real disease-related consequence for neurons in affected regions. One of the hallmarks of Alzheimer’s disease is a vicious cycle in which amyloid makes neurons too easily excited, and overexcitement causes neurons to produce more amyloid. The team measured the excitability of neurons in the mammillary body of 5XFAD mice and found they were more excitable than otherwise similar mice that did not harbor the 5XFAD set of genetic alterations. In a preview of a potential future therapeutic strategy, when the researchers used a genetic approach to silence the neurons in the mammillary body of some 5XFAD mice but left neurons in others unaffected, the mice with silenced neurons produced less amyloid. While the study findings help explain much about how amyloid spreads in the brain over space and time, they also raise new questions, Huang said. How might the mammillary body affect memory, and what types of cells are most affected there? “This study sets a stage for further investigation of how dysfunction in these brain regions and circuits contributes to the symptoms of Alzheimer’s disease,” he says. In addition to Huang, Canter, Choi, Tsai, and Chung, the paper’s other authors are Jun Wang, Lauren Ashley Watson, Christine Yao, Fatema Abdurrob, Stephanie Bousleiman, Jennie Young, David Bennett and Ivana Dellalle. The National Institutes of Health, the JPB Foundation, Norman B. Leventhal and Barbara Weedon fellowships, The Burroughs Wellcome Fund, the Searle Scholars Program, a Packard Award, a NARSAD Young Investigator Award, and the NCSOFT Cultural Foundation funded the research. "
MIT News,A look at Japan’s evolving intelligence efforts,Research,2019-10-08,-,http://news.mit.edu/2019/special-duty-japan-intelligence-1008,"  Once upon a time — from the 1600s through the 1800s — Japan had a spy corps so famous we know their name today: the ninjas, intelligence agents serving the ruling Tokugawa family. Over the last 75 years, however, as international spying and espionage has proliferated, Japan has mostly been on the sidelines of this global game. Defeat in World War II, and demilitarization afterward, meant that Japanese intelligence services were virtually nonexistent for decades. Japan’s interest in spycraft has returned, however. In addition to a notable military expansion — as of last year, the country has aircraft carriers again — Japan is also ramping up its formal intelligence apparatus, as a response to what the country’s chief cabinet secretary has called “the drastically changing security environment” around it. “Intelligence is a critical element of any national security strategy,” says MIT political scientist Richard Samuels, a leading expert on Japanese politics and foreign policy. “It’s just a question of how robust, and openly robust, any country is willing to make it.” Examining the status of Japan’s intelligence efforts, then, helps us understand Japan’s larger strategic outlook and goals. And now Samuels has written a wide-ranging new history of Japan’s intelligence efforts, right up to the present. The book, “Special Duty: A History of the Japanese Intelligence Community,” is being published this week by Cornell University Press. “Japan didn’t have a comprehensive intelligence capability, but they’re heading in that direction,” says Samuels, who is the director of the Center for International Studies and the Ford International Professor of Political Science at MIT. As firm as Japan’s taboo on military and intelligence activity once was, he adds, “that constraint is coming undone.” Ruffians and freelance agents Aside from the ninjas, who focused on domestic affairs, Japan’s international intelligence efforts have seen a few distinct phases: a patchy early period, a big buildup before World War II, the dismantling of the system under the postwar U.S. occupation, and — especially during the current decade — a restoration of intelligence capabilities. Famously, Japan was closed off to much of the rest of the world until the late 19th century. It did not formally pursue international intelligence activities until the late 1860s. By the early 1900s, Japanese agents had found some success: They decoded Russian cables in the Russo-Japanese War of 1904-05 and cut off Russian raids during the conflict. But as Samuels details in the book, during this period Japan heavily relied on a colorful array of spies and agents working on an unofficial basis, an arrangement that gave the country “plausible deniability” in case these operatives were caught. “There was an interesting reliance upon scoundrels, ruffians, and freelance agents,” Samuels says. Some of these figures were quite successful. One agent, Uchida Ryohei, founded an espionage group, the Amur River Society (also sometimes called the Black Dragon Society), which opened its own training school, created Japan’s best battlefield maps and conducted all manner of operations meant to limit Russian expansion. In the 1930s, another undercover agent, Doihara Kenji, became so successful at creating pro-Japanese local governments in northern China, that he became known as “Lawrence of Manchuria.” Meanwhile, Japan’s official intelligence units had a chronic lack of coordination; they divided along military branches and between military and diplomatic bureaucracies. Still, in the decades before World War II, Japan leveraged some existing strengths in the study of foreign cultures — “The Japanese invented area studies before we did,” says Samuels — and used technological advances to make huge strides in information-gathering. “They had strengths, they had weaknesses, they had official intelligence, they had nonofficial intelligence, but overall that was a period of great growth in their intelligence capability,” Samuels says. “That of course comes to a crashing halt at the end of the war, when the entire military apparatus was taken down. So there was this period immediately after the war where there was no formal intelligence.” Japan’s subsequent postwar political reorientation toward the U.S. created many advantages for the country but was simultaneously a source of frustration to some. The country became an economic powerhouse while lacking the same covert capabilities as other countries.   “The Cold War was a period in which many Japanese in the intelligence world resented having to accommodate to American power in the intelligence world, and resented it,” Samuels says. “They had economic intelligence capability. They were very good at doing foreign economic analysis and were all over the world, but they were underperforming on the diplomatic and military fronts.” The Asian pivot In “Special Duty,” Samuels suggests three main reasons why any country reforms its intelligence services: Shifts in the strategic environment, technological innovations, and intelligence failures. The first of these seems principally responsible for the current revival of Japan’s intelligence operations. As Samuels notes, some Japanese officials wanted to change the country’s intelligence structure during the 1980s — to little avail. The end of the Cold War, and the more complicated geopolitcal map that resulted, provided a more compelling rationale for doing so, without producing many tangible results. Instead, more recent events in Asia have had a much bigger impact in Japan: namely, North Korean missile testing and China’s massive surge in economic and military power. In 2005, Samuels notes, Japan’s GDP was still twice that of China. A decade later, China’s economy was two and a half times as large as Japan’s, and its military budget was twice as big. U.S. power relative to China has also declined. Those developments have altered Japanese security priorities. “There’s been a Japanese pivot in Asia,” Samuels notes. “That’s really very important.” Moreover, he adds, from the Japanese perspective, “The question about China is obvious. Is its rise going to be disjunctive, or is it going to be stabilizing?” These regional changes have led Japan to chart a course of greater confidence in foreign policy — reflected in its growing intelligence function. Since 2013 in particular, after Prime Minister Shinzo Abe took office for a second time, Japan has built up its own intelligence function as never before, making operations more unified and better-supported. Japan still coordinates extensively with the U.S. in some areas of intelligence but is also taking intelligence matters into its own hands, in a way not seen for several decades. As Samuels notes, Japan’s increasing foreign-policy independence is also supported by voters. “Japanese public opinion has changed,” Samuels says. “They see the issues now, they talk about it now. Used to be, you couldn’t talk about intelligence in polite company. But people talk about it now, and they’re much more willing to go forward.” “Special Duty” has been praised by other scholars in the field of Japanese security studies and foreign policy. Sheila Smith of the Council on Foreign Relations in Washington calls it a “truly wonderful book” that “offers much needed insight to academics and policymakers alike as they seek to understand the changes in Japan's security choices. ” By looking at intelligence issues in this way, Samuels has also traced larger contours in Japanese history: first, an opening up to the world, then the alignment with the U.S. in the postwar world, and now a move toward greater capabilities. On the intelligence front, those capabilities include enhanced analysis and streamlined relations across units, heading toward the full spectrum of functions seen in the other major states.   “It’s been the assumption that the Japanese just don’t do [intelligence activities], except economics,” Samuels reflects. “Well, I hope after people see this book they will understand that’s no longer the case, and hasn’t been for some time.” "
MIT News,New capsule can orally deliver drugs that usually have to be injected,Research,2019-10-07,-,http://news.mit.edu/2019/orally-deliver-drugs-injected-1007,"  Many drugs, especially those made of proteins, cannot be taken orally because they are broken down in the gastrointestinal tract before they can take effect. One example is insulin, which patients with diabetes have to inject daily or even more frequently. In hopes of coming up with an alternative to those injections, MIT engineers, working with scientists from Novo Nordisk, have designed a new drug capsule that can carry insulin or other protein drugs and protect them from the harsh environment of the gastrointestinal tract. When the capsule reaches the small intestine, it breaks down to reveal dissolvable microneedles that attach to the intestinal wall and release drug for uptake into the bloodstream. “We are really pleased with the latest results of the new oral delivery device our lab members have developed with our collaborators, and we look forward to hopefully seeing it help people with diabetes and others in the future,” says Robert Langer, the David H. Koch Institute Professor at MIT and a member of the Koch Institute for Integrative Cancer Research. In tests in pigs, the researchers showed that this capsule could load a comparable amount of insulin to that of an injection, enabling fast uptake into the bloodstream after the microneedles were released. Langer and Giovanni Traverso, an assistant professor in MIT’s Department of Mechanical Engineering and a gastroenterologist at Brigham and Women’s Hospital, are the senior authors of the study, which appears today in Nature Medicine. The lead authors of the paper are recent MIT PhD recipient Alex Abramson and former MIT postdoc Ester Caffarel-Salvador. Microneedle delivery Langer and Traverso have previously developed several novel strategies for oral delivery of drugs that usually have to be injected. Those efforts include a pill coated with many tiny needles, as well as star-shaped structures that unfold and can remain in the stomach from days to weeks while releasing drugs. “A lot of this work is motivated by the recognition that both patients and health care providers prefer the oral route of administration over the injectable one,” Traverso says. Earlier this year, they developed a blueberry-sized capsule containing a small needle made of compressed insulin. Upon reaching the stomach, the needle injects the drug into the stomach lining. In the new study, the researchers set out to develop a capsule that could inject its contents into the wall of the small intestine. Most drugs are absorbed through the small intestine, Traverso says, in part because of its extremely large surface area --- 250 square meters, or about the size of a tennis court. Also, Traverso noted that pain receptors are lacking in this part of the body, potentially enabling pain-free micro-injections in the small intestine for delivery of drugs like insulin. To allow their capsule to reach the small intestine and perform these micro-injections, the researchers coated it with a polymer that can survive the acidic environment of the stomach, which has a pH of 1.5 to 3.5. When the capsule reaches the small intestine, the higher pH (around 6) triggers it to break open, and three folded arms inside the capsule spring open. Each arm contains patches of 1-millimeter-long microneedles that can carry insulin or other drugs. When the arms unfold open, the force of their release allows the tiny microneedles to just penetrate the topmost layer of the small intestine tissue. After insertion, the needles dissolve and release the drug. “We performed numerous safety tests on animal and human tissue to ensure that the penetration event allowed for drug delivery without causing a full thickness perforation or any other serious adverse events,” Abramson says. To reduce the risk of blockage in the intestine, the researchers designed the arms so that they would break apart after the microneedle patches are applied. The new capsule represents an important step toward achieving oral delivery of protein drugs, which has been very difficult to do, says David Putnam, a professor of biomedical engineering and chemical and biomolecular engineering at Cornell University.   “It’s a compelling paper,” says Putnam, who was not involved in the study. “Delivering proteins is the holy grail of drug delivery. People have been trying to do it for decades.” Insulin demonstration In tests in pigs, the researchers showed that the 30-millimeter-long capsules could deliver doses of insulin effectively and generate an immediate blood-glucose-lowering response. They also showed that no blockages formed in the intestine and the arms were excreted safely after applying the microneedle patches. “We designed the arms such that they maintained sufficient strength to deliver the insulin microneedles to the small intestine wall, while still dissolving within several hours to prevent obstruction of the gastrointestinal tract,” Caffarel-Salvador says. Although the researchers used insulin to demonstrate the new system, they believe it could also be used to deliver other protein drugs such as hormones, enzymes, or antibodies, as well as RNA-based drugs. “We can deliver insulin, but we see applications for many other therapeutics and possibly vaccines,” Traverso says. “We’re working very closely with our collaborators to identify the next steps and applications where we can have the greatest impact.” The research was funded by Novo Nordisk and the National Institutes of Health. Other authors of the paper include Vance Soares, Daniel Minahan, Ryan Yu Tian, Xiaoya Lu, David Dellal, Yuan Gao, Soyoung Kim, Jacob Wainer, Joy Collins, Siddartha Tamang, Alison Hayward, Tadayuki Yoshitake, Hsiang-Chieh Lee, James Fujimoto, Johannes Fels, Morten Revsgaard Frederiksen, Ulrik Rahbek, and Niclas Roxhed. "
MIT News,Scientists observe a single quantum vibration under ordinary conditions,Research,2019-10-06,-,http://news.mit.edu/2019/single-quantum-vibration-normal-1007,"  When a guitar string is plucked, it vibrates as any vibrating object would, rising and falling like a wave, as the laws of classical physics predict. But under the laws of quantum mechanics, which describe the way physics works at the atomic scale, vibrations should behave not only as waves, but also as particles. The same guitar string, when observed at a quantum level, should vibrate as individual units of energy known as phonons. Now scientists at MIT and the Swiss Federal Institute of Technology have for the first time created and observed a single phonon in a common material at room temperature. Until now, single phonons have only been observed at ultracold temperatures and in precisely engineered, microscopic materials that researchers must probe in a vacuum. In contrast, the team has created and observed single phonons in a piece of diamond sitting in open air at room temperature. The results, the researchers write in a paper published today in Physical Review X, “bring quantum behavior closer to our daily life.” “There is a dichotomy between our daily experience of what a vibration is — a wave — and what quantum mechanics tells us it must be — a particle,” says Vivishek Sudhir, a postdoc in MIT’s Kavli Institute for Astrophysics and Space Research. “Our experiment, because it is conducted at very tangible conditions, breaks this tension between our daily experience and what physics tells us must be the case.” The technique the team developed can now be used to probe other common materials for quantum vibrations. This may help researchers characterize the atomic processes in solar cells, as well as identify why certain materials are superconducting at high temperatures. From an engineering perspective, the team’s technique can be used to identify common phonon-carrying materials that may make ideal interconnects, or transmission lines, between the quantum computers of the future. “What our work means is that we now have access to a much wider palette of systems to choose from,” says Sudhir, one of the paper’s lead authors. Sudhir’s co-authors are Santiago Tarrago Velez, Kilian Seibold, Nils Kipfer, Mitchell Anderson, and Christophe Galland, of the Swiss Federal Institute of Technology. “Democratizing quantum mechanics” Phonons, the individual particles of vibration described by quantum mechanics, are also associated with heat. For instance, when a crystal, made from orderly lattices of interconnected atoms, is heated at one end, quantum mechanics predicts that heat travels through the crystal in the form of phonons, or individual vibrations of the bonds between molecules. Single phonons have been extremely difficult to detect, mainly because of their sensitivity to heat. Phonons are susceptible to any thermal energy that is greater than their own. If phonons are inherently low in energy, then exposure to higher thermal energies could trigger a material’s phonons to excite en masse, making detection of a single photon a needle-in-a-haystack endeavor. The first efforts to observe single phonons did so with materials specially engineered to harbor very few phonons, at relatively high energies. These researchers then submerged the materials in near-absolute-zero refrigerators Sudhir describes as “brutally, aggressively cold,” to ensure that the surrounding thermal energy was lower than the energy of the phonons in the material. “If that’s the case, then the [phonon] vibration cannot borrow energy from the thermal environment to excite more than one phonon,” Sudhir explains. The researchers then shot a pulse of photons (particles of light) into the material, hoping that one photon would interact with a single phonon. When that happens, the photon, in a process known as Raman scattering, should reflect back out at a different energy imparted to it by the interacting phonon. In this way, researchers were able to detect single phonons, though at ultracold temperatures, and in carefully engineered materials. “What we’ve done here is to ask the question, how do you get rid of this complicated environment you’ve created around this object, and bring this quantum effect to our setting, to see it in more common materials,” Sudhir says. “It’s like democratizing quantum mechanics in some sense.” One in a million For the new study, the team looked to diamond as a test subject. In diamond, phonons naturally operate at high frequencies, of tens of terahertz — so high that, at room temperature, the energy of a single phonon is higher than the surrounding thermal energy. “When this crystal of diamond sits at room temperature, phonon motion does not even exist, because there’s no energy at room temperature to excite anything,” Sudhir says. Within this vibrationally quiet mix of phonons, the researchers aimed to excite just a single phonon. They sent high-frequency laser pulses, consisting of 100 million photons each, into the diamond — a crystal made up of carbon atoms — on the off chance that one of them would interact and reflect off a phonon. The team would then measure the decreased frequency of the photon involved in the collision — confirmation that it had indeed hit upon a phonon, though this operation wouldn’t be able to discern whether one or more phonons were excited in the process. To decipher the number of phonons excited, the researchers sent a second laser pulse into the diamond, as the phonon’s energy gradually decayed. For each phonon excited by the first pulse, this second pulse can de-excite it, taking away that energy in the form of a new, higher-energy photon. If only one phonon was initially excited, then one new, higher-frequency photon should be created. To confirm this, the researchers placed a semitransparent glass through which this new, higher-frequency photon would exit the diamond, along with two detectors on either side of the glass. Photons do not split, so if multiple phonons were excited then de-excited, the resulting photons should pass through the glass and scatter randomly into both detectors. If just one detector “clicks,” indicating the detection of a single photon, the team can be sure that that photon interacted with a single phonon. “It’s a clever trick we play to make sure we are observing just one phonon,” Sudhir says. The probability of a photon interacting with a phonon is about one in 10 billion. In their experiments, the researchers blasted the diamond with 80 million pulses per second — what Sudhir describes as a “train of millions of billions of photons” over several hours, in order to detect about 1 million photon-phonon interactions. In the end, they found, with statistical significance, that they were able to create and detect a single quantum of vibration. “This is sort of an ambitious claim, and we have to be careful the science is rigorously done, with no room for reasonable doubt,” Sudhir says. When sending in their second laser pulse to verify that single phonons were indeed being created, the researchers delayed this pulse, sending in into the diamond as the excited phonon was beginning to ebb in energy. In this way, they were able to glean the manner in which the phonon itself decayed. “So, not only are we able to probe the birth of a single phonon, but also we’re able to probe its death,” Sudhir says. “Now we can say, ‘go use this technique to study how long it takes for a single phonon to die out in your material of choice.’ That number is very useful. If the time it takes to die is very long, then that material can support coherent phonons. If that’s the case, you can do interesting things with it, like thermal transport in solar cells, and interconnects between quantum computers.” "
MIT News,Oobleck’s weird behavior is now predictable,Research,2019-10-05,-,http://news.mit.edu/2019/oobleck-behavior-predict-cornstarch-1006,"  It’s a phenomenon many preschoolers know well: When you mix cornstarch and water, weird things happen. Swish it gently in a bowl, and the mixture sloshes around like a liquid. Squeeze it, and it starts to feel like paste. Roll it between your hands, and it solidifies into a rubbery ball. Try to hold that ball in the palm of your hand, and it will dribble away as a liquid. Most of us who have played with this stuff know it as “oobleck,” named after a sticky green goo in Dr. Seuss’ “Bartholomew and the Oobleck.” Scientists, on the other hand, refer to cornstarch and water as a “non-Newtonian fluid” — a material that appears thicker or thinner depending on how it is physically manipulated. Now MIT engineers have developed a mathematical model that predicts oobleck’s weird behavior. Using their model, the researchers accurately simulated how oobleck turns from a liquid to a solid and back again, under various conditions. Aside from predicting what the stuff might do in the hands of toddlers, the new model can be useful in predicting how oobleck and other solutions of ultrafine particles might behave for military and industrial applications. Could an oobleck-like substance fill highway potholes and temporarily harden as a car drives over it? Or perhaps the slurry could pad the lining of bulletproof vests, morphing briefly into an added shield against sudden impacts. With the team’s new oobleck model, designers and engineers can start to explore such possibilities. “It’s a simple material to make — you go to the grocery store, buy cornstarch, then turn on your faucet,” says Ken Kamrin, associate professor of mechanical engineering at MIT. “But it turns out the rules that govern how this material flows are very nuanced.” Kamrin, along with graduate student Aaron Baumgarten, have published their results today in the Proceedings of the National Academy of Sciences.         A clumpy model Kamrin’s primary work focuses on characterizing the flow of granular material such as sand. Over the years, he’s developed a mathematical model that accurately predicts the flow of dry grains under a number of different conditions and environments. When Baumgarten joined the group, the researchers started work on a model to describe how saturated wet sand moves. It was around this time that Kamrin and Baumgarten saw a scientific talk on oobleck. “We’d seen this talk, and we had a lengthy debate over what is oobleck, and how is it different from wet sand,” Kamrin says. “After some vigorous back and forth with Aaron, he decided to see if we could turn this wet sand model into one for oobleck.” Granular material in oobleck is much finer than sand: A single particle of cornstarch is about 1 to 10 microns wide and about one-hundredth the size of a grain of sand. Kamrin says particles at such a small scale experience effects that larger particles such as sand do not. For instance, because cornstarch particles are so small, they can be influenced by temperature, and by electric charges that build up between particles, causing them to slightly repel against each other. “As long as you squish slowly, the grains will repel, keeping a layer of fluid between them, and just slide past each other, like a fluid,” Kamrin says. “But if you do anything too fast, you’ll overcome that little repulsion, the particles will touch, there will be friction, and it’ll act as a solid.” This repulsion happening at the small scale brings out a key difference between large and ultrafine grain mixtures at the lab scale: The viscosity, or consistency of wet sand at a given packing density remains the same, whether you stir it gently or slam a fist into it. In contrast, oobleck has a low, liquid-like viscosity when slowly stirred. But if its surface is punched, a rapidly growing zone of the slurry adjacent to the contact point becomes more viscous, causing oobleck’s surface to bounce back and resist the impact, like a solid trampoline. They found that stress was the main factor in determining whether a material was more or less viscous. For instance, the faster and more forcefully oobleck is disturbed, the “clumpier” it is — that is, the more the underlying particles make frictional, as opposed to lubricated, contact. If it is slowly and gently deformed, oobleck is less viscous, with particles that are more evenly distributed and that repel against each other, like a liquid. The team looked to model the effect of repulsion of fine particles, with the idea that perhaps a new “clumpiness variable” could be added to their model of wet sand to make an accurate model of oobleck. In their model, they included mathematical terms to describe how this variable would grow and shrink under a certain stress or force. “Now we have a robust way of modeling how clumpy any chunk of the material in the body will be as you deform it in an arbitrary way,” Baumgarten says. Wheels spinning The researchers incorporated this new variable into their more general model for wet sand, and looked to see whether it would predict oobleck’s behavior. They used their model to simulate previous experiments by others, including a simple setup of oobleck being squeezed and sheared between two plates, and a set of experiments in which a small projectile is shot into a tank of oobleck at different speeds. In all scenarios, the simulations matched the experimental data and reproduced the motion of the oobleck, replicating the regions where it morphed from liquid to solid, and back again. To see how their model could predict oobleck’s behavior in more complex conditions, the team simulated a pronged wheel driving at different speeds over a deep bed of the slurry. They found the faster the wheel spun, the more the mixture formed what Baumgarten calls a “solidification front” in the oobleck, that momentarily supports the wheel so that it can roll across without sinking. Kamrin and Baumgarten say the new model can be used to explore how various ultrafine-particle solutions such as oobleck behave when put to use as, for instance, fillings for potholes, or bulletproof vests. They say the model could also help to identify ways to redirect slurries through systems such as industrial plants. “With industrial waste products, you could get fine particle suspensions that don’t flow the way you expect, and you have to move them from this vat to that vat, and there may be best practices that people don’t know yet, because there’s no model for it,” Kamrin says. “Maybe now there is.” This research was supported, in part, by the Army Research Office and the National Science Foundation. "
MIT News,3 Questions: How artificial intelligence is supercharging materials science,Research,2019-10-03,-,http://news.mit.edu/2019/3-questions-how-ai-supercharging-materials-science-juejun-hu-1003,"  Machine learning and artificial intelligence are increasingly being used in materials science research. For example, MIT associate professor of materials science and engineering Juejun ""JJ"" Hu developed an algorithm that enhances the performance of a chip-based spectrometer, and Atlantic Richfield Associate Professor of Energy Studies Elsa A. Olivetti built an artificial-intelligence system that scours through scientific papers to deduce materials science “recipes.” These and other MIT professors, as well as keynote speaker Brian Storey, Toyota Research Institute’s director of accelerated materials design and discovery, will discuss insights and breakthroughs in their research using machine learning at the MIT Materials Research Laboratory’s annual Materials Day Symposium on Wednesday, Oct. 9 in Kresge Auditorium. Associate Professor Hu recently explained what led to his breakthrough spectrometer, and why he is optimistic that machine learning and artificial intelligence are becoming an everyday tool in materials research.  Q: Your spectrometer work in particular made use of machine learning techniques. How is the new approach changing the process of discovery in materials science? A: Basically, we developed a new spectrometer technology that allows us to shrink big components onto a small silicon chip and still maintain high performance. We developed an algorithm that allows us to extract the information with much better signal-to-noise ratio. We have validated the algorithm for many different kinds of spectrum. The algorithm identifies separate colors of light by comparing two repeated measurements to mitigate the impact of measurement noises. The algorithm improves resolution by 100 percent compared to the textbook limits, called the Rayleigh limits.  Q: How are you using machine learning to identify new optical materials and designs for your work on mid-infrared lenses composed of optical antenna arrays? A: We are collaborating with a group at UMass [the University of Massachusetts] to develop a deep learning algorithm for designing “metasurfaces,” which are a kind of optical device where instead of using conventional geometric curvature to construct, say, a lens, you use an array of specially designed optical antennas to impart phase delay on the incoming light, and therefore we can achieve all kind of functionalities. One big problem with metasurfaces is that conventionally, when people would design these metasurfaces, they would do it essentially by trial and error. We have set up a deep learning algorithm. The algorithm allows us to train it with existing data. So as we train it, eventually the algorithm becomes “smart.” The algorithm can evaluate the workability of irregular shapes that go beyond conventional shapes likes circles and rectangles. It can recognize hidden connections between complex geometries and the electromagnetic response, which is usually not trivial, and it can find these hidden relations faster than conventional full-scale simulations. The algorithm can also screen out potential combinations of materials and functions that just won’t work. If you use conventional methods, you have to waste lots of time to exhaust all the possible design space and then come to this conclusion, but now our algorithm can tell you really quickly. Q: What other advances are facilitating use of machine learning in materials science? A: The other thing we are seeing is now we also have much easier access to very powerful, cloud-based computational facilities that are commercially available. So that combination of hardware, easy access, very powerful computing resources, and the new algorithms, that's what enables us to make new innovations. Again, for example, with metasurfaces, if you look at old designs, people were pretty much using regular geometries like circles, squares, rectangles, but we, as well as many others in the community, are all now moving on to topologically optimized optical devices. And to design those structures, the combination of new algorithms and powerful computational resources is the key to design huge devices like macroscopic, topologically optimized optics in three-dimensional space.  "
MIT News,SMART develops a way to commercially manufacture integrated silicon III-V chips,Research,2019-10-03,-,http://news.mit.edu/2019/mit-singapore-smart-way-to-manufacture-integrated-silicon-iii-v-chips-1003,"  The Singapore-MIT Alliance for Research and Technology (SMART), MIT’s research enterprise in Singapore, has announced the successful development of a commercially viable way to manufacture integrated silicon III-V chips with high-performance III-V devices inserted into their design. In most devices today, silicon-based CMOS chips are used for computing, but they are not efficient for illumination and communications, resulting in low efficiency and heat generation. This is why current 5G mobile devices on the market get very hot upon use and can shut down after a short time. This is where III-V semiconductors are valuable. III-V chips are made with compounds including elements in the third and fifth columns of the periodic table, such as gallium nitride (GaN) and indium gallium arsenide (InGaAs). Due to their unique properties, they are exceptionally well-suited for optoelectronics (such as LEDs) and communications (such as 5G wireless), boosting efficiency substantially. “By integrating III-V into silicon, we can build upon existing manufacturing capabilities and low-cost volume production techniques of silicon and include the unique optical and electronic functionality of III-V technology,” says Eugene Fitzgerald, CEO and director of SMART and the Merton C. Flemings-SMA Professor of Materials Science and Engineering at MIT. “The new chips will be at the heart of future product innovation and power the next generation of communications devices, wearables, and displays.” Kenneth Lee, senior scientific director of the SMART Low Energy Electronic Systems (LEES) research program, adds: “Integrating III-V semiconductor devices with silicon in a commercially viable way is one of the most difficult challenges faced by the semiconductor industry, even though such integrated circuits have been desired for decades. Current methods are expensive and inefficient, which is delaying the availability of the chips the industry needs. With our new process, we can leverage existing capabilities to manufacture these new integrated silicon III-V chips cost-effectively and accelerate the development and adoption of new technologies that will power economies.” The new technology developed by SMART builds two layers of silicon and III-V devices on separate substrates and integrates them vertically together within a micron, which is 1/50th the diameter of a human hair. The process can use existing 200 micrometer manufacturing tools, which will allow semiconductor manufacturers in Singapore and around the world to make new use of their current equipment. Today, the cost of investing in a new manufacturing technology is in the range of tens of billions of dollars; the new integrated circuit platform is highly cost-effective, and will result in much lower-cost novel circuits and electronic systems. SMART is focusing on creating new chips for pixelated illumination/display and 5G markets, which has a combined potential market of over $100 billion. Other markets that SMART’s new integrated silicon III-V chips will disrupt include wearable mini-displays, virtual reality applications, and other imaging technologies. The patent portfolio has been exclusively licensed by New Silicon Corporation (NSC), a Singapore-based spinoff from SMART. NSC is the first fabless silicon integrated circuit company with proprietary materials, processes, devices, and design for monolithic integrated silicon III-V circuits. SMART’s new integrated Silicon III-V chips will be available next year and expected in products by 2021. SMART’s LEES Interdisciplinary Research Group is creating new integrated circuit technologies that result in increased functionality, lower power consumption, and higher performance for electronic systems. These integrated circuits of the future will impact applications in wireless communications, power electronics, LED lighting, and displays. LEES has a vertically-integrated research team possessing expertise in materials, devices, and circuits, comprising multiple individuals with professional experience within the semiconductor industry. This ensures that the research is targeted to meet the needs of the semiconductor industry both within Singapore and globally. "
MIT News,Artificial gut aims to expose the elusive microbiome,Research,2019-10-03,-,http://news.mit.edu/2019/artificial-gut-aims-to-expose-human-microbiome-1003,"  The microbiome is a collection of trillions of bacteria that reside in and on our bodies. Each person’s microbiome is unique — just like a fingerprint — and researchers are finding more and more ways in which it impacts our health and daily lives. One example involves an apparent link between the brain and the bacteria in the gut. This brain-gut “axis” is believed to influence conditions such as Parkinson’s disease, depression, and irritable bowel syndrome. However, many studies into the brain-gut axis have stalled because of one central problem: the lack of an adequate testable model of the gut. Current testing platforms cannot emulate the human gut accurately and cheaply enough for large-scale studies. The research community needs something new, which is what a team at MIT Lincoln Laboratory is tackling in a project funded through the Technology Office. Researchers there aim to create the perfect artificial gut. “The question from the mechanical side is, how do you emulate the colon?” says Todd Thorsen, the project’s principal investigator from the Biological and Chemical Technologies Group. “Bacteria in the colon occupy lots of ecological niches.” Thorsen is referring to the complexity of the human gut, which includes a community of 100 trillion microbes that all have specific, and sometimes clashing, needs. For example, certain types of bacteria in the gut will die in the presence of oxygen, while others need it to survive. The gut also contains both hard and soft mucus that allows different types of bacteria to grow. All of these conditions need to be mimicked in a single platform in order to properly maintain and test microbiome samples — and that’s not an easy task. “Until now, no one has been able to culture a microbiome sample and maintain it,” says David Walsh from the Biological and Chemical Technologies Group, who led the device’s development and fabrication. “If we can maintain a culture, we can do things like add toxins and therapeutics to see how they change the culture over time.” To address the problem, the laboratory team developed a multimaterial platform made of permeable silicon rubber and other plastics, such as polystyrene, all of which are cheap and can be rapidly prototyped. The two components of the platform emulate the essential oxygen and mucosal gradients. The above photo (left) shows the component that controls the oxygen gradient. Air diffuses through the plastic while the blue ports allow researchers to change the local oxygen concentrations at different positions within the adjacent microculture chambers. The right photo shows the component that controls mucus, which is welled up into the device from below. Both components implement careful geometry to yield the precise conditions found in the gut. “The final system will allow us to tackle real-world problems,” Walsh says. Those problems, in addition to unraveling the brain-gut axis, include developing resilience to current and emerging pathogens, combating biological warfare, and more. This year, the research team is partnering with the University of Alabama at Birmingham, Northeastern University, and the University of California at San Francisco to implement their first tests of microbiome samples to study links to Parkinson’s disease. The laboratory’s role is to use the artificial gut to culture microbiome samples taken from people with and without Parkinson’s disease and test what happens when different suspected adverse influencers are added. The goal is to correlate how changes in the microbiome caused by exposure to certain toxins may induce Parkinson’s-like nerve damage. The laboratory will also continue advancing other aspects of the project. Some examples include building a tubular core-shell origami-like gut that rolls up during assembly to emulate the colon and the surrounding vascularized tissue, and developing modeling software to predict how microbial communities might change over time. "
MIT News,Engineered viruses could fight drug resistance,Research,2019-10-03,-,http://news.mit.edu/2019/engineered-phage-viruses-drug-resistance-1003,"  In the battle against antibiotic resistance, many scientists have been trying to deploy naturally occurring viruses called bacteriophages that can infect and kill bacteria. Bacteriophages kill bacteria through different mechanisms than antibiotics, and they can target specific strains, making them an appealing option for potentially overcoming multidrug resistance. However, quickly finding and optimizing well-defined bacteriophages to use against a bacterial target is challenging. In a new study, MIT biological engineers showed that they could rapidly program bacteriophages to kill different strains of E. coli by making mutations in a viral protein that binds to host cells. These engineered bacteriophages are also less likely to provoke resistance in bacteria, the researchers found. “As we’re seeing in the news more and more now, bacterial resistance is continuing to evolve and is increasingly problematic for public health,” says Timothy Lu, an MIT associate professor of electrical engineering and computer science and of biological engineering. “Phages represent a very different way of killing bacteria than antibiotics, which is complementary to antibiotics, rather than trying to replace them.” The researchers created several engineered phages that could kill E. coli grown in the lab. One of the newly created phages was also able to eliminate two E. coli strains that are resistant to naturally occurring phages from a skin infection in mice. Lu is the senior author of the study, which appears in the Oct. 3 issue of Cell. MIT postdoc Kevin Yehl and former postdoc Sebastien Lemire are the lead authors of the paper. Engineered viruses The Food and Drug Administration has approved a handful of bacteriophages for killing harmful bacteria in food, but they have not been widely used to treat infections because finding naturally occurring phages that target the right kind of bacteria can be a difficult and time-consuming process. To make such treatments easier to develop, Lu’s lab has been working on engineered viral “scaffolds” that can be easily repurposed to target different bacterial strains or different resistance mechanisms. “We think phages are a good toolkit for killing and knocking down bacteria levels inside a complex ecosystem, but in a targeted way,” Lu says. In 2015, the researchers used a phage from the T7 family, which naturally kills E.coli, and showed that they could program it to target other bacteria by swapping in different genes that code for tail fibers, the protein that bacteriophages use to latch onto receptors on the surfaces of host cells. While that approach did work, the researchers wanted to find a way to speed up the process of tailoring phages to a particular type of bacteria. In their new study, they came up with a strategy that allows them to rapidly create and test a much greater number of tail fiber variants. From previous studies of tail fiber structure, the researchers knew that the protein consists of segments called beta sheets that are connected by loops. They decided to try systematically mutating only the amino acids that form the loops, while preserving the beta sheet structure. “We identified regions that we thought would have minimal effect on the protein structure, but would be able to change its binding interaction with the bacteria,” Yehl says. They created phages with about 10,000,000 different tail fibers and tested them against several strains of E. coli that had evolved to be resistant to the nonengineered bacteriophage. One way that E. coli can become resistant to bacteriophages is by mutating “LPS” receptors so that they are shortened or missing, but the MIT team found that some of their engineered phages could kill even strains of E. coli with mutated or missing LPS receptors. This helps to overcome one of the limiting factors in using phages as antimicrobials, which is that bacteria can generate resistance by mutating receptors that the phages use to enter bacteria, says Rotem Sorek, a professor of molecular genetics at the Weizmann Institute of Science. “Through deep understanding of the biology entailing the phage-bacteria recognition, together with smart bioengineering approaches, Lu and his team managed to design a large library of phage variants, each of which has the potential to recognize a slightly different receptor. They show that treating bacteria with this library rather than with a single phage limits the emergence of resistance,” says Sorek, who was not involved in the study. Other targets Lu and Yehl now plan to apply this approach to targeting other resistance mechanisms used by E. coli, and they also hope to develop phages that can kill other types of harmful bacteria. “This is just the beginning, as there are many other viral scaffolds and bacteria to target,” Yehl says. The researchers are also interested in using bacteriophages as a tool to target specific strains of bacteria that live in the human gut and cause health problems. “Being able to selectively hit those nonbeneficial strains could give us a lot of benefits in terms of human clinical outcomes,” Lu says. The research was funded by the Defense Threat Reduction Agency, the National Institutes of Health, the U.S. Army Research Laboratory/Army Research Office through the MIT Institute for Soldier Nanotechnologies, and the Koch Institute Support (core) Grant from the National Cancer Institute. "
MIT News,This is how a “fuzzy” universe may have looked ,Research,2019-10-03,-,http://news.mit.edu/2019/early-galaxy-fuzzy-universe-simulation-1003,"  Dark matter was likely the starting ingredient for brewing up the very first galaxies in the universe. Shortly after the Big Bang, particles of dark matter would have clumped together in gravitational “halos,” pulling surrounding gas into their cores, which over time cooled and condensed into the first galaxies. Although dark matter is considered the backbone to the structure of the universe, scientists know very little about its nature, as the particles have so far evaded detection. Now scientists at MIT, Princeton University, and Cambridge University have found that the early universe, and the very first galaxies, would have looked very different depending on the nature of dark matter. For the first time, the team has simulated what early galaxy formation would have looked like if dark matter were “fuzzy,” rather than cold or warm. In the most widely accepted scenario, dark matter is cold, made up of slow-moving particles that, aside from gravitational effects, have no interaction with ordinary matter. Warm dark matter is thought to be a slightly lighter and faster version of cold dark matter. And fuzzy dark matter, a relatively new concept, is something entirely different, consisting of ultralight particles, each about 1 octillionth (10-27) the mass of an electron (a cold dark matter particle is far heavier — about 105 times more massive than an electron). In their simulations, the researchers found that if dark matter is cold, then galaxies in the early universe would have formed in nearly spherical halos. But if the nature of dark matter is fuzzy or warm, the early universe would have looked very different, with galaxies forming first in extended, tail-like filaments. In a fuzzy universe, these filaments would have appeared striated, like star-lit strings on a harp.   As new telescopes come online, with the ability to see further back into the early universe, scientists may be able to deduce, from the pattern of galaxy formation, whether the nature of dark matter, which today makes up nearly 85 percent of the matter in the universe, is fuzzy as opposed to cold or warm. “The first galaxies in the early universe may illuminate what type of dark matter we have today,” says Mark Vogelsberger, associate professor of physics in MIT’s Kavli Institute for Astrophysics and Space Research. “Either we see this filament pattern, and fuzzy dark matter is plausible, or we don’t, and we can rule that model out. We now have a blueprint for how to do this.” Vogelsberger is a co-author of a paper appearing today in Physical Review Letters, along with the paper’s lead author, Philip Mocz of Princeton University, and Anastasia Fialkov of Cambridge University and previously the University of Sussex. Fuzzy waves While dark matter has yet to be directly detected, the hypothesis that describes dark matter as cold has proven successful at describing the large-scale structure of the observable universe. As a result, models of galaxy formation are based on the assumption that dark matter is cold. “The problem is, there are some discrepancies between observations and predictions of cold dark matter,” Vogelsberger points out. “For example, if you look at very small galaxies, the inferred distribution of dark matter within these galaxies doesn’t perfectly agree with what theoretical models predict. So there is tension there.” Enter, then, alternative theories for dark matter, including warm, and fuzzy, which researchers have proposed in recent years. “The nature of dark matter is still a mystery,” Fialkov says. “Fuzzy dark matter is motivated by fundamental physics, for instance, string theory, and thus is an interesting dark matter candidate. Cosmic structures hold the key to validating or ruling out such dark matter modles.” Fuzzy dark matter is made up of particles that are so light that they act in a quantum, wave-like fashion, rather than as individual particles. This quantum, fuzzy nature, Mocz says, could have produced early galaxies that look entirely different from what standard models predict for cold dark matter. “Even though in the late universe these different dark matter scenarios may predict similar shapes for galaxies, the first galaxies would be strikingly different, which will give us a clue about what dark matter is,” Mocz says. To see how different a cold and a fuzzy early universe could be, the researchers simulated a small, cubic space of the early universe, measuring about 3 million light years across, and ran it forward in time to see how galaxies would form given one of the three dark matter scenarios: cold, warm, and fuzzy. The team began each simulation by assuming a certain distribution of dark matter, which scientists have some idea of, based on measurements of the cosmic microwave background — “relic radiation” that was emitted by, and was detected just 400,000 years after, the Big Bang. “Dark matter doesn’t have a constant density, even at these early times,” Vogelsberger says. “There are tiny perturbations on top of a constant density field.” The researchers were able to use existing algorithms to simulate galaxy formation under scenarios of cold and warm dark matter. But to simulate fuzzy dark matter, with its quantum nature, they needed a new approach. A map of harp strings The researchers modified their simulation of cold dark matter, enabling it to solve two extra equations in order to simulate galaxy formation in a fuzzy dark matter universe. The first, Schrödinger’s equation, describes how a quantum particle acts as a wave, while the second, Poisson’s equation, describes how that wave generates a density field, or distribution of dark matter, and how that distribution leads to gravity — the force that eventually pulls in matter to form galaxies. They then coupled this simulation to a model that describes the behavior of gas in the universe, and the way it condenses into galaxies in response to gravitational effects. In all three scenarios, galaxies formed wherever there were over-densities, or large concentrations of gravitationally collapsed dark matter. The pattern of this dark matter, however, was different, depending on whether it was cold, warm, or fuzzy.  In a scenario of cold dark matter, galaxies formed in spherical halos, as well as smaller subhalos. Warm dark matter produced  first galaxies in tail-like filaments, and no subhalos. This may be due to warm dark matter’s lighter, faster nature, making particles less likely to stick around in smaller, subhalo clumps. Similar to warm dark matter, fuzzy dark matter formed stars along filaments. But then quantum wave effects took over in shaping the galaxies, which formed more striated filaments, like strings on an invisible harp. Vogelsberger says this striated pattern is due to interference, an effect that occurs when two waves overlap. When this occurs, for instance in waves of light, the points where the crests and troughs of each wave align form darker spots, creating an alternating pattern of bright and dark regions. In the case of fuzzy dark matter, instead of bright and dark points, it generates an alternating pattern of over-dense and under-dense concentrations of dark matter. “You would get a lot of gravitational pull at these over-densities, and the gas would follow, and at some point would form galaxies along those over-densities, and not the under-densities,” Vogelsberger explains. “This picture would be replicated throughout the early universe.” The team is developing more detailed predictions of what early galaxies may have looked like in a universe dominated by fuzzy dark matter. Their goal is to provide a map for upcoming telescopes, such as the James Webb Space Telescope, that may be able to look far enough back in time to spot the earliest galaxies. If they see filamentary galaxies such as those simulated by Mocz, Fialkov, Vogelsberger, and their colleagues, it could be the first signs that dark matter’s nature is fuzzy. “It’s this observational test we can provide for the nature of dark matter, based on observations of the early universe, which will become feasible in the next couple of years,” Vogelsberger says. This research was supported, in part, by NASA. "
MIT News,System helps smart devices find their position,Research,2019-10-02,-,http://news.mit.edu/2019/iot-smart-device-position-1003,"  A new system developed by researchers at MIT and elsewhere helps networks of smart devices cooperate to find their positions in environments where GPS usually fails. Today, the “internet of things” concept is fairly well-known: Billions of interconnected sensors around the world — embedded in everyday objects, equipment, and vehicles, or worn by humans or animals — collect and share data for a range of applications. An emerging concept, the “localization of things,” enables those devices to sense and communicate their position. This capability could be helpful in supply chain monitoring, autonomous navigation, highly connected smart cities, and even forming a real-time “living map” of the world. Experts project that the localization-of-things market will grow to $128 billion by 2027. The concept hinges on precise localization techniques. Traditional methods leverage GPS satellites or wireless signals shared between devices to establish their relative distances and positions from each other. But there’s a snag: Accuracy suffers greatly in places with reflective surfaces, obstructions, or other interfering signals, such as inside buildings, in underground tunnels, or in “urban canyons” where tall buildings flank both sides of a street. Researchers from MIT, the University of Ferrara, the Basque Center of Applied Mathematics (BCAM), and the University of Southern California have developed a system that captures location information even in these noisy, GPS-denied areas. A paper describing the system appears in the Proceedings of the IEEE. When devices in a network, called “nodes,” communicate wirelessly in a signal-obstructing, or “harsh,” environment, the system fuses various types of positional information from dodgy wireless signals exchanged between the nodes, as well as digital maps and inertial data. In doing so, each node considers information associated with all possible locations — called “soft information” — in relation to those of all other nodes. The system leverages machine-learning techniques and techniques that reduce the dimensions of processed data to determine possible positions from measurements and contextual data. Using that information, it then pinpoints the node’s position. In simulations of harsh scenarios, the system operates significantly better than traditional methods. Notably, it consistently performed near the theoretical limit for localization accuracy. Moreover, as the wireless environment got increasingly worse, traditional systems’ accuracy dipped dramatically while the new soft information-based system held steady. “When the tough gets tougher, our system keeps localization accurate,” says Moe Win, a professor in the Department of Aeronautics and Astronautics and the Laboratory for Information and Decision Systems (LIDS), and head of the Wireless Information and Network Sciences Laboratory. “In harsh wireless environments, you have reflections and echoes that make it far more difficult to get accurate location information. Places like the Stata Center [on the MIT campus] are particularly challenging, because there are surfaces reflecting signals everywhere. Our soft information method is particularly robust in such harsh wireless environments.” Joining Win on the paper are: Andrea Conti of the University of Ferrara; Santiago Mazuelas of BCAM; Stefania Bartoletti of the University of Ferrara; and William C. Lindsey of the University of Southern California. Capturing “soft information” In network localization, nodes are generally referred to as anchors or agents. Anchors are nodes with known positions, such as GPS satellites or wireless base stations. Agents are nodes that have unknown positions — such as autonomous cars, smartphones, or wearables. To localize, agents can use anchors as reference points, or they can share information with other agents to orient themselves. That involves transmitting wireless signals, which arrive at the receiver carrying positional information. The power, angle, and time-of-arrival of the received waveform, for instance, correlate to the distance and orientation between nodes. Traditional localization methods extract one feature of the signal to estimate a single value for, say, the distance or angle between two nodes. Localization accuracy relies entirely on the accuracy of those inflexible (or “hard”) values, and accuracy has been shown to decrease drastically as environments get harsher. Say a node transmits a signal to another node that’s 10 meters away in a building with many reflective surfaces. The signal may bounce around and reach the receiving node at a time corresponding to 13 meters away. Traditional methods would likely assign that incorrect distance as a value. For the new work, the researchers decided to try using soft information for localization. The method leverages many signal features and contextual information to create a probability distribution of all possible distances, angles, and other metrics. “It’s called ‘soft information’ because we don’t make any hard choices about the values,” Conti says. The system takes many sample measurements of signal features, including its power, angle, and time of flight. Contextual data come from external sources, such as digital maps and models that capture and predict how the node moves. Back to the previous example: Based on the initial measurement of the signal’s time of arrival, the system still assigns a high probability that the nodes are 13 meters apart. But it assigns a small possibility that they’re 10 meters apart, based on some delay or power loss of the signal. As the system fuses all other information from surrounding nodes, it updates the likelihood for each possible value. For instance, it could ping a map and see that the room’s layout shows it’s highly unlikely both nodes are 13 meters apart. Combining all the updated information, it decides the node is far more likely to be in the position that is 10 meters away. “In the end, keeping that low-probability value matters,” Win says. “Instead of giving a definite value, I’m telling you I’m really confident that you’re 13 meters away, but there’s a smaller possibility you’re also closer. This gives additional information that benefits significantly in determining the positions of the nodes.” Reducing complexity Extracting many features from signals, however, leads to data with large dimensions that can be too complex and inefficient for the system. To improve efficiency, the researchers reduced all signal data into a reduced-dimension and easily computable space. To do so, they identified aspects of the received waveforms that are the most and least useful for pinpointing location based on “principal component analysis,” a technique that keeps the most useful aspects in multidimensional datasets and discards the rest, creating a dataset with reduced dimensions. If received waveforms contain 100 sample measurements each, the technique might reduce that number to, say, eight. A final innovation was using machine-learning techniques to learn a statistical model describing possible positions from measurements and contextual data. That model runs in the background to measure how that signal-bouncing may affect measurements, helping to further refine the system’s accuracy. The researchers are now designing ways to use less computation power to work with resource-strapped nodes that can’t transmit or compute all necessary information. They’re also working on bringing the system to “device-free” localization, where some of the nodes can’t or won’t share information. This will use information about how the signals are backscattered off these nodes, so other nodes know they exist and where they are located. "
MIT News,Using algorithms to build a map of the placenta,Research,2019-10-02,-,http://news.mit.edu/2019/using-algorithms-to-build-placenta-map-1002,"  The placenta is one of the most vital organs when a woman is pregnant. If it’s not working correctly, the consequences can be dire: Children may experience stunted growth and neurological disorders, and their mothers are at increased risk of blood conditions like preeclampsia, which can impair kidney and liver function.  Unfortunately, assessing placental health is difficult because of the limited information that can be gleaned from imaging. Traditional ultrasounds are cheap, portable, and easy to perform, but they can’t always capture enough detail. This has spurred researchers to explore the potential of magnetic resonance imaging (MRI). Even with MRIs, though, the curved surface of the uterus makes images difficult to interpret. This problem got the attention of a team of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), who wondered whether the placenta’s scrunched shape could be flattened out using some fancy geometry. Next month they’re publishing a paper showing that it can. Their new algorithm unfolds images from MRI scans to better visualize the organ. For example, their images more clearly show the “cotyledons,” circular structures that allow for the exchange of nutrients between the mother and her developing child or children. Being able to visualize such structures could allow doctors to diagnose and treat placental issues much earlier in the pregnancy.  “The idea is to unfold the image of the placenta while it’s in the body, so that it looks similar to how doctors are used to seeing it after delivery,” says PhD student Mazdak Abulnaga, lead author of the new paper with MIT professors Justin Solomon and Polina Golland. “While this is just a first step, we think an approach like this has the potential to become a standard imaging method for radiologists.”  Golland says that the algorithm could also be used in clinical research to find specific  biomarkers associated with poor placental health. Such research could help radiologists save time and more accurately locate problem areas without having to examine many different slices of the placenta. Chris Kroenke, an associate professor at Oregon Health and Science University, says that the project opens up many new possibilities for monitoring placental health.  “The biological processes that underlie cotyledon patterning are not completely understood, nor is it known whether a standard pattern should be expected for a given population,” says Kroenke, who was not involved in the paper. “The tools provided by this work will certainly aid researchers to address these questions in the future.”        Abulnaga, Solomon, and Golland co-wrote the paper with former CSAIL postdoc Mikhail Bessmeltsev and their collaborators, Esra Abaci Turk and P. Ellen Grant of Boston Children’s Hospital (BCH). Grant is the director of BCH’s Fetal-Neonatal Neuroimaging and Development Science Center, and a professor of radiology and pediatrics at Harvard Medical School. The team also worked closely with collaborators at Massachusetts General Hospital (MGH) and MIT Professor Elfar Adalsteinsson. The paper will be presented Oct. 14 in Shenzhen, China, at the International Conference on Medical Image Computing and Computer-Assisted Intervention.  The team’s algorithm first models the placenta’s shape by subdividing it into thousands of tiny pyramids, or tetrahedra. This serves an efficient representation for computers to perform operations to manipulate the shape. The algorithm then arranges those pyramids into a template that resembles the flattened shape that a placenta holds once it’s out of the body. (The algorithm does this by essentially moving the corners of the pyramids on the surface of the placenta to match the two parallel planes of the template and letting the rest fill the new shape.) The model has to make a tradeoff between the pyramids matching the shape of the template and minimizing the amount of distortion. The team showed the system can ultimately achieve accuracy at the scale of less than one voxel (a 3-D pixel).  The project is far from the first aimed at improving medical imaging by actually manipulating said images. There have been recent efforts to unfold scans of ribs, and researchers have also spent many years developing ways to flatten images of the brain’s cerebral cortex to better visualize areas between the folds. Meanwhile, work involving the womb is much newer. Previous approaches to this problem focused on flattening different layers of the placenta separately. The team says that they feel that the new volumetric method results in more consistency and less distortion because it maps the whole 3-D placenta at once, enabling it to more closely model the physical unfolding process. “The team’s work provides a very elegant tool to address the issue of the placenta’s irregular shape being difficult to image,” says Kroenke.  As a next step, the team hopes to work with MGH and BCH to directly compare in-utero images with ones of the same placentas post-birth. Because the placenta loses fluid and changes shape during the birth process, this will require using a special chamber designed by MGH and BCH where researchers can put the placenta directly after the birth. The source code for the project is available on github. The work was supported in part by the National Institute of Child Health and Human Development, the National Institute of Biomedical Imaging and Bioengineering, the National Science Foundation, the U.S. Air Force, and the Natural Sciences and Engineering Research Council of Canada. "
MIT News,“Electroadhesive” stamp picks up and puts down microscopic structures,Nanotech,2019-10-11,-,http://news.mit.edu/2019/electroadhesive-stamp-microscopic-manufacturing-1011,"  If you were to pry open your smartphone, you would see an array of electronic chips and components laid out across a circuit board, like a miniature city. Each component might contain even smaller “chiplets,” some no wider than a human hair. These elements are often assembled with robotic grippers designed to pick up the components and place them down in precise configurations. As circuit boards are packed with ever smaller components, however, robotic grippers’ ability to manipulate these objects is approaching a limit.   “Electronics manufacturing requires handling and assembling small components in a size similar to or smaller than grains of flour,” says Sanha Kim, a former MIT postdoc and research scientist who worked in the lab of mechanical engineering associate professor John Hart. “So a special pick-and-place solution is needed, rather than simply miniaturizing [existing] robotic grippers and vacuum systems.” Now Kim, Hart, and others have developed a miniature “electroadhesive” stamp that can pick up and place down objects as small as 20 nanometers wide — about 1,000 times finer than a human hair. The stamp is made from a sparse forest of ceramic-coated carbon nanotubes arranged like bristles on a tiny brush. When a small voltage is applied to the stamp, the carbon nanotubes become temporarily charged, forming prickles of electrical attraction that can attract a minute particle. By turning the voltage off, the stamp’s “stickiness” goes away, enabling it to release the object onto a desired location. Hart says the stamping technique can be scaled up to a manufacturing setting to print micro- and nanoscale features, for instance to pack more elements onto ever smaller computer chips. The technique may also be used to pattern other small, intricate features, such as cells for artificial tissues. And, the team envisions macroscale, bioinspired electroadhesive surfaces, such as voltage-activated pads for grasping everyday objects and for gecko-like climbing robots. “Simply by controlling voltage, you can switch the surface from basically having zero adhesion to pulling on something so strongly, on a per unit area basis, that it can act somewhat like a gecko’s foot,” Hart says. The team has published its results today in the journal Science Advances. Like dry Scotch tape Existing mechanical grippers are unable to pick up objects smaller than about 50 to 100 microns, mainly because at smaller scales surface forces tend to win over gravity. You may see this when pouring flour from a spoon — inevitably, some tiny particles stick to the spoon’s surface, rather than letting gravity drag them off. “The dominance of surface forces over gravity forces becomes a problem when trying to precisely place smaller things — which is the foundational process by which electronics are assembled into integrated systems,” Hart says. He and his colleagues noted that electroadhesion, the process of adhering materials via an applied voltage, has been used in some industrial settings to pick and place large objects, such as fabrics, textiles, and whole silicon wafers. But this same electroadhesion had never been applied to objects at the microscopic level, because a new material design for controlling electroadhesion at smaller scales was needed. Hart’s group has previously worked with carbon nanotubes (CNTs) — atoms of carbon linked in a lattice pattern and rolled into microscopic tubes. CNTs are known for their exceptional mechanical, electrical, and chemical properties, and they have been widely studied as dry adhesives. “Previous work on CNT-based dry adhesives focused on maximizing the contact area of the nanotubes to essentially create a dry Scotch tape,” Hart says. “We took the opposite approach, and said, ‘let’s design a nanotube surface to minimize the contact area, but use electrostatics to turn on adhesion when we need it.’”  New electroadhesive stamp picks and places a 170-micrometer sized LED chiplet, using an external voltage of 30V to temporarily “stick” to the LED. Courtesy of the researchers A sticky on/off switch The team found that if they coated CNTs with a thin dielectric material such as aluminum oxide, when they applied a voltage to the nanotubes, the ceramic layer became polarized, meaning its positive and negative charges became temporarily separated. For instance, the positive charges of the tips of the nanotubes induced an opposite polarization in any nearby conducting material, such as a microscopic electronic element. As a result, the nanotube-based stamp adhered to the element, picking it up like tiny, electrostatic fingers. When the researchers turned the voltage off, the nanotubes and the element depolarized, and the “stickiness” went away, allowing the stamp to detach and place the object onto a given surface. The team explored various formulations of stamp designs, altering the density of carbon nanotubes grown on the stamp, as well as the thickness of the ceramic layer that they used to coat each nanotube. They found that the thinner the ceramic layer and the more sparsely spaced the carbon nanotubes were, the greater the stamp’s on/off ratio, meaning the greater the stamp’s stickiness was when the voltage was on, versus when it was off. In their experiments, the team used the stamp to pick up and place down films of nanowires, each about 1,000 times thinner than a human hair. They also used the technique to pick and place intricate patterns of polymer and metal microparticles, as well as micro-LEDs. Hart says the electroadhesive printing technology could be scaled up to manufacture circuit boards and systems of miniature electronic chips, as well as displays with microscale LED pixels. “With ever-advancing capabilities of semiconductor devices, an important need and opportunity is to integrate smaller and more diverse components, such as microprocessors, sensors, and optical devices,” Hart says. “Often, these are necessarily made separately but must be integrated together to create next-generation electronic systems. Our technology possibly bridges the gap necessary for scalable, cost-effective assembly of these systems.” This research was supported in part by the Toyota Research Insititute, the National Science Foundation, and the MIT-Skoltech Next Generation Program. "
MIT News,Diagnosing cellular nanomechanics,Nanotech,2019-10-07,-,http://news.mit.edu/2019/smart-mit-diagnosing-cellular-nanomechanics-1007,"  Researchers at Singapore-MIT Alliance for Research and Technology (SMART) and MIT’s Laser Biomedical Research Center (LBRC) have developed a new way to study cells, paving the way for a better understanding of how cancers spread and become killers. The new technology is explained in a paper published recently in Nature Communications. A new confocal reflectance interferometric microscope provides 1.5 microns depth resolution and better than 200 picometers height measurement sensitivity for high-speed characterization of nanometer-scale nucleic envelope and plasma membrane fluctuations in biological cells. It enables researchers to use these fluctuations to understand key biological questions, such as the role of nuclear stiffness in cancer metastasis and genetic diseases. “Current methods for nuclear mechanics are invasive, as they either require mechanical manipulation, such as stretching, or require injecting fluorescent probes that ‘light up’ the nucleus to observe its shape. Both these approaches would undesirably change cells' intrinsic properties, limiting study of cellular mechanisms, disease diagnosis, and cell-based therapies,” say Vijay Raj Singh, SMART research scientist, and Zahid Yaqoob, LBRC principal investigator. “With the confocal reflectance interferometric microscope, we can study nuclear mechanics of biological cells without affecting their native properties.” While the scientists can study about a hundred cells in a few minutes, they believe that the system can be upgraded in the future to improve the throughput to tens of thousands of cells. “Today, many disease mechanisms are not fully understood because we lack a way to look at how cells’ nucleus changes when it undergoes stress,” says Peter So, SMART BioSyM principal investigator, MIT professor, and LBRC director. “For example, people often do not die from the primary cancer, but from the secondary cancers that form after the cancer cells metastasize from the primary site — and doctors do not know why cancer becomes aggressive and when it happens. Nuclear mechanics plays a vital role in cancer metastasis as the cancer cells must ‘squeeze’ through the blood vessel walls into the bloodstream, and again when they enter a new location. This is why the ability to study nuclear mechanics is so important to our understanding of cancer formation, diagnostics, and treatment.” With the new interferometric microscope, scientists at LBRC are studying cancer cells when they undergo mechanical stress, especially during extravasation process, paving the way for new cancer treatments. Further, the scientists are also able to use the same technology to study the effect of “lamin mutations” on nuclear mechanics, which result in rare genetic diseases such as Progeria, which leads to fast aging in young children. The confocal reflectance interferometric microscope also has applications in other sectors. For example, this technology has the potential for studying cellular mechanics within intact living tissues. With the new technology, the scientists could shed new light on biological processes within the body’s major organs such as liver, allowing safer and more accurate cell therapies. Cell therapy is a major focus area for Singapore, with the government recently announcing a S$80 million (US $58 million) boost to the manufacturing of living cells as medicine. About BioSyM BioSystems and Micromechanics (BioSyM) Inter-Disciplinary Research Group brings together a multidisciplinary team of faculties and researchers from MIT and the universities and research institutes of Singapore. BioSyM’s research deals with the development of new technologies to address critical medical and biological questions applicable to a variety of diseases with an aim to provide novel solutions to the health care industry and to the broader research infrastructure in Singapore. The guiding tenet of BioSyM is that accelerated progress in biology and medicine will critically depend upon the development of modern analytical methods and tools that provide a deep understanding of the interactions between mechanics and biology at multiple length scales — from molecules to cells to tissues — that impact maintenance or disruption of human health. About Singapore-MIT Alliance for Research and Technology (SMART) Singapore-MIT Alliance for Research and Technology (SMART) is MIT’s research enterprise in Singapore, established in partnership with the National Research Foundation of Singapore (NRF) since 2007. SMART is the first entity in the Campus for Research Excellence and Technological Enterprise (CREATE) developed by NRF. SMART serves as an intellectual and innovation hub for research interactions between MIT and Singapore. Cutting-edge research projects in areas of interest to both Singapore and MIT are undertaken at SMART. SMART currently comprises an Innovation Centre and six Interdisciplinary Research Groups: Antimicrobial Resistance, BioSystems and Micromechanics, Critical Analytics for Manufacturing Personalized-Medicine, Disruptive & Sustainable Technologies for Agricultural Precision, Future Urban Mobility, and Low Energy Electronic Systems. SMART research is funded by the National Research Foundation Singapore under the CREATE program. About the Laser Biomedical Research Center (LBRC) Established in 1985, the Laser Biomedical Research Center is a National Research Resource Center supported by the National Institute of Biomedical Imaging and Bioengineering, a Biomedical Technology Resource Center of the National Institutes of Health. The LBRC’s mission is to develop the basic scientific understanding and new techniques required for advancing the clinical applications of lasers and spectroscopy. Researchers at the LBRC develop laser-based microscopy and spectroscopy techniques for medical applications, such as the spectral diagnosis of various diseases and investigation of biophysical and biochemical properties of cells and tissues. A unique feature of the LBRC is its ability to form strong clinical collaborations with outside investigators in areas of common interest that further the center’s mandated research objectives. "
MIT News,A new mathematical approach to understanding zeolites,Nanotech,2019-10-07,-,http://news.mit.edu/2019/identify-zeolites-transform-1007,"  Zeolites are a class of natural or manufactured minerals with a sponge-like structure, riddled with tiny pores that make them useful as catalysts or ultrafine filters. But of the millions of zeolite compositions that are theoretically possible, so far only about 248 have ever been discovered or made. Now, research from MIT helps explain why only this small subset has been found, and could help scientists find or produce more zeolites with desired properties. The new findings are being reported this week in the journal Nature Materials, in a paper by MIT graduate students Daniel Schwalbe-Koda and Zach Jensen, and professors Elsa Olivetti and Rafael Gomez-Bombarelli. Previous attempts to figure out why only this small group of possible zeolite compositions has been identified, and to explain why certain types of zeolites can be transformed into specific other types, have failed to come up with a theory that matches the observed data. Now, the MIT team has developed a mathematical approach to describing the different molecular structures. The approach is based on graph theory, which can predict which pairs of zeolite types can be transformed from one to the other. This could be an important step toward finding ways of making zeolites tailored for specific purposes. It could also lead to new pathways for production, since it predicts certain transformations that have not been previously observed. And, it suggests the possibility of producing zeolites that have never been seen before, since some of the predicted pairings would lead to transformations into new types of zeolite structures. Interzeolite tranformations Zeolites are widely used today in applications as varied as catalyzing the “cracking” of petroleum in refineries and absorbing odors as components in cat litterbox filler. Even more applications may become possible if researchers can create new types of zeolites, for example with pore sizes suited to specific types of filtration. All kinds of zeolites are silicate minerals, similar in chemical composition to quartz. In fact, over geological timescales, they will all eventually turn into quartz — a much denser form of the mineral — explains Gomez-Bombarelli, who is the Toyota Assistant Professor in Materials Processing. But in the meantime, they are in a “metastable” form, which can sometimes be transformed into a different metastable form by applying heat or pressure or both. Some of these transformations are well-known and already used to produce desired zeolite varieties from more readily available natural forms. Currently, many zeolites are produced by using chemical compounds known as OSDAs (organic structure-directing agents), which provide a kind of template for their crystallization. But Gomez-Bombarelli says that if instead they can be produced through the transformation of another, readily available form of zeolite, “that’s really exciting. If we don’t need to use OSDAs, then it’s much cheaper [to produce the material].The organic material is pricey. Anything we can make to avoid the organics gets us closer to industrial-scale production.” Traditional chemical modeling of the structure of different zeolite compounds, researchers have found, provides no real clue to finding the pairs of zeolites that can readily transform from one to the other. Compounds that appear structurally similar sometimes are not subject to such transformations, and other pairs that are quite dissimilar turn out to easily interchange. To guide their research, the team used an artificial intelligence system previously developed by the Olivetti group to “read” more than 70,000 research papers on zeolites and select those that specifically identify interzeolite transformations. They then studied those pairs in detail to try to identify common characteristics. What they found was that a topological description based on graph theory, rather than traditional structural modeling, clearly identified the relevant pairings. These graph-based descriptions, based on the number and locations of chemical bonds in the solids rather than their actual physical arrangement, showed that all the known pairings had nearly identical graphs. No such identical graphs were found among pairs that were not subject to transformation. The finding revealed a few previously unknown pairings, some of which turned out to match with preliminary laboratory observations that had not previously been identified as such, thus helping to validate the new model. The system also was successful at predicting which forms of zeolites can intergrow — forming combinations of two types that are interleaved like the fingers on two clasped hands. Such combinations are also commercially useful, for example for sequential catalysis steps using different zeolite materials. Ripe for further research   The new findings might also help explain why many of the theoretically possible zeolite formations don’t seem to actually exist. Since some forms readily transform into others, it may be that some of them transform so quickly that they are never observed on their own. Screening using the graph-based approach may reveal some of these unknown pairings and show why those short-lived forms are not seen. Some zeolites, according to the graph model, “have no hypothetical partners with the same graph, so it doesn’t make sense to try to transform them, but some have thousands of partners” and thus are ripe for further research, Gomez-Bombarelli says. In principle, the new findings could lead to the development of a variety of new catalysts, tuned to the exact chemical reactions they are intended to promote. Gomez-Bombarelli says that almost any desired reaction could hypothetically find an appropriate zeolite material to promote it. “Experimentalists are very excited to find a language to describe their transformations that is predictive,” he says. This work is “a major advancement in the understanding of interzeolite transformations, which has become an increasingly important topic owing to the potential for using these processes to improve the efficiency and economics of commercial zeolite production,” says Jeffrey Rimer, an associate professor of chemical and biomolecular engineering at the University of Houston, who was not involved in this research. Manuel Moliner, a tenured scientist at the Technical University of Valencia, in Spain, who also was not connected to this research, says: “Understanding the pairs involved in particular interzeolite transformations, considering not only known zeolites but also hundreds of hypothetical zeolites that have not ever been synthesized, opens extraordinary practical opportunities to rationalize and direct the synthesis of target zeolites with potential interest as industrial catalysts.” This research was supported, in part, by the National Science Foundation and the Office of Naval Research. "
MIT News,A new way to corrosion-proof thin atomic sheets,Nanotech,2019-10-04,-,http://news.mit.edu/2019/corrosion-proof-atomic-sheets-1004,"  A variety of two-dimensional materials that have promising properties for optical, electronic, or optoelectronic applications have been held back by the fact that they quickly degrade when exposed to oxygen and water vapor. The protective coatings developed thus far have proven to be expensive and toxic, and cannot be taken off. Now, a team of researchers at MIT and elsewhere has developed an ultrathin coating that is inexpensive, simple to apply, and can be removed by applying certain acids. The new coating could open up a wide variety of potential applications for these “fascinating” 2D materials, the researchers say. Their findings are reported this week in the journal PNAS, in a paper by MIT graduate student Cong Su; professors Ju Li, Jing Kong, Mircea Dinca, and Juejun Hu; and 13 others at MIT and in Australia, China, Denmark, Japan, and the U.K. Research on 2D materials, which form thin sheets just one or a few atoms thick, is “a very active field,” Li says. Because of their unusual electronic and optical properties, these materials have promising applications, such as highly sensitive light detectors. But many of them, including black phosphorus and a whole category of materials known as transition metal dichalcogenides (TMDs), corrode when exposed to humid air or to various chemicals. Many of them degrade significantly in just hours, precluding their usefulness for real-world applications. “It’s a key issue” for the development of such materials, Li says. “If you cannot stabilize them in air, their processability and usefulness is limited.” One reason silicon has become such a ubiquitous material for electronic devices, he says, is because it naturally forms a protective layer of silicon dioxide on its surface when exposed to air, preventing further degradation of the surface. But that’s more difficult with these atomically thin materials, whose total thickness could be even less than the silicon dioxide protective layer. There have been attempts to coat various 2D materials with a protective barrier, but so far they have had serious limitations. Most coatings are much thicker than the 2D materials themselves. Most are also very brittle, easily forming cracks that let through the corroding liquid or vapor, and many are also quite toxic, creating problems with handling and disposal. The new coating, based on a family of compounds known as linear alkylamines, improves on these drawbacks, the researchers say. The material can be applied in ultrathin layers, as little as 1 nanometer (a billionth of a meter) thick, and further heating of the material after application heals tiny cracks to form a contiguous barrier. The coating is not only impervious to a variety of liquids and solvents but also significantly blocks the penetration of oxygen. And, it can be removed later if needed by certain organic acids. “This is a unique approach” to protecting thin atomic sheets, Li says, that produces an extra layer just a single molecule thick, known as a monolayer, that provides remarkably durable protection. “This gives the material a factor of 100 longer lifetime,” he says, extending the processability and usability of some of these materials from a few hours up to months. And the coating compound is “very cheap and easy to apply,” he adds. In addition to theoretical modeling of the molecular behavior of these coatings, the team made a working photodetector from flakes of TMD material protected with the new coating, as a proof of concept. The coating material is hydrophobic, meaning that it strongly repels water, which otherwise would diffuse into the coating and dissolve away a naturally formed protective oxide layer within the coating, leading to rapid corrosion. The application of the coating is a very simple process, Su explains. The 2D material is simply placed into bath of liquid hexylamine, a form of the linear alkylamine, which builds up the protective coating after about 20 minutes, at a temperature of 130 degrees Celsius at normal pressure. Then, to produce a smooth, crack-free surface, the material is immersed for another 20 minutes in vapor of the same hexylamine. “You just put the wafer into this liquid chemical and let it be heated,” Su says. “Basically, that’s it.” The coating “is pretty stable, but it can be removed by certain very specific organic acids.” The use of such coatings could open up new areas of research on promising 2D materials, including the TMDs and black phosphorous, but potentially also silicene, stanine, and other related materials. Since black phosphorous is the most vulnerable and easily degraded of all these materials, that’s what the team used for their initial proof of concept. The new coating could provide a way of overcoming “the first hurdle to using these fascinating 2D materials,” Su says. “Practically speaking, you need to deal with the degradation during processing before you can use these for any applications,” and that step has now been accomplished, he says. The team included researchers in MIT’s departments of Nuclear Science and Engineering, Chemistry, Materials Science and Engineering, Electrical Engineering and Computer Science, and the Research Laboratory of Electronics, as well as others at the Australian National University, the University of Chinese Academy of Sciences, Aarhus University in Denmark, Oxford University, and Shinshu University in Japan. The work was supported by the Center for Excitonics and the Energy Frontier Research Center funded by the U.S. Department of Energy, and by the National Science Foundation, the Chinese Academy of Sciences, the Royal Society, the U.S. Army Research Office through the MIT Institute for Soldier Nanotechnologies, and Tohoku University. "
MIT News,Controlling 2-D magnetism with stacking order,Nanotech,2019-09-30,-,http://news.mit.edu/2019/controlling-2d-magnetism-stacking-order-0930,"  Researchers led by MIT Department of Physics Professor Pablo Jarillo-Herrero last year showed that rotating layers of hexagonally structured graphene at a particular “magic angle” could change the material’s electronic properties from an insulating state to a superconducting state. Now researchers in the same group and their collaborators have demonstrated that in a different ultra-thin material that also features a honeycomb-shaped atomic structure — chromium trichloride (CrCl3) — they can alter the material’s magnetic properties by shifting the stacking order of layers. The researchers peeled away two-dimensional (2-D) layers of chromium trichloride using tape in the same way researchers peel away graphene from graphite. Then they studied the 2-D chromium trichloride’s magnetic properties using electron tunneling. They found that the magnetism is different in 2-D and 3-D crystals due to different stacking arrangements between atoms in adjacent layers. At high temperatures, each chromium atom in chromium trichloride has a magnetic moment that fluctuates like a tiny compass needle. Experiments show that as the temperature drops below 14 kelvins (-434.47 degrees Fahrenheit), deep in the cryogenic temperature range, these magnetic moments freeze into an ordered pattern, pointing in opposite directions in alternating layers (antiferromagnetism). The magnetic direction of all the layers of chromium trichloride can be aligned by applying a magnetic field. But the researchers found that in its 2-D form, this alignment needs a magnetic force 10 times stronger than in the 3-D crystal. The results were recently published online in Nature Physics. “What we’re seeing is that it’s 10 times harder to align the layers in the thin limit compared to the bulk, which we measure using electron tunneling in a magnetic field,” says MIT physics graduate student Dahlia R. Klein, a National Science Foundation graduate research fellow and one of the paper’s lead authors. Physicists call the energy required to align the magnetic direction of opposing layers the interlayer exchange interaction. “Another way to think of it is that the interlayer exchange interaction is how much the adjacent layers want to be anti-aligned,” fellow lead author and MIT postdoc David MacNeill suggests. The researchers attribute this change in energy to the slightly different physical arrangement of the atoms in 2-D chromium chloride. “The chromium atoms form a honeycomb structure in each layer, so it’s basically stacking the honeycombs in different ways,” Klein says. “The big thing is we’re proving that the magnetic and stacking orders are very strongly linked in these materials.” ""Our work highlights how the magnetic properties of 2-D magnets can differ very substantially from their 3-D counterparts,” says senior author Pablo Jarillo-Herrero, the Cecil and Ida Green Professor of Physics. “This means that we have now a new generation of highly tunable magnetic materials, with important implications for both new fundamental physics experiments and potential applications in spintronics and quantum information technologies."" Layers are very weakly coupled in these materials, known as van der Waals magnets, which is what makes it easy to remove a layer from the 3-D crystal with adhesive tape. “Just like with graphene, the bonds within the layers are very strong, but there are only very weak interactions between adjacent layers, so you can isolate few-layer samples using tape,” Klein says. MacNeill and Klein grew the chromium chloride samples, built and tested nanoelectronic devices, and analyzed their results. The researchers also found that as chromium trichloride is cooled from room temperature to cryogenic temperatures, 3-D crystals of the material undergo a structural transition that the 2-D crystals do not. This structural difference accounts for the higher energy required to align the magnetism in the 2-D crystals. The researchers measured the stacking order of 2-D layers through the use of Raman spectroscopy and developed a mathematical model to explain the energy involved in changing the magnetic direction. Co-author and Harvard University postdoc Daniel T. Larson says he analyzed a plot of Raman data that showed variations in peak location with the rotation of the chromium trichloride sample, determining that the variation was caused by the stacking pattern of the layers. “Capitalizing on this connection, Dahlia and David have been able to use Raman spectroscopy to learn details about the crystal structure of their devices that would be very difficult to measure otherwise,” Larson explains. “I think this technique will be a very useful addition to the toolbox for studying ultra-thin structures and devices.” Department of Materials Science and Engineering graduate student Qian Song carried out the Raman spectroscopy experiments in the lab of MIT assistant professor of physics Riccardo Comin. Both also are co-authors of the paper. “This research really highlights the importance of stacking order on understanding how these van der Waals magnets behave in the thin limit,” Klein says. MacNeill adds, “The question of why the 2-D crystals have different magnetic properties had been puzzling us for a long time. We were very excited to finally understand why this is happening, and it’s because of the structural transition.” This work builds on two years of prior research into 2-D magnets in which Jarillo-Herrero’s group collaborated with researchers at the University of Washington, led by Professor Xiaodong Xu, who holds joint appointments in the departments of Materials Science and Engineering, Physics, and Electrical and Computer Engineering, and others. Their work, which was published in a Nature letter in June 2017, showed for the first time that a different material with a similar crystal structure — chromium triiodide (CrI3) — also behaved differently in the 2-D form than in the bulk, with few-layer samples showing antiferromagnetism unlike the ferromagnetic 3-D crystals. Jarillo-Herrero’s group went on to show in a May 2018 Science paper that chromium triiodide exhibited a sharp change in electrical resistance in response to an applied magnetic field at low temperature. This work demonstrated that electron tunneling is a useful probe for studying magnetism of 2-D crystals. Klein and MacNeill were also the first authors of this paper. University of Washington Professor Xiaodong Xu says of the latest findings, “The work presents a very clever approach, namely the combined tunneling measurements with polarization resolved Raman spectroscopy. The former is sensitive to the interlayer antiferromagnetism, while the latter is a sensitive probe of crystal symmetry. This approach gives a new method to allow others in the community to uncover the magnetic properties of layered magnets.” “This work is in concert with several other recently published works,” Xu says. “Together, these works uncover the unique opportunity provided by layered van der Waals magnets, namely engineering magnetic order via controlling stacking order. It is useful for arbitrary creation of new magnetic states, as well as for potential application in reconfigurable magnetic devices.” Other authors contributing to this work include Efthimious Kaxiras, the John Hasbrouck Van Vleck Professor of Pure and Applied Physics at Harvard University; Harvard graduate student Shiang Fang; Iowa State University Distinguished Professor (Condensed Matter Physics) Paul C. Canfield; Iowa State graduate student Mingyu Xu; and Raquel A. Ribeiro, of Iowa State University and the Federal University of ABC, Santo André, Brazil. This work was supported in part by the Center for Integrated Quantum Materials, the U.S. Department of Energy Office of Science Basic Energy Sciences Program, the Gordon and Betty Moore Foundation’s EPiQS Initiative, and the Alfred P. Sloan Foundation. "
MIT News,MIT.nano awards inaugural NCSOFT seed grants for gaming technologies,Nanotech,2019-09-30,-,http://news.mit.edu/2019/mitnano-awards-inaugural-ncsoft-seed-grants-gaming-technologies-0930,"  MIT.nano has announced the first recipients of NCSOFT seed grants to foster hardware and software innovations in gaming technology. The grants are part of the new MIT.nano Immersion Lab Gaming program, with inaugural funding provided by video game developer NCSOFT, a founding member of the MIT.nano Consortium. The newly awarded projects address topics such as 3-D/4-D data interaction and analysis, behavioral learning, fabrication of sensors, light field manipulation, and micro-display optics.  “New technologies and new paradigms of gaming will change the way researchers conduct their work by enabling immersive visualization and multi-dimensional interaction,” says MIT.nano Associate Director Brian W. Anthony. “This year’s funded projects highlight the wide range of topics that will be enhanced and influenced by augmented and virtual reality.” In addition to the sponsored research funds, each awardee will be given funds specifically to foster a community of collaborative users of MIT.nano’s Immersion Lab. The MIT.nano Immersion Lab is a new, two-story immersive space dedicated to visualization, augmented and virtual reality (AR/VR), and the depiction and analysis of spatially related data. Currently being outfitted with equipment and software tools, the facility will be available starting this semester for use by researchers and educators interested in using and creating new experiences, including the seed grant projects.  The five projects to receive NCSOFT seed grants are: Stefanie Mueller: connecting the virtual and physical world Virtual game play is often accompanied by a prop — a steering wheel, a tennis racket, or some other object the gamer uses in the physical world to create a reaction in the virtual game. Build-it-yourself cardboard kits have expanded access to these props by lowering costs; however, these kits are pre-cut, and thus limited in form and function. What if users could build their own dynamic props that evolve as they progress through the game? Department of Electrical Engineering and Computer Science (EECS) Professor Stefanie Mueller aims to enhance the user’s experience by developing a new type of gameplay with tighter virtual-physical connection. In Mueller’s game, the player unlocks a physical template after completing a virtual challenge, builds a prop from this template, and then, as the game progresses, can unlock new functionalities to that same item. The prop can be expanded upon and take on new meaning, and the user learns new technical skills by building physical prototypes. Luca Daniel and Micha Feigin-Almon: replicating human movements in virtual characters Athletes, martial artists, and ballerinas share the ability to move their body in an elegant manner that efficiently converts energy and minimizes injury risk. Professor Luca Daniel, EECS and Research Laboratory of Electronics, and Micha Feigin-Almon, research scientist in mechanical engineering, seek to compare the movements of trained and untrained individuals to learn the limits of the human body with the goal of generating elegant, realistic movement trajectories for virtual reality characters. In addition to use in gaming software, their research on different movement patterns will predict stresses on joints, which could lead to nervous system models for use by artists and athletes. Wojciech Matusik: using phase-only holograms Holographic displays are optimal for use in augmented and virtual reality. However, critical issues show a need for improvement. Out-of-focus objects look unnatural, and complex holograms have to be converted to phase-only or amplitude-only in order to be physically realized. To combat these issues, EECS Professor Wojciech Matusik proposes to adopt machine learning techniques for synthesis of phase-only holograms in an end-to-end fashion. Using a learning-based approach, the holograms could display visually appealing three-dimensional objects. “While this system is specifically designed for varifocal, multifocal, and light field displays, we firmly believe that extending it to work with holographic displays has the greatest potential to revolutionize the future of near-eye displays and provide the best experiences for gaming,” says Matusik. Fox Harrell: teaching socially impactful behavior Project VISIBLE — Virtuality for Immersive Socially Impactful Behavioral Learning Enhancement — utilizes virtual reality in an educational setting to teach users how to recognize, cope with, and avoid committing microaggressions. In a virtual environment designed by Comparative Media Studies Professor Fox Harrell, users will encounter micro-insults, followed by major micro-aggression themes. The user’s physical response drives the narrative of the scenario, so one person can play the game multiple times and reach different conclusions, thus learning the various implications of social behavior. Juejun Hu: displaying a wider field of view in high resolution Professor Juejun Hu from the Department of Materials Science and Engineering seeks to develop high-performance, ultra-thin immersive micro-displays for AR/VR applications. These displays, based on metasurface optics, will allow for a large, continuous field of view, on-demand control of optical wavefronts, high-resolution projection, and a compact, flat, lightweight engine. While current commercial waveguide AR/VR systems offer less than 45 degrees of visibility, Hu and his team aim to design a high-quality display with a field of view close to 180 degrees. "
MIT News,Quantum sensing on a chip,Nanotech,2019-09-25,-,http://news.mit.edu/2019/quantum-sensing-chip-0925,"  MIT researchers have, for the first time, fabricated a diamond-based quantum sensor on a silicon chip. The advance could pave the way toward low-cost, scalable hardware for quantum computing, sensing, and communication. “Nitrogen-vacancy (NV) centers” in diamonds are defects with electrons that can be manipulated by light and microwaves. In response, they emit colored photons that carry quantum information about surrounding magnetic and electric fields, which can be used for biosensing, neuroimaging, object detection, and other sensing applications. But traditional NV-based quantum sensors are about the size of a kitchen table, with expensive, discrete components that limit practicality and scalability. In a paper published in Nature Electronics, the researchers found a way to integrate all those bulky components — including a microwave generator, optical filter, and photodetector — onto a millimeter-scale package, using traditional semiconductor fabrication techniques. Notably, the sensor operates at room temperature with capabilities for sensing the direction and magnitude of magnetic fields. The researchers demonstrated the sensor’s use for magnetometry, meaning they were able to measure atomic-scale shifts in the frequency due to surrounding magnetic fields, which could contain information about the environment. With further refining, the sensor could have a range of applications, from mapping electrical impulses in the brain to detecting objects, even without a line of sight. “It’s very difficult to block magnetic fields, so that’s a huge advantage for quantum sensors,” says co-author Christopher Foy, a graduate student in the Department of Electrical Engineering and Computer Science (EECS). “If there’s a vehicle traveling in, say, an underground tunnel below you, you’d be able to detect it even if you don’t see it there.” Joining Foy on the paper are: Mohamed Ibrahim, a graduate student in EECS; Donggyu Kim PhD ’19; Matthew E. Trusheim, a postdoc in EECS; Ruonan Han, an associate professor in EECS and head of the Terahertz Integrated Electronics Group, which is part of MIT's Microsystems Technology Laboratories (MTL); and Dirk Englund, an MIT associate professor of electrical engineering and computer science, a researcher in Research Laboratory of Electronics (RLE), and head of the Quantum Photonics Laboratory. Shrinking and stacking NV centers in diamonds occur where carbon atoms in two adjacent places in the lattice structure are missing — one atom is replaced by a nitrogen atom, and the other space is an empty “vacancy.” That leaves missing bonds in the structure, where the electrons are extremely sensitive to tiny variations in electrical, magnetic, and optical characteristics in the surrounding environment. The NV center essentially functions as an atom, with a nucleus and surrounding electrons. It also has photoluminescent properties, meaning it absorbs and emits colored photons. Sweeping microwaves across the center can make it change states — positive, neutral, and negative — which in turn changes the spin of its electrons. Then, it emits different amounts of red photons, depending on the spin. A technique, called optically detected magnetic resonance (ODMR), measures how many photons are emitted by interacting with the surrounding magnetic field. That interaction produces further, quantifiable information about the field. For all of that to work, traditional sensors require bulky components, including a mounted laser, power supply, microwave generator, conductors to route the light and microwaves, an optical filter and sensor, and a readout component. The researchers instead developed a novel chip architecture that positions and stacks tiny, inexpensive components in a certain way using standard complementary metal-oxide-semiconductor (CMOS) technology, so they function like those components. “CMOS technologies enable very complex 3-D structures on a chip,” Ibrahim says. “We can have a complete system on the chip, and we only need  a piece of diamond and green light source on top. But that can be a regular chip-scale LED.” NV centers within a diamond slab are positioned in a “sensing area” of the chip. A small green pump laser excites the NV centers, while a nanowire placed close to the NV centers generates sweeping microwaves in response to current. Basically, the light and microwave work together to make the NV centers emit a different amount of red photons — with the difference being the target signal for readout in the researchers’ experiments. Below the NV centers is a photodiode, designed to eliminate noise and measure the photons. In between the diamond and photodiode is a metal grating that acts as a filter that absorbs the green laser photons while allowing the red photons to reach the photodiode. In short, this enables an on-chip ODMR device, which measures resonance frequency shifts with the red photons that carry information about the surrounding magnetic field. But how can one chip do the work of a large machine? A key trick is simply moving the conducting wire, which produces the microwaves, at an optimal distance from the NV centers. Even if the chip is very small, this precise distance enables the wire current to generate enough magnetic field to manipulate the electrons. The tight integration and codesign of the microwave conducting wires and generation circuitry also help. In their paper, the researchers were able to generate enough magnetic field to enable practical applications in object detection. Only the beginning In another paper presented earlier this year at the International Solid-State Circuits Conference, the researchers describe a second-generation sensor that makes various improvements on this design to achieve 100-fold greater sensitivity. Next, the researchers say they have a “roadmap” for how to increase sensitivity by 1,000 times. That basically involves scaling up the chip to increase the density of the NV centers, which determines sensitivity. If they do, the sensor could be used even in neuroimaging applications. That means putting the sensor near neurons, where it can detect the intensity and direction of firing neurons. That could help researchers map connections between neurons and see which neurons trigger each other. Other future applications including a GPS replacement for vehicles and airplanes. Because the magnetic field on Earth has been mapped so well, quantum sensors can serve as extremely precise compasses, even in GPS-denied environments. “We’re only at the beginning of what we can accomplish,” Han says. “It’s a long journey, but we already have two milestones on the track, with the first-and second-generation sensors. We plan to go from sensing to communication to computing. We know the way forward and we know how to get there.” “I am enthusiastic about this quantum sensor technology and foresee major impact in several fields,” says Ron Walsworth, a senior lecturer at Harvard University whose group develops high-resolution magnetometry tools using NV centers. “They have taken a key step in the integration of quantum-diamond sensors with CMOS technology, including on-chip microwave generation and delivery, as well as on-chip filtering and detection of the information-carrying fluorescent light from the quantum defects in diamond. The resulting unit is compact and relatively low-power. Next steps will be to further enhance the sensitivity and bandwidth of the quantum diamond sensor [and] integrate the CMOS-diamond sensor with wide-ranging applications, including chemical analysis, NMR spectroscopy, and materials characterization.” "
MIT News,"3 Questions: Why sensing, why now, what next?",Nanotech,2019-09-20,-,http://news.mit.edu/2019/3-questions-why-sensing-why-now-brian-anthony-mit-0920,"  Sensors are everywhere today, from our homes and vehicles to medical devices, smart phones, and other useful tech. More and more, sensors help detect our interactions with the environment around us — and shape our understanding of the world. SENSE.nano is an MIT.nano Center of Excellence, with a focus on sensors, sensing systems, and sensing technologies. The 2019 SENSE.nano Symposium, taking place on Sept. 30 at MIT, will dive deep into the impact of sensors on two topics: sensing for augmented and virtual reality (AR/VR) and sensing for advanced manufacturing.  MIT Principal Research Scientist Brian W. Anthony is the associate director of MIT.nano and faculty director of the Industry Immersion Program in Mechanical Engineering. He weighs in on why sensing is ubiquitous and how advancements in sensing technologies are linked to the challenges and opportunities of big data. Q: What do you see as the next frontier for sensing as it relates to augmented and virtual reality? A: Sensors are an enabling technology for AR/VR. When you slip on a VR headset and enter an immersive environment, sensors map your movements and gestures to create a convincing virtual experience. But sensors have a role beyond the headset. When we're interacting with the real world we're constrained by our own senses — seeing, hearing, touching, and feeling. But imagine sensors providing data within AR/VR to enhance your understanding of the physical environment, such as allowing you to see air currents, thermal gradients, or the electricity flowing through wires superimposed on top of the real physical structure. That's not something you could do any place else other than a virtual environment. Another example: MIT.nano is a massive generator of data. Could AR/VR provide a more intuitive and powerful way to study information coming from the metrology instruments in the basement, or the fabrication tools in the clean room? Could it allow you to look at data on a massive scale, instead of always having to look under a microscope or on a flat screen that's the size of your laptop? Sensors are also critical for haptics, which are interactions related to the sensation of touch. As I apply pressure to a device or pick up an object — real or virtual — can I receive physical feedback that conveys that state of interaction to me? You can’t be an engineer or a scientist without being involved with sensing instrumentation in some way. Recognizing the widespread presence of sensing on campus, SENSE.nano and MIT.nano — with MIT.nano’s new Immersion Lab providing the tools and facility — are trying to bring together researchers on both the hardware and software sides to explore the future of these technologies. Q: Why is SENSE.nano focusing on sensing for advanced manufacturing? A: In this era of big data, we sometimes forget that data comes from someplace: sensors and instruments. As soon as the data industry as a whole has solved the big data challenges we have now with the data that's coming from current sensors — wearable physiological monitors, or from factories, or from your automobiles — it is going to be starved for new sensors with improved functionality. Coupled with that, there are a large number of manufacturing technologies — in the U.S. and worldwide — that are either coming to maturity or receiving a lot of investment. For example, researchers are looking at novel ways to make integrated photonics devices combining electronics and optics for on-chip sensors; exploring novel fiber manufacturing approaches to embed sensors into your clothing or composites; and developing flexible materials that mold to the body or to the shape of an automobile as the substrate for integrated circuits or as a sensor. These various manufacturing technologies enable us to think of new, innovative ways to create sensors that are lower in cost and more readily immersed into our environment. Q: You’ve said that a factory is not just a place that produces products, but also a machine that produces information. What does that mean? A: Today’s manufacturers have to approach a factory not just as a physical place, but also as a data center. Seeing physical operation and data as interconnected can improve quality, drive down costs, and increase the rate of production. And sensors and sensing systems are the tools to collect this data and improve the manufacturing process. Communications technologies now make it easy to transmit data from a machine to a central location. For example, we can apply sensing techniques to individual machines and then collect data across an entire factory so that information on how to debug one computer-controlled machine can be used to improve another in the same facility. Or, suppose I'm the producer of those machines and I've deployed them to any number of manufacturers. If I can get a little bit of information from each of my customers to optimize the machine’s operating performance, I can turn around and share improvements with all the companies who purchase my equipment. When information is shared amongst manufacturers, it helps all of them drive down their costs and improve quality.  "
MIT News,Uncovering the hidden “noise” that can kill qubits,Nanotech,2019-09-16,-,http://news.mit.edu/2019/non-gaussian-noise-detect-qubits-0916,"  MIT and Dartmouth College researchers have demonstrated, for the first time, a tool that detects new characteristics of environmental “noise” that can destroy the fragile quantum state of qubits, the fundamental components of quantum computers. The advance may provide insights into microscopic noise mechanisms to help engineer new ways of protecting qubits.   Qubits can represent the two states corresponding to the classic binary bits, a 0 or 1. But, they can also maintain a “quantum superposition” of both states simultaneously, enabling quantum computers to solve complex problems that are practically impossible for classical computers. But a qubit’s quantum “coherence” — meaning  its ability to maintain the superposition state — can fall apart due to noise coming from environment around the qubit. Noise can arise from control electronics, heat, or impurities in the qubit material itself, and can also cause serious computing errors that may be difficult to correct. Researchers have developed statistics-based models to estimate the impact of unwanted noise sources surrounding qubits to create new ways to protect them, and to gain insights into the noise mechanisms themselves. But, those tools generally capture simplistic “Gaussian noise,” essentially the collection of random disruptions from a large number of sources. In short, it’s like white noise coming from the murmuring of a large crowd, where there’s no specific disruptive pattern that stands out, so the qubit isn’t particularly affected by any one particular source. In this type of model, the probability distribution of the noise would form a standard symmetrical bell curve, regardless of the statistical significance of individual contributors. In a paper published today in the journal Nature Communications, the researchers describe a new tool that, for the first time, measures “non-Gaussian noise” affecting a qubit. This noise features distinctive patterns that generally stem from a few particularly strong noise sources. The researchers designed techniques to separate that noise from the background Gaussian noise, and then used signal-processing techniques to reconstruct highly detailed information about those noise signals. Those reconstructions can help researchers build more realistic noise models, which may enable more robust methods to protect qubits from specific noise types. There is now a need for such tools, the researchers say: Qubits are being fabricated with fewer and fewer defects, which could increase the presence of non-Gaussian noise. “It’s like being in a crowded room. If everyone speaks with the same volume, there is a lot of background noise, but I can still maintain my own conversation. However, if a few people are talking particularly loudly, I can’t help but lock on to their conversation. It can be very distracting,” says William Oliver, an associate professor of electrical engineering and computer science, professor of the practice of physics, MIT Lincoln Laboratory Fellow, and associate director of the Research Laboratory for Electronics (RLE). “For qubits with many defects, there is noise that decoheres, but we generally know how to handle that type of aggregate, usually Gaussian noise. However, as qubits improve and there are fewer defects, the individuals start to stand out, and the noise may no longer be simply of a Gaussian nature. We can find ways to handle that, too, but we first need to know the specific type of non-Gaussian noise and its statistics.” “It is not common for theoretical physicists to be able to conceive of an idea and also find an experimental platform and experimental colleagues willing to invest in seeing it through,” says co-author Lorenza Viola, a professor of physics at Dartmouth. “It was great to be able to come to such an important result with the MIT team.” Joining Oliver and Viola on the paper are: first author Youngkyu Sung, Fei Yan, Jack Y. Qiu, Uwe von Lüpke, Terry P. Orlando, and Simon Gustavsson, all of RLE; David K. Kim and Jonilyn L. Yoder of the Lincoln Laboratory; and Félix Beaudoin and Leigh M. Norris of Dartmouth. Pulse filters For their work, the researchers leveraged the fact that superconducting qubits are good sensors for detecting their own noise. Specifically, they use a “flux” qubit, which consists of a superconducting loop that is capable of detecting a particular type of disruptive noise, called magnetic flux, from its surrounding environment. In the experiments, they induced non-Gaussian “dephasing” noise by injecting engineered flux noise that disturbs the qubit and makes it lose coherence, which in turn is then used as a measuring tool. “Usually, we want to avoid decoherence, but in this case, how the qubit decoheres tells us something about the noise in its environment,” Oliver says. Specifically, they shot 110 “pi-pulses” — which are used to flip the states of qubits — in specific sequences over tens of microseconds. Each pulse sequence effectively created a narrow frequency “filter” which masks out much of the noise, except in a particular band of frequency. By measuring the response of a qubit sensor to the bandpass-filtered noise, they extracted the noise power in that frequency band. By modifying the pulse sequences, they could move filters up and down to sample the noise at different frequencies. Notably, in doing so, they tracked how the non-Gaussian noise distinctly causes the qubit to decohere, which provided a high-dimensional spectrum of the non-Gaussian noise. Error suppression and correction The key innovation behind the work is carefully engineering the pulses to act as specific filters that extract properties of the “bispectrum,” a two-dimension representation that gives information about distinctive time correlations of non-Gaussian noise. Essentially, by reconstructing the bispectrum, they could find properties of non-Gaussian noise signals impinging on the qubit over time — ones that don’t exist in Gaussian noise signals. The general idea is that, for Gaussian noise, there will be only correlation between two points in time, which is referred to as a “second-order time correlation.” But, for non-Gaussian noise, the properties at one point in time will directly correlate to properties at multiple future points. Such “higher-order” correlations are the hallmark of non-Gaussian noise. In this work, the authors were able to extract noise with correlations between three points in time. This information can help programmers validate and tailor dynamical error suppression and error-correcting codes for qubits, which fixes noise-induced errors and ensures accurate computation. Such protocols use information from the noise model to make implementations that are more efficient for practical quantum computers. But, because the details of noise aren’t yet well-understood, today’s error-correcting codes are designed with that standard bell curve in mind. With the researchers’ tool, programmers can either gauge how their code will work effectively in realistic scenarios or start to zero in on non-Gaussian noise. Keeping with the crowded-room analogy, Oliver says: “If you know there’s only one loud person in the room, then you’ll design a code that effectively muffles that one person, rather than trying to address every possible scenario.” "
MIT News,Astronomers use giant galaxy cluster as X-ray magnifying lens,Nasa,2019-10-14,-,http://news.mit.edu/2019/astronomers-galaxy-x-ray-magnifying-1014,"  Astronomers at MIT and elsewhere have used a massive cluster of galaxies as an X-ray magnifying glass to peer back in time, to nearly 9.4 billion years ago. In the process, they spotted a tiny dwarf galaxy in its very first, high-energy stages of star formation. While galaxy clusters have been used to magnify objects at optical wavelengths, this is the first time scientists have leveraged these massive gravitational giants to zoom in on extreme, distant, X-ray-emitting phenomena. What they detected appears to be a blue speck of an infant galaxy, about 1/10,000 the size of our Milky Way, in the midst of churning out its first stars — supermassive, cosmically short-lived objects that emit high-energy X-rays, which the researchers detected in the form of a bright blue arc. “It’s this little blue smudge, meaning it’s a very small galaxy that contains a lot of super-hot, very massive young stars that formed recently,” says Matthew Bayliss, a research scientist in MIT’s Kavli Institute for Astrophysics and Space Research. “This galaxy is similar to the very first galaxies that formed in the universe … the kind of which no one has ever seen in X-ray in the distant universe before.” Bayliss says the detection of this single, distant galaxy is proof that scientists can use galaxy clusters as natural X-ray magnifiers, to pick out extreme, highly energetic phenomena in the universe’s early history. “With this technique, we could, in the future, zoom in on a distant galaxy and age-date different parts of it — to say, this part has stars that formed 200 million years ago, versus another part that formed 50 million years ago, and pick them apart in a way you cannot otherwise do,” says Bayliss, who will be moving on to the University of Cincinnati as an assistant professor of physics. He and his co-authors, including Michael McDonald, assistant professor of physics at MIT, have published their results today in the journal Nature Astronomy.  A candle in the light Galaxy clusters are the most massive objects in the universe, composed of thousands of galaxies, all bound together by gravity as one enormous, powerful force. Galaxy clusters are so massive, and their gravitational pull is so strong, that they can distort the fabric of space-time, bending the universe and any surrounding light, much like an elephant would stretch and warp a trapeze net. Scientists have used galaxy clusters as cosmic magnifying glasses, with a technique known as gravitational lensing. The idea is that if scientists can approximate the mass of a galaxy cluster, they can estimate its gravitational effects on any surrounding light, as well as the angle at which a cluster may deflect that light. For instance, imagine if an observer, facing a galaxy cluster, were trying to detect an object, such as a single galaxy, behind that cluster. The light emitted by that object would travel straight toward the cluster, then bend around the cluster. It would continue traveling toward the observer, though at slightly different angles, appearing to the observer as mirrored images of the same object, which in the end can be combined as a single, “magnified” image. Scientists have used galaxy clusters to magnify objects at optical wavelengths, but never in the X-ray band of the electromagnetic spectrum, mainly because galaxy clusters themselves emit an enormous amount of X-rays. Scientists have thought that any X-rays coming from a background source would be impossible to discern from the cluster’s own glare. “If you’re trying to see an X-ray source behind a cluster, it’s like trying to see a candle next to a really bright light,” Bayliss says. “So we knew this was a challenging measurement to make.” X-ray subtraction The researchers wondered: Could they subtract that bright light and see the candle behind it? In other words, could they remove the X-ray emissions coming from the galaxy cluster, to view the much fainter X-rays coming from an object, behind and magnified by the cluster? The team tested this idea with observations taken by NASA’s Chandra X-ray Observatory, one of the world’s most powerful X-ray space telescopes. They looked in particular at Chandra’s measurements of the Phoenix cluster, a distant galaxy cluster located 5.7 billion light-years from Earth, which has been estimated to be about a quadrillion times as massive as the sun, with gravitational effects that should make it a powerful, natural magnifying lens. “The idea is to take whatever your best X-ray telescope is — in this case, Chandra — and use a natural lens to magnify and effectively make Chandra bigger, so you can see more distant things,” Bayliss says. He and his colleagues analyzed observations of the Phoenix cluster, taken continuously by Chandra for over a month. They also looked at images of the cluster taken by two optical and infrared telescopes — the Hubble Space Telescope and the Magellan telescope in Chile. With all these various views, the team developed a model to characterize the cluster’s optical effects, which allowed the researchers to precisely measure the X-ray emissions from the cluster itself, and subtract it from the data. They were left with two similar patterns of X-ray emissions around the cluster, which they determined were “lensed,” or gravitationally bent, by the cluster. When they traced the emissions backward in time, they found that they all originated from a single, distant source: a tiny dwarf galaxy from 9.4 billion years ago, when the universe itself was roughly 4.4 billion years old — about a third of its current age. “Previously, Chandra had seen only a handful of things at this distance,” Bayliss says. “In less than 10 percent of the time, we discovered this object, similarly far away. And gravitational lensing is what let us do it.” The combination of Chandra and the Phoenix cluster’s natural lensing power enabled the team to see the tiny galaxy hiding behind the cluster, magnified about 60 times. At this resolution, they were able to zoom in to discern two distinct clumps within the galaxy, one producing many more X-rays than the other. As X-rays are typically produced during extreme, short-lived phenomena, the researchers believe that the first X-ray-rich clump signals a part of the dwarf galaxy that has very recently formed supermassive stars, while the quieter region is an older region that contains more mature stars. “We’re catching this galaxy at a very useful stage, where it’s got these really young stars,” Bayliss says. “Every galaxy had to start out in this phase, but we don’t see a lot of these kinds of galaxies in our own neighborhood. Now we can go back in time, look in the distant universe, find galaxies in this early phase of their life, and start to study how star formation is different there.” This research was funded, in part, by NASA, and by the Space Telescope Science Institute. "
MIT News,Computing and the search for new planets,Nasa,2019-09-23,-,http://news.mit.edu/2019/computing-and-search-for-new-planets-tess-mit-0923,"  When MIT launched the MIT Stephen A. Schwarzman College of Computing this fall, one of the goals was to drive further innovation in computing across all of MIT’s schools. Researchers are already expanding beyond traditional applications of computer science and using these techniques to advance a range of scientific fields, from cancer medicine to anthropology to design — and to the discovery of new planets. Computation has already proven useful for the Transiting Exoplanet Survey Satellite (TESS), a NASA-funded mission led by MIT. Launched from Cape Canaveral in April 2018, TESS is a satellite that takes images of the sky as it orbits the Earth. These images can help researchers find planets orbiting stars beyond our sun, called exoplanets. This work, which is now halfway complete, will reveal more about the other planets within what NASA calls our “solar neighborhood.”  “TESS just completed the first of its two-year prime mission, surveying the southern night sky,” says Sara Seager, an astrophysicist and planetary scientist at MIT and deputy director of science for TESS. “TESS found over 1,000 planet candidates and about 20 confirmed planets, some in multiple-planet systems.” While TESS has enabled some impressive discoveries so far, finding these exoplanets is no simple task. TESS is collecting images of more than 200,000 distant stars, saving an image of these planets every two minutes, as well as saving an image of a large swath of sky every 30 minutes. Seager says every two weeks, which is how long it takes the satellite to orbit the Earth, TESS sends about 350 gigabytes of data (once uncompressed) to Earth. While Seager says this is not as much data as people might expect (a 2019 Macbook Pro has up to 512 gigabytes of storage), analyzing the data involves taking many complex factors into consideration. Seager, who says she has long been interested in how computation can be used as a tool for science, began discussing the project with Victor Pankratius, a former principal research scientist in MIT’s Kavli Institute for Astrophysics and Space Research, who is now the director and head of global software engineering at Bosch Sensortec. A trained computer scientist, Pankratius says that after arriving at MIT in 2013, he started thinking about scientific fields that produce big data, but that have not yet fully benefited from computing techniques. After speaking with astronomers like Seager, he learned more about the data their instruments collect and became interested in applying computer-aided discovery techniques to the search for exoplanets. “The universe is a big place,” Pankratius says. “So I think leveraging what we have on the computer science side is a great thing.”  The basic idea underlying TESS’ mission is that like our own solar system, in which the Earth and other planets revolve around a central star (the sun), there are other planets beyond our solar system revolving around different stars. The images TESS collects produce light curves — data that show how the brightness of the star changes over time. Researchers are analyzing these light curves to find drops in brightness, which could indicate that a planet is passing in front of the star and temporarily blocking some of its light.  “Every time a planet orbits, you would see this brightness go down,” Pankratius says. “It's almost like a heartbeat.”  The trouble is that not every dip in brightness is necessarily caused by a passing planet. Seager says machine learning currently comes into play during the “triage” phase of their TESS data analysis, helping them distinguish between potential planets and other things that could cause dips in brightness, like variable stars, which naturally vary in their brightness, or instrument noise. Analysis on planets that pass through triage is still done by scientists who have learned how to “read” light curves. But the team is now using thousands of light curves that have been classified by eye to teach neural networks how to identify exoplanet transits. Computation is helping them narrow down which light curves they should examine in more detail. Liang Yu PhD ’19, a recent physics graduate, built upon an existing code to write the machine learning tool that the team is now using. While helpful for homing in on the most relevant data, Seager says machine learning cannot yet be used to simply find exoplanets. “We still have a lot of work to do,” she says. Pankratius agrees. “What we want to do is basically create computer-aided discovery systems that do this for all [stars] all the time,” he says. “You want to just press a button and say, show me everything. But right now it's still people with some automation vetting all of these light curves.” Seager and Pankratius also co-taught a course that focused on various aspects of computation and artificial intelligence (AI) development in planetary science. Seager says inspiration for the course arose from a growing interest from students to learn about AI and its applications to cutting-edge data science. In 2018, the course allowed students to use actual data collected by TESS to explore machine learning applications for this data. Modeled after another course Seager and Pankratius taught, students in the course were able to choose a scientific problem and learn the computation skills to solve that problem. In this case, students learned about AI techniques and applications to TESS. Seager says students had a great response to the unique class.  “As a student, you could actually make a discovery,” Pankratius says. “You can build a machine learning algorithm, run it on this data, and who knows, maybe you will find something new.” Much of the data TESS collects is also readily available as part of a larger citizen science project. Pankratius says anyone with the right tools could start making discoveries of their own. Thanks to cloud connectivity, this is even possible on a cell phone.  “If you get bored on your bus ride home, why not search for planets?” he says. Pankratius says this type of collaborative work allows experts in each domain to share their knowledge and learn from each other, rather than each trying to get caught up in the other’s field.   “Over time, science has become more specialized, so we need ways to integrate the specialists better,” Pankratius says. The college of computing could help forge more such collaborations, he adds. Pankratius also says it could attract researchers who work at the intersection of these disciplines, who can bridge gaps in understanding between experts. This type of work integrating computer science is already becoming increasingly common across scientific fields, Seager notes. “Machine learning is ‘in vogue’ right now,” she says.  Pankratius says that is in part because there is more evidence that leveraging computer science techniques is an effective way to address various types of problems and growing data sets. “We now have demonstrations in different areas that the computer-aided discovery approach doesn’t just work,” Pankratius says. “It actually leads to new discoveries.” "
MIT News,Scientists detect tones in the ringing of a newborn black hole for the first time,Nasa,2019-09-12,-,http://news.mit.edu/2019/ringing-new-black-hole-first-0912,"  If Albert Einstein’s theory of general relativity holds true, then a black hole, born from the cosmically quaking collisions of two massive black holes, should itself “ring” in the aftermath, producing gravitational waves much like a struck bell reverbates sound waves. Einstein predicted that the particular pitch and decay of these gravitational waves should be a direct signature of the newly formed black hole’s mass and spin. Now, physicists from MIT and elsewhere have studied the ringing of an infant black hole, and found that the pattern of this ringing does, in fact, predict the black hole’s mass and spin — more evidence that Einstein was right all along. The findings, published today in Physical Review Letters, also favor the idea that black holes lack any sort of “hair” — a metaphor referring to the idea that black holes, according to Einstein’s theory, should exhibit just three observable properties: mass, spin, and electric charge. All other characteristics, which the physicist John Wheeler termed “hair,” should be swallowed up by the black hole itself, and would therefore be unobservable. The team’s findings today support the idea that black holes are, in fact, hairless. The researchers were able to identify the pattern of a black hole’s ringing, and, using Einstein’s equations, calculated the mass and spin that the black hole should have, given its ringing pattern. These calculations matched measurements of the black hole’s mass and spin made previously by others. If the team’s calculations deviated significantly from the measurements, it would have suggested that the black hole’s ringing encodes properties other than mass, spin, and electric charge — tantalizing evidence of physics beyond what Einstein’s theory can explain. But as it turns out, the black hole’s ringing pattern is a direct signature of its mass and spin, giving support to the notion that black holes are bald-faced giants, lacking any extraneous, hair-like properties. “We all expect general relativity to be correct, but this is the first time we have confirmed it in this way,” says the study’s lead author, Maximiliano Isi, a NASA Einstein Fellow in MIT’s Kavli Institute for Astrophysics and Space Research. “This is the first experimental measurement that succeeds in directly testing the no-hair theorem. It doesn’t mean black holes couldn’t have hair. It means the picture of black holes with no hair lives for one more day.” A chirp, decoded On Sept. 14, 2015, scientists made the first-ever detection of gravitational waves — infinitesimal ripples in space-time, emanating from distant, violent cosmic phenomena. The detection, named GW150914, was made by LIGO, the Laser Interferometer Gravitational-wave Observatory. Once scientists cleared away the noise and zoomed in on the signal, they observed a waveform that quickly crescendoed before fading away. When they translated the signal into sound, they heard something resembling a “chirp.” Scientists determined that the gravitational waves were set off by the rapid inspiraling of two massive black holes. The peak of the signal — the loudest part of the chirp — linked to the very moment when the black holes collided, merging into a single, new black hole. While this infant black hole gave off gravitational waves of its own, its signature ringing, physicists assumed, would be too faint to decipher amid the clamor of the initial collision. Thus, traces of this ringing were only identified some time after the peak, where the signal was too faint to study in detail. Isi and his colleagues, however, found a way to extract the black hole’s reverberation from the moments immediately after the signal’s peak. In previous work led by Isi’s co-author, Matthew Giesler of Caltech, the team showed through simulations that such a signal, and particularly the portion right after the peak, contains “overtones” — a family of loud, short-lived tones. When they reanalyzed the signal, taking overtones into account, the researchers discovered that they could successfully isolate a ringing pattern that was specific to a newly formed black hole. In the team’s new paper, the researchers applied this technique to actual data from the GW150914 detection, concentrating on the last few milliseconds of the signal, immediately following the chirp’s peak. Taking into account the signal’s overtones, they were able to discern a ringing coming from the new, infant black hole. Specifically, they identified two distinct tones, each with a pitch and decay rate that they were able to measure. “We detect an overall gravitational wave signal that’s made up of multiple frequencies, which fade away at different rates, like the different pitches that make up a sound,” Isi says. “Each frequency or tone corresponds to a vibrational frequency of the new black hole.” Listening beyond Einstein Einstein’s theory of general relativity predicts that the pitch and decay of a black hole’s gravitational waves should be a direct product of its mass and spin. That is, a black hole of a given mass and spin can only produce tones of a certain pitch and decay. As a test of Einstein’s theory, the team used the equations of general relativity to calculate the newly formed black hole’s mass and spin, given the pitch and decay of the two tones they detected. They found their calculations matched with measurements of the black hole’s mass and spin previously made by others. Isi says the results demonstrate that researchers can, in fact, use the very loudest, most detectable parts of a gravitational wave signal to discern a new black hole’s ringing, where before, scientists assumed that this ringing could only be detected within the much fainter end of the gravitational wave signal, and identifying many tones would require much more sensitive instruments than what currently exist. “This is exciting for the community because it shows these kinds of studies are possible now, not in 20 years,” Isi says. As LIGO improves its resolution, and more sensitive instruments come online in the future, researchers will be able to use the group’s methods to “hear” the ringing of other newly born black holes. And if they happen to pick up tones that don’t quite match up with Einstein’s predictions, that could be an even more exciting prospect. “In the future, we’ll have better detectors on Earth and in space, and will be able to see not just two, but tens of modes, and pin down their properties precisely,” Isi says. “If these are not black holes as Einstein predicts, if they are more exotic objects like wormholes or boson stars, they may not ring in the same way, and we’ll have a chance of seeing them.” This research was supported, in part, by NASA, the Sherman Fairchild Foundation, the Simons Foundation, and the National Science Foundation. "
