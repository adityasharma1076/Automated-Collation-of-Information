Source,Heading,Category,Date,Time,URL,Text
Science Daily,Gut Bacteria May Be Linked to High Blood Pressure and Depression,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905161408.htm,"   ""People are 'meta-organisms' made up of roughly equal numbers of human cells and bacteria. Gut bacteria ecology interacts with our bodily physiology and brains, which may steer some people towards developing high blood pressure and depression,"" said Bruce R. Stevens, Ph.D., lead author of the study and professor of physiology & functional genomics, medicine and psychiatry at the University of Florida College of Medicine in Gainesville, Florida. ""In the future, health professionals may target your gut in order to prevent, diagnose and selectively treat different forms of high blood pressure."" Stevens said there's potential for this research to uncover treatment approaches that could improve outcomes in people with treatment-resistant hypertension. Nearly 20 percent of patients with high blood pressure don't respond well to treatment, even with multiple medications. The researchers isolated DNA (deoxyribonucleic acid, the carrier of genetic information) from gut bacteria obtained from the stool samples of 105 volunteers. They used a new technique involving artificial-intelligence software to analyze the bacteria, which revealed four distinct types of bacterial genes and signature molecules. Surprisingly, the investigators discovered unique patterns of bacteria from people with 1) high blood pressure plus depression; 2) high blood pressure without depression; 3) depression with healthy blood pressure; or 4) healthy subjects without depression or high blood pressure. Stevens said the results suggest different medical mechanisms of high blood pressure that correlate with signature molecules produced by gut bacteria. These molecules are thought to impact the cardiovascular system, metabolism, hormones and the nervous system. ""We believe we have uncovered new forms of high blood pressure: 'Depressive Hypertension' (high blood pressure with depression), which may be a completely different disease than 'Non-Depressive Hypertension' (high blood pressure without depression), which are each different from 'Non-Hypertensive Depression,'"" Stevens said. "
Science Daily,Having an Elder Brother Is Associated With Slower Language Development,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905124522.htm,"   What is even more surprising is that apparently only elder brothers impact the language skills of their younger siblings, as a study conducted by a research team from the CNRS, the AP-HP, the EHESS, the ENS and the INSERM has just shown. The study finds that children who have grown up with an elder sister have identical development to children with no elder sibling. More than 1000 children have been followed from birth to five and a half years old in the mother-child cohort EDEN.* Their language skills were evaluated at 2, 3 and 5.5 years old by tests measuring several aspects of language, such as vocabulary, syntax and verbal reasoning. Children who have an elder brother had on average a two-month delay in language development compared with children with an elder sister. The scientists propose two hypotheses that may explain this result. The first is that elder sisters, in being more willing to talk to their younger siblings than brothers, may compensate for their parents being less available. Another hypothesis would be that elder sisters compete less than elder brothers for parental attention. Though this study cannot separate these two hypotheses, it does show that early language development in a younger sibling tends to be slower when the elder is a boy. For their next project, the scientists want to examine the impact of culture (specifically geographical origin) on these results. *- The EDEN cohort recruited families between 2003 and 2006 in the CHU in Nancy and Poitiers. "
Science Daily,Report Cards on Women in STEM Fields Finds Much Room for Improvement,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111636.htm,"   ""The data suggest that we are making headway,"" says Reshma Jagsi, a radiation oncologist and director of the Center for Bioethics and Social Sciences in Medicine at the University of Michigan and one of the corresponding authors. ""That said, there are still many institutions that have few women in senior-most faculty positions. There also remains quite a bit of room for improvement in certain areas, including the representation of women in certain roles, such as speaking at scientific meetings."" The researchers obtained their data through the use of institutional report cards that were collected when individual researchers applied for grants from NYSCF. The report cards were part of a 2014 NYSCF project that put forward a number of strategies aimed at helping to achieve gender parity in science, technology, engineering, and math (STEM). Of the 1,287 report cards that were submitted, 741 provided complete information for a given year, and some included multiple years. Overall, the data in the paper represent 541 institutions in 38 countries in North America (72%) and Europe (18%). The investigators found that although women made up more than half of the population among undergraduate, graduate, and post-graduate students, the picture became different as seniority increased. Women made up 42% of assistant professors, 34% of associate professors, and 23% of full professors. These rates varied greatly by institution: At about one-third of the institutions surveyed, women made up less than 10% of tenured faculty recruits. ""We expected to find that women would be better represented at more junior ranks compared with senior ranks,"" Jagsi says. ""But I found it noteworthy that there were regional differences. For example, institutions in Europe come closer to achieving gender parity."" The researchers say their findings suggest that the primary issue is not recruiting women into STEM roles but retaining them and promoting them into more influential positions. They also point out the important part that funding organizations can play. ""Funding organizations are in a unique position to require institutional leaders to pay attention to equity, diversity, and inclusion within their organizations,"" Jagsi says. ""By requiring these report cards, they can promote actions that help all scientists thrive. We hope that other funding bodies, like the NIH, will adopt a similar report card."" The next phase of IWISE will focus on highlighting best practices undertaken by institutions. This will provide comparative data and allow the researchers to monitor progress over time. The researchers will also look at other factors that may influence the recruitment and retention of women scientists, such as the presence of women in top leadership roles, the rates at which tenured women stay in their positions, and equity in salaries across gender, race, and ethnicity. ""For my own work, I plan to begin to focus more on issues of intersectionality,"" Jagsi concludes. ""A particularly understudied area involves the career experiences in women with other minority identities, such as race. Further research is needed to understand the challenges these women face."" "
Science Daily,The Future of Mind Control,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905094048.htm,"   In a recent perspective titled ""Precision Electronic Medicine,"" published in Nature Biotechnology, Patel, a faculty member at the Harvard Medical School and Massachusetts General Hospital, and Lieber, the Joshua and Beth Friedman University Professor, argue that neurotechnology is on the cusp of a major renaissance. Throughout history, scientists have blurred discipline lines to tackle problems larger than their individual fields. The Human Genome Project, for example, convened international teams of scientists to map human genes faster than otherwise possible. ""The next frontier is really the merging of human cognition with machines,"" Patel said. He and Lieber see mesh electronics as the foundation for those machines, a way to design personalized electronic treatment for just about anything related to the brain. ""Everything manifests in the brain fundamentally. Everything. All your thoughts, your perceptions, any type of disease,"" Patel said. Scientists can pinpoint the general areas of the brain where decision-making, learning, and emotions originate, but tracing behaviors to specific neurons is still a challenge. Right now, when the brain's complex circuitry starts to misbehave or degrade due to psychiatric illnesses like addiction or Obsessive-Compulsive Disorder, neurodegenerative diseases like Parkinson's or Alzheimer's, or even natural aging, patients have only two options for medical intervention: drugs or, when those fail, implanted electrodes. Drugs like L-dopa can quiet the tremors that prevent someone with Parkinson's from performing simple tasks like dressing and eating. But because drugs affect more than just their target, even common L-dopa side effects can be severe, ranging from nausea to depression to abnormal heart rhythms. When drugs no longer work, FDA-approved electrodes can provide relief through Deep Brain Stimulation. Like a pace maker, a battery pack set beneath the clavicle sends automated electrical pulses to two brain implants. Lieber said each electrode ""looks like a pencil. It's big."" During implantation, Parkinson's patients are awake, so surgeons can calibrate the electrical pulses. Dial the electricity up, and the tremors calm. ""Almost instantly, you can see the person regain control of their limbs,"" Patel said. ""It blows my mind."" But, like with L-dopa, the large electrodes stimulate more than their intended targets, causing sometimes severe side effects like speech impediments. And, over time, the brain's immune system treats the stiff implants as foreign objects: Neural immune cells (glia cells) engulf the perceived invader, displacing or even killing neurons and reducing the device's ability to maintain treatment. In contrast, Lieber's mesh electronics provoke almost no immune response. With close, long-term proximity to the same neurons, the implants can collect robust data on how individual neurons communicate over time or, in the case of neurological disorders, fail to communicate. Eventually, such technology could track how specific neural subtypes talk, too, all of which could lead to a cleaner, more precise map of the brain's communication network. With higher resolution targets, future electrodes can act with greater precision, eliminating unwanted side effects. If that happens, Patel said, they could be tuned to treat any neurological disorder. And, unlike current electrodes, Lieber's have already demonstrated a valuable trick of their own: They encourage neural migration, potentially guiding newborn neurons to damaged areas, like pockets created by stroke. ""The potential for it is outstanding,"" Patel said. ""In my own mind, I see this at the level of what started with the transistor or telecommunications."" The potential reaches beyond therapeutics: Adaptive electrodes could provide heightened control over prosthetic or even paralyzed limbs. In time, they could act like neural substitutes, replacing damaged circuitry to re-establish broken communication networks and recalibrate based on live feedback. ""If you could actually interact in a precise and long-term way and also provide feedback information,"" Lieber said, ""you could really communicate with the brain in the same way that the brain is communicating within itself."" A few major technology companies are also eager to champion brain-machine interfaces. Some, like Elon Musk's Neuralink, which plans to give paralyzed patients the power to work computers with their minds, are focused on assistive applications. Others have broader plans: Facebook wants people to text by imaging the words, and Brian Johnson's Kernel hopes to enhance cognitive abilities. During his postdoctoral studies, Patel saw how just a short pulse of electricity -- no more than 500 milliseconds of stimulation -- could control a person's ability to make a safe or impulsive decision. After a little zap, subjects who almost always chose the risky bet, instead went with the safe option. ""You would have no idea that it's happened,"" Patel said. ""You're unaware of it. It's beyond your conscious awareness."" Such power demands intense ethical scrutiny. For people struggling to combat addiction or obsessive-compulsive disorder, an external pulse regulator could significantly improve their quality of life. But, companies that operate those regulators could access their client's most personal data -- their thoughts. And, if enhanced learning and memory are for sale, who gets to buy a better brain? ""One does need to be a little careful about the ethics involved if you're trying to make a superhuman,"" Lieber said. ""Being able to help people is much more important to me at this time."" Mesh electronics still have several major challenges to overcome: scaling up the number of implanted electrodes, processing the data flood those implants deliver, and feeding that information back into the system to enable live recalibration. ""I always joke in talks that I'm doing this because my memory has gotten a little worse than it used to be,"" Lieber said. ""That's natural aging. But does it have to be that way? What if you could correct it?"" If he and Patel succeed in galvanizing researchers around mesh electronics, the question might not be if but when. "
Science Daily,People Can See Beauty in Complex Mathematics,Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905090944.htm,"   The study, published in science journal Cognition, showed people even agreed on what made such abstract mathematical arguments beautiful. The findings may have implications for teaching schoolchildren, who may not be entirely convinced that there is beauty in mathematics. The similarities between mathematics and music have long been noted but the study co-authors, Yale mathematician Stefan Steinerberger and University of Bath psychologist Dr. Samuel G.B.Johnson, wanted to add art to the mix to see if there was something universal at play in the people judge aesthetics and beauty -- be they in art, music or abstract mathematics. The research was sparked when Steinerberger, while teaching his students, likened a mathematical proof to a 'really good Schubert sonata' -- but couldn't put his finger on why. He approached Johnson, assistant professor of marketing at the University of Bath School of Management, who was completing his Ph.D. in psychology at Yale. Johnson designed an experiment to test his question of whether people share the same aesthetic sensibilities about maths that they do about art or music -- and if this would hold true for an average person, not just a career mathematician. For the study, they chose four mathematical proof, four landscape paintings, and four classical piano pieces. None of the participants was a mathematician. The mathematical proofs used were: the sum of an infinite geometric series, Gauss's summation trick for positive integers, the Pigeonhole principle, and a geometric proof of a Faulhaber formula. A mathematical proof is an argument which convinces people something is true. The piano pieces were Schubert's Moment Musical No. 4, D 780 (Op. 94), Bach's Fugue from Toccata in E Minor (BWV 914), Beethoven's Diabelli Variations (Op. 120) and Shostakovich's Prelude in D-flat major (Op.87 No. 15). The landscape paintings were Looking Down Yosemite Valley, California by Albert Bierstadt; A Storm in the Rocky Mountains, Mt. Rosalie by Albert Bierstadt; The Hay Wain by John Constable; and The Heart of the Andes by Frederic Edwin Church. Johnson divided the study into three parts. The first task required a sample of individuals to match the four maths proofs to the four landscape paintings based on how aesthetically similar they found them. The second task required a different group of people to compare the four maths proofs to the four piano sonatas. Finally, the third asked another sample group to rate each of the four works of art and mathematical arguments for nine different criteria -- seriousness, universality, profundity, novelty, clarity, simplicity, elegance, intricacy, and sophistication. Participants in the third group agreed with each other about how elegant, profound, clear, etc., each of the mathematical arguments and paintings was. But Steinerberger and Johnson were most impressed that these ratings could be used to predict how similar participants in the first group believed that each argument and painting were to each other. This finding suggests that perceived correspondences between maths and art really have to do with their underlying beauty. Overall, the results showed there was considerable consensus in comparing mathematical arguments to artworks. And there was some consensus in judging the similarity of classical piano music and mathematics. ""Laypeople not only had similar intuitions about the beauty of math as they did about the beauty of art but also had similar intuitions about beauty as each other. In other words, there was consensus about what makes something beautiful, regardless of modality,"" Johnson said. However, it was not clear whether the results would be the same with different music. ""I'd like to see our study done again but with different pieces of music, different proofs, different artwork,"" said Steinerberger. ""We demonstrated this phenomenon, but we don't know the limits of it. Where does it stop existing? Does it have to be classical music? Do the paintings have to be of the natural world, which is highly aesthetic?"" Both Steinerberger and Johnson believe the research may have implications for maths education, especially at the secondary-school level. ""There might be opportunities to make the more abstract, more formal aspects of mathematics more accessible and more exciting to students at that age,"" said Johnson, ""And that might be useful in terms of encouraging more people to enter the field of mathematics."" "
Science Daily,"Hunter-Gatherers Agree on What Is Moral, but Not Who Is Moral",Mind & Brain,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905080039.htm,"   The research appears in the journal Social Psychological and Personality Science. Interviewing Hadza hunter-gatherers to rank the people they live with on a number of traits, such as who has a good heart, who shares the most, and who works the hardest, the team found that Hadza agreed on how important generosity and hard work was to moral character, but disagreed on who most exemplified these traits. ""They disagreed on who among them had the most moral character,"" says Kristopher Smith, a Penn postdoctoral fellow and lead paper author. Smith and Coren Apicella, an associate professor of Psychology, had 94 judges rank their campmates on global character and relevant character traits for a total of 824 observations. The Hadza live in small nomadic groups and move from group to group so that the social structure of each group frequently changes. Scientists suspect this nomadic way of living gives insight into the origins of human cooperation. Although the researchers note that the Hadza are a modern people, living in modern times, their way of living is more similar to how human ancestors lived than in western societies. ""It's not that the Hadza do not have a concept of morality or don't care about it. They agreed on what traits contributed to moral character. But they cannot agree on who exemplifies it,"" says Smith. Previous work by Apicella and Smith looking at generosity found that this trait changed depending on the group dynamic. A generous group led members to be more generous, and a group that didn't share much led to others not sharing as well. It may be this mobility and changing group dynamics that drive their current findings as well, as Apicella notes that ""the changing of groups and behavior may make it difficult for individuals to track and agree on moral reputations."" From the 1,000-foot morality perspective, ""this suggests that, for the Hadza, there is little consistent moral behavior across situations, and that reputation may have played a smaller role in the evolution of morality,"" says Smith. ""Understanding how moral psychology differs across cultures and in different social systems may provide insight into ways to improve our interactions with one another, and maybe overcome moral disagreements in our society,"" summarizes Smith. "
Science Daily,"'Mental Rigidity' at Root of Intense Political Partisanship on Both Left and Right, Study Finds",Mind & Brain,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829081401.htm,"   This ""mental rigidity"" makes it harder for people to change their ways of thinking or adapt to new environments, say researchers. Importantly, mental rigidity was found in those with the most fervent beliefs and affiliations on both the left and right of the political divide. The study of over 700 US citizens, conducted by scientists from the University of Cambridge, is the largest -- and first for over 20 years -- to investigate whether the more politically ""extreme"" have a certain ""type of mind"" through the use of objective psychological testing. The findings suggest that the basic mental processes governing our ability to switch between different concepts and tasks are linked to the intensity with which we attach ourselves to political doctrines -- regardless of the ideology. ""Relative to political moderates, participants who indicated extreme attachment to either the Democratic or Republican Party exhibited mental rigidity on multiple objective neuropsychological tests,"" said Dr Leor Zmigrod, a Cambridge Gates Scholar and lead author of the study, now published in the Journal of Experimental Psychology. ""While political animosity often appears to be driven by emotion, we find that the way people unconsciously process neutral stimuli seems to play an important role in how they process ideological arguments."" ""Those with lower cognitive flexibility see the world in more black-and-white terms, and struggle with new and different perspectives. The more inflexible mind may be especially susceptible to the clarity, certainty, and safety frequently offered by strong loyalty to collective ideologies,"" she said. The research is the latest in a series of studies from Zmigrod and her Cambridge colleagues, Dr Jason Rentfrow and Professor Trevor Robbins, on the relationship between ideology and cognitive flexibility. Their previous work over the last 18 months has suggested that mental rigidity is linked to more extreme attitudes with regards to religiosity, nationalism, and a willingness to endorse violence and sacrifice one's life for an ideological group. For the latest study, the Cambridge team recruited 743 men and women of various ages and educational backgrounds from across the political spectrum through the Amazon Mechanical Turk platform. Participants completed three psychological tests online: a word association game, a card-sorting test -- where colours, shapes and numbers are matched according to shifting rules -- and an exercise in which participants have a two-minute window to imagine possible uses for everyday objects. ""These are established and standardized cognitive tests which quantify how well individuals adapt to changing environments and how flexibly their minds process words and concepts,"" said Zmigrod. The participants were also asked to score their feelings towards various divisive social and economic issues -- from abortion and marriage to welfare -- and the extent of ""overlap"" between their personal identity and the US Republican and Democrat parties. Zmigrod and colleagues found that ""partisan extremity"" -- the intensity of participants' attachment to their favoured political party -- was a strong predictor of rigidity in all three cognitive tests. They also found that self-described Independents displayed greater cognitive flexibility compared to both Democrats and Republicans. Other cognitive traits, such as originality or fluency of thought, were not related to heightened political partisanship, which researchers argue suggests the unique contribution of cognitive inflexibility. ""In the context of today's highly divided politics, it is important we work to understand the psychological underpinnings of dogmatism and strict ideological adherence,"" said Zmigrod. ""The aim of this research is not to draw false equivalences between different, and sometimes opposing, ideologies. We want to highlight the common psychological factors that shape how people come to hold extreme views and identities,"" said Zmigrod. ""Past studies have shown that it is possible to cultivate cognitive flexibility through training and education. Our findings raise the question of whether heightening our cognitive flexibility might help build more tolerant societies, and even develop antidotes to radicalization."" ""While the conservatism and liberalism of our beliefs may at times divide us, our capacity to think about the world flexibly and adaptively can unite us,"" she added. "
Science Daily,Brain Circuit Connects Feeding and Mood in Response to Stress,Mind & Brain,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904213722.htm,"   ""This study was initiated by first author Dr. Na Qu, a psychiatrist of Wuhan Mental Health Center, China, when she was visiting my lab,"" said corresponding author Dr. Yong Xu, associate professor of pediatrics and of molecular and cellular biology at Baylor College of Medicine. Qu, a practicing psychiatrist who also conducts basic brain research, was interested in investigating whether there was a neurological basis for the association between depression and other psychiatric disorders and alterations in metabolism, such as obesity or lack of appetite, she had observed in a number of her patients. Xu, Qu and their colleagues worked with a mouse model of depression induced by chronic stress and observed that depressed animals ate less and lost weight. Then, they applied a number of experimental techniques to identify the neuronal circuits that changed activity when the animals were depressed. ""We found that POMC neurons in the hypothalamus, which are essential for regulating body weight and feeding behavior, extend physical connections into another region of the brain that has numerous dopamine neurons that are implicated in the regulation of mood,"" said Xu, who also is a researcher at the USDA/ARS Children's Nutrition Research Center at Baylor and Texas Children's Hospital. ""We know that a decrease in dopamine may trigger depression."" In addition to the physical connection between the feeding and the mood centers of the brain, the researchers also discovered that when they triggered depression in mice, the POMC neurons were activated and this led to inhibition of the dopamine neurons. Interestingly, when the researchers inhibited the neuronal circuit connecting the feeding and the mood centers, the animals ate more, gained weight and looked less depressed. ""We have discovered that a form of chronic stress triggered a neuronal circuit that starts in a population of cells that are known to regulate metabolism and feeding behavior and ends in a group of neurons that are famous for their regulation of mood,"" Xu said. ""Stress-triggered activation of the feeding center led to inhibition of dopamine-producing neurons in the mood center."" Although more research is needed, Xu, Qu and their colleagues propose that their findings provide a new biological basis that may explain some of the connections between mood alterations and changes in metabolism observed in people, and may provide solutions in the future. ""Our findings only explain one scenario, when depression is associated with poor appetite. But in other cases depression has been linked to overeating. We are interested in investigating this second association between mood and eating behavior to identify the neuronal circuits that may explain that response,"" Xu said. "
Science Daily,How Your Brain Remembers Motor Sequences,Mind & Brain,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828100539.htm,"   Contrary to the common assumption, the researchers found that overlapping regions in the premotor and parietal cortices represent the sequences in multiple levels of motor hierarchy (e.g., chunks of a few finger movements, or chunks of a few chunks), whereas the individual finger movements (i.e., the lowest level in the hierarchy) were uniquely represented in the primary motor cortex. These results uncovered the first detailed map of cortical sequence representation in the human brain. The results may also provide some clue for locating new candidate brain areas as signal sources for motor BCI application or developing more sophisticated algorithm to reconstruct complex motor behavior. The results were published online as Yokoi and Diedrichsen ""Neural Organization of Hierarchical Motor Sequence Representations in the Human Neocortex"" in Neuron on July 22, 2019. Achievements The best way to remember/produce long and complex motor sequences is to divide them into several smaller pieces recursively. For example, a musical piece may be remembered as a sequence of smaller chunks, with each chunk representing a group of often co-occurring notes. Such hierarchical organization has long been thought to underlie our control of motor sequences from the highly skillful actions, like playing music, to daily behavior, like making a cup of tea. Yet, very little is known about how these hierarchies are implemented in our brain. In a new study published in a journal Neuron, Atsushi Yokoi, Center for Information and Neural Networks (CiNet), NICT, and Jörn Diedrichsen, Brain and Mind Institute, Western Univ., provide the first direct evidence of how hierarchically organized sequences are represented through the population activity across the human cerebral cortex. The researchers measured the fine-grained fMRI activity patterns, while human participants produced 8 different remembered sequences of 11 finger presses. ""Remembering 8 different sequences of 11 finger presses is a tough task, so you will definitely need to organize them hierarchically,"" says Diedrichsen, the study's senior author and a Western Research Chair for Motor Control and Computational Neuroscience at the Western University, Canada. ""To study a hierarchy, you would really need the sequences to have this much of complexity. And currently it's very hard to train animals to learn such sequences,"" added Yokoi, the study's lead author who is a former postdoctoral researcher at the Diedrichsen's group since both were at the Institute of Cognitive Neuroscience, University College London in UK, and now a Researcher at the CiNet, NICT, Japan. Through a series of careful behavioural analyses, the researchers could show that participants encoded the sequences in terms of a three-level hierarchy; (1) individual finger presses, (2) chunks consisting of two or three finger presses, and (3) entire sequences consisting of four chunks. They could then characterize the fMRI activity patterns with respect to these hierarchies using machine learning techniques. As expected, the patterns in primary motor cortex, the area that controls finger movements, seemed to only depend on each individual finger moved, independent of it positioning in the sequence. Activity in higher-order motor areas, such as premotor and parietal cortices, clearly could be shown to encode the sequential context at the level of chunks or entire sequence. Thus, in contrast to primary motor cortex, these areas ""know"" what was played before and what comes after the ongoing finger press. For the first time the study now allowed insights into the organization of these higher-order representations. Surprisingly, different levels of sequence information overlapped greatly. An unsupervised clustering approach further subdivided these areas into distinct clusters, each had a different mixing ratio of the representations, just like how one's iPhone storage is used. These results uncovered the first detailed map of cortical sequence representation in the human brain. Study's impact One common assumption in the cognitive neuroscience has been that each level in the functional hierarchy would mirror the anatomical hierarchy, from the higher, association cortices (e.g., premotor or parietal cortices) down to the primary sensorimotor cortices. The mysterious coexistence of a clear anatomical separation (i.e., individual finger vs. other representations) and an overlap (i.e., chunk and sequence representations) sheds new light on the classical question of the correspondence between functional and anatomical hierarchies. ""It can be said the brain represents motor sequences in partly hierarchical, yet partly flat ways."" ""Although its functional role is still unclear, the anatomical overlap between chunk and sequence representations may suggest these representations in upper movement hierarchy may influence with each other to support flexible sequence production. This needs to be tested in the future study,"" Yokoi concluded. Future prospects The study also suggests possible loci from which we can record brain signal to control neural prosthetics to make fluent movement sequences in potential BCI applications. The researchers also hope that it could also contribute in developing a new decoding algorithm that effectively combines the information in different hierarchies to reproduce movements. The study was conducted under an international collaboration between NICT (Japan), UCL (UK), and Western University (Canada). "
Science Daily,Patients in the US and Canada Are Likely to Receive Opioids After Surgery,Mind & Brain,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165221.htm,"   To compare international opioid prescribing rates after surgery, researchers analyzed data on four frequently performed procedures: surgery to remove the appendix, surgery to remove the gallbladder, a minimally invasive procedure to treat a torn meniscus cartilage in the knee and a procedure to remove a breast lump. Within seven days of discharge, about 75 percent of the patients in the United States and Canada filled an opioid prescription, compared to just 11 percent of the patients in Sweden. By the one-month mark, nearly half of U.S patients had received high-dose opioid prescriptions (i.e., prescriptions totaling more than 200 MME) -- nearly double the rate in Canada (25 percent) and nine times higher than the rate in Sweden (5 percent). ""Our findings reveal stark differences in prescribing practices across the three countries and suggest real opportunities to encourage more judicious use of opioids before and after surgery for patients in the United States and Canada,"" said the study's corresponding author Mark D. Neuman, MD, an associate professor of Anesthesiology and Critical Care and Chair of the Penn Medicine Opioid Task Force. ""While innovative strategies, like enhanced recovery protocols, have helped to reduce the number of prescribed opioids, it's clear that we need to continue to identify ways to improve prescribing practices in the United States and Canada."" Opioids, such as codeine, tramadol and morphine, are routinely prescribed for postoperative pain management in many countries. However, recent research suggests that overprescribing opioid medications for short-term pain may be widespread in the United States. The excessive prescribing can increase the risk of drug diversion, new long-term opioid use and the development of opioid use disorder. In the last decade, opioid overdose deaths have significantly increased in countries across the world, including the United States and Canada -- which have the highest opioid use per capita in the world. While the use of opioids varies in countries worldwide, there has been little research -- until now -- that characterizes the international disparities in opioid use for specific indications, such as pain relief after surgery. In their analysis, researchers examined data from more than 220,000 cases -- ranging from 2013 to 2016 -- to identify differences in the percentage of opioid prescriptions filled within seven and 30 days of the procedures, as well as the quantity and types of opioids dispensed. They specifically sought patients who shared similar characteristics, including age and medical history, and who had not received an opioid in the 90 days prior to the surgery. Researchers found that at least 65 percent of patients in the United States and Canada filled an opioid prescription in the first seven days after each procedure. In Sweden, the prescribing rate didn't exceed 20 percent for any of the procedures. Meanwhile, the average dosage of the initial prescription in the United States was 247 MME -much higher than the dosage dispensed in Sweden (197) and Canada (169). In addition to the disparities in prescribing rate and dosage, researchers also identified a significant variation in the types of opioid medication prescribed. For example, codeine and tramadol accounted for 58 percent of the postoperative prescriptions dispensed in Canada and 45 percent of the prescriptions in Sweden, but just 7 percent of prescriptions in the United States. In the United States, hydrocodone and oxycodone were the most commonly dispensed opioid medications. ""Our findings point to systematic differences in practitioners' approaches to opioid prescribing, public attitudes regarding the role of opioids in treating pain and broader structural factors related to drug marketing and regulation,"" said Dr. Karim Ladha, a clinician-scientist at the Li Ka Shing Knowledge Institute of St. Michael's Hospital and co-author of the study. The work was supported, in part, by a grant from the National Institutes of Health (1-R01-DA042299) and ICES. "
Science Daily,Closing in on Elusive Particles,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905161426.htm,"   While the Standard Model of Particle Physics has remained mostly unchanged since its initial conception, experimental observations for neutrinos have forced the neutrino part of the theory to be reconsidered in its entirety. Neutrino oscillation was the first observation inconsistent with the predictions and proves that neutrinos have non-zero masses, a property that contradicts the Standard Model. In 2015, this discovery was rewarded with the Nobel Prize. Are neutrinos their own antiparticles? Additionally, there is the longstanding conjecture that neutrinos are so-called Majorana particles: Unlike all other constituents of matter, neutrinos might be their own antiparticles. This would also help explain why there is so much more matter than antimatter in the Universe. The GERDA experiment is designed to scrutinize the Majorana hypothesis by searching for the neutrinoless double beta decay of the germanium isotope 76-Ge: Two neutrons inside a 76-Ge nucleus simultaneously transform into two protons with the emission of two electrons. This decay is forbidden in the Standard Model because the two antineutrinos -- the balancing antimatter -- are missing. The Technical University of Munich (TUM) has been a key partner of the GERDA project (GERmanium Detector Array) for many years. Prof. Stefan Schönert, who heads the TUM research group, is the speaker of the new LEGEND project. The GERDA experiment achieves extreme levels of sensitivity GERDA is the first experiment to reach exceptionally low levels of background noise and has now surpassed the half-life sensitivity for decay of 10^26 years. In other words: GERDA proves that the process has a half-life of at least 10^26 years, or 10,000,000,000,000,000 times the age of the Universe. Physicists know that neutrinos are at least 100,000 times lighter than electrons, the next heaviest particles. What mass they have exactly, however, is still unknown and another important research topic. In the standard interpretation, the half-life of the neutrinoless double beta decay is related to a special variant of the neutrino mass called the Majorana mass. Based the new GERDA limit and those from other experiments, this mass must be at least a million times smaller than that of an electron, or in the terms of physicists, less than 0.07 to 0.16 eV/c^2. Consistent with other experiments Also other experiments limit the neutrino mass: the Planck mission provides a limit on another variant of the neutrino mass: The sum of the masses of all known neutrino types is less than 0.12 to 0.66 eV/c^2. The tritium decay experiment KATRIN at the Karlsruhe Institute of Technology (KIT) is set-up to measure the neutrino mass with a sensitivity of about 0.2 eV/c^2 in the coming years. These masses are not directly comparable, but they provide a cross check on the paradigm that neutrinos are Majorana particles. So far, no discrepancy has been observed. From GERDA to LEGEND During the reported data collection period, GERDA operated detectors with a total mass of 35.6 kg of 76-Ge. Now, a newly formed international collaboration, LEGEND, will increase this mass to 200 kg of 76-Ge until 2021 and further reduce the background noise. The aim is to achieve a sensitivity of 10^27 years within the next five years. "
Science Daily,Time Saving Software in an Age of Ever-Expanding Data,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145448.htm,"   It is hard to get people excited about research software says Eliza Grames, a PhD candidate in ecology and evolutionary biology. Yet, the software she has developed is exciting and to understand why, it is important to put yourself into the shoes of a researcher. Before embarking on a new research project, a thorough and exhaustive review of existing literature must be done to make sure the new project is novel. Researchers can also explore the entire body of previously published data on a subject to answer a new question using that same data. This is a daunting task, especially considering that millions of new research articles are published each year. Where does one even begin to explore all of that data? ""Each new study contributes more to what we know about a topic, adding nuance and complexity that helps improve our understanding of the natural world. To make sense of this wealth of evidence and get closer to a complete picture of the world, researchers are increasingly turning to systematic review methods as a way to synthesize this information,"" says Grames. ""It is important to find all of the relevant information and to not find too much of it,"" says Grames. The way to perform this search is through something called a systematic review, which Grames says started in the fields of medicine and public health, where keeping current with research can be a question of life or death. ""In those fields, there is an established system with Medical Subject Headers where articles get tagged with keywords associated with the work, but ecology does not have that."" Other fields of research across the scientific spectrum were in the same boat. The project sprang out of need. In her own process of reviewing, Grames noted she would miss articles and key terms and was interested in finding out how to identify those missing terms on the first try. ""As we were working on this software, we realized there was a much faster way to do the reviews than how others were doing them,"" says Grames, ""The traditional way was mostly going through papers and pulling out a term and then reading the rest of the article to identify more terms to use."" Even with fairly specific key words, Grames notes the average systematic review in her field of conservation biology initially yields about 10,000 research papers for bigger projects. It is important to retrieve relevant information, without also retrieving too much irrelevant information. ""Each year, the amount of data just keeps increasing. There are some systematic reviews that if you look at the amount of time they would have taken just three years ago, they would take about 300 days to perform. If the same reviews were done today, they would take about 350 days because the number of publications just keeps going up and up."" Grames says it took about a month or two to hash out ideas for the software, then she spent a summer writing and fixing the code. The result is an open-source software package called litsearchr. How it works, says Grames, is that a user will input their best faith effort of putting together a search into a few databases. ""The keywords should be fairly relevant to retrieve articles that are entered into the algorithm to extract all of the potential keywords, which are then put into a network. The original keywords are at the center of the network and are the most well-connected."" Grames says the time required to develop a search strategy has been decreased by 90%. Presented with the most relevant articles, researchers then have significantly fewer papers to parse through manually. This review stage itself is partially automated now too, adds Grames. Litsearchr is part of a collaborative effort by researchers, called metaverse, where the goal is to link several software packages together so researchers can perform their research from start to finish in the same coding language, R. ""Researchers can develop their systematic reviews, import data, and there is even a package that can write up the results section for the systematic review,"" says Grames. Grames and her team set up the software so that it could be used by anyone, whether they can code or not, using templates ready to upload information to. There is also a detailed step-by-step video to take users through the process. By keeping the software open source, Grames says debugging and editing is improved because users can point out details that need attention, ""Every time I get an email, it is so exciting. It is nice to have it open because people can let me know when there is a typo."" The software is currently being used by researchers in scientific fields such as nutritional science and psychology and for a massive undertaking of screening all papers pertaining to insect populations across the globe. Grames says it is nice to have the software in place to be able to take on such a big project. ""There is no way we could do this project without the level of automation we get using litsearchr. I built this out of a need from another project, but this software is making it possible to do even bigger analyses than before."" "
Science Daily,Using Nature to Produce a Revolutionary Optical Material,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905124512.htm,"   The work, published in the journal Nature Communications, also describes a superior manner of telecom switching without the use of electronics; instead, they use an all-optical method that could improve the speed and capacity of internet communications. That could remove a roadblock in moving from 4GLTE to 5G networks. The team reported that a material created using tellurium nanorods -- produced by naturally occurring bacteria -- is an effective nonlinear optical material, capable of protecting electronic devices against high-intensity bursts of light, including those emitted by inexpensive household lasers targeted at aircraft, drones or other critical systems. The researchers describe the material and its performance as a material of choice for next-generation optoelectronic and photonic devices. Seamus Curran, a physics professor at the University of Houston and one of the paper's authors, said while most optical materials are chemically synthesized, using a biologically-based nanomaterial proved less expensive and less toxic. ""We found a cheaper, easier, simpler way to manufacture the material,"" he said. ""We let Mother Nature do it."" The new findings grew out of earlier work by Curran and his team, working in collaboration with Werner J. Blau of Trinity College Dublin and Ron Oremland with the U.S. Geological Survey. Curran initially synthesized the nanocomposites to examine their potential in the photonics world. He holds a U.S. and international series of patents for that work. The researchers noted that using bacteria to create the nanocrystals suggests an environmentally friendly route of synthesis, while generating impressive results. ""Nonlinear optical measurements of this material reveal the strong saturable absorption and nonlinear optical extinctions induced by Mie scattering overbroad temporal and wavelength ranges,"" they wrote. ""In both cases, Te [tellurium] particles exhibit superior optical nonlinearity compared to graphene."" Light at very high intensity, such as that emitted by a laser, can have unpredictable polarizing effects on certain materials, Curran said, and physicists have been searching for suitable nonlinear materials that can withstand the effects. One goal, he said, is a material that can effectively reduce the light intensity, allowing for a device to be developed that could prevent damage by that light. The researchers used the nanocomposite, made up of biologically generated elemental tellurium nanocrystals and a polymer to build an electro-optic switch -- an electrical device used to modulate beams of light -- that is immune to damage from a laser, he said. Oremland noted that the current work grew out of 30 years of basic research, stemming from their initial discovery of selenite-respiring bacteria and the fact that the bacteria form discrete packets of elemental selenium. ""From there, it was a step down the Periodic Table to learn that the same could be done with tellurium oxyanions,"" he said. ""The fact that tellurium had potential application in the realm of nanophotonics came as a serendipitous surprise."" Blau said the biologically generated tellurium nanorods are especially suitable for photonic device applications in the mid-infrared range. ""This wavelength region is becoming a hot technological topic as it is useful for biomedical, environmental and security-related sensing, as well as laser processing and for opening up new windows for fiber optical and free-space communications."" Work will continue to expand the material's potential for use in all-optical telecom switches, which Curran said is critical in expanding broadband capacity. ""We need a massive investment in optical fiber,"" he said. ""We need greater bandwidth and switching speeds. We need all-optical switches to do that."" "
Science Daily,Report Cards on Women in STEM Fields Finds Much Room for Improvement,Matter & Energy,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111636.htm,"   ""The data suggest that we are making headway,"" says Reshma Jagsi, a radiation oncologist and director of the Center for Bioethics and Social Sciences in Medicine at the University of Michigan and one of the corresponding authors. ""That said, there are still many institutions that have few women in senior-most faculty positions. There also remains quite a bit of room for improvement in certain areas, including the representation of women in certain roles, such as speaking at scientific meetings."" The researchers obtained their data through the use of institutional report cards that were collected when individual researchers applied for grants from NYSCF. The report cards were part of a 2014 NYSCF project that put forward a number of strategies aimed at helping to achieve gender parity in science, technology, engineering, and math (STEM). Of the 1,287 report cards that were submitted, 741 provided complete information for a given year, and some included multiple years. Overall, the data in the paper represent 541 institutions in 38 countries in North America (72%) and Europe (18%). The investigators found that although women made up more than half of the population among undergraduate, graduate, and post-graduate students, the picture became different as seniority increased. Women made up 42% of assistant professors, 34% of associate professors, and 23% of full professors. These rates varied greatly by institution: At about one-third of the institutions surveyed, women made up less than 10% of tenured faculty recruits. ""We expected to find that women would be better represented at more junior ranks compared with senior ranks,"" Jagsi says. ""But I found it noteworthy that there were regional differences. For example, institutions in Europe come closer to achieving gender parity."" The researchers say their findings suggest that the primary issue is not recruiting women into STEM roles but retaining them and promoting them into more influential positions. They also point out the important part that funding organizations can play. ""Funding organizations are in a unique position to require institutional leaders to pay attention to equity, diversity, and inclusion within their organizations,"" Jagsi says. ""By requiring these report cards, they can promote actions that help all scientists thrive. We hope that other funding bodies, like the NIH, will adopt a similar report card."" The next phase of IWISE will focus on highlighting best practices undertaken by institutions. This will provide comparative data and allow the researchers to monitor progress over time. The researchers will also look at other factors that may influence the recruitment and retention of women scientists, such as the presence of women in top leadership roles, the rates at which tenured women stay in their positions, and equity in salaries across gender, race, and ethnicity. ""For my own work, I plan to begin to focus more on issues of intersectionality,"" Jagsi concludes. ""A particularly understudied area involves the career experiences in women with other minority identities, such as race. Further research is needed to understand the challenges these women face."" "
Science Daily,Future of LEDs Gets Boost from Verification of Localization States in InGaN Quantum Wells,Matter & Energy,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904135642.htm,"   In the Journal of Applied Physics, from AIP Publishing, researchers in China report an InGaN LED structure with high luminescence efficiency and what is believed to be the first direct observation of transition carriers between different localization states within InGaN. The localization states were confirmed by temperature-dependent photoluminescence and excitation power-dependent photoluminescence. Localization states theory is commonly used to explain the high luminescence efficiency gained via the large number of dislocations within InGaN materials. Localization states are the energy minima states believed to exist within the InGaN quantum well region (discrete energy values), but a direct observation of localization states was elusive until now. ""Based primarily on indium content fluctuations, we explored the 'energy minima' that remain within the InGaN quantum well region,"" said Yangfeng Li, the paper's lead author and a now postdoctoral fellow at the Hong Kong University of Science and Technology. ""Such energy minima will capture the charge carriers -- electrons and holes -- and prevent them from being captured by defects (dislocations). This means that the emission efficiency is less affected by the large number of defects."" The group's direct observation of localization states is an important discovery for the future of LEDs, because it verifies their existence, which was a long-standing open scientific question. ""Segregation of indium may be one of the reasons causing localization states,"" said Li. ""Due to the existence of localization states, the charge carriers will mainly be captured in the localization states rather than by nonradiative recombination defects. This improves the high luminescence efficiency of light-emitting devices."" Based on the group's electroluminescence spectra, ""the InGaN sample with stronger localization states provides more than a twofold enhancement of the light-output at the same current-injection conditions as samples of weaker localization states,"" Li said. The researchers' work can serve as a reference about the emission properties of InGaN materials for use in manufacturing LEDs and laser diodes. They plan to continue to explore gallium nitride-related materials and devices ""not only to gain a better understanding of their localizations but also the properties of InGaN quantum dots, which are semiconductor particles with potential applications in solar cells and electronics,"" Li said. ""We hope that other researchers will also conduct in-depth theoretical studies of localization states."" "
Science Daily,Closing in on Elusive Particles,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905161426.htm,"   While the Standard Model of Particle Physics has remained mostly unchanged since its initial conception, experimental observations for neutrinos have forced the neutrino part of the theory to be reconsidered in its entirety. Neutrino oscillation was the first observation inconsistent with the predictions and proves that neutrinos have non-zero masses, a property that contradicts the Standard Model. In 2015, this discovery was rewarded with the Nobel Prize. Are neutrinos their own antiparticles? Additionally, there is the longstanding conjecture that neutrinos are so-called Majorana particles: Unlike all other constituents of matter, neutrinos might be their own antiparticles. This would also help explain why there is so much more matter than antimatter in the Universe. The GERDA experiment is designed to scrutinize the Majorana hypothesis by searching for the neutrinoless double beta decay of the germanium isotope 76-Ge: Two neutrons inside a 76-Ge nucleus simultaneously transform into two protons with the emission of two electrons. This decay is forbidden in the Standard Model because the two antineutrinos -- the balancing antimatter -- are missing. The Technical University of Munich (TUM) has been a key partner of the GERDA project (GERmanium Detector Array) for many years. Prof. Stefan Schönert, who heads the TUM research group, is the speaker of the new LEGEND project. The GERDA experiment achieves extreme levels of sensitivity GERDA is the first experiment to reach exceptionally low levels of background noise and has now surpassed the half-life sensitivity for decay of 10^26 years. In other words: GERDA proves that the process has a half-life of at least 10^26 years, or 10,000,000,000,000,000 times the age of the Universe. Physicists know that neutrinos are at least 100,000 times lighter than electrons, the next heaviest particles. What mass they have exactly, however, is still unknown and another important research topic. In the standard interpretation, the half-life of the neutrinoless double beta decay is related to a special variant of the neutrino mass called the Majorana mass. Based the new GERDA limit and those from other experiments, this mass must be at least a million times smaller than that of an electron, or in the terms of physicists, less than 0.07 to 0.16 eV/c^2. Consistent with other experiments Also other experiments limit the neutrino mass: the Planck mission provides a limit on another variant of the neutrino mass: The sum of the masses of all known neutrino types is less than 0.12 to 0.66 eV/c^2. The tritium decay experiment KATRIN at the Karlsruhe Institute of Technology (KIT) is set-up to measure the neutrino mass with a sensitivity of about 0.2 eV/c^2 in the coming years. These masses are not directly comparable, but they provide a cross check on the paradigm that neutrinos are Majorana particles. So far, no discrepancy has been observed. From GERDA to LEGEND During the reported data collection period, GERDA operated detectors with a total mass of 35.6 kg of 76-Ge. Now, a newly formed international collaboration, LEGEND, will increase this mass to 200 kg of 76-Ge until 2021 and further reduce the background noise. The aim is to achieve a sensitivity of 10^27 years within the next five years. "
Science Daily,Time Saving Software in an Age of Ever-Expanding Data,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905145448.htm,"   It is hard to get people excited about research software says Eliza Grames, a PhD candidate in ecology and evolutionary biology. Yet, the software she has developed is exciting and to understand why, it is important to put yourself into the shoes of a researcher. Before embarking on a new research project, a thorough and exhaustive review of existing literature must be done to make sure the new project is novel. Researchers can also explore the entire body of previously published data on a subject to answer a new question using that same data. This is a daunting task, especially considering that millions of new research articles are published each year. Where does one even begin to explore all of that data? ""Each new study contributes more to what we know about a topic, adding nuance and complexity that helps improve our understanding of the natural world. To make sense of this wealth of evidence and get closer to a complete picture of the world, researchers are increasingly turning to systematic review methods as a way to synthesize this information,"" says Grames. ""It is important to find all of the relevant information and to not find too much of it,"" says Grames. The way to perform this search is through something called a systematic review, which Grames says started in the fields of medicine and public health, where keeping current with research can be a question of life or death. ""In those fields, there is an established system with Medical Subject Headers where articles get tagged with keywords associated with the work, but ecology does not have that."" Other fields of research across the scientific spectrum were in the same boat. The project sprang out of need. In her own process of reviewing, Grames noted she would miss articles and key terms and was interested in finding out how to identify those missing terms on the first try. ""As we were working on this software, we realized there was a much faster way to do the reviews than how others were doing them,"" says Grames, ""The traditional way was mostly going through papers and pulling out a term and then reading the rest of the article to identify more terms to use."" Even with fairly specific key words, Grames notes the average systematic review in her field of conservation biology initially yields about 10,000 research papers for bigger projects. It is important to retrieve relevant information, without also retrieving too much irrelevant information. ""Each year, the amount of data just keeps increasing. There are some systematic reviews that if you look at the amount of time they would have taken just three years ago, they would take about 300 days to perform. If the same reviews were done today, they would take about 350 days because the number of publications just keeps going up and up."" Grames says it took about a month or two to hash out ideas for the software, then she spent a summer writing and fixing the code. The result is an open-source software package called litsearchr. How it works, says Grames, is that a user will input their best faith effort of putting together a search into a few databases. ""The keywords should be fairly relevant to retrieve articles that are entered into the algorithm to extract all of the potential keywords, which are then put into a network. The original keywords are at the center of the network and are the most well-connected."" Grames says the time required to develop a search strategy has been decreased by 90%. Presented with the most relevant articles, researchers then have significantly fewer papers to parse through manually. This review stage itself is partially automated now too, adds Grames. Litsearchr is part of a collaborative effort by researchers, called metaverse, where the goal is to link several software packages together so researchers can perform their research from start to finish in the same coding language, R. ""Researchers can develop their systematic reviews, import data, and there is even a package that can write up the results section for the systematic review,"" says Grames. Grames and her team set up the software so that it could be used by anyone, whether they can code or not, using templates ready to upload information to. There is also a detailed step-by-step video to take users through the process. By keeping the software open source, Grames says debugging and editing is improved because users can point out details that need attention, ""Every time I get an email, it is so exciting. It is nice to have it open because people can let me know when there is a typo."" The software is currently being used by researchers in scientific fields such as nutritional science and psychology and for a massive undertaking of screening all papers pertaining to insect populations across the globe. Grames says it is nice to have the software in place to be able to take on such a big project. ""There is no way we could do this project without the level of automation we get using litsearchr. I built this out of a need from another project, but this software is making it possible to do even bigger analyses than before."" "
Science Daily,Report Cards on Women in STEM Fields Finds Much Room for Improvement,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111636.htm,"   ""The data suggest that we are making headway,"" says Reshma Jagsi, a radiation oncologist and director of the Center for Bioethics and Social Sciences in Medicine at the University of Michigan and one of the corresponding authors. ""That said, there are still many institutions that have few women in senior-most faculty positions. There also remains quite a bit of room for improvement in certain areas, including the representation of women in certain roles, such as speaking at scientific meetings."" The researchers obtained their data through the use of institutional report cards that were collected when individual researchers applied for grants from NYSCF. The report cards were part of a 2014 NYSCF project that put forward a number of strategies aimed at helping to achieve gender parity in science, technology, engineering, and math (STEM). Of the 1,287 report cards that were submitted, 741 provided complete information for a given year, and some included multiple years. Overall, the data in the paper represent 541 institutions in 38 countries in North America (72%) and Europe (18%). The investigators found that although women made up more than half of the population among undergraduate, graduate, and post-graduate students, the picture became different as seniority increased. Women made up 42% of assistant professors, 34% of associate professors, and 23% of full professors. These rates varied greatly by institution: At about one-third of the institutions surveyed, women made up less than 10% of tenured faculty recruits. ""We expected to find that women would be better represented at more junior ranks compared with senior ranks,"" Jagsi says. ""But I found it noteworthy that there were regional differences. For example, institutions in Europe come closer to achieving gender parity."" The researchers say their findings suggest that the primary issue is not recruiting women into STEM roles but retaining them and promoting them into more influential positions. They also point out the important part that funding organizations can play. ""Funding organizations are in a unique position to require institutional leaders to pay attention to equity, diversity, and inclusion within their organizations,"" Jagsi says. ""By requiring these report cards, they can promote actions that help all scientists thrive. We hope that other funding bodies, like the NIH, will adopt a similar report card."" The next phase of IWISE will focus on highlighting best practices undertaken by institutions. This will provide comparative data and allow the researchers to monitor progress over time. The researchers will also look at other factors that may influence the recruitment and retention of women scientists, such as the presence of women in top leadership roles, the rates at which tenured women stay in their positions, and equity in salaries across gender, race, and ethnicity. ""For my own work, I plan to begin to focus more on issues of intersectionality,"" Jagsi concludes. ""A particularly understudied area involves the career experiences in women with other minority identities, such as race. Further research is needed to understand the challenges these women face."" "
Science Daily,The Future of Mind Control,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905094048.htm,"   In a recent perspective titled ""Precision Electronic Medicine,"" published in Nature Biotechnology, Patel, a faculty member at the Harvard Medical School and Massachusetts General Hospital, and Lieber, the Joshua and Beth Friedman University Professor, argue that neurotechnology is on the cusp of a major renaissance. Throughout history, scientists have blurred discipline lines to tackle problems larger than their individual fields. The Human Genome Project, for example, convened international teams of scientists to map human genes faster than otherwise possible. ""The next frontier is really the merging of human cognition with machines,"" Patel said. He and Lieber see mesh electronics as the foundation for those machines, a way to design personalized electronic treatment for just about anything related to the brain. ""Everything manifests in the brain fundamentally. Everything. All your thoughts, your perceptions, any type of disease,"" Patel said. Scientists can pinpoint the general areas of the brain where decision-making, learning, and emotions originate, but tracing behaviors to specific neurons is still a challenge. Right now, when the brain's complex circuitry starts to misbehave or degrade due to psychiatric illnesses like addiction or Obsessive-Compulsive Disorder, neurodegenerative diseases like Parkinson's or Alzheimer's, or even natural aging, patients have only two options for medical intervention: drugs or, when those fail, implanted electrodes. Drugs like L-dopa can quiet the tremors that prevent someone with Parkinson's from performing simple tasks like dressing and eating. But because drugs affect more than just their target, even common L-dopa side effects can be severe, ranging from nausea to depression to abnormal heart rhythms. When drugs no longer work, FDA-approved electrodes can provide relief through Deep Brain Stimulation. Like a pace maker, a battery pack set beneath the clavicle sends automated electrical pulses to two brain implants. Lieber said each electrode ""looks like a pencil. It's big."" During implantation, Parkinson's patients are awake, so surgeons can calibrate the electrical pulses. Dial the electricity up, and the tremors calm. ""Almost instantly, you can see the person regain control of their limbs,"" Patel said. ""It blows my mind."" But, like with L-dopa, the large electrodes stimulate more than their intended targets, causing sometimes severe side effects like speech impediments. And, over time, the brain's immune system treats the stiff implants as foreign objects: Neural immune cells (glia cells) engulf the perceived invader, displacing or even killing neurons and reducing the device's ability to maintain treatment. In contrast, Lieber's mesh electronics provoke almost no immune response. With close, long-term proximity to the same neurons, the implants can collect robust data on how individual neurons communicate over time or, in the case of neurological disorders, fail to communicate. Eventually, such technology could track how specific neural subtypes talk, too, all of which could lead to a cleaner, more precise map of the brain's communication network. With higher resolution targets, future electrodes can act with greater precision, eliminating unwanted side effects. If that happens, Patel said, they could be tuned to treat any neurological disorder. And, unlike current electrodes, Lieber's have already demonstrated a valuable trick of their own: They encourage neural migration, potentially guiding newborn neurons to damaged areas, like pockets created by stroke. ""The potential for it is outstanding,"" Patel said. ""In my own mind, I see this at the level of what started with the transistor or telecommunications."" The potential reaches beyond therapeutics: Adaptive electrodes could provide heightened control over prosthetic or even paralyzed limbs. In time, they could act like neural substitutes, replacing damaged circuitry to re-establish broken communication networks and recalibrate based on live feedback. ""If you could actually interact in a precise and long-term way and also provide feedback information,"" Lieber said, ""you could really communicate with the brain in the same way that the brain is communicating within itself."" A few major technology companies are also eager to champion brain-machine interfaces. Some, like Elon Musk's Neuralink, which plans to give paralyzed patients the power to work computers with their minds, are focused on assistive applications. Others have broader plans: Facebook wants people to text by imaging the words, and Brian Johnson's Kernel hopes to enhance cognitive abilities. During his postdoctoral studies, Patel saw how just a short pulse of electricity -- no more than 500 milliseconds of stimulation -- could control a person's ability to make a safe or impulsive decision. After a little zap, subjects who almost always chose the risky bet, instead went with the safe option. ""You would have no idea that it's happened,"" Patel said. ""You're unaware of it. It's beyond your conscious awareness."" Such power demands intense ethical scrutiny. For people struggling to combat addiction or obsessive-compulsive disorder, an external pulse regulator could significantly improve their quality of life. But, companies that operate those regulators could access their client's most personal data -- their thoughts. And, if enhanced learning and memory are for sale, who gets to buy a better brain? ""One does need to be a little careful about the ethics involved if you're trying to make a superhuman,"" Lieber said. ""Being able to help people is much more important to me at this time."" Mesh electronics still have several major challenges to overcome: scaling up the number of implanted electrodes, processing the data flood those implants deliver, and feeding that information back into the system to enable live recalibration. ""I always joke in talks that I'm doing this because my memory has gotten a little worse than it used to be,"" Lieber said. ""That's natural aging. But does it have to be that way? What if you could correct it?"" If he and Patel succeed in galvanizing researchers around mesh electronics, the question might not be if but when. "
Science Daily,People Can See Beauty in Complex Mathematics,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905090944.htm,"   The study, published in science journal Cognition, showed people even agreed on what made such abstract mathematical arguments beautiful. The findings may have implications for teaching schoolchildren, who may not be entirely convinced that there is beauty in mathematics. The similarities between mathematics and music have long been noted but the study co-authors, Yale mathematician Stefan Steinerberger and University of Bath psychologist Dr. Samuel G.B.Johnson, wanted to add art to the mix to see if there was something universal at play in the people judge aesthetics and beauty -- be they in art, music or abstract mathematics. The research was sparked when Steinerberger, while teaching his students, likened a mathematical proof to a 'really good Schubert sonata' -- but couldn't put his finger on why. He approached Johnson, assistant professor of marketing at the University of Bath School of Management, who was completing his Ph.D. in psychology at Yale. Johnson designed an experiment to test his question of whether people share the same aesthetic sensibilities about maths that they do about art or music -- and if this would hold true for an average person, not just a career mathematician. For the study, they chose four mathematical proof, four landscape paintings, and four classical piano pieces. None of the participants was a mathematician. The mathematical proofs used were: the sum of an infinite geometric series, Gauss's summation trick for positive integers, the Pigeonhole principle, and a geometric proof of a Faulhaber formula. A mathematical proof is an argument which convinces people something is true. The piano pieces were Schubert's Moment Musical No. 4, D 780 (Op. 94), Bach's Fugue from Toccata in E Minor (BWV 914), Beethoven's Diabelli Variations (Op. 120) and Shostakovich's Prelude in D-flat major (Op.87 No. 15). The landscape paintings were Looking Down Yosemite Valley, California by Albert Bierstadt; A Storm in the Rocky Mountains, Mt. Rosalie by Albert Bierstadt; The Hay Wain by John Constable; and The Heart of the Andes by Frederic Edwin Church. Johnson divided the study into three parts. The first task required a sample of individuals to match the four maths proofs to the four landscape paintings based on how aesthetically similar they found them. The second task required a different group of people to compare the four maths proofs to the four piano sonatas. Finally, the third asked another sample group to rate each of the four works of art and mathematical arguments for nine different criteria -- seriousness, universality, profundity, novelty, clarity, simplicity, elegance, intricacy, and sophistication. Participants in the third group agreed with each other about how elegant, profound, clear, etc., each of the mathematical arguments and paintings was. But Steinerberger and Johnson were most impressed that these ratings could be used to predict how similar participants in the first group believed that each argument and painting were to each other. This finding suggests that perceived correspondences between maths and art really have to do with their underlying beauty. Overall, the results showed there was considerable consensus in comparing mathematical arguments to artworks. And there was some consensus in judging the similarity of classical piano music and mathematics. ""Laypeople not only had similar intuitions about the beauty of math as they did about the beauty of art but also had similar intuitions about beauty as each other. In other words, there was consensus about what makes something beautiful, regardless of modality,"" Johnson said. However, it was not clear whether the results would be the same with different music. ""I'd like to see our study done again but with different pieces of music, different proofs, different artwork,"" said Steinerberger. ""We demonstrated this phenomenon, but we don't know the limits of it. Where does it stop existing? Does it have to be classical music? Do the paintings have to be of the natural world, which is highly aesthetic?"" Both Steinerberger and Johnson believe the research may have implications for maths education, especially at the secondary-school level. ""There might be opportunities to make the more abstract, more formal aspects of mathematics more accessible and more exciting to students at that age,"" said Johnson, ""And that might be useful in terms of encouraging more people to enter the field of mathematics."" "
Science Daily,Role of Earthquake Motions in Triggering a 'Surprise' Tsunami,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905090931.htm,"   The tsunami was as surprising to scientists as it was devastating to communities in Sulawesi. It occurred near an active plate boundary, where earthquakes are common. Surprisingly, the earthquake caused a major tsunami, although it primarily offset the ground horizontally -- normally, large-scale tsunamis are typically caused by vertical motions. Researchers were at a loss -- what happened? How was the water displaced to create this tsunami: by landslides, faulting, or both? Satellite data of the surface rupture suggests relatively straight, smooth faults, but do not cover areas offshore, such as the critical Palu Bay. Researchers wondered -- what is the shape of the faults beneath Palu Bay and is this important for generating the tsunami? This earthquake was extremely fast. Could rupture speed have amplified the tsunami? Using a supercomputer operated by the Leibniz Supercomputing Centre, a member of the Gauss Centre for Supercomputing, the team showed that the earthquake-induced movement of the seafloor beneath Palu Bay itself could have generated the tsunami, meaning the contribution of landslides is not required to explain the tsunami's main features. The team suggests an extremely fast rupture on a straight, tilted fault within the bay. In their model, slip is mostly lateral, but also downward along the fault, resulting in anywhere from 0.8 metres to 2.8 metres vertical seafloor change that averaged 1.5 metres across the area studied. Critical to generating this tsunami source are the tilted fault geometry and the combination of lateral and extensional strains exerted on the region by complex tectonics. The scientists come to this conclusion using a cutting-edge, physics-based earthquake-tsunami model. The earthquake model, based on earthquake physics, differs from conventional data-driven earthquake models, which fit observations with high accuracy at the cost of potential incompatibility with real-world physics. It instead incorporates models of the complex physical processes occurring at and off of the fault, allowing researchers to produce a realistic scenario compatible both with earthquake physics and regional tectonics. The researchers evaluated the earthquake-tsunami scenario against multiple available datasets. Sustained supershear rupture velocity, or when the earthquake front moves faster than the seismic waves near the slipping faults, is required to match simulation to observations. The modeled tsunami wave amplitudes match the available wave measurements and the modeled inundation elevation (defined as the sum of the ground elevation and the maximum water height) qualitatively match field observations. This approach offers a rapid, physics-based evaluation of the earthquake-tsunami interactions during this puzzling sequence of events. ""Finding that earthquake displacements probably played a critical role generating the Palu tsunami is as surprising as the very fast movements during the earthquake itself,"" said Thomas Ulrich, PhD student at Ludwig Maximilian University of Munich and lead author of the paper. ""We hope that our study will launch a much closer look on the tectonic settings and earthquake physics potentially favouring localized tsunamis in similar fault systems worldwide."" "
Science Daily,Silicon as a Semiconductor: Silicon Carbide Would Be Much More Efficient,Computers & Math,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111650.htm,"   Energy consumption is growing across the globe; electric power is being relied upon more and more, and sustainable energy supplies such as wind and solar power are becoming increasingly important. Electric power, however, is often generated a long distance away from the consumer. Efficient distribution and transport systems are thus just as crucial as transformer stations and power converters that turn the generated direct current into alternating current. Huge savings are possible Modern power electronics must be able to handle large currents and high voltages. Current transistors made of semiconductor materials for field-effect transistors are now mainly based on silicon technology. Significant physical and chemical advantages, however, arise from the use of SiC over silicon: in addition to a much higher heat resistance, this material provides significantly better energy efficiency, which could lead to massive savings. It is known that these advantages are significantly compromised by defects at the interface between silicon carbide and the insulating material silicon dioxide. This damage is based on tiny, irregular clusters of carbon rings bound in the crystal lattice, as experimentally demonstrated by researchers led by Professor Thomas Jung at the Swiss Nanoscience Institute and Department of Physics from the University of Basel and the Paul Scherrer Institute. Using atomic force microscope analysis and Raman spectroscopy, they showed that the defects are generated in the vicinity of the interface by the oxidation process. Experimentally confirmed The interfering carbon clusters, which are only a few nanometers in size, are formed during the oxidation process of silicon carbide to silicon dioxide under high temperatures. ""If we change certain parameters during oxidation, we can influence the occurrence of the defects,"" says doctoral student Dipanwita Dutta. For example, a nitrous oxide atmosphere in the heating process leads to significantly fewer carbon clusters. The experimental results were confirmed by the team led by Professor Stefan Gödecker at the Department of Physics and Swiss Nanoscience Institute from the University of Basel. Computer simulations confirmed the structural and chemical changes induced by graphitic carbon atoms as observed experimentally. Beyond experiments, atomistic insight has been gained in the generation of the defects and their impact on the electron flow in the semiconductor material. Better use of electricity ""Our studies provide important insight to drive the onward development of field-effect transistors based on silicon carbide. Therefore we expect to provide a significant contribution to the more effective use of electrical power,"" comments Jung. The work was initiated as part of the Nano Argovia program for applied research projects. "
Science Daily,Role of Earthquake Motions in Triggering a 'Surprise' Tsunami,Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905090931.htm,"   The tsunami was as surprising to scientists as it was devastating to communities in Sulawesi. It occurred near an active plate boundary, where earthquakes are common. Surprisingly, the earthquake caused a major tsunami, although it primarily offset the ground horizontally -- normally, large-scale tsunamis are typically caused by vertical motions. Researchers were at a loss -- what happened? How was the water displaced to create this tsunami: by landslides, faulting, or both? Satellite data of the surface rupture suggests relatively straight, smooth faults, but do not cover areas offshore, such as the critical Palu Bay. Researchers wondered -- what is the shape of the faults beneath Palu Bay and is this important for generating the tsunami? This earthquake was extremely fast. Could rupture speed have amplified the tsunami? Using a supercomputer operated by the Leibniz Supercomputing Centre, a member of the Gauss Centre for Supercomputing, the team showed that the earthquake-induced movement of the seafloor beneath Palu Bay itself could have generated the tsunami, meaning the contribution of landslides is not required to explain the tsunami's main features. The team suggests an extremely fast rupture on a straight, tilted fault within the bay. In their model, slip is mostly lateral, but also downward along the fault, resulting in anywhere from 0.8 metres to 2.8 metres vertical seafloor change that averaged 1.5 metres across the area studied. Critical to generating this tsunami source are the tilted fault geometry and the combination of lateral and extensional strains exerted on the region by complex tectonics. The scientists come to this conclusion using a cutting-edge, physics-based earthquake-tsunami model. The earthquake model, based on earthquake physics, differs from conventional data-driven earthquake models, which fit observations with high accuracy at the cost of potential incompatibility with real-world physics. It instead incorporates models of the complex physical processes occurring at and off of the fault, allowing researchers to produce a realistic scenario compatible both with earthquake physics and regional tectonics. The researchers evaluated the earthquake-tsunami scenario against multiple available datasets. Sustained supershear rupture velocity, or when the earthquake front moves faster than the seismic waves near the slipping faults, is required to match simulation to observations. The modeled tsunami wave amplitudes match the available wave measurements and the modeled inundation elevation (defined as the sum of the ground elevation and the maximum water height) qualitatively match field observations. This approach offers a rapid, physics-based evaluation of the earthquake-tsunami interactions during this puzzling sequence of events. ""Finding that earthquake displacements probably played a critical role generating the Palu tsunami is as surprising as the very fast movements during the earthquake itself,"" said Thomas Ulrich, PhD student at Ludwig Maximilian University of Munich and lead author of the paper. ""We hope that our study will launch a much closer look on the tectonic settings and earthquake physics potentially favouring localized tsunamis in similar fault systems worldwide."" "
Science Daily,Diversity Increases Ecosystem Stability,Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905094056.htm,"   As the researchers state, there is increasing scientific evidence of positive relationships between the diversity of tree species and ecosystem functioning. However most studies on this relationship to date have used either data from forests where the influence of biodiversity cannot be separated from other factors, or from young planted experiments, which do not provide data on longer periods of time. Therefore, the Freiburg research team analyzed data from the Sardinilla experiment which was planted in Panama in 2001. This experiment covers 22 plots planted with one, two, three or five native tree species. Since these grow at different rates, the plots with a greater variety of species also have a greater structural diversity with regard to the height and diameter of the trees. Annual data on the size and height of the trees, which are seen as indicators of the productivity and stability of the ecosystem, come from the period 2006 to 2016. The study concludes that mixtures of two and three tree species have on average a 25 to 30 per cent higher productivity than monocultures, and those with five species even 50 percent higher. The differences during a severe dry period caused by the tropical climate phenomenon El Niño were especially pronounced. This indicates that forests with a greater diversity of tree species are not only more productive, but also more stable and resilient under drought stress -- the researchers believe this is a particularly important finding in view of global climate change. In the context of initiatives that aim to reduce atmospheric CO2 with extensive reforestation, these results indicate that to store the same amount of CO2 in biomass, far less space is needed with mixed-species forests. According to the team, these results offer new insights into the dynamics of tropical plantation forests and emphasize the importance of analyses that cover a longer development period, since they contribute to a better understanding of the connections between the diversity, productivity and stability of ecosystems. The study is based on Florian Schnabel's master thesis, for which he will be receiving the Hansjürg-Steinlin prize, a University of Freiburg award for new talent, in October at the start of the 2019/20 academic year. Florian Schnabel is now a PhD student involved in the TreeDì project at the German Centre for Integrative Biodiversity Research (iDiv) in Leipzig. "
Science Daily,Groundwater Studies Can Be Tainted by 'Survivor Bias',Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905080043.htm,"   Researchers at the University of Waterloo uncovered the problem while examining a discrepancy between scientific data and anecdotal evidence in southern India. Reports on thousands of wells and satellite images taken between 1996 and 2016 suggested groundwater levels were rising, good news in an area where it is vitally important for agriculture. At the same time, however, fieldworkers were hearing more stories from farmers about wells running dry, suggesting levels were actually declining. Researchers solved the apparent paradox by first obtaining census data that backed up the anecdotal evidence. It showed, for example, that more farmers were digging expensive deep wells in the hard-rock aquifer. ""If indeed groundwater levels are going up, why would farmers choose to pay more and dig deeper wells?"" asked Nandita Basu, a civil and environmental engineering professor. ""It didn't make sense."" Researchers then examined the well data and found that those with missing water level data were often excluded from analysis because they were considered unreliable. When the excluded wells were added back into the mix, the results confirmed the evidence from farmers that groundwater levels were decreasing, not increasing. ""They were systematically picking the wells with a lot of data and potentially ignoring the wells that were going dry because they had incomplete data,"" said Tejasvi Hora, an engineering PhD student who led the research. The culprit was identified as something called 'survivor bias,' a statistical phenomenon that results in the exclusion of negative data. When wells ran dry, there were no water levels to report. That created gaps in reports for those wells, and their incomplete data was then discarded as inferior to the complete data from good wells that hadn't run dry. Basu, also a professor of earth and environmental sciences and a member of the Water Institute at Waterloo, said the lesson from southern India is applicable anywhere in the world that groundwater levels are monitored and analyzed. ""Our main point is that bad data is good data,"" she said. ""When you have wells with a lot of missing data points, that is telling you something important. Take notice of it."" ""Whenever you're focusing only on complete data, you should take a step back and ask if there is a reason for the incomplete data, a systematic bias in your data source,"" Hora said. "
Science Daily,"Plant Research Could Benefit Wastewater Treatment, Biofuels and Antibiotics",Earth & Climate,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905080134.htm,"   The study is in the journal Proceedings of the National Academy of Sciences. The researchers used a new DNA sequencing approach to study the genome of Spirodela polyrhiza, one of 37 species of duckweed, which are small, fast-growing aquatic plants found worldwide. The scientists discovered how the immune system of Spirodela polyrhiza adapts to a polluted environment in a way that differs from land plants. They identified the species' powerful genes that protect against a wide range of harmful microbes and pests, including waterborne fungi and bacteria. The study could help lead to the use of duckweed strains for bioreactors that recycle wastes, and to make drugs and other products, treat agricultural and industrial wastewater and make biofuels such as ethanol for automobiles. Duckweed could also be used to generate electricity. ""The new gene sequencing approach is a major step forward for the analysis of entire genomes in plants and could lead to many societal benefits,"" said co-author Joachim Messing, Distinguished University Professor and director of the Waksman Institute of Microbiology at Rutgers University-New Brunswick. Duckweed can also serve as protein- and mineral-rich food for people, farmed fish, chickens and livestock, especially in developing countries, according to Eric Lam, a Distinguished Professor in Rutgers' School of Environmental and Biological Sciences who was not part of this study. Lam's lab is at the vanguard of duckweed farming research and development. His team houses the world's largest collection of duckweed species and their 900-plus strains. The lead author was in Messing's laboratory and now has her own laboratory at Shanghai Jiao Tong University in China. Scientists at the Chinese Academy of Sciences and Chinese Academy of Agricultural Sciences contributed to the study. "
Science Daily,"From the Tropics to the Boreal, Temperature Drives Ecosystem Functioning",Earth & Climate,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904165229.htm,"   ""Temperature influences many ecological processes and has been used to explain patterns of biodiversity for over a century; however, we still don't have a clear understanding of how temperature influences the functioning of ecosystems,"" Buzzard said. But by measuring and comparing the traits of diverse species to understand how they function in their environment across a range of temperatures, the team uncovered how temperature influences an ecosystem. They found that temperature drives coordinated shifts in the functional traits between plants and microbes that influence ecosystems, according to Buzzard, who is the lead author on the paper published in Nature Ecology and Evolution on Aug. 19. ""The work represents an unprecedented monitoring of soils and forests from hot tropical forests to the cold boreal forests and fills important gaps in our ecological understanding of how organisms within different levels of an ecosystem's food chain are linked via temperature,"" Enquist said of the project that began in 2011. ""The work involved much field work in remote locations, lab work associated with analyzing soil microbial DNA and computer analyses using large datasets."" As an example, bacteria within certain communities have genes tailored by evolution for cycling the nutrients that are naturally available within their ecosystem. The team saw a shift in the genes tied to nutrient cycling for bacteria as temperatures differed across sites. ""As you increase the latitude -- so, cooler temperatures -- we have a nitrogen limitation. We expect that to influence the structuring of these communities, both plants and microbes,"" Enquist said. In tropical forests, on the other hand, trees quickly grow and shed very broad leaves. That means these ""throw-away leaves,"" as Buzzard put it, constantly fall to the floor for microbes to consume. The research team saw a lower abundance of genes in the local microbes for processing carbon. In forests with pine trees that sprout and drop dense, narrow leaves, the local bacteria had different functional traits: They have a greater abundance of carbon-cycling genes to handle the complex-difficult to access large pools of carbon that is available in temperate regions. It's like eating lettuce (tropics) verse eating bark (temperate regions), according to Buzzard. ""We can use this understanding to make predictions about how we expect soil microbial communities to function as climate changes,"" Buzzard said. ""If temperature drives the observed shift in plant and bacterial functioning, ecosystems subjected to climate warming should also experience directional shifts in functional diversity and biogeochemistry."" That shift might happen too quickly for ecosystems to adapt. She also added that diversity is not solely driven by temperature. There could be other constraining factors that could be teased out in another study. Next, Buzzard said the team will install more sites to collect more data. They also want to monitor how growth rates in plants vary across ecosystems with differing temperatures. This means much more time in the field, but that's no problem for Buzzard. She spent at least six months a year for the first three years of the study in the field: ""I really enjoyed being in the field. There are long days. They're hard, but you get to go see these amazing places and have unique interactions with the wildlife."" "
Science Daily,Underwater Soundscapes Reveal Differences in Marine Environments,Earth & Climate,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904153958.htm,"   Using underwater acoustic monitors, researchers listened in on Stellwagen Bank National Marine Sanctuary off the coast of Boston; Glacier Bay National Park and Preserve in Alaska; National Park of American Samoa; and Buck Island Reef National Monument in the Virgin Islands. They found that the ambient sounds varied widely across the sites and were driven by differences in animal vocalization rates, human activity and weather. The findings demonstrate that sound monitoring is an effective tool for assessing conditions and monitoring changes, said Samara Haver, a doctoral candidate in the College of Agricultural Sciences at OSU and the study's lead author. ""This is a relatively economical way for us to get a ton of information about the environment,"" said Haver, who studies marine acoustics and works out of the Cooperative Institute for Marine Resources Studies, a partnership between OSU and the National Oceanic and Atmospheric Administration at the Hatfield Marine Science Center in Newport. ""Documenting current and potentially changing conditions in the ocean soundscape can provide important information for managing the ocean environment."" The findings were published recently in the journal Frontiers in Marine Science. Co-authors include Robert Dziak, an acoustics scientist with NOAA who holds a courtesy appointment in OSU's College of Earth, Ocean, and Atmospheric Sciences; and other researchers from OSU, NOAA, Cornell University and the National Park Service. Passive acoustic monitoring is seen as a cost-effective and low-impact method for monitoring the marine environment. The researchers' goal was to test how effective acoustic monitoring would be for long-term assessment of underwater conditions. ""Ocean noise levels have been identified as a potential measure for effectiveness of conservation efforts, but until now comparing sound across different locations has been challenging,"" Haver said. ""Using equipment that was calibrated across all of the sites, we were able to compare the sound environments of these diverse areas in the ocean."" The researchers collected low frequency, passive acoustic recordings from each of the locations between 2014 and 2018. They compared ambient sounds as well as sounds of humpback whales, a species commonly found in all four locations. The inclusion of the humpback whale sounds -- mostly songs associated with mating in the southern waters, and feeding or social calls in the northern waters -- gives researchers a way to compare the sounds of biological resources across all the soundscapes, Haver said. The researchers found that ambient sound levels varied across all four study sites and sound levels were driven by differences in animal vocalization rates, human activity and weather. The highest sound levels were found in Stellwagen Bank during the winter/spring, driven by higher animal sound rates, vessel activity and high wind speeds. The lowest sound levels were found in Glacier Bay in the summer. ""Generally, the Atlantic areas were louder, especially around Stellwagen, than the Pacific sites,"" Haver said. ""That makes sense, as there is generally more human-made sound activity in the Atlantic. There also was a lot of vessel noise in the Caribbean."" The researchers also were able to hear how sound in the ocean changes before, during and after hurricanes and other severe storms; the monitoring equipment captured Hurricanes Maria and Irma in the Virgin Islands and Tropical Cyclone Winston in American Samoa. Ultimately, the study provides a baseline for these four regions and can be used for comparison over time. Documenting current and potentially changing conditions in the ocean soundscape can provide important information for managing the ocean environment, particularly in and around areas that have been designated as protected, Haver said. "
Science Daily,Mathematical Model Provides New Support for Environmental Taxes,Earth & Climate,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904141300.htm,"   A worldwide ""green development"" movement calls for reducing pollution and increasing resource utilization efficiency without hindering economic expansion. Many governments have proposed or imposed environmental taxes, such as taxes on carbon emissions, to promote environmentally friendly economic practices. However, few studies have rigorously quantified the effects of environmental taxes on the interconnected factors involved in green development. To help clarify the impact of environmental taxation, Fan and colleagues developed and validated a mathematical model that reflects the closely integrated relationships between environmental taxes, economic growth, pollution emissions, and utilization of resources, such as water and fossil fuels. Then they applied the model to real-world data in order to analyze the effects of environmental taxes on green development in China. The analysis suggests that environmental taxes can indeed help to stimulate economic growth, decrease emissions, and improve resource utilization. The researchers explored several different scenarios, finding that the beneficial effects of an environmental tax are enhanced by advanced technology, elevated consumer awareness, and -- especially -- firm government control. The authors suggest that their model could be applied to explore the effects of environmental taxes in other countries beyond China. Researchers may also seek to modify the model for application to different industries or economic sectors, as opposed to countries or regions. The model could potentially be improved by identification and incorporation of more sophisticated mathematical relationships between the various green development factors. "
Science Daily,Solutions to Urban Heat Differ Between Tropical and Drier Climes,Earth & Climate,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904135649.htm,"   Understanding a city's heat island effect is critical for developing strategies to reduce energy use and stave off dangerously high temperatures, said Elie Bou-Zeid, one of the study's authors and a professor in Princeton's Department of Civil and Environmental Engineering. The analysis suggests that cooling cities by planting more vegetation may be more effective in drier regions than in wetter ones. Using summer temperature data from more than 30,000 world cities, Bou-Zeid worked with colleagues at ETH Zurich and Duke University to develop a new model for urban heat islands. The novelty of the model is that it uses population and precipitation as proxies for a complex array of factors involving climate, environment and urban engineering. One of the model's benefits is its simplicity. Although it cannot capture details of individual cities, it can give planners a quick and broadly accurate view of possible solutions and their effects on a city's temperature. ""There are a few cities -- New York, London, Baltimore -- that are studied intensively, and we don't know very much about a large range of other cities,"" said Bou-Zeid. ""With a reduced model that only needs information on precipitation and population, we are hoping to provide a simple framework that can give guidance to any city"" in planning heat mitigation efforts. The heat island effect, defined in the study as the surface temperature difference between urban and rural areas, is somewhat larger for cities with higher populations. One key reason is that these cities tend to have larger areas as well as more high-rise buildings that do not dissipate heat as effectively as lower structures. The researchers also found that the heat island effect increases as a city's average annual precipitation increases, since its surroundings become greener and cooler, but only up to a point. Beyond a precipitation level of about 39 inches (100 centimeters) per year -- similar to that of Washington, D.C. -- a city's temperature boost does not rise much above 2 degrees Fahrenheit (1.25 degrees Celsius). This has implications for approaches to cooling wetter cities. Although planting vegetation can lower city temperatures through evapotranspiration, there are limits to this approach. Southeast Asian cities such as Singapore have high precipitation and large areas of green cover, but have strong urban heat island effects because nearby rainforests inevitably contain far more plant life than the city. On the other hand, drier cities such as Phoenix can be even cooler than surrounding areas in the summer if irrigation is used to grow plants in the city. ""In places that are already wet and vegetated, adding more vegetation is not going to help,"" explained Bou-Zeid. Lowering summer temperatures in these cities will require different solutions, such as increasing shading or ventilation, or building with novel materials. Still, all cities can reap other benefits from green areas, such as improved air quality and opportunities for recreation. ""Our results show that there is no one-size-fits-all solution to reduce city-scale warming,"" said Gabriele Manoli, a research fellow at ETH Zurich and the study's lead author. ""The efficiency of heat mitigation strategies varies across geographic regions, and any effort aimed at greening and cooling world cities should be put in the context of local hydro-climatic conditions."" Given that urban areas will face the combined effects of global climate change and population growth, these results can provide guidance for the climate-sensitive design of future cities. Bou-Zeid and his colleagues are working to extend their model to examine seasonal variations in the urban heat island effect. Their framework could also be used to create more tailored models for specific regions of the world. "
Science Daily,Report Cards on Women in STEM Fields Finds Much Room for Improvement,Science & Society,2019-09-05,-,https://www.sciencedaily.com/releases/2019/09/190905111636.htm,"   ""The data suggest that we are making headway,"" says Reshma Jagsi, a radiation oncologist and director of the Center for Bioethics and Social Sciences in Medicine at the University of Michigan and one of the corresponding authors. ""That said, there are still many institutions that have few women in senior-most faculty positions. There also remains quite a bit of room for improvement in certain areas, including the representation of women in certain roles, such as speaking at scientific meetings."" The researchers obtained their data through the use of institutional report cards that were collected when individual researchers applied for grants from NYSCF. The report cards were part of a 2014 NYSCF project that put forward a number of strategies aimed at helping to achieve gender parity in science, technology, engineering, and math (STEM). Of the 1,287 report cards that were submitted, 741 provided complete information for a given year, and some included multiple years. Overall, the data in the paper represent 541 institutions in 38 countries in North America (72%) and Europe (18%). The investigators found that although women made up more than half of the population among undergraduate, graduate, and post-graduate students, the picture became different as seniority increased. Women made up 42% of assistant professors, 34% of associate professors, and 23% of full professors. These rates varied greatly by institution: At about one-third of the institutions surveyed, women made up less than 10% of tenured faculty recruits. ""We expected to find that women would be better represented at more junior ranks compared with senior ranks,"" Jagsi says. ""But I found it noteworthy that there were regional differences. For example, institutions in Europe come closer to achieving gender parity."" The researchers say their findings suggest that the primary issue is not recruiting women into STEM roles but retaining them and promoting them into more influential positions. They also point out the important part that funding organizations can play. ""Funding organizations are in a unique position to require institutional leaders to pay attention to equity, diversity, and inclusion within their organizations,"" Jagsi says. ""By requiring these report cards, they can promote actions that help all scientists thrive. We hope that other funding bodies, like the NIH, will adopt a similar report card."" The next phase of IWISE will focus on highlighting best practices undertaken by institutions. This will provide comparative data and allow the researchers to monitor progress over time. The researchers will also look at other factors that may influence the recruitment and retention of women scientists, such as the presence of women in top leadership roles, the rates at which tenured women stay in their positions, and equity in salaries across gender, race, and ethnicity. ""For my own work, I plan to begin to focus more on issues of intersectionality,"" Jagsi concludes. ""A particularly understudied area involves the career experiences in women with other minority identities, such as race. Further research is needed to understand the challenges these women face."" "
Science Daily,"'Mental Rigidity' at Root of Intense Political Partisanship on Both Left and Right, Study Finds",Science & Society,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829081401.htm,"   This ""mental rigidity"" makes it harder for people to change their ways of thinking or adapt to new environments, say researchers. Importantly, mental rigidity was found in those with the most fervent beliefs and affiliations on both the left and right of the political divide. The study of over 700 US citizens, conducted by scientists from the University of Cambridge, is the largest -- and first for over 20 years -- to investigate whether the more politically ""extreme"" have a certain ""type of mind"" through the use of objective psychological testing. The findings suggest that the basic mental processes governing our ability to switch between different concepts and tasks are linked to the intensity with which we attach ourselves to political doctrines -- regardless of the ideology. ""Relative to political moderates, participants who indicated extreme attachment to either the Democratic or Republican Party exhibited mental rigidity on multiple objective neuropsychological tests,"" said Dr Leor Zmigrod, a Cambridge Gates Scholar and lead author of the study, now published in the Journal of Experimental Psychology. ""While political animosity often appears to be driven by emotion, we find that the way people unconsciously process neutral stimuli seems to play an important role in how they process ideological arguments."" ""Those with lower cognitive flexibility see the world in more black-and-white terms, and struggle with new and different perspectives. The more inflexible mind may be especially susceptible to the clarity, certainty, and safety frequently offered by strong loyalty to collective ideologies,"" she said. The research is the latest in a series of studies from Zmigrod and her Cambridge colleagues, Dr Jason Rentfrow and Professor Trevor Robbins, on the relationship between ideology and cognitive flexibility. Their previous work over the last 18 months has suggested that mental rigidity is linked to more extreme attitudes with regards to religiosity, nationalism, and a willingness to endorse violence and sacrifice one's life for an ideological group. For the latest study, the Cambridge team recruited 743 men and women of various ages and educational backgrounds from across the political spectrum through the Amazon Mechanical Turk platform. Participants completed three psychological tests online: a word association game, a card-sorting test -- where colours, shapes and numbers are matched according to shifting rules -- and an exercise in which participants have a two-minute window to imagine possible uses for everyday objects. ""These are established and standardized cognitive tests which quantify how well individuals adapt to changing environments and how flexibly their minds process words and concepts,"" said Zmigrod. The participants were also asked to score their feelings towards various divisive social and economic issues -- from abortion and marriage to welfare -- and the extent of ""overlap"" between their personal identity and the US Republican and Democrat parties. Zmigrod and colleagues found that ""partisan extremity"" -- the intensity of participants' attachment to their favoured political party -- was a strong predictor of rigidity in all three cognitive tests. They also found that self-described Independents displayed greater cognitive flexibility compared to both Democrats and Republicans. Other cognitive traits, such as originality or fluency of thought, were not related to heightened political partisanship, which researchers argue suggests the unique contribution of cognitive inflexibility. ""In the context of today's highly divided politics, it is important we work to understand the psychological underpinnings of dogmatism and strict ideological adherence,"" said Zmigrod. ""The aim of this research is not to draw false equivalences between different, and sometimes opposing, ideologies. We want to highlight the common psychological factors that shape how people come to hold extreme views and identities,"" said Zmigrod. ""Past studies have shown that it is possible to cultivate cognitive flexibility through training and education. Our findings raise the question of whether heightening our cognitive flexibility might help build more tolerant societies, and even develop antidotes to radicalization."" ""While the conservatism and liberalism of our beliefs may at times divide us, our capacity to think about the world flexibly and adaptively can unite us,"" she added. "
Science Daily,Snack Tax May Be More Effective Than a Sugary Drink Tax to Tackle Obesity,Science & Society,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904194447.htm,"   The researchers say this option ""is worthy of further research and consideration as part of an integrated approach to tackling obesity."" Obesity rates are increasing across the world. In the UK, obesity is estimated to affect around 1 in every 4 adults and around 1 in every 5 children aged 10 to 11, with higher rates among those living in more deprived areas. The use of taxes to lower sugar and energy intake have mainly focused on sugar sweetened drinks. But in the UK, high sugar snacks, such as biscuits, cakes, chocolates and sweets make up more free sugar and energy intake than sugary drinks. Reducing purchases of high sugar snacks therefore has the potential to make a greater impact on population health than reducing the purchase of sugary drinks. To explore this in more detail, researchers used economic modelling to assess the impact of a 20% price increase on high sugar snack foods in the UK. Modelling was based on food purchase data for 36,324 UK households and National Diet and Nutrition Survey data for 2,544 adults. Results were grouped by household income and body mass index (BMI) to estimate changes in weight and prevalence of obesity over one year. The results suggest that for all income groups combined, increasing the price of biscuits, cakes, chocolates, and sweets by 20% would reduce annual average energy intake by around 8,900 calories, leading to an average weight loss of 1.3 kg over one year. In contrast, a similar price increase on sugary drinks would result in an average weight loss of just 203 g over one year. What's more, the model predicts that the impact of the price increase would be largest in low income households with the highest rates of obesity, suggesting that taxing high sugar snacks could help to reduce health inequalities driven by diet related diseases, say the researchers. They point to some possible study limitations, such as the relatively short, one-year, time-frame over which weight changes were modelled, but say findings were based on information from high quality databases and remained largely unchanged after varying some key assumptions. As such, they say that a 20% price increase in high sugar snacks, ""has the potential to reduce overall energy purchased among all body mass index and income groups in the UK, leading to an estimated population level reduction in obesity prevalence of 2.7 percentage points after the first year."" ""The results also suggest that price increases in high sugar snacks could also make an important contribution to reducing health inequalities driven by diet related disease,"" they conclude. There is a strong rationale for using fiscal policy to improve diet and health, but caution is needed, say researchers in a linked editorial. For example, they point out that substitution and displacement effects in response to food tax and subsidy policies are complicated and difficult to predict, while product reformulation in response to consumer demand can also have unintended consequences. They also argue that fiscal policies aimed at reducing sugar, salt, and saturated fat intake ""might be useful, but they fail to incentivise the consumption of healthy foods."" Ultimately, tackling obesity and diet related disease ""requires close scrutiny of the social determinants of food environments and a systemic, sustained group of initiatives aimed at reducing health inequalities,"" they conclude. "
Science Daily,Mathematical Model Provides New Support for Environmental Taxes,Science & Society,2019-09-04,-,https://www.sciencedaily.com/releases/2019/09/190904141300.htm,"   A worldwide ""green development"" movement calls for reducing pollution and increasing resource utilization efficiency without hindering economic expansion. Many governments have proposed or imposed environmental taxes, such as taxes on carbon emissions, to promote environmentally friendly economic practices. However, few studies have rigorously quantified the effects of environmental taxes on the interconnected factors involved in green development. To help clarify the impact of environmental taxation, Fan and colleagues developed and validated a mathematical model that reflects the closely integrated relationships between environmental taxes, economic growth, pollution emissions, and utilization of resources, such as water and fossil fuels. Then they applied the model to real-world data in order to analyze the effects of environmental taxes on green development in China. The analysis suggests that environmental taxes can indeed help to stimulate economic growth, decrease emissions, and improve resource utilization. The researchers explored several different scenarios, finding that the beneficial effects of an environmental tax are enhanced by advanced technology, elevated consumer awareness, and -- especially -- firm government control. The authors suggest that their model could be applied to explore the effects of environmental taxes in other countries beyond China. Researchers may also seek to modify the model for application to different industries or economic sectors, as opposed to countries or regions. The model could potentially be improved by identification and incorporation of more sophisticated mathematical relationships between the various green development factors. "
Science Daily,Parental Burnout Can Lead to Harmful Outcomes for Parent and Child,Education & Learning,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828080538.htm,"   ""In the current cultural context, there is a lot of pressure on parents,"" says lead researcher Moïra Mikolajczak of UCLouvain. ""But being a perfect parent is impossible and attempting to be one can lead to exhaustion. Our research suggests that whatever allows parents to recharge their batteries, to avoid exhaustion, is good for children."" Mikolajczak and coauthors James J. Gross of Stanford University and Isabelle Roskam of UCLouvain became interested in the issue through their clinical encounters with good parents who, as a result of their exhaustion, had become the opposite of what they were trying to be. Although previous research had explored the causes of parental burnout, relatively little was known about its consequences. The researchers decided to directly examine the outcomes associated with parental burnout in two studies that followed parents over time. In the first study, Mikolajczak and colleagues recruited parents through social networks, schools, pediatricians, and other sources to participate in research on ""parental well-being and exhaustion."" The parents, mostly French-speaking adults in Belgium, completed three batches of online surveys spaced about 5.5 months apart. The surveys included a 22-item measure of parental burnout that gauged parents' emotional exhaustion, emotional distancing, and feelings of inefficacy; a six-item measure that gauged their thoughts about escaping their family; a 17-item measure that gauged the degree to which they neglected their childrens' physical, educational and emotional needs; and a 15-item measure that gauged their tendency to engage in verbal, physical, or psychological violence. Because many of the questions asked about sensitive topics, the researchers also measured participants' tendency to choose the most socially desirable responses when confronted with probing questions. A total of 2,068 parents participated in the first survey, with 557 still participating at the third survey. Participants' data revealed a strong association between burnout and the three variables -- escape ideation, parental neglect, and parental violence -- at each of the three time points. Parental burnout at the first and second survey was associated with later parental neglect, parental violence, and escape ideation. The researchers found that parental burnout and parental neglect had a circular relationship: Parental burnout led to increased parental neglect, which led to increased burnout, and so on. Parental violence appeared to be a clear consequence of burnout. Importantly, all of these patterns held even when the researchers took participants' tendency toward socially desirable responding into account. A second online study with mostly English-speaking parents in the UK produced similar findings. Together, the data suggest that parental burnout is likely the cause of escape ideation, parental neglect, and parental violence. ""We were a bit surprised by the irony of the results,"" says Mikolajczak. ""If you want to do the right thing too much, you can end up doing the wrong thing. Too much pressure on parents can lead them to exhaustion which can have damaging consequences for the parent and for the children."" Additional studies are needed to confirm and extend these findings with broader samples and measures. Nonetheless, the robust pattern of results suggests that there are important lessons to be learned from these findings, the researchers say. ""Parents need to know that self-care is good for the child and that when they feel severely exhausted, they should seek help. Health and child services professionals need to be informed about parental burnout so that they can accurately diagnose it and provide parents with the most appropriate care. And those engaged in policy and public health need to help raise awareness and lift the taboo on parental burnout, which will encourage parents to seek the help they need,"" Mikolajczak concludes. "
Science Daily,Mindfulness for Middle School Students,Education & Learning,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826153630.htm,"   ""By definition, mindfulness is the ability to focus attention on the present moment, as opposed to being distracted by external things or internal thoughts. If you're focused on the teacher in front of you, or the homework in front of you, that should be good for learning,"" says John Gabrieli, the Grover M. Hermann Professor in Health Sciences and Technology, a professor of brain and cognitive sciences, and a member of MIT's McGovern Institute for Brain Research. The researchers also showed, for the first time, that mindfulness training can alter brain activity in students. Sixth-graders who received mindfulness training not only reported feeling less stressed, but their brain scans revealed reduced activation of the amygdala, a brain region that processes fear and other emotions, when they viewed images of fearful faces. Together, the findings suggest that offering mindfulness training in schools could benefit many students, says Gabrieli, who is the senior author of both studies. ""We think there is a reasonable possibility that mindfulness training would be beneficial for children as part of the daily curriculum in their classroom,"" he says. ""What's also appealing about mindfulness is that there are pretty well-established ways of teaching it."" In the moment Both studies were performed at charter schools in Boston. In one of the papers, which appears today in the journal Behavioral Neuroscience, the MIT team studied about 100 sixth-graders. Half of the students received mindfulness training every day for eight weeks, while the other half took a coding class. The mindfulness exercises were designed to encourage students to pay attention to their breath, and to focus on the present moment rather than thoughts of the past or the future. Students who received the mindfulness training reported that their stress levels went down after the training, while the students in the control group did not. Students in the mindfulness training group also reported fewer negative feelings, such as sadness or anger, after the training. About 40 of the students also participated in brain imaging studies before and after the training. The researchers measured activity in the amygdala as the students looked at pictures of faces expressing different emotions. At the beginning of the study, before any training, students who reported higher stress levels showed more amygdala activity when they saw fearful faces. This is consistent with previous research showing that the amygdala can be overactive in people who experience more stress, leading them to have stronger negative reactions to adverse events. ""There's a lot of evidence that an overly strong amygdala response to negative things is associated with high stress in early childhood and risk for depression,"" Gabrieli says. After the mindfulness training, students showed a smaller amygdala response when they saw the fearful faces, consistent with their reports that they felt less stressed. This suggests that mindfulness training could potentially help prevent or mitigate mood disorders linked with higher stress levels, the researchers say. Evaluating mindfulness In the other paper, which appeared in the journal Mind, Brain, and Education in June, the researchers did not perform any mindfulness training but used a questionnaire to evaluate mindfulness in more than 2,000 students in grades 5-8. The questionnaire was based on the Mindfulness Attention Awareness Scale, which is often used in mindfulness studies on adults. Participants are asked to rate how strongly they agree with statements such as ""I rush through activities without being really attentive to them."" The researchers compared the questionnaire results with students' grades, their scores on statewide standardized tests, their attendance rates, and the number of times they had been suspended from school. Students who showed more mindfulness tended to have better grades and test scores, as well as fewer absences and suspensions. ""People had not asked that question in any quantitative sense at all, as to whether a more mindful child is more likely to fare better in school,"" Gabrieli says. ""This is the first paper that says there is a relationship between the two."" The researchers now plan to do a full school-year study, with a larger group of students across many schools, to examine the longer-term effects of mindfulness training. Shorter programs like the two-month training used in the Behavioral Neuroscience study would most likely not have a lasting impact, Gabrieli says. ""Mindfulness is like going to the gym. If you go for a month, that's good, but if you stop going, the effects won't last,"" he says. ""It's a form of mental exercise that needs to be sustained."" "
