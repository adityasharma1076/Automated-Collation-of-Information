Source,Heading,Category,Date,Time,URL,Text
Science Daily,Suggested Move to Plant-Based Diets Risks Worsening Brain Health Nutrient Deficiency,Mind & Brain,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829184143.htm,"   To make matters worse, the UK government has failed to recommend or monitor dietary levels of this nutrient -- choline -- found predominantly in animal foods, says Dr Emma Derbyshire, of Nutritional Insight, a consultancy specialising in nutrition and biomedical science. Choline is an essential dietary nutrient, but the amount produced by the liver is not enough to meet the requirements of the human body. Choline is critical to brain health, particularly during fetal development. It also influences liver function, with shortfalls linked to irregularities in blood fat metabolism as well as excess free radical cellular damage, writes Dr Derbyshire. The primary sources of dietary choline are found in beef, eggs, dairy products, fish, and chicken, with much lower levels found in nuts, beans, and cruciferous vegetables, such as broccoli. In 1998, recognising the importance of choline, the US Institute of Medicine recommended minimum daily intakes. These range from 425 mg/day for women to 550 mg/day for men, and 450 mg/day and 550 mg/day for pregnant and breastfeeding women, respectively, because of the critical role the nutrient has in fetal development. In 2016, the European Food Safety Authority published similar daily requirements. Yet national dietary surveys in North America, Australia, and Europe show that habitual choline intake, on average, falls short of these recommendations. ""This is....concerning given that current trends appear to be towards meat reduction and plant-based diets,"" says Dr Derbyshire. She commends the first report (EAT-Lancet) to compile a healthy food plan based on promoting environmental sustainability, but suggests that the restricted intakes of whole milk, eggs and animal protein it recommends could affect choline intake. And she is at a loss to understand why choline does not feature in UK dietary guidance or national population monitoring data. ""Given the important physiological roles of choline and authorisation of certain health claims, it is questionable why choline has been overlooked for so long in the UK,"" she writes. ""Choline is presently excluded from UK food composition databases, major dietary surveys, and dietary guidelines,"" she adds. It may be time for the UK government's independent Scientific Advisory Committee on Nutrition to reverse this, she suggests, particularly given the mounting evidence on the importance of choline to human health and growing concerns about the sustainability of the planet's food production. ""More needs to be done to educate healthcare professionals and consumers about the importance of a choline-rich diet, and how to achieve this,"" she writes. ""If choline is not obtained in the levels needed from dietary sources per se then supplementation strategies will be required, especially in relation to key stages of the life cycle, such as pregnancy, when choline intakes are critical to infant development,"" she concludes. "
Science Daily,Delivering Immunotherapy Directly to Brain Tumors,Mind & Brain,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150145.htm,"   The study, published this week on the peer-reviewed open access journal Nature Communications, demonstrated that a new type of nano-immunotherapy traversed the blood-brain barrier in laboratory mice, inducing a local immune response in brain tissue surrounding the tumors. The tumor cells stopped multiplying, and survival rates increased. For patients with glioblastoma, the most common and also most deadly form of brain cancer, immunotherapies like this could hold the key to longer survival, said Julia Ljubimova, MD, PhD, senior author of the study and professor of Neurosurgery and Biomedical Sciences at Cedars-Sinai. ""This study showed a promising and exciting outcome,"" Ljubimova said. ""Current clinically proven methods of brain cancer immunotherapy do not ensure that therapeutic drugs cross the blood-brain barrier. Although our findings were not made in humans, they bring us closer to developing a treatment that might effectively attack brain tumors with systematic drug administration."" Harnessing the power of the body's own immune system to attack tumors is a concept that has intrigued investigators for decades. Scientists have been studying ways to persuade the immune system to attack tumors in the same way that it attacks, for example, a virus. While promising, this idea presents a few key challenges, especially when it comes to brain tumors. The environment of the brain can be hard to penetrate with drugs or other therapies. The blood-brain barrier, which the body uses to naturally block toxins and other harmful substances in the bloodstream from getting into the brain, can keep out potentially lifesaving treatments. In addition, brain tumors seem to have the effect of suppressing their local immune systems. Tumors accumulate immunological guards such as T regulatory cells (Tregs) and special macrophages, which block the body's anti-cancer immune cells, protecting the tumor from attack, Ljubimova said. In order to allow tumor-killing immune cells to activate, investigators needed to find a way to arrest or deactivate the tumor-protecting Tregs and macrophages. Other immunotherapies have been successful in triggering an immune response in the whole body, which can slow the growth of tumors and extend the life of patients, but this treatment is one of the first of its kind to demonstrate the activation of both whole body and local immune systems around the tumor in laboratory mice. The immunotherapy tested in this study works by delivering checkpoint inhibitors, a type of antibody drug that can arrest and block Tregs and macrophages, so the tumor can't use them to block the incoming tumor-killing immune cells. Those checkpoint inhibitors are attached with a biodegradable polymer to a protein or peptide that enables the drug to traverse the blood-brain barrier. ""The checkpoint inhibitors can then block the Tregs and macrophages, allowing the local immune cells to get activated and do their job-fight the tumor,"" Ljubimova said. With the tumor-shielding cells blocked, immune cells like cytotoxic lymphocytes and microglial cells can then attack and destroy the cancer cells. ""Drug delivery is the major obstacle for the treatment of central nervous system diseases, including brain conditions,"" Ljubimova said. ""The horizon for treatment of brain cancer is getting clearer. We hope that by delivering multifunctional new-generation drugs through the blood-brain barrier, we can explore new therapies for many neurological conditions."" Ljubimova says that further tests are needed before this therapy can be tested in humans. "
Science Daily,New Medication May Be Able to Improve Effects of Psychological Treatment for PTSD,Mind & Brain,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829115420.htm,"   ""We have used a medication that blocks the way the body breaks down its own cannabis-like substances, or 'endocannabinoids'. Our study shows that this class of medications, called FAAH inhibitors, may offer a new way to treat PTSD and perhaps also other stress-related psychiatric conditions. The next important step will be to see if this type of medication works in patients, particularly those with PTSD,"" says Leah Mayo, senior post-doctoral fellow and lead investigator on the study, which was carried out in the laboratory of Professor Markus Heilig at the Center for Social and Affective Neuroscience, CSAN, Linköping University. Post-traumatic stress disorder, PTSD, arises in some -- but not all -- people who have experienced life-threatening events. A person affected by PTSD avoids reminders of the trauma, even when the danger is long gone. Over time, these patients become tense, withdrawn, and experience sleep difficulties. This condition is particularly common among women, where it is often the result of physical or sexual abuse. It is highly debilitating, and current treatment options are limited. PTSD is currently best treated using prolonged exposure therapy, PE. In this treatment, patients are repeatedly exposed to their traumatic memory with the help of a therapist. This ultimately allows patients to acquire new learning: that these memories no longer signal imminent danger. Although clinically useful, effects of PE are limited. Many patients do not benefit, and among those who do, fears frequently return over time. The scientists who carried out the current study examined whether fear extinction learning, the principle behind PE therapy, can be boosted by a medication. The researchers tested a pharmaceutical that affects the endocannabinoid system, which uses the body's own cannabis-like substances to regulate fear and stress-related behaviors. The experimental medication results in increased levels of anandamide, a key endocannabinoid, in regions of the brain that control fear and anxiety. The medication accomplishes this by blocking an enzyme, FAAH (fatty acid amide hydrolase), that normally breaks down anandamide. The FAAH inhibitor tested by the researchers was originally developed for use as a pain killer, but was not effective enough when tested clinically. This early-stage experimental study was randomised, placebo-controlled and double-blind, which means that neither the participants nor the scientists knew who was receiving the active drug (16 people) and who was receiving placebo (29 people). Participants were healthy volunteers. After taking the drug for 10 days, they underwent several psychological and physiological tests. In one of these, participants learned to associate a highly unpleasant sound, that of fingernails scraping across a blackboard, with a specific visual cue -- an image of a red or blue lamp. Once they had learned to respond with fear to the previously innocuous image of the lamp, they were repeatedly re-exposed to it, but now in the absence of the unpleasant sound. This allowed them to unlearn the fear memory. The following day, the scientists measured how well participants remembered this new learning: that the lamp was no longer a threat signal. This process of un-learning fear is the same principle on which PE therapy for PTSD is based. ""We saw that participants who had received the FAAH inhibitor remembered the fear extinction memory much better. This is very exciting,"" say Leah Mayo. ""Numerous promising treatments coming out of basic research on psychiatric disorders have failed when tested in humans. This has created quite a disappointment in the field. This is the first mechanism in a long time where promising results from animal experiments seem to hold up when put to test in people. The next step, of course, is to see whether the treatment works in people with PTSD,"" adds professor Markus Heilig. "
Science Daily,The 'Inflammation' of Opioid Use,Mind & Brain,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829104621.htm,"   Researchers in the lab of James Schwaber at the Daniel Baugh Institute for Functional Genomics and Computational Biology at Thomas Jefferson University are studying how inflammation contributes to drug withdrawal and dependence. Their study was published in Frontiers of Neuroscience on July 3. Opioids can cause inflammation in the brain by inducing immune cells to release inflammatory molecules called cytokines. The main immune cells in the brain are microglia and astrocytes. Inflammatory responses induced by opioids have been observed in the central amygdala, a brain region that has been strongly implicated in opioid dependence because of its role in emotion and motivation. The central amygdala can also be affected by inflammation in other parts of the body, like the gut. In fact, the communication between the gut and the brain can shape a variety of motivated behaviors and emotional states, including those associated with drug dependence and withdrawal. The researchers including first author Sean O'Sullivan in Dr. Schwaber's lab isolated single neurons, microglia, and astrocytes from the central amygdala and studied their genetic profiles in normal, opioid-dependent, and withdrawn rats. They were surprised to find that the profile of astrocytes changed the most, shifting genetic expression to a more activated state. This shift correlated strongly with opioid withdrawal. Furthermore, all three cell types showed a considerable increase in an inflammatory cytokine called TNF alpha in withdrawn animals. In addition, the researchers also assayed different types of bacteria in the gut of rats and found that certain anti-inflammatory bacteria were suppressed in withdrawn animals, shifting the ratio of gut microbiota and causing a phenomenon called dysbiosis, which can cause inflammation in the digestive system. It is unclear how these changes influence opioid withdrawal, but the authors propose that the simultaneous inflammation in the gut and central amygdala may be linked to the negative emotional experience of withdrawal. The findings underscore the highly complex relationship between the gut and the brain, and suggest that inflammation in the gut and brain may exacerbate symptoms associated with withdrawal. Targeting inflammation in these regions may alleviate the negative experience of drug withdrawal, and therefore prevent dependence. "
Science Daily,How to Simulate Softness,Mind & Brain,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150804.htm,"   The findings provide fundamental insights into designing tactile materials and haptic interfaces that can recreate realistic touch sensations, for applications such as electronic skin, prostheses and medical robotics. Researchers detail their findings in the Aug. 30 issue of Science Advances. ""We provide a formula to recreate a spectrum of softness. In doing so, we are helping close the gap in understanding what it takes to recreate some aspects of touch,"" said Charles Dhong, who co-led the study as a postdoctoral fellow at UC San Diego and is now an assistant professor in biomedical engineering at the University of Delaware. Dhong worked with Darren Lipomi, a professor of nanoengineering at UC San Diego and the study's co-corresponding author. Based on the results from their experiments, the researchers created equations that can calculate how soft or hard a material will feel based on material thickness, Young's modulus (a measure of a material's stiffness), and micropatterned areas. The equations can also do the reverse and calculate, for example, how thick or micropatterned a material needs to be to feel a certain level of softness. ""What's interesting about this is that we've found two new ways to tune the perceived softness of an object -- micropatterning and changing the thickness,"" Dhong said. ""Young's modulus is what scientists typically turn to in terms of what's soft or hard. It is a factor, but now we show that it's only one part of the equation."" Recreating softness The researchers began by examining two parameters engineers use to measure a material's perceived softness: indentation depth (how deep a fingertip presses into a material) and contact area between the fingertip and the material. Normally, these parameters both change simultaneously as a fingertip presses into an object. Touch a piece of soft rubber, for example, and the contact area will increase the deeper a fingertip presses in. Dhong, Lipomi and colleagues were curious how indentation depth and contact area independently affect the perception of softness. To answer this question, they specially engineered materials that decoupled the two parameters and then tested them on human subjects. The researchers created nine different elastomeric slabs, each with its own unique ratio of indentation depth to contact area. The slabs differed in amount of micropatterning on the surface, thickness and Young's modulus. Micropatterning is key to the design. It consists of arrays of raised microscopic pillars dotted on the surface of the slabs. These tiny pillars allow a fingertip to press deeper without changing the contact area. This is similar to pressing against the metal pins of a Pinscreen toy, where arrays of pins slide in and out to make a 3D impression. ""By creating these micropatterned surface structures, we produce discontinuous regions of contact where the finger presses in that are much smaller than the shadow it would cast on the surface,"" Lipomi said. The team tested the slabs on 15 subjects and instructed them to perform two tasks. In the first task, they presented subjects with multiple pairs of slabs and asked them to identify the softer one in each pair. In the second task, the researchers had subjects rank the nine slabs from softest to hardest. Overall, the slabs that subjects perceived as softer were thicker, had little to no micropatterning on the surface, and had a low Young's modulus. Meanwhile slabs that felt harder were thinner, had more micropatterning and a high Young's modulus. Softness: a basic ingredient of touch Experiments also led the researchers to an interesting conclusion: the perception of softness is a basic sensation, not a combination of other sensations. ""This means softness is a primary ingredient of the human sense of touch. It's like how we have RGB for color displays,"" Lipomi said. ""If we can find the other 'pixels of touch,' can we combine them to make any tactile image we want? These are the fundamental things we would like to know going forward."" Paper title: ""Role of Indentation Depth and Contact Area on Human Perception of Softness for Haptic Interfaces."" Co-authors include Rachel Miller, Nicholas Root, Sumit Gupta, Laure V. Kayser, Cody W. Carpenter, Kenneth J. Loh and Vilayanur S. Ramachandran, all at UC San Diego. This work was supported by the National Institutes of Health Director's New Innovator Award (grant 1DP2EB022358) and the Office of Naval Research (grant N00014-18-1-2483). "
Science Daily,Creation of New Brain Cells Plays an Underappreciated Role in Alzheimer's Disease,Mind & Brain,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830112832.htm,"   Aß plaques will continue to be a major focus for Alzheimer's researchers. However, new work by neuroscientists at the University of Chicago looks at another process that plays an underappreciated role in the progression of the disease. In a new study published in the Journal of Neuroscience, Sangram Sisodia, PhD, the Thomas Reynolds Sr. Family Professor of Neurosciences at UChicago, and his colleagues show how in genetic forms of Alzheimer's, a process called neurogenesis, or the creation of new brain cells, can be disrupted by the brain's own immune cells. Some types of early onset, hereditary Alzheimer's are caused by mutations in two genes called presenilin 1 (PS1) and presenilin 2 (PS2). Previous research has shown that when healthy mice are placed into an ""enriched"" environment where they can exercise, play and interact with each other, they have a huge increase in new brain cells being created in the hippocampus, part of the brain that is important for memory. But when mice carrying mutations to PS1 and PS2 are placed in an enriched environment, they don't show the same increase in new brain cells. They also start to show signs of anxiety, a symptom often reported by people with early onset Alzheimer's. This led Sisodia to think that something besides genetics had a role to play. He suspected that the process of neurogenesis in mice both with and without Alzheimer's mutations could also be influenced by other cells that interact with the newly forming brain cells. Focus on the microglia The researchers focused on microglia, a kind of immune cell in the brain that usually repairs synapses, destroys dying cells and clears out excess Aß proteins. When the researchers gave the mice a drug that causes microglial cells to die, neurogenesis returned to normal. The mice with presenilin mutations were then placed into an enriched environment and they were fine; they didn't show any memory deficits or signs of anxiety, and they were creating the normal, expected number of new neurons. ""It's the most astounding result to me,"" Sisodia said. ""Once you wipe out the microglia, all these deficits that you see in these mice with the mutations are completely restored. You get rid of one cell type, and everything is back to normal."" Sisodia thinks the microglia could be overplaying their immune system role in this case. Alzheimer's disease normally causes inflammation in the microglia, so when they encounter newly formed brain cells with presenilin mutations they may overreact and kill them off prematurely. He feels that this discovery about the microglia's role opens another important avenue toward understanding the biology of Alzheimer's disease. ""I've been studying amyloid for 30 years, but there's something else going on here, and the role of neurogenesis is really underappreciated,"" he said. ""This is another way to understand the biology of these genes that we know significantly affect the progression of disease and loss of memory."" "
Science Daily,Victorian Child Hearing-Loss Databank to Go Global,Mind & Brain,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830092107.htm,"   The Victorian Childhood Hearing Impairment Longitudinal Databank, which has collected information for eight years, is featured in the latest International Journal of Epidemiology. Its data shows that language development and speech in hearing-impaired children lags behind other children, despite advancements in earlier detection and intervention in the past decade. The paper's* lead author, Murdoch Children's Research Institute's (MCRI) Dr Valerie Sung, says researchers world-wide can use the databank to answer questions around childhood hearing loss. ""This register can help us understand why some children with a hearing loss do so well, while others experience greater difficulties,"" she says. ""Universal newborn hearing screening is detecting hearing loss earlier than ever before, usually within a few weeks of birth. ""Children with hearing loss have very early access to hearing aids, early intervention services and for some, cochlear implantation. It was expected that hearing-impaired children would quickly come to enjoy the same language and educational outcomes as their hearing peers. ""However, early clinical diagnosis and intervention does not guarantee equality in health outcomes, with language and related outcomes of children with hearing loss remaining on average well below population means and the children's true cognitive potential. ""Demonstrating the reasons for this inequality has been hampered until now by the lack of population based prospective research."" The Victorian Childhood Hearing Impairment Longitudinal Databank (VicCHILD) is a population-based longitudinal databank open to every child with permanent hearing loss in Victoria. VicCHILD started in 2012 and stems from 25 years of work by The Royal Children's Hospital and MCRI. At the end 2018, 807 children were enrolled and provided baseline data. By 2020 more than 1000 children will be taking part, making it the largest hearing databank in the world. VicCHILD collects data at enrolment, two years of age, school entry and late primary /early high school. It involves parent questionnaires, child assessments and taking saliva samples. Dr Sung, who is also a honorary fellow at the University of Melbourne, says about 600 Australian infants each year are diagnosed with congenital hearing loss within weeks of birth. ""As these children grow, they can face challenges in things that come naturally to others like language and learning. This can impact their quality of life,"" she says. ""Hearing loss incurs significant burden and medical costs and impacts adversely on educational attainment and employment opportunities. ""This important bank of information could improve interventions and ultimately the lives of children with hearing loss and their families. It will also act as a platform for research trials to understand the effectiveness of different interventions."" "
Science Daily,"Number of Years in NFL, Certain Positions Portend Greater Risk for Cognitive, Mental Health Problems",Mind & Brain,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830082619.htm,"   The study is believed to be the first to explore the interplay between career length, position and cognitive and mental health outcomes among professional football players. The analysis -- based on a survey of nearly 3,500 former NFL players -- was conducted by investigators at the Harvard T.H. Chan School of Public Health and Harvard Medical School as part of the ongoing Football Players Health Study at Harvard University. The study results show that players who experienced concussions had elevated risk for serious cognitive problems, depression and anxiety, which persisted over time, as long as 20 years following injury. The investigators caution that their analysis relied on players' memories of experiencing concussion rather than on diagnosis at the time of injury. And the findings do not mean that everyone with concussion will necessarily experience cognitive or mental health problems, they add. Contrary to previous reports, the new research did not find a link between starting football at a young age and cognitive problems in adulthood. On one level, the researchers say, many of their findings make intuitive sense and confirm what some might have already suspected: The longer players remain in the game, the more likely they are to suffer a head injury, which increases the risk for neurocognitive problems. It also affirms that certain positions are more prone to concussions and, therefore, players in them face greater risk for experiencing the downstream of effects of head injury. Nonetheless, the researchers said, the analysis is the first to document and quantify the risk that stems from lengthier careers and certain high-impact positions. Specifically, the analysis showed that players who reported the most concussion symptoms had 22-fold risk of reporting serious long-term cognitive problems and six times the risk of having symptoms of depression and anxiety, compared with those who reported the fewest symptoms. ""Our findings confirm what some have suspected -- a consistently and persistently elevated risk for men who play longer and who play in certain positions,"" said study lead investigator Andrea Roberts, a research scientist at the Harvard T.H. Chan School of Public Health. ""Our results underscore the importance of preventing concussions, vigilant monitoring of those who suffer them and finding new ways to mitigate the damage from head injury."" For the study, former players, average age 53, were asked about the number of seasons played in the NFL, their positions and any history of blows to the head or neck followed by symptoms of concussion such as dizziness, confusion, vision problems, loss of consciousness, nausea, headaches and seizures, among other symptoms. Based on the number and severity of symptoms, players were given a concussion score. Overall, one in eight players (12 percent) reported signs of serious cognitive problems. By comparison, about 2 percent of people in the general population in the United States report such problems. Age made no difference in the interplay between concussion and cognitive problems, the study showed. Those under age 52 reported serious cognitive problems at a similar rate as the rest (13 percent), a finding that suggests neurocognitive decline was likely not a function of mere aging. Alarmingly, that risk remained magnified even in those 45 and younger. Indeed, 30 percent of players 45 and younger who had the most concussions reported serious cognitive problems. To gauge whether the number of seasons played and position type were linked to depression, anxiety and cognitive problems, the researchers used standard questionnaires commonly used to screen for the presence of such disorders. The researchers compared the proportion of players with serious cognitive problems among individuals with various career lengths -- one season, two to four seasons, five to six seasons, seven to nine seasons and 10 seasons or more. Overall, those with the longest careers -- 10 seasons or more -- were twice as likely to report severe cognitive problems compared with players who'd played a single season -- 12.6 percent in the 10-plus season group reported signs of severe cognitive problems, compared with 5.8 percent in the single-season category. The risk crept up proportionally with the number of seasons played, growing progressively higher as the number of years increased. Every five seasons of play carried a nearly 20 percent increase in risk for serious cognitive problems. Which position one played also mattered. To evaluate the risk-position link, the researchers divided players into three groups based on the average concussion symptoms per year that players reported in each position. Kickers, punters and quarterbacks had the fewest symptoms per year, followed by wide receivers, defensive backs, linemen and tight ends. The groups with the highest number of symptoms included running backs, linebackers and special teams. Those in the group with the most concussion symptoms had twice the risk for serious cognitive problems -- 15 percent of those in this group had cognitive difficulties -- compared with those reporting the fewest concussion symptoms (6 percent). Those with the most concussions also had a nearly 50 percent greater risk for depression and anxiety, compared with those playing in the group with the fewest concussion symptoms. One in four in the first group had symptoms indicative of depression, compared with 15 percent of players reporting problems in the latter one, while 27 percent had signs of anxiety, compared with 16 percent in the group with the fewest concussions. Those who played in the mid-range group had a 75 percent higher risk of cognitive problems and a 40 percent elevation in risk for depression and anxiety, compared with players in the group with the fewest symptoms. Nearly one in four players reported symptoms of anxiety (26 percent) and depression (24 percent), and nearly one in five (18 percent) reported symptoms of both conditions. Career length influenced risk for depression, with every five seasons boosting the risk by 9 percent. The number of seasons, however, was not linked to greater anxiety risk. The age at which an individual started playing organized football did not affect risk. Indeed, outcomes were similar between those who began playing the game before age 12 and those who began later. The findings, however, pertain solely to former NFL players and not necessarily to the general population, the researchers caution. The question of when a child should start playing organized football remains very much open, and should be made by each individual family, the researchers said. ""The overarching goal of the Football Players Health Study is to unravel risk factors and disease mechanisms and to inform interventions that preserve and optimize player health and wellness,"" said study senior author Marc Weisskopf, the Cecil K. and Philip Drinker Professor of Environmental Epidemiology and Physiology at the Harvard T.H. Chan School of Public Health. ""These latest findings confirm much of what we know but they add much needed granularity and specificity to risk magnitude by career length and position."" ""Clearly, not everyone who sustains a concussion is destined for cognitive trouble, but the results of the research highlight just how critical it is to continue to find ways to prevent head injuries from occurring in the first place because of the many downstream and long-lasting effects on physical, cognitive and mental health,"" said Ross Zafonte, the Earle P. and Ida S. Charlton Professor of Physical Medicine and Rehabilitation and head of the Department of Physical Medicine and Rehabilitation at Harvard Medical School. Zafonte is also principal investigator of the Football Players Health Study. The research was supported by National Football League Players Association (NFLPA). "
Science Daily,White Matter Affects How People Respond to Brain Stimulation Therapy,Mind & Brain,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829214757.htm,"   The technique, called non-invasive electrical brain stimulation, involves applying an electrical current to the surface of a patient's head to stimulate brain cells, altering the patient's brain activity. It is being trialled for a range of neurological problems including recovery from stroke, traumatic brain injury, dementia, and depression, but research to date has found the effects to be inconsistent Now, a team led by researchers at Imperial College London has shed more light on why these inconsistencies occur and may provide physical evidence for why some patients respond better than others -- because of the fine structure of their brain tissue. The new research suggests it may be possible to target the therapy to those patients most likely to benefit. They found that differences in the makeup of the brain's white matter -- the tissue deep in the brain and rich in the branching 'tails' of nerve cells -- were key. The research revealed that those who had more connectivity in the regions being stimulated were more likely to respond better to the treatment. According to the team, the findings, published in the journal Brain, could help to personalise the non-invasive electrical brain stimulation, targeting the treatment to patients who are most likely to gain clinical benefits. Dr Lucia Li, a clinical lecturer in neurology in the Department of Brain Sciences at Imperial College London, and lead author of the study, said: ""With all the current buzz around brain stimulation for altering brain activity, it's important to understand who will benefit most from this technique in the clinic. ""Problems with white matter structure are a feature of a range of different neurological conditions. Our study is a step towards more personalised use of brain stimulation, which will improve the outcomes using this technique, as well as reduce the number of people treated un-necessarily."" In the study, researchers looked at 24 healthy patients and 35 patients recovering from a moderate or severe traumatic brain injury (TBI). Participants performed a task inside an MRI scanner (see 'The Stop Signal Task' in notes to editors) while receiving small amounts of electrical current through electrodes on the surface of the scalp or a placebo. They were unable to tell whether they were receiving brain stimulation or not. They found that healthy participants who received brain stimulation performed better in the task than when they didn't receive the treatment. For patient with TBI, task performance in response to stimulation varied widely. However, when they analysed MRI scans, they found that those participants with highly-connected white matter in the brain region being stimulated responded best to the treatment, and those who had damaged or less-connected regions of white matter showed less improvement. They also found that brain stimulation could partially reverse some of the abnormalities in brain activity caused by TBI. The team cautions that while more work is needed to confirm the findings, it could mean brain stimulation might prove a useful treatment approach for other neurological conditions with abnormal brain activity as a feature, such as dementia. ""We found that people with stronger white matter connections in their brain had better improvement with stimulation,"" Dr Li explained. ""This might be an important reason why previous studies have found that some people benefit from stimulation, whilst others don't and means we can start using brain stimulation in a more personalised way."" According to the researchers, the study is limited in that in that they only investigated one type of cognitive behaviour and would need to be replicated in other types of behaviour to show if the findings apply more generally. In addition, they only stimulated one region of the brain, so they don't know whether the effects are specific to this region, or whether other regions can be stimulated. Dr Li explains the team will now focus on larger studies with more participants to investigate what other factors influence someone's response to brain stimulation. They will also apply the technique to other conditions with abnormalities in brain activity to see if they can alter activity and improve brain function. The work was supported by the Wellcome Trust and the National Institute for Health Research. Patients were predominantly recruited from St Mary's Hospital (Imperial College Healthcare NHS Trust) and imaged at the Imperial College Clinical Imaging Facility. ' "
Science Daily,Changing Treatment Practices for Alcohol Use Disorder Could Save Lives,Mind & Brain,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829184150.htm,"   More than 1 million Canadians have alcohol use disorders in any given year, but the vast majority never receives professional help. Despite interventions for alcohol use disorders being effective and -- if performed according to current guidelines -- cost-saving, they are rare in Canada and elsewhere in the world. According to senior author Dr. Jürgen Rehm, Senior Scientist at Institute for Mental Health and Policy Research at the Centre for Addiction and Mental Health (CAMH), improved and routine screening should start in primary care and should be followed by accessible specialized care when required. ""Clinical interventions for alcohol use disorders need to start at the primary care level, if average consumption exceeds more than two drinks a day,"" said Dr. Rehm, ""General practitioners should regularly be asking their patients about alcohol intake, and initiate interventions if required."" Stigma is one of the main reasons for a lack of intervention in primary care. While stigmatization of other mental disorders -- for instance for major depression -- has markedly improved over the last decades, no such improvement has been seen for alcohol use disorders. Stigma may lead patients to conceal their heavy alcohol consumption, and general practitioners may fail to ask them about it. If a disorder is detected, safe and effective medications are available for use in primary health care. Co-author Markus Heilig, an international expert on the pharmacology of addictive disorders and director of the Center for Social and Affective Neuroscience at Linköping University, Sweden, adds: ""Approved medications for alcohol use disorders are no less effective than other widely used medical treatments. They are also safe, well tolerated. And they are cheap. Yet they are only prescribed to a small minority of patients. This needs to change."" Two additional best practices can help ensure that specialized treatment is effective, according to the authors: wait lists for specialist treatment should be minimal and primary care providers should be involved in the patient's after care. "
Science Daily,Mystery Solved About the Machines That Move Your Genes,Matter & Energy,2019-09-02,-,https://www.sciencedaily.com/releases/2019/09/190902113627.htm,"   The spindle divides chromosomes in half during cell division, ensuring that both offspring cells contain a full set of genetic material. The spindle is made up of tens of thousands of stiff, hollow tubes called microtubules connected by biological motors. Microtubules are only propelled forward when connected to a neighbor pointed in the opposite direction. Previous observations, however, showed microtubules cruising at full speed even when linked only to neighbors facing the same way. In a new paper published September 2 in Nature Physics, the researchers provide an answer to this puzzle. The microtubules are so entangled with one another that even those not actively launched forward get dragged along at full speed by the crowd. ""It's like a New York City crosswalk,"" says study lead author Sebastian Fürthauer, a research scientist at the Flatiron Institute's Center for Computational Biology (CCB) in New York City. ""People walking different ways are all mixed together, yet everyone is able to move at full speed and flow smoothly past one another."" The findings will help scientists better understand the cellular machinery that segregates chromosomes during cell division and why this process sometimes goes wrong. If a spindle does its job incorrectly, it can introduce errors such as missing or extra chromosomes that can lead to complications like infertility and cancer, Fürthauer says. Fürthauer and CCB director Michael Shelley, both applied mathematicians, worked on the project alongside an interdisciplinary team of experimental biologists and physicists from Harvard University, the Massachusetts Institute of Technology, Indiana University, and the University of California, Santa Barbara. One of the overarching goals of biophysics is to link the activity of small-scale components to the large-scale dynamics of cells and organisms. The properties of the main spindle components are relatively well studied. Microtubules are long, stiff polymer rods akin to drinking straws, each with a 'minus' end and a 'plus' end. Molecular motors latch onto and move along microtubules using a pair of molecular 'feet.' Kinesin motors, for instance, have two pairs of feet, one at either end. Kinesin molecules can attach to two different microtubules, with each pair of feet marching from the minus end to the plus end of each microtubule. If the plus and minus ends of both microtubules are aligned, the two pairs of feet walk in the same direction and the microtubules don't move relative to one another. If the microtubules are anti-aligned, the feet move in opposite directions, causing the microtubules to slide past one another. The collective motion of all the microtubules determines the spindle's growth and form. Previous studies mostly focused on situations where motors were scarce. Scientists had assumed that this was an accurate representation of what happens in actual cells. In such a scenario, a microtubule's movement would depend on its neighbors' orientation. Microtubules aligned with their neighbors would stay put while those that defied the crowd would zoom forward. Real spindles, however, don't exhibit this expected behavior. Microtubules surrounded by neighbors facing the same way still move at full speed. So what's pushing them forward? Fürthauer and colleagues investigated how the microtubules would collectively move if the system were packed with lots of motors, resulting in lots of connections between microtubules. They developed a mathematical theory of how mechanical stresses develop in the collective when microtubules are pushed and pulled against each other by the numerous motors. Their theory predicts that the microtubules line up, with every microtubule facing one of two opposing directions. Where microtubules of opposite orientation mingle, they are propelled forward as expected. Microtubules elsewhere, the theory states, are so entangled with their neighbors that they too are pulled along for the ride. Every microtubule, therefore, moves at precisely the speed of the walking motors regardless of its place in the crowd. Experiments conducted by the researchers using microtubules and abundant kinesin motors matched these predictions. Additionally, the theory and experiments matched real-world spindles: In the eggs of African clawed frogs, microtubules in spindles move at roughly the same speed that the motors connecting them are known to walk. The frog spindle behavior is ""very suggestive that the actual biology lives in the regime we see in our experiments,"" Fürthauer says. ""With this new understanding, we can now ask: How can we build a spindle? Can we reconstruct this complex biological machine in a computer simulation, or even in the test tube?"" He and his colleagues are hopeful that they are getting closer. "
Science Daily,Fleet of Autonomous Shapeshifting Boats,Matter & Energy,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150648.htm,"   The autonomous boats -- rectangular hulls equipped with sensors, thrusters, microcontrollers, GPS modules, cameras, and other hardware -- are being developed as part of the ongoing ""Roboat"" project between MIT and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute). The project is led by MIT professors Carlo Ratti, Daniela Rus, Dennis Frenchman, and Andrew Whittle. In the future, Amsterdam wants the roboats to cruise its 165 winding canals, transporting goods and people, collecting trash, or self-assembling into ""pop-up"" platforms -- such as bridges and stages -- to help relieve congestion on the city's busy streets. In 2016, MIT researchers tested a roboat prototype that could move forward, backward, and laterally along a preprogrammed path in the canals. Last year, researchers designed low-cost, 3-D-printed, one-quarter scale versions of the boats, which were more efficient and agile, and came equipped with advanced trajectory-tracking algorithms. In June, they created an autonomous latching mechanism that let the boats target and clasp onto each other, and keep trying if they fail. In a new paper presented at the last week's IEEE International Symposium on Multi-Robot and Multi-Agent Systems, the researchers describe an algorithm that enables the roboats to smoothly reshape themselves as efficiently as possible. The algorithm handles all the planning and tracking that enables groups of roboat units to unlatch from one another in one set configuration, travel a collision-free path, and reattach to their appropriate spot on the new set configuration. In demonstrations in an MIT pool and in computer simulations, groups of linked roboat units rearranged themselves from straight lines or squares into other configurations, such as rectangles and ""L"" shapes. The experimental transformations only took a few minutes. More complex shapeshifts may take longer, depending on the number of moving units -- which could be dozens -- and differences between the two shapes. ""We've enabled the roboats to now make and break connections with other roboats, with hopes of moving activities on the streets of Amsterdam to the water,"" says Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. ""A set of boats can come together to form linear shapes as pop-up bridges, if we need to send materials or people from one side of a canal to the other. Or, we can create pop-up wider platforms for flower or food markets."" Joining Rus on the paper are: Ratti, director of MIT's Senseable City Lab, and, also from the lab, first author Banti Gheneti, Ryan Kelly, and Drew Meyers, all researchers; postdoc Shinkyu Park; and research fellow Pietro Leoni. Collision-free trajectories For their work, the researchers had to tackle challenges with autonomous planning, tracking, and connecting groups of roboat units. Giving each unit unique capabilities to, for instance, locate each other, agree on how to break apart and reform, and then move around freely, would require complex communication and control techniques that could make movement inefficient and slow. To enable smoother operations, the researchers developed two types of units: coordinators and workers. One or more workers connect to one coordinator to form a single entity, called a ""connected-vessel platform"" (CVP). All coordinator and worker units have four propellers, a wireless-enabled microcontroller, and several automated latching mechanisms and sensing systems that enable them to link together. Coordinators, however, also come equipped with GPS for navigation, and an inertial measurement unit (IMU), which computes localization, pose, and velocity. Workers only have actuators that help the CVP steer along a path. Each coordinator is aware of and can wirelessly communicate with all connected workers. Structures comprise multiple CVPs, and individual CVPs can latch onto one another to form a larger entity. During shapeshifting, all connected CVPs in a structure compare the geometric differences between its initial shape and new shape. Then, each CVP determines if it stays in the same spot and if it needs to move. Each moving CVP is then assigned a time to disassemble and a new position in the new shape. Each CVP uses a custom trajectory-planning technique to compute a way to reach its target position without interruption, while optimizing the route for speed. To do so, each CVP precomputes all collision-free regions around the moving CVP as it rotates and moves away from a stationary one. After precomputing those collision-free regions, the CVP then finds the shortest trajectory to its final destination, which still keeps it from hitting the stationary unit. Notably, optimization techniques are used to make the whole trajectory-planning process very efficient, with the precomputation taking little more than 100 milliseconds to find and refine safe paths. Using data from the GPS and IMU, the coordinator then estimates its pose and velocity at its center of mass, and wirelessly controls all the propellers of each unit and moves into the target location. In their experiments, the researchers tested three-unit CVPs, consisting of one coordinator and two workers, in several different shapeshifting scenarios. Each scenario involved one CVP unlatching from the initial shape and moving and relatching to a target spot around a second CVP. Three CVPs, for instance, rearranged themselves from a connected straight line -- where they were latched together at their sides -- into a straight line connected at front and back, as well as an ""L."" In computer simulations, up to 12 roboat units rearranged themselves from, say, a rectangle into a square or from a solid square into a Z-like shape. Scaling up Experiments were conducted on quarter-sized roboat units, which measure about 1 meter long and half a meter wide. But the researchers believe their trajectory-planning algorithm will scale well in controlling full-sized units, which will measure about 4 meters long and 2 meters wide. In about a year, the researchers plan to use the roboats to form into a dynamic ""bridge"" across a 60-meter canal between the NEMO Science Museum in Amsterdam's city center and an area that's under development. The project, called RoundAround, will employ roboats to sail in a continuous circle across the canal, picking up and dropping off passengers at docks and stopping or rerouting when they detect anything in the way. Currently, walking around that waterway takes about 10 minutes, but the bridge can cut that time to around two minutes. ""This will be the world's first bridge comprised of a fleet of autonomous boats,"" Ratti says. ""A regular bridge would be super expensive, because you have boats going through, so you'd need to have a mechanical bridge that opens up or a very high bridge. But we can connect two sides of canal [by using] autonomous boats that become dynamic, responsive architecture that float on the water."" To reach that goal, the researchers are further developing the roboats to ensure they can safely hold people, and are robust to all weather conditions, such as heavy rain. They're also making sure the roboats can effectively connect to the sides of the canals, which can vary greatly in structure and design. "
Science Daily,Scientists Explore Aged Paint in Microscopic Detail to Inform Preservation Efforts,Matter & Energy,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829115421.htm,"   The formation of metal soaps in artwork composed with oil paints can cause ""art acne"" -- including pimpling and more severe deterioration -- which poses a pressing challenge for art conservation around the globe. It is affecting the works of Georgia O'Keefe, Vincent Van Gogh, Francisco de Goya, and Jackson Pollock, among many others, and researchers haven't yet found a good solution to stop its effects. To learn more about the chemical processes involved in aging oil paints in microscopic and nanoscale detail, an international team led by researchers at the National Gallery of Art and the National Institute of Standards and Technology (NIST) conducted a range of studies that included 3D X-ray imaging of a paint sample at the Advanced Light Source (ALS), a synchrotron at the Department of Energy's Lawrence Berkeley National Laboratory (Berkeley Lab) . ""An estimated 70 percent of oil paintings might already have or will have these metal-soap problems,"" said Xiao Ma, Charles E. Culpeper Fellow at the National Gallery of Art who was the lead author of the team's study, published in the journal Angewandte Chemie International Edition. ""In our collections we see soaps in the paintings -- I would say it's not uncommon,"" he noted. ""They might not already show at the surface, but exist at the 'ground,' or priming layers."" The same damaging chemistry, which previous studies have traced to the mixing of fatty acids with metal ions present in paint pigments including lead, zinc, copper, cadmium, and manganese, has been found occurring in some organic coatings, too, such as those used for bronze sculptures and in industry, Ma noted. The latest study focused on one paint called ""Soft Titanium White"" that was painted on a canvas in 1995 by a paint manufacturer. In addition to titanium dioxide (TiO2), it contains zinc oxide (ZnO), which is known to form soaps. Paints like it have been in use since around 1930, Ma said. The aged sample hasn't been treated in any way and has remained in a controlled environment. The study found that clusters of a compound called aluminum stearate are distributed randomly in the paint, and that zinc carboxylates, known as soaps, are intermixed within them. The high spatial resolution analysis showed that one sort of zinc soap, zinc stearate, aggregates in proximity to these clusters. And while the paint sample didn't yet show physical deterioration, researchers found signs that paint fragmentation and chipping (spalling) could eventually occur if zinc soaps become more concentrated and localized within the paint over time. ""We're trying to get a handle on the very beginning processes to understand where the soaps might be forming and where they might be moving -- if they're moving,"" said Barbara Berrie, who leads the Scientific Research Department at the National Gallery of Art and served as a co-leader of the study. ""We want to make sure we understand what's going on in more contemporary paintings so that these works are here for the future."" The study could have broader implications for developing better methods for conservation based around the observed chemistry in oil paints, she said. ""I can see this maybe being applied generally to issues of preservation and treatments for all kinds of works of art,"" she said. Dula Parkinson, a staff scientist at the ALS who participated in the study, said the X-rays revealed the size, shape, and distribution of tiny spots resembling bubbles in a paint sample that measured just a couple of millimeters across. ""They wanted to understand the basic chemistry and basic processes of what was going on,"" he said. ""These structures that they see are really common in lots of paintings, and so they're trying to see why these structures are here."" The imaging, using a technique called X-ray microtomography, mapped varied thicknesses in the paint and revealed some microscopic cracking. Microtomography at the ALS has also been used to provide microscopic views of a wide range of samples, from plant stems to spacecraft heat shields. Besides the X-ray exploration of a paint sample at the microscale, the team also used a technique known as photothermal induced resonance (PTIR) that exceeded the magnification limits of conventional light-based microscopes. PTIR couples infrared (IR) lasers with an atomic force microscope to provide a nanoscale window into the paint's chemistry at a scale much smaller than is achievable with conventional IR microscopes. Another technique, called Fourier transform infrared (FTIR) micro spectroscopy, provided a broader view of the chemical composition across varying layers of paint samples. Andrea Centrone, a project leader for the Nanoscale Spectroscopy Group at NIST who co-led the study with Berrie, noted that the PTIR technique provides chemical mapping with resolution similar to that of atomic-force microscopy -- which offers a scan of the sample via a process that is similar to a record player's needle moving over a surface and mapping it. While the tip scans over the sample, infrared pulses are absorbed locally and the sample heats up and expands rapidly. This ""kicks"" the tip like a struck tuning fork and provides chemically specific information about the sample. The paint sample had a very rough, sticky surface that was difficult to chemically map, so Centrone worked with collaborators at NIST to adapt the technique so that the scanning tip oscillated above the sample surface, touching it gently instead of dragging across it, allowing the capture of high-resolution data. ""We are able to capture very small details down to 10 or 20 nanometers,"" or billionths of a meter, Centrone said. ""We were able to detect which kind of metal soap had formed in the paint samples."" The study notes that the same techniques that were used in combination to explore the paint chemistry could be applied more broadly in other fields where the samples are challenging because their chemistry isn't uniform, and detailed knowledge of chemistry over different scales is required, such as in biomedicine and energy storage. Berrie said she looks forward to future studies that apply the same techniques to explore different types and layers of paint and other issues for preservation of works of art. ""We hope that we will be able to do some comparing and contrasting of different combinations of oil-pigment interactions,"" she said. ""We will be able to explore some of the underlying chemistry of paintings that we still don't know much about,"" to provide insight for art preservation, too. ""And, we are trying to help inform the range of choices that art conservators have."" "
Science Daily,Biochar: A Better Start to Rain Forest Restoration,Matter & Energy,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830162300.htm,"   A carbon-based soil amendment called biochar is a cheap and effective way to support tree seedling survival during reforestation efforts in the Amazon rain forest, according to new research from Wake Forest University's Center for Amazonian Scientific Innovation (CINCIA). Restoring and recovering rain forests has become increasingly important for combatting climate change, since these wide swaths of trees can absorb billions of tons of carbon dioxide each year. The problem is particularly acute in areas mined for alluvial gold deposits, which devastate not only rain forest trees but also soils. High costs can be a huge barrier to replanting, fertilizing and nurturing trees to replace those lost in the rain forest. The scientists found that using biochar combined with fertilizer significantly improved height and diameter growth of tree seedlings while also increasing the number of leaves the seedlings developed. The experiment, based in a Peruvian Amazon region called Madre de Dios, the heart of illegal gold mining trade in that country, used two tropical tree species: the fast-growing Guazuma crinita and Terminalia amazonia, a late successional tree often used as timber. ""The most difficult period in a tree seedling's life is the first few months after transplanting,"" said Miles Silman, CINCIA associate director of science and Wake Forest's Andrew Sabin Presidential Chair of Conservation Biology. ""But just a little bit of biochar does wonderful things to the soil, and it really shines when you add organic fertilizer."" The CINCIA scientists make biochar out of Brazil nut husks discarded by large-scale processors in Peru. They burn the husks slowly in 55-gallon barrels, a low-tech, inexpensive and easily scalable method for producing biochar. The study, ""Biochar effects on two native tropical tree species and its potential as a tool for reforestation,"" appears online this month in the peer-reviewed journal Forests. Until this study, little was known about whether biochar could benefit tree growth in tropical tree seedlings. ""We show that while both biochar and fertilizer can improve tree seedling growth, combining them makes seedlings thrive beyond either amendment alone,"" said Silman. The native peoples of the Amazon created ""dark earths"" using biochar thousands of years ago, and those soils are still productive today. Biochar's benefits are many: It improves the soil's ability to hold water and makes it less acidic. It provides a welcoming habitat for microbes, which support plant growth. It holds onto fertilizer and releases it over time, decreasing the need for repeat applications of fertilizer, which cuts labor and supply costs.The scientists used soils recovered from the San Jacinto native community, where gold mining has ravaged the land. Silman explained that the dirt that comes from the mining sluice is devoid of the organic matter and microbes that supports plant life. ""These are the kinds of landscapes we have to recover, and we are still trying to determine how to grow plants in them,"" he said. ""This soil is extremely limiting for natural regrowth, but treating them with biochar turns it into something that plants can grow in. That's good for biodiversity and good for the people that have to make a living from the land."" "
Science Daily,Defrosting Surfaces in Seconds,Matter & Energy,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830112811.htm,"   A group of researchers at the University of Illinois at Urbana-Champaign and Kyushu University has developed a way to remove ice and frost from surfaces extremely efficiently, using less than 1% of the energy and less than 0.01% of the time needed for traditional defrosting methods. The group describes the method in Applied Physics Letters, from AIP Publishing. Instead of conventional defrosting, which melts all the ice or frost from the top layer down, the researchers established a technique that melts the ice where the surface and the ice meet, so the ice can simply slide off. ""The work was motivated by the large energy efficiency losses of building energy systems and refrigeration systems due to the need to do intermittent defrosting. The systems must be shut down, the working fluid is heated up, then it needs to be cooled down again,"" said author Nenad Miljkovic, at UIUC. ""This eats up a lot of energy when you think of the yearly operational costs of running intermittent defrosting cycles."" According to the authors, the biggest source of inefficiency in conventional systems is that much of the energy used for de-icing goes into heating other components of the system rather than directly heating the frost or ice. This increases energy consumption and system downtime. Instead, the researchers proposed delivering a pulse of very high current where the ice and the surface meet to create a layer of water. To ensure the pulse reaches the intended space rather than melting the exposed ice, the researchers apply a thin coating of indium tin oxide (ITO) -- a conductive film often used for defrosting -- to the surface of the material. Then, they leave the rest to gravity. To test this, the scientists defrosted a small glass surface cooled to minus 15.1 degrees Celsius -- about as cold as the warmest parts of Antarctica -- and to minus 71 degrees Celsius -- colder than the coldest parts of Antarctica. These temperatures were chosen to model heating, ventilation, air conditioning and refrigeration applications and aerospace applications, respectively. In all tests, the ice was removed with a pulse lasting less than one second. In a real, 3D system, gravity would be assisted by air flow. ""At scale, it all depends on the geometry,"" Miljkovic said. ""However, the efficiency of this approach should definitely still be much better than conventional approaches."" The group hasn't studied more complicated surfaces like airplanes yet, but they think it's an obvious future step. ""They are a natural extension as they travel fast, so the shear forces on the ice are large, meaning only a very thin layer at the interface needs to be melted in order to remove the ice,"" Miljkovic said. ""Work would be needed to figure out how we can coat curved components conformally with the ITO and to figure out how much energy we would need."" The researchers hope to work with external companies on scaling up their approach for commercialization. "
Science Daily,How to Simulate Softness,Matter & Energy,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150804.htm,"   The findings provide fundamental insights into designing tactile materials and haptic interfaces that can recreate realistic touch sensations, for applications such as electronic skin, prostheses and medical robotics. Researchers detail their findings in the Aug. 30 issue of Science Advances. ""We provide a formula to recreate a spectrum of softness. In doing so, we are helping close the gap in understanding what it takes to recreate some aspects of touch,"" said Charles Dhong, who co-led the study as a postdoctoral fellow at UC San Diego and is now an assistant professor in biomedical engineering at the University of Delaware. Dhong worked with Darren Lipomi, a professor of nanoengineering at UC San Diego and the study's co-corresponding author. Based on the results from their experiments, the researchers created equations that can calculate how soft or hard a material will feel based on material thickness, Young's modulus (a measure of a material's stiffness), and micropatterned areas. The equations can also do the reverse and calculate, for example, how thick or micropatterned a material needs to be to feel a certain level of softness. ""What's interesting about this is that we've found two new ways to tune the perceived softness of an object -- micropatterning and changing the thickness,"" Dhong said. ""Young's modulus is what scientists typically turn to in terms of what's soft or hard. It is a factor, but now we show that it's only one part of the equation."" Recreating softness The researchers began by examining two parameters engineers use to measure a material's perceived softness: indentation depth (how deep a fingertip presses into a material) and contact area between the fingertip and the material. Normally, these parameters both change simultaneously as a fingertip presses into an object. Touch a piece of soft rubber, for example, and the contact area will increase the deeper a fingertip presses in. Dhong, Lipomi and colleagues were curious how indentation depth and contact area independently affect the perception of softness. To answer this question, they specially engineered materials that decoupled the two parameters and then tested them on human subjects. The researchers created nine different elastomeric slabs, each with its own unique ratio of indentation depth to contact area. The slabs differed in amount of micropatterning on the surface, thickness and Young's modulus. Micropatterning is key to the design. It consists of arrays of raised microscopic pillars dotted on the surface of the slabs. These tiny pillars allow a fingertip to press deeper without changing the contact area. This is similar to pressing against the metal pins of a Pinscreen toy, where arrays of pins slide in and out to make a 3D impression. ""By creating these micropatterned surface structures, we produce discontinuous regions of contact where the finger presses in that are much smaller than the shadow it would cast on the surface,"" Lipomi said. The team tested the slabs on 15 subjects and instructed them to perform two tasks. In the first task, they presented subjects with multiple pairs of slabs and asked them to identify the softer one in each pair. In the second task, the researchers had subjects rank the nine slabs from softest to hardest. Overall, the slabs that subjects perceived as softer were thicker, had little to no micropatterning on the surface, and had a low Young's modulus. Meanwhile slabs that felt harder were thinner, had more micropatterning and a high Young's modulus. Softness: a basic ingredient of touch Experiments also led the researchers to an interesting conclusion: the perception of softness is a basic sensation, not a combination of other sensations. ""This means softness is a primary ingredient of the human sense of touch. It's like how we have RGB for color displays,"" Lipomi said. ""If we can find the other 'pixels of touch,' can we combine them to make any tactile image we want? These are the fundamental things we would like to know going forward."" Paper title: ""Role of Indentation Depth and Contact Area on Human Perception of Softness for Haptic Interfaces."" Co-authors include Rachel Miller, Nicholas Root, Sumit Gupta, Laure V. Kayser, Cody W. Carpenter, Kenneth J. Loh and Vilayanur S. Ramachandran, all at UC San Diego. This work was supported by the National Institutes of Health Director's New Innovator Award (grant 1DP2EB022358) and the Office of Naval Research (grant N00014-18-1-2483). "
Science Daily,Storage and Release of Mechanical Waves Without Energy Loss,Matter & Energy,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150746.htm,"   In a newly published paper in Science Advances, a group of researchers led by Andrea Alù, founding director of the Photonics Initiative at the Advanced Science Research Center (ASRC) at The Graduate Center, CUNY, and by Massimo Ruzzene, professor of Aeronautics Engineering at Georgia Tech, have experimentally shown that it is possible to efficiently capture and store a wave intact then guide it towards a specific location. ""Our experiment proves that unconventional forms of excitation open new opportunities to gain control over wave propagation and scattering,"" said Alù. ""By carefully tailoring the time dependence of the excitation, it is possible to trick the wave to be efficiently stored in a cavity, and then release it on demand towards the desired direction."" Methodology To achieve their goal, the scientists had to devise a way for changing the basic interaction between waves and materials. When a light or sound wave hits an obstacle, it is either partially absorbed or reflected and scattered. The absorption process entails immediately converting of the wave into heat or other forms of energy. Materials that can't absorb waves only reflect and scatter them. The researchers' goal was to find a way to mimic the absorbtion process without converting the wave into other forms of energy and instead storing it in the material. This concept, introduced theoretically two years ago by the ASRC group, is known as coherent virtual absorption. To prove their theory, the researchers reasoned that they needed to tailor the waves' time evolution so that when they came in contact with non-abosorbing materials, they wouldn't be reflected, scattered, or transmitted. This would prevent the wave impinging on the structure from escaping, and it would be efficiently trapped inside as if it were being absorbed. The stored wave could then be released on demand. During their experiment, researchers propagated two mechanical waves traveling in opposite directions along a carbon steel waveguide bar that contained a cavity. The time variations of each wave were carefully controlled to ensure that the cavity would retain all of the impinging energy. Then, by stopping the excitation or detuning one of the waves, they were able to control the release of the stored energy and send it towards a desired direction on demand. ""While we ran our proof-of-concept experiment using elastic waves traveling in a solid material, our findings are also applicable to radiowaves and light, offering exciting prospects for efficient energy harvesting, wireless power transfer, low-energy photonics, and generally enhanced control over wave propagation,"" said Ruzzene. Research Funding This study was funded by the the Air Force Office of Scientific Research, the National Science Foundation, and the Simons Foundation. "
Science Daily,Ultra-White Beetle Scales Hold Secret to Creating Sustainable Paint from Recycled Plastic,Matter & Energy,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829081405.htm,"   Cyphochilus beetle scales are one of the brightest whites in nature and their ultra-white appearance is created by the nanostructure in their tiny scales, as opposed to the use of pigment or dyes. Experts have now been able to recreate and improve on this structure in the lab using low cost materials -- via a technique which could be used as a sustainable alternative to titanium dioxide in white paint. Dr Andrew Parnell, from the University of Sheffield's Department of Physics and Astronomy, who led the research, said: ""In the natural world, whiteness is usually created by a foamy, Swiss cheese-like structure made of a solid interconnected network and air. Until now, how these structures form and develop and how they have evolved light-scattering properties has remained a mystery. ""Having understood these structures we were able to take plastic and structure it in the same way. Ideally, we could recycle plastic waste that would normally be burnt or sent to landfill, structure it just like the beetle scale and then use it to make super white paint. This would make paint with a much lower carbon footprint and help tackle the challenge of recycling single-use plastics."" The findings show that the foamy structure of the beetles' scales had the right proportion of empty spaces, which optimise the scattering of light -- creating the ultra-white colouring. Conventional white paint contains nanoparticles of titanium dioxide, which scatter light very strongly. However, the use of titanium dioxide is harmful to the environment as it contributes to nearly 75 per cent of the carbon footprint of each tin of paint that is produced. To measure the tiny individual beetle scales, researchers used a technique called X-ray tomography, which is similar to a CT scan but on a miniscule scale. The scientists used the X-ray imaging facilities at the instrument ID16B at the European Synchrotron Research Facility (ESRF) in Grenoble, France. The intense X-ray source at the ESRF meant whole intact scales could be measured, which was pivotal to understanding them and modelling how they scatter light. To follow how the synthetic material formed, they again used the ESRF to confirm the formation mechanism as the layer dried and became structured. Dr Stephanie Burg, a PhD researcher at the University of Sheffield said: ""This research answers long-standing questions about how the structure inside these scales actually form and we hope these lessons from nature will help inform the future of sustainable manufacturing for paint."" The team also used the instrument Larmor at the ISIS Spallation Neutron Source, which measured the nanostructure of the synthetic whites they made. This was at the Rutherford Appleton Laboratory in Oxfordshire -- part of the Science and Technologies Facilities Council. The work was carried out in collaboration with the coatings company AkzoNobel, makers of Dulux paint. "
Science Daily,Making Waves in Water Desalination,Matter & Energy,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830112827.htm,"   Now, with a new publication and new research project funded by the National Science Foundation, he continues to build on his highly praised work to develop new methods of deionizing saltwater. The paper, ""Effect of Conductive Additives on the Transport Properties of Porous Flow-Through Electrodes with Insulative Particles and their Optimization for Faradaic Deionization,"" published this week in Water Research, demonstrated promising results for energy-efficient desalination of alternative water resources. Smith's newest work, spear-headed by his doctoral student Erik Reale, involves deionization devices that can reversibly store and release cations using intercalation materials, a class of materials commonly used for rechargeable batteries. This work in particular addresses the challenge of cycling intercalation materials with fast rates of electron, ion, and fluid transport, features that are difficult to achieve simultaneously in a single system. His team fabricated optimized electrodes containing insulative Prussian Blue analogue particles, and used them in an experimental cation intercalation desalination (CID) cell with symmetric electrodes. They witnessed results of a nearly 10-fold increase in the rate of salt removal at similar energy consumption levels to past CID demonstrations. ""High salt removal rates are needed in electrochemical water treatment devices because smaller units can be constructed to achieve the same total production of treated water if salt can be removed faster. Following that line of thinking, the capital cost to construct a system will be lower for a fixed water productivity level,"" said Smith. In his new three-year NSF-funded research project, ""Enabling Minimal Brine Discharge Desalination Using Intercalation Reactions,"" Smith will be using battery materials to overcome the limitation in the volume of waste brine that is produced during water desalination using reverse osmosis (RO). Brine disposal has major environmental sustainability issues, including increased earthquakes when injected into the earth and danger to aquatic ecosystems when disposed of in bodies of water. While RO brine generation is dictated by the pressure driving force used (and thus imposes mechanical limitations), Smith plans to use electric fields to concentrate salt ions, which, he proposes, could concentrate salts to levels near saturation in solution. The University of Illinois previously reported, in 2016, that Smith had discovered the technology that charges batteries for electronic devices could provide fresh water from salty seas. He developed a novel device -- a saltwater-filled battery with electricity running through it -- that deionized water using the least amount of energy possible at the time. This work earned a spot on the list of top 10 most-read articles from the Journal of the Electrochemical Society in 2016. Just a year later, in 2017, Smith and his team took saltwater desalination a step further, focusing on new materials to improve the economic viability and energy efficiency of the process in collaboration with Wetsus, the European Centre of Excellence for Water Technology. They created a battery-like device that uses electrodes made from a material that could remove not only sodium ions but also potassium, calcium, magnesium, and others -- an important technological improvement because saltwater and brackish waters often contain a mix of other salts like potassium, calcium, and manganese chloride. This work was published in the journal Electrochimica Acta. The present experimental work also follows work published by Smith and his students using computational modeling of electrochemical transport to guide the design of battery-based desalination cells. Their group has also recently used quantum mechanical modeling, combined with experiments and thermodynamic analysis, to understand how the battery materials used in their desalination cells absorb sodium, as well as magnesium and calcium, at the atomic scale. More recently, Smith won the 2018 ISE-Elsevier Prize for Applied Electrochemistry -- a recognition based entirely on his mathematical modeling of battery-based desalination devices, lithium-ion batteries, and flow batteries. "
Science Daily,Psychosensory Electronic Skin Technology for Future AI and Humanoid Development,Matter & Energy,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830092113.htm,"   The attempt to mimic human's five senses led to the development of innovative electronic devices such as camera and TV, which are inventions that dramatically changed human life. As a result, many scientists are continuously performing research to imitate tactile, olfactory, and palate senses and tactile sensing is expected to be the next mimetic technology for various reasons. Currently, most tactile sensor researches are focusing on physical mimetic technologies that measure the pressure used for a robot to grab an object, but psychosensory tactile research on how to mimic human tactile feeling such like soft, smooth or rough has a long way to go. As a result, Professor Jae Eun Jang's team developed a tactile sensor that can feel pain and temperature like human through a joint research with Professor Cheil Moon's team in the Department of Brain and Cognitive Science, Professor Ji-woong Choi's team in the Department of Information and Communication Engineering, and Professor Hongsoo Choi's team in the Department of Robotics Engineering. Its key strengths are that it has simplified the sensor structure and can measure pressure and temperature at the same time and can be applied on various tactile systems regardless of the measurement principle of the sensor. For this, the research team focused on zinc oxide nano-wire (ZnO Nano-wire) technology, which was applied as a self-power tactile sensor that does not need a battery thanks to its piezoelectric effect, which generates electrical signals by detecting pressure. Also, a temperature sensor using Seebeck effect1) was applied at the same time for one sensor to do two jobs. The research team arranged electrodes on polyimide flexible substrate, grew the ZnO nano-wire, and could measure the piezoelectric effect by pressure and the Seebeck effect by temperature change at the same time. The research team also succeeded in developing a signal processing technique that judges the generation of pain signals considering the pressure level, stimulated area and temperature. Professor Jang in the Department of Information and Communication Engineering said ""We have developed a core base technology that can effectively detect pain, which is necessary for developing future-type tactile sensor. As an achievement of convergence research by experts in nano engineering, electronic engineering, robotics engineering, and brain sciences, it will be widely applied on electronic skin that feels various senses as well as new human-machine interactions. If robots can also feel pain, our research will expand further into technology to control robots' aggressive tendency, which is one of the risk factors of AI development."" 1 Seebeck effect: Forms electric circuit by connecting different metals and generates thermoelectromotive forces on the circuit if there is a temperature difference on both access points. "
Science Daily,Hints of a Volcanically Active Exo-Moon,Space & Time,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829115425.htm,"   Sodium gas as circumstantial evidence Astronomers have not yet discovered a rocky moon beyond our solar system and it's on the basis of circumstantial evidence that the researchers in Bern conclude that the exo-Io exists: Sodium gas was detected at the WASP 49-b at an anomalously high-altitude. ""The neutral sodium gas is so far away from the planet that it is unlikely to be emitted solely by a planetary wind,"" says Oza. Observations of Jupiter and Io in our solar system, by the international team, along with mass loss calculations show that an exo-Io could be a very plausible source of sodium at WASP 49-b. ""The sodium is right where it should be"" says the astrophysicist. Tides keep the system stable Already in 2006, Bob Johnson of the University of Virginia and the late Patrick Huggins at New York University, USA had shown that large amounts of sodium at an exoplanet could point to a hidden moon or ring of material, and ten years ago, researchers at Virginia calculated that such a compact system of three bodies: star, close-in giant planet and moon, can be stable over billions of years. Apurva Oza was then a student at Virginia, and after his PhD on moons atmospheres in Paris, decided to pick up the theoretical calculations of these researchers. He now publishes the results of his work together with Johnson and colleagues in the Astrophysical Journal. ""The enormous tidal forces in such a system are the key to everything,"" explains the astrophysicist. The energy released by the tides to the planet and its moon keeps the moon's orbit stable, simultaneously heating it up and making it volcanically active. In their work, the researchers were able to show that a small rocky moon can eject more sodium and potassium into space through this extreme volcanism than a large gas planet, especially at high altitudes. ""Sodium and potassium lines are quantum treasures to us astronomers because they are extremely bright,"" says Oza, ""the vintage street lamps that light up our streets with yellow haze, is akin to the gas we are now detecting in the spectra of a dozen exoplanets."" ""We need to find more clues"" The researchers compared their calculations with these observations and found five candidate systems where a hidden exomoon can survive against destructive thermal evaporation. For WASP 49-b the observed data can be best explained by the existence of an exo-Io. However, there are other options. For example, the exoplanet could be surrounded by a ring of ionized gas, or non-thermal processes. ""We need to find more clues,"" Oza admits. The researchers are therefore relying on further observations with ground-based and space-based instruments. ""While the current wave of research is going towards habitability and biosignatures, our signature is a signature of destruction,"" says the astrophysicist. A few of these worlds could be destroyed in a few billion years due to the extreme mass loss. ""The exciting part is that we can monitor these destructive processes in real time, like fireworks,"" says Oza. "
Science Daily,Busy Older Stars Outpace Stellar Youngsters,Space & Time,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828193809.htm,"   The findings provide fresh insights into the history of our Galaxy and increase our understanding of how stars form and evolve. Researchers calculate that the old stars are moving more quickly in and out of the disc -- the pancake-shaped mass at the heart of the Galaxy where most stars are located. A number of theories could explain this movement -- it all depends where the star is in the disc. Stars towards the outskirts could be knocked by gravitational interactions with smaller galaxies passing by. Towards the inner parts of the disc, the stars could be disturbed by massive gas clouds which move along with the stars inside the disc. They could also be thrown out of the disc by the movement of its spiral structure. Dr Ted Mackereth, a galactic archaeologist at the University of Birmingham, is lead author on the paper. He explains: ""The specific way that the stars move tells us which of these processes has been dominant in forming the disc we see today. We think older stars are move active because they have been around the longest, and because they were formed during a period when the Galaxy was a bit more violent, with lots of star formation happening and lots of disturbance from gasses and smaller satellite galaxies. There are lots of different processes at work, and untangling all these helps us to build up a picture of the history of our Galaxy."" The study uses data from the Gaia satellite, currently working to chart the movements of around 1 billion stars in the Milky Way. It also takes information from APOGEE, an experiment run by the Sloan Digital Sky Survey that uses spectroscopy to measure the distribution of elements in stars, as well as images from the recently-retired Kepler space telescope. Measurements provided by Kepler show how the brightness of stars varies over time, which gives insights into how they vibrate. In turn, that yields information about their interior structure, which enables scientists to calculate their age. The Birmingham team, working with colleagues at the University of Toronto and teams involved with the Sloan Digital Sky Survey, were able to take these different data strands and calculate the differences in velocity between different sets of stars grouped by age. They found that the older stars were moving in many different directions with some moving very quickly out from the galactic disk. Younger stars move closely together at much slower speeds out from the disc, although they are faster than the older stars as they rotate around the Galaxy within the disc. The eventual goal of the research is to link what is known about the Milky Way with information about how other galaxies in the universe formed, ultimately being able to place our Galaxy within the very earliest signatures of the universe. The research is published in the Monthly Notices of the Royal Astronomical Society and funded by the Science and Technology Facilities Council, the Royal Astronomical Society and the European Research Council. "
Science Daily,Earth's Fingerprint Hints at Finding Habitable Planets Beyond the Solar System,Space & Time,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828140132.htm,"   McGill Physics student Evelyn Macdonald and her supervisor Prof. Nicolas Cowan used over a decade of observations of Earth's atmosphere taken by the SCISAT satellite to construct a transit spectrum of Earth, a sort of fingerprint for Earth's atmosphere in infrared light, which shows the presence of key molecules in the search for habitable worlds. This includes the simultaneous presence of ozone and methane, which scientists expect to see only when there is an organic source of these compounds on the planet. Such a detection is called a ""biosignature."" ""A handful of researchers have tried to simulate Earth's transit spectrum, but this is the first empirical infrared transit spectrum of Earth,"" says Prof. Cowan. ""This is what alien astronomers would see if they observed a transit of Earth."" The findings, published Aug. 28 in the journal Monthly Notices of the Royal Astronomical Society, could help scientists determine what kind of signal to look for in their quest to find Earth-like exoplanets (planets orbiting a star other than our Sun). Developed by the Canadian Space Agency, SCISAT was created to help scientists understand the depletion of Earth's ozone layer by studying particles in the atmosphere as sunlight passes through it. In general, astronomers can tell what molecules are found in a planet's atmosphere by looking at how starlight changes as it shines through the atmosphere. Instruments must wait for a planet to pass -- or transit -- over the star to make this observation. With sensitive enough telescopes, astronomers could potentially identify molecules such as carbon dioxide, oxygen or water vapour that might indicate if a planet is habitable or even inhabited. Cowan was explaining transit spectroscopy of exoplanets at a group lunch meeting at the McGill Space Institute (MSI) when Prof. Yi Huang, an atmospheric scientist and fellow member of the MSI, noted that the technique was similar to solar occultation studies of Earth's atmosphere, as done by SCISAT. Since the first discovery of an exoplanet in the 1990s, astronomers have confirmed the existence of 4,000 exoplanets. The holy grail in this relatively new field of astronomy is to find planets that could potentially host life -- an Earth 2.0. A very promising system that might hold such planets, called TRAPPIST-1, will be a target for the upcoming James Webb Space Telescope, set to launch in 2021. Macdonald and Cowan built a simulated signal of what an Earth-like planet's atmosphere would look like through the eyes of this future telescope which is a collaboration between NASA, the Canadian Space Agency and the European Space Agency. The TRAPPIST-1 system located 40 light years away contains seven planets, three or four of which are in the so-called ""habitable zone"" where liquid water could exist. The McGill astronomers say this system might be a promising place to search for a signal similar to their Earth fingerprint since the planets are orbiting an M-dwarf star, a type of star which is smaller and colder than our Sun. ""TRAPPIST-1 is a nearby red dwarf star, which makes its planets excellent targets for transit spectroscopy. This is because the star is much smaller than the Sun, so its planets are relatively easy to observe,"" explains Macdonald. ""Also, these planets orbit close to the star, so they transit every few days. Of course, even if one of the planets harbours life, we don't expect its atmosphere to be identical to Earth's since the star is so different from the Sun."" According to their analysis, Macdonald and Cowan affirm that the Webb Telescope will be sensitive enough to detect carbon dioxide and water vapour using its instruments. It may even be able to detect the biosignature of methane and ozone if enough time is spent observing the target planet. Prof. Cowan and his colleagues at the Montreal-based Institute for Research on Exoplanets are hoping to be some of the first to detect signs of life beyond our home planet. The fingerprint of Earth assembled by Macdonald for her senior undergraduate thesis could tell other astronomers what to look for in this search. She will be starting her Ph.D. in the field of exoplanets at the University of Toronto in the Fall. "
Science Daily,Newly Discovered Giant Planet Slingshots Around Its Star,Space & Time,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828092502.htm,"   ""This planet is unlike the planets in our solar system, but more than that, it is unlike any other exoplanets we have discovered so far,"" says Sarah Blunt, a Caltech graduate student and first author on the new study publishing in The Astronomical Journal. ""Other planets detected far away from their stars tend to have very low eccentricities, meaning that their orbits are more circular. The fact that this planet has such a high eccentricity speaks to some difference in the way that it either formed or evolved relative to the other planets."" The planet was discovered using the radial velocity method, a workhorse of exoplanet discovery that detects new worlds by tracking how their parent stars ""wobble"" in response to gravitational tugs from those planets. However, analyses of these data usually require observations taken over a planet's entire orbital period. For planets orbiting far from their stars, this can be difficult: a full orbit can take tens or even hundreds of years. The California Planet Search, led by Caltech Professor of Astronomy Andrew W. Howard, is one of the few groups that watches stars over the decades-long timescales necessary to detect long-period exoplanets using radial velocity. The data needed to make the discovery of the new planet were first provided by W. M. Keck Observatory in Hawaii. In 1997, the team began using the High-Resolution Echelle Spectrometer (HIRES) on the Keck I telescope to take measurements of the planet's star, called HR 5183. ""The key was persistence,"" said Howard. ""Our team followed this star with Keck Observatory for more than two decades and only saw evidence for the planet in the past couple years! Without that long-term effort, we never would have found this planet."" In addition to Keck Observatory, the California Planet Search also used the Lick Observatory in Northern California and the McDonald Observatory in Texas. The astronomers have been watching HR 5183 since the 1990s, but do not have data corresponding to one full orbit of the planet, called HR 5183 b, because it circles its star roughly every 45 to 100 years. The team instead found the planet because of its strange orbit. ""This planet spends most of its time loitering in the outer part of its star's planetary system in this highly eccentric orbit, then it starts to accelerate in and does a slingshot around its star,"" explains Howard. ""We detected this slingshot motion. We saw the planet come in and now it's on its way out. That creates such a distinctive signature that we can be sure that this is a real planet, even though we haven't seen a complete orbit."" The new findings show that it is possible to use the radial velocity method to make detections of other far-flung planets without waiting decades. And, the researchers suggest, looking for more planets like this one could illuminate the role of giant planets in shaping their solar systems. Planets take shape out of disks of material left over after stars form. That means that planets should start off in flat, circular orbits. For the newly detected planet to be on such an eccentric orbit, it must have gotten a gravitational kick from some other object. The most plausible scenario, the researchers propose, is that the planet once had a neighbor of similar size. When the two planets got close enough to each other, one pushed the other out of the solar system, forcing HR 5183 b into a highly eccentric orbit. ""This newfound planet basically would have come in like a wrecking ball,"" says Howard, ""knocking anything in its way out of the system."" This discovery demonstrates that our understanding of planets beyond our solar system is still evolving. Researchers continue to find worlds that are unlike anything in our solar system or in solar systems we have already discovered. ""Copernicus taught us that Earth is not the center of the solar system, and as we expanded into discovering other solar systems of exoplanets, we expected them to be carbon copies of our own solar system,"" Howard explains, ""But it's just been one surprise after another in this field. This newfound planet is another example of a system that is not the image of our solar system but has remarkable features that make our universe incredibly rich in its diversity."" "
Science Daily,Astronomers Find a Golden Glow from a Distant Stellar Collision,Space & Time,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827123524.htm,"   The impact also created a kilonova -- a turbocharged explosion that instantly forged several hundred planets' worth of gold and platinum. The observations provided the first compelling evidence that kilonovae produce large quantities of heavy metals, a finding long predicted by theory. Astronomers suspect that all of the gold and platinum on Earth formed as a result of ancient kilonovae created during neutron star collisions. Based on data from the 2017 event, first spotted by the Laser Interferometer Gravitational-wave Observatory (LIGO), astronomers began to adjust their assumptions of how a kilonova should appear to Earth-bound observers. A team led by Eleonora Troja, an associate research scientist in the University of Maryland's Department of Astronomy, re-examined data from a gamma-ray burst spotted in August 2016 and found new evidence for a kilonova that went unnoticed during the initial observations. NASA's Neil Gehrels Swift Observatory began tracking the 2016 event, named GRB160821B, minutes after it was detected. The early catch enabled the research team to gather new insights that were missing from the kilonova observations of the LIGO event, which did not begin until nearly 12 hours after the initial collision. Troja and her colleagues reported these new findings in the journal Monthly Notices of the Royal Astronomical Society on August 27, 2019. ""The 2016 event was very exciting at first. It was nearby and visible with every major telescope, including NASA's Hubble Space Telescope. But it didn't match our predictions -- we expected to see the infrared emission become brighter and brighter over several weeks,"" said Troja, who also has an appointment at NASA's Goddard Space Flight Center. ""Ten days after the event, barely any signal remained. We were all so disappointed. Then, a year later, the LIGO event happened. We looked at our old data with new eyes and realized we had indeed caught a kilonova in 2016. It was a nearly perfect match. The infrared data for both events have similar luminosities and exactly the same time scale."" The similarities between the two events suggest that the 2016 kilonova also resulted from the merger of two neutron stars. Kilonovae may also result from the merger of a black hole and neutron star, but it is unknown whether such an event would yield a different signature in X-ray, infrared, radio and optical light observations. According to Troja, the information collected from the 2016 event does not contain as much detail as the observations of the LIGO event. But the coverage of those first few hours -- missing from the record of the LIGO event -- revealed important new insights into the early stages of a kilonova. For example, the team got their first look at the new object that remained after the collision, which was not visible in the LIGO event data. ""The remnant could be a highly magnetized, hypermassive neutron star known as a magnetar, which survived the collision and then collapsed into a black hole,"" said Geoffrey Ryan, a Joint Space-Science Institute (JSI) Prize Postdoctoral Fellow in the UMD Department of Astronomy and a co-author of the research paper. ""This is interesting, because theory suggests that a magnetar should slow or even stop the production of heavy metals, which is the ultimate source of a kilonova's infrared light signature. Our analysis suggests that heavy metals are somehow able to escape the quenching influence of the remnant object."" Troja and her colleagues plan to apply the lessons they learned to re-evaluate past events, while also improving their approach to future observations. A number of candidate events have been identified with optical light observations, but Troja is more interested in events with a strong infrared light signature -- the telltale indicator of heavy metal production. ""The very bright infrared signal from this event arguably makes it the clearest kilonova we have observed in the distant universe,"" Troja said. ""I'm very much interested in how kilonova properties change with different progenitors and final remnants. As we observe more of these events, we may learn that there are many different types of kilonovae all in the same family, as is the case with the many different types of supernovae. It's so exciting to be shaping our knowledge in real time."" "
Science Daily,The Dark Side of Extrasolar Planets Share Surprisingly Similar Temperatures,Space & Time,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827111114.htm,"   Using data from the Spitzer Space and the Hubble Space telescopes, the researchers from the McGill Space Institute found that the nightside temperature of 12 hot Jupiters they studied was about 800°C. Unlike our familiar planet Jupiter, so-called hot Jupiters circle very close to their host star -- so close that it typically takes fewer than three days to complete an orbit. As a result, hot Jupiters have daysides that permanently face their host stars and nightsides that always face the darkness of space, similarly to how the same side of the Moon always faces the Earth. The tight orbit also means these planets receive more light from their star, which is what makes them extremely hot on the dayside. But scientists had previously measured significant amounts of heat on the nightside of hot Jupiters, as well, suggesting some kind of energy transfer from one side to the other. ""Atmospheric circulation models predicted that the nightside temperatures should vary much more than they do,"" said Dylan Keating, a Physics PhD student under the supervision of McGill professor Nicolas Cowan. ""This is surprising because the planets we studied all receive different amounts of irradiation from their host stars and the dayside temperatures among them varies by almost 1700°C."" Keating, the first author of a new Nature Astronomy study describing the findings, said the nightside temperatures are probably the result of condensation of vaporized rock in these very hot atmospheres. ""The uniformity of the nightside temperatures suggests that clouds on this side of the planets are likely similar to one another in composition. Our analysis suggests that these clouds are likely made of minerals such as manganese sulfide or silicates: in other words, rocks,"" Keating explained. According to Cowan, because the basic physics of cloud formation are universal, the study of the nightside clouds on hot Jupiters could give insight into cloud formation elsewhere in the Universe, including on Earth. Keating said that future space telescope missions -- such as the James Webb Space Telescope and the European Space Agency's ARIEL mission -- could be used to further characterize the dominant cloud composition on hot Jupiter nightsides, as well as to improve models of atmospheric circulation and cloud formation of these planets. ""Observing hot Jupiters at both shorter and longer wavelengths will help us determine what types of clouds are on the nightsides of these planets,"" Keating explained. "
Science Daily,Mystery Solved About the Machines That Move Your Genes,Computers & Math,2019-09-02,-,https://www.sciencedaily.com/releases/2019/09/190902113627.htm,"   The spindle divides chromosomes in half during cell division, ensuring that both offspring cells contain a full set of genetic material. The spindle is made up of tens of thousands of stiff, hollow tubes called microtubules connected by biological motors. Microtubules are only propelled forward when connected to a neighbor pointed in the opposite direction. Previous observations, however, showed microtubules cruising at full speed even when linked only to neighbors facing the same way. In a new paper published September 2 in Nature Physics, the researchers provide an answer to this puzzle. The microtubules are so entangled with one another that even those not actively launched forward get dragged along at full speed by the crowd. ""It's like a New York City crosswalk,"" says study lead author Sebastian Fürthauer, a research scientist at the Flatiron Institute's Center for Computational Biology (CCB) in New York City. ""People walking different ways are all mixed together, yet everyone is able to move at full speed and flow smoothly past one another."" The findings will help scientists better understand the cellular machinery that segregates chromosomes during cell division and why this process sometimes goes wrong. If a spindle does its job incorrectly, it can introduce errors such as missing or extra chromosomes that can lead to complications like infertility and cancer, Fürthauer says. Fürthauer and CCB director Michael Shelley, both applied mathematicians, worked on the project alongside an interdisciplinary team of experimental biologists and physicists from Harvard University, the Massachusetts Institute of Technology, Indiana University, and the University of California, Santa Barbara. One of the overarching goals of biophysics is to link the activity of small-scale components to the large-scale dynamics of cells and organisms. The properties of the main spindle components are relatively well studied. Microtubules are long, stiff polymer rods akin to drinking straws, each with a 'minus' end and a 'plus' end. Molecular motors latch onto and move along microtubules using a pair of molecular 'feet.' Kinesin motors, for instance, have two pairs of feet, one at either end. Kinesin molecules can attach to two different microtubules, with each pair of feet marching from the minus end to the plus end of each microtubule. If the plus and minus ends of both microtubules are aligned, the two pairs of feet walk in the same direction and the microtubules don't move relative to one another. If the microtubules are anti-aligned, the feet move in opposite directions, causing the microtubules to slide past one another. The collective motion of all the microtubules determines the spindle's growth and form. Previous studies mostly focused on situations where motors were scarce. Scientists had assumed that this was an accurate representation of what happens in actual cells. In such a scenario, a microtubule's movement would depend on its neighbors' orientation. Microtubules aligned with their neighbors would stay put while those that defied the crowd would zoom forward. Real spindles, however, don't exhibit this expected behavior. Microtubules surrounded by neighbors facing the same way still move at full speed. So what's pushing them forward? Fürthauer and colleagues investigated how the microtubules would collectively move if the system were packed with lots of motors, resulting in lots of connections between microtubules. They developed a mathematical theory of how mechanical stresses develop in the collective when microtubules are pushed and pulled against each other by the numerous motors. Their theory predicts that the microtubules line up, with every microtubule facing one of two opposing directions. Where microtubules of opposite orientation mingle, they are propelled forward as expected. Microtubules elsewhere, the theory states, are so entangled with their neighbors that they too are pulled along for the ride. Every microtubule, therefore, moves at precisely the speed of the walking motors regardless of its place in the crowd. Experiments conducted by the researchers using microtubules and abundant kinesin motors matched these predictions. Additionally, the theory and experiments matched real-world spindles: In the eggs of African clawed frogs, microtubules in spindles move at roughly the same speed that the motors connecting them are known to walk. The frog spindle behavior is ""very suggestive that the actual biology lives in the regime we see in our experiments,"" Fürthauer says. ""With this new understanding, we can now ask: How can we build a spindle? Can we reconstruct this complex biological machine in a computer simulation, or even in the test tube?"" He and his colleagues are hopeful that they are getting closer. "
Science Daily,Fleet of Autonomous Shapeshifting Boats,Computers & Math,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150648.htm,"   The autonomous boats -- rectangular hulls equipped with sensors, thrusters, microcontrollers, GPS modules, cameras, and other hardware -- are being developed as part of the ongoing ""Roboat"" project between MIT and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute). The project is led by MIT professors Carlo Ratti, Daniela Rus, Dennis Frenchman, and Andrew Whittle. In the future, Amsterdam wants the roboats to cruise its 165 winding canals, transporting goods and people, collecting trash, or self-assembling into ""pop-up"" platforms -- such as bridges and stages -- to help relieve congestion on the city's busy streets. In 2016, MIT researchers tested a roboat prototype that could move forward, backward, and laterally along a preprogrammed path in the canals. Last year, researchers designed low-cost, 3-D-printed, one-quarter scale versions of the boats, which were more efficient and agile, and came equipped with advanced trajectory-tracking algorithms. In June, they created an autonomous latching mechanism that let the boats target and clasp onto each other, and keep trying if they fail. In a new paper presented at the last week's IEEE International Symposium on Multi-Robot and Multi-Agent Systems, the researchers describe an algorithm that enables the roboats to smoothly reshape themselves as efficiently as possible. The algorithm handles all the planning and tracking that enables groups of roboat units to unlatch from one another in one set configuration, travel a collision-free path, and reattach to their appropriate spot on the new set configuration. In demonstrations in an MIT pool and in computer simulations, groups of linked roboat units rearranged themselves from straight lines or squares into other configurations, such as rectangles and ""L"" shapes. The experimental transformations only took a few minutes. More complex shapeshifts may take longer, depending on the number of moving units -- which could be dozens -- and differences between the two shapes. ""We've enabled the roboats to now make and break connections with other roboats, with hopes of moving activities on the streets of Amsterdam to the water,"" says Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. ""A set of boats can come together to form linear shapes as pop-up bridges, if we need to send materials or people from one side of a canal to the other. Or, we can create pop-up wider platforms for flower or food markets."" Joining Rus on the paper are: Ratti, director of MIT's Senseable City Lab, and, also from the lab, first author Banti Gheneti, Ryan Kelly, and Drew Meyers, all researchers; postdoc Shinkyu Park; and research fellow Pietro Leoni. Collision-free trajectories For their work, the researchers had to tackle challenges with autonomous planning, tracking, and connecting groups of roboat units. Giving each unit unique capabilities to, for instance, locate each other, agree on how to break apart and reform, and then move around freely, would require complex communication and control techniques that could make movement inefficient and slow. To enable smoother operations, the researchers developed two types of units: coordinators and workers. One or more workers connect to one coordinator to form a single entity, called a ""connected-vessel platform"" (CVP). All coordinator and worker units have four propellers, a wireless-enabled microcontroller, and several automated latching mechanisms and sensing systems that enable them to link together. Coordinators, however, also come equipped with GPS for navigation, and an inertial measurement unit (IMU), which computes localization, pose, and velocity. Workers only have actuators that help the CVP steer along a path. Each coordinator is aware of and can wirelessly communicate with all connected workers. Structures comprise multiple CVPs, and individual CVPs can latch onto one another to form a larger entity. During shapeshifting, all connected CVPs in a structure compare the geometric differences between its initial shape and new shape. Then, each CVP determines if it stays in the same spot and if it needs to move. Each moving CVP is then assigned a time to disassemble and a new position in the new shape. Each CVP uses a custom trajectory-planning technique to compute a way to reach its target position without interruption, while optimizing the route for speed. To do so, each CVP precomputes all collision-free regions around the moving CVP as it rotates and moves away from a stationary one. After precomputing those collision-free regions, the CVP then finds the shortest trajectory to its final destination, which still keeps it from hitting the stationary unit. Notably, optimization techniques are used to make the whole trajectory-planning process very efficient, with the precomputation taking little more than 100 milliseconds to find and refine safe paths. Using data from the GPS and IMU, the coordinator then estimates its pose and velocity at its center of mass, and wirelessly controls all the propellers of each unit and moves into the target location. In their experiments, the researchers tested three-unit CVPs, consisting of one coordinator and two workers, in several different shapeshifting scenarios. Each scenario involved one CVP unlatching from the initial shape and moving and relatching to a target spot around a second CVP. Three CVPs, for instance, rearranged themselves from a connected straight line -- where they were latched together at their sides -- into a straight line connected at front and back, as well as an ""L."" In computer simulations, up to 12 roboat units rearranged themselves from, say, a rectangle into a square or from a solid square into a Z-like shape. Scaling up Experiments were conducted on quarter-sized roboat units, which measure about 1 meter long and half a meter wide. But the researchers believe their trajectory-planning algorithm will scale well in controlling full-sized units, which will measure about 4 meters long and 2 meters wide. In about a year, the researchers plan to use the roboats to form into a dynamic ""bridge"" across a 60-meter canal between the NEMO Science Museum in Amsterdam's city center and an area that's under development. The project, called RoundAround, will employ roboats to sail in a continuous circle across the canal, picking up and dropping off passengers at docks and stopping or rerouting when they detect anything in the way. Currently, walking around that waterway takes about 10 minutes, but the bridge can cut that time to around two minutes. ""This will be the world's first bridge comprised of a fleet of autonomous boats,"" Ratti says. ""A regular bridge would be super expensive, because you have boats going through, so you'd need to have a mechanical bridge that opens up or a very high bridge. But we can connect two sides of canal [by using] autonomous boats that become dynamic, responsive architecture that float on the water."" To reach that goal, the researchers are further developing the roboats to ensure they can safely hold people, and are robust to all weather conditions, such as heavy rain. They're also making sure the roboats can effectively connect to the sides of the canals, which can vary greatly in structure and design. "
Science Daily,How to Simulate Softness,Computers & Math,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150804.htm,"   The findings provide fundamental insights into designing tactile materials and haptic interfaces that can recreate realistic touch sensations, for applications such as electronic skin, prostheses and medical robotics. Researchers detail their findings in the Aug. 30 issue of Science Advances. ""We provide a formula to recreate a spectrum of softness. In doing so, we are helping close the gap in understanding what it takes to recreate some aspects of touch,"" said Charles Dhong, who co-led the study as a postdoctoral fellow at UC San Diego and is now an assistant professor in biomedical engineering at the University of Delaware. Dhong worked with Darren Lipomi, a professor of nanoengineering at UC San Diego and the study's co-corresponding author. Based on the results from their experiments, the researchers created equations that can calculate how soft or hard a material will feel based on material thickness, Young's modulus (a measure of a material's stiffness), and micropatterned areas. The equations can also do the reverse and calculate, for example, how thick or micropatterned a material needs to be to feel a certain level of softness. ""What's interesting about this is that we've found two new ways to tune the perceived softness of an object -- micropatterning and changing the thickness,"" Dhong said. ""Young's modulus is what scientists typically turn to in terms of what's soft or hard. It is a factor, but now we show that it's only one part of the equation."" Recreating softness The researchers began by examining two parameters engineers use to measure a material's perceived softness: indentation depth (how deep a fingertip presses into a material) and contact area between the fingertip and the material. Normally, these parameters both change simultaneously as a fingertip presses into an object. Touch a piece of soft rubber, for example, and the contact area will increase the deeper a fingertip presses in. Dhong, Lipomi and colleagues were curious how indentation depth and contact area independently affect the perception of softness. To answer this question, they specially engineered materials that decoupled the two parameters and then tested them on human subjects. The researchers created nine different elastomeric slabs, each with its own unique ratio of indentation depth to contact area. The slabs differed in amount of micropatterning on the surface, thickness and Young's modulus. Micropatterning is key to the design. It consists of arrays of raised microscopic pillars dotted on the surface of the slabs. These tiny pillars allow a fingertip to press deeper without changing the contact area. This is similar to pressing against the metal pins of a Pinscreen toy, where arrays of pins slide in and out to make a 3D impression. ""By creating these micropatterned surface structures, we produce discontinuous regions of contact where the finger presses in that are much smaller than the shadow it would cast on the surface,"" Lipomi said. The team tested the slabs on 15 subjects and instructed them to perform two tasks. In the first task, they presented subjects with multiple pairs of slabs and asked them to identify the softer one in each pair. In the second task, the researchers had subjects rank the nine slabs from softest to hardest. Overall, the slabs that subjects perceived as softer were thicker, had little to no micropatterning on the surface, and had a low Young's modulus. Meanwhile slabs that felt harder were thinner, had more micropatterning and a high Young's modulus. Softness: a basic ingredient of touch Experiments also led the researchers to an interesting conclusion: the perception of softness is a basic sensation, not a combination of other sensations. ""This means softness is a primary ingredient of the human sense of touch. It's like how we have RGB for color displays,"" Lipomi said. ""If we can find the other 'pixels of touch,' can we combine them to make any tactile image we want? These are the fundamental things we would like to know going forward."" Paper title: ""Role of Indentation Depth and Contact Area on Human Perception of Softness for Haptic Interfaces."" Co-authors include Rachel Miller, Nicholas Root, Sumit Gupta, Laure V. Kayser, Cody W. Carpenter, Kenneth J. Loh and Vilayanur S. Ramachandran, all at UC San Diego. This work was supported by the National Institutes of Health Director's New Innovator Award (grant 1DP2EB022358) and the Office of Naval Research (grant N00014-18-1-2483). "
Science Daily,Storage and Release of Mechanical Waves Without Energy Loss,Computers & Math,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150746.htm,"   In a newly published paper in Science Advances, a group of researchers led by Andrea Alù, founding director of the Photonics Initiative at the Advanced Science Research Center (ASRC) at The Graduate Center, CUNY, and by Massimo Ruzzene, professor of Aeronautics Engineering at Georgia Tech, have experimentally shown that it is possible to efficiently capture and store a wave intact then guide it towards a specific location. ""Our experiment proves that unconventional forms of excitation open new opportunities to gain control over wave propagation and scattering,"" said Alù. ""By carefully tailoring the time dependence of the excitation, it is possible to trick the wave to be efficiently stored in a cavity, and then release it on demand towards the desired direction."" Methodology To achieve their goal, the scientists had to devise a way for changing the basic interaction between waves and materials. When a light or sound wave hits an obstacle, it is either partially absorbed or reflected and scattered. The absorption process entails immediately converting of the wave into heat or other forms of energy. Materials that can't absorb waves only reflect and scatter them. The researchers' goal was to find a way to mimic the absorbtion process without converting the wave into other forms of energy and instead storing it in the material. This concept, introduced theoretically two years ago by the ASRC group, is known as coherent virtual absorption. To prove their theory, the researchers reasoned that they needed to tailor the waves' time evolution so that when they came in contact with non-abosorbing materials, they wouldn't be reflected, scattered, or transmitted. This would prevent the wave impinging on the structure from escaping, and it would be efficiently trapped inside as if it were being absorbed. The stored wave could then be released on demand. During their experiment, researchers propagated two mechanical waves traveling in opposite directions along a carbon steel waveguide bar that contained a cavity. The time variations of each wave were carefully controlled to ensure that the cavity would retain all of the impinging energy. Then, by stopping the excitation or detuning one of the waves, they were able to control the release of the stored energy and send it towards a desired direction on demand. ""While we ran our proof-of-concept experiment using elastic waves traveling in a solid material, our findings are also applicable to radiowaves and light, offering exciting prospects for efficient energy harvesting, wireless power transfer, low-energy photonics, and generally enhanced control over wave propagation,"" said Ruzzene. Research Funding This study was funded by the the Air Force Office of Scientific Research, the National Science Foundation, and the Simons Foundation. "
Science Daily,"Researchers Develop Low-Power,  Low-Cost Network for 5G Connectivity",Computers & Math,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829081355.htm,"   With 75 billion Internet of Things (IoT) devices expected to be in place by 2025, a growing strain will be placed on requirements of wireless networks. Contemporary WiFi and cellular networks won't be enough to support the influx of IoT devices, the researchers highlighted in their new study. Millimeter wave (mmWave), a network that offers multi-gigahertz of unlicensed bandwidth -- more than 200 times that allocated to today's WiFi and cellular networks, can be used to address the looming issue. In fact, 5G networks are going to be powered by mmWave technology. However, the hardware required to use mmWave is expensive and power-hungry, which are significant deterrents to it being deployed in many IoT applications. ""To address the existing challenges in exploiting mmWave for IoT applications we created a novel mmWave network called mmX,"" said Omid Abari, an assistant professor in Waterloo's David R. Cheriton School of Computer Science. ""mmX significantly reduces cost and power consumption of a mmWave network enabling its use in all IoT applications."" In comparison to WiFi and Bluetooth, which are slow for many IoT applications, mmX provides much higher bitrate. ""mmX will not only improve our WiFi and wireless experience, as we will receive much faster internet connectivity for all IoT devices, but it can also be used in applications, such as, virtual reality, autonomous cars, data centers and wireless cellular networks,"" said Ali Abedi, a postdoctoral fellow at the Cheriton School of Computer Science. ""Any sensor you have in your home, which traditionally used WiFi and lower frequency can now communicate using high-speed millimeter wave networks. ""Autonomous cars are also going to use a huge number of sensors in them which will be connected through wire; now you can make all of them wireless and more reliable."" The study, A Millimeter Wave Network for Billions of Things, authored by Waterloo's Faculty of Mathematics researchers Abari, Abedi, and research assistants Mohammed Mazaheri and Soroush Ameli, was recently presented at the ACM SIGCOMM 2019 conference. "
Science Daily,Psychosensory Electronic Skin Technology for Future AI and Humanoid Development,Computers & Math,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830092113.htm,"   The attempt to mimic human's five senses led to the development of innovative electronic devices such as camera and TV, which are inventions that dramatically changed human life. As a result, many scientists are continuously performing research to imitate tactile, olfactory, and palate senses and tactile sensing is expected to be the next mimetic technology for various reasons. Currently, most tactile sensor researches are focusing on physical mimetic technologies that measure the pressure used for a robot to grab an object, but psychosensory tactile research on how to mimic human tactile feeling such like soft, smooth or rough has a long way to go. As a result, Professor Jae Eun Jang's team developed a tactile sensor that can feel pain and temperature like human through a joint research with Professor Cheil Moon's team in the Department of Brain and Cognitive Science, Professor Ji-woong Choi's team in the Department of Information and Communication Engineering, and Professor Hongsoo Choi's team in the Department of Robotics Engineering. Its key strengths are that it has simplified the sensor structure and can measure pressure and temperature at the same time and can be applied on various tactile systems regardless of the measurement principle of the sensor. For this, the research team focused on zinc oxide nano-wire (ZnO Nano-wire) technology, which was applied as a self-power tactile sensor that does not need a battery thanks to its piezoelectric effect, which generates electrical signals by detecting pressure. Also, a temperature sensor using Seebeck effect1) was applied at the same time for one sensor to do two jobs. The research team arranged electrodes on polyimide flexible substrate, grew the ZnO nano-wire, and could measure the piezoelectric effect by pressure and the Seebeck effect by temperature change at the same time. The research team also succeeded in developing a signal processing technique that judges the generation of pain signals considering the pressure level, stimulated area and temperature. Professor Jang in the Department of Information and Communication Engineering said ""We have developed a core base technology that can effectively detect pain, which is necessary for developing future-type tactile sensor. As an achievement of convergence research by experts in nano engineering, electronic engineering, robotics engineering, and brain sciences, it will be widely applied on electronic skin that feels various senses as well as new human-machine interactions. If robots can also feel pain, our research will expand further into technology to control robots' aggressive tendency, which is one of the risk factors of AI development."" 1 Seebeck effect: Forms electric circuit by connecting different metals and generates thermoelectromotive forces on the circuit if there is a temperature difference on both access points. "
Science Daily,Entanglement Sent Over 50 Km of Optical Fiber,Computers & Math,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150732.htm,"   The quantum internet promises absolutely tap-proof communication and powerful distributed sensor networks for new science and technology. However, because quantum information cannot be copied, it is not possible to send this information over a classical network. Quantum information must be transmitted by quantum particles, and special interfaces are required for this. The Innsbruck-based experimental physicist Ben Lanyon, who was awarded the Austrian START Prize in 2015 for his research, is researching these important intersections of a future quantum Internet. Now his team at the Department of Experimental Physics at the University of Innsbruck and at the Institute of Quantum Optics and Quantum Information of the Austrian Academy of Sciences has achieved a record for the transfer of quantum entanglement between matter and light. For the first time, a distance of 50 kilometers was covered using fiber optic cables. ""This is two orders of magnitude further than was previously possible and is a practical distance to start building inter-city quantum networks,"" says Ben Lanyon. Converted photon for transmission Lanyon's team started the experiment with a calcium atom trapped in an ion trap. Using laser beams, the researchers write a quantum state onto the ion and simultaneously excite it to emit a photon in which quantum information is stored. As a result, the quantum states of the atom and the light particle are entangled. But the challenge is to transmit the photon over fiber optic cables. ""The photon emitted by the calcium ion has a wavelength of 854 nanometers and is quickly absorbed by the optical fiber,"" says Ben Lanyon. His team therefore initially sends the light particle through a nonlinear crystal illuminated by a strong laser. Thereby the photon wavelength is converted to the optimal value for long-distance travel: the current telecommunications standard wavelength of 1550 nanometers. The researchers from Innsbruck then send this photon through a 50-kilometer-long optical fiber line. Their measurements show that atom and light particle are still entangled even after the wavelength conversion and this long journey. Even greater distances in sight As a next step, Lanyon and his team show that their methods would enable entanglement to be generated between ions 100 kilometers apart and more. Two nodes send each an entangled photon over a distance of 50 kilometers to an intersection where the light particles are measured in such a way that they lose their entanglement with the ions, which in turn would entangle them. With 100-kilometer node spacing now a possibility, one could therefore envisage building the world's first intercity light-matter quantum network in the coming years: only a handful of trapped ion-systems would be required on the way to establish a quantum internet between Innsbruck and Vienna, for example. "
Science Daily,Lab-on-a-Chip May Help Identify New Treatments for Liver Disease,Computers & Math,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829122141.htm,"   Now a team led by investigators at Massachusetts General Hospital (MGH) has developed a ""lab on a chip"" technology that can simulate different levels of NAFLD progression in cells across a single continuous tissue. For the research, which is described in an article published in the journal Lab-on-a-Chip, the scientists used their new platform to evaluate the effects of different drivers of NAFLD -- such as fat and oxygen concentrations -- on liver cells. In this way, the platform can allow for detailed studies of NAFLD progression. Other influences such as inflammatory cues can also be superimposed onto the platform to examine their impacts. In addition, the lab on a chip platform can be used to assess investigational drugs' effects on NAFLD progression, therefore revealing their potential for further testing in clinical trials. ""This platform is unique in that in one continuous liver tissue on a single chip, we are able to look at different severities of the disease and to study how liver tissue might respond to both triggers of NAFLD as well as different therapeutic approaches,"" said senior author O. Berk Usta, PhD, an investigator in the Center for Engineering in Medicine at MGH and assistant professor of Surgery at Harvard Medical School. ""While further studies into more complex pathologies of NAFLD and its progressive forms are needed to establish a more complete recapitulation, the current platform establishes a basis for lab-based drug efficacy screening for NAFLD,"" noted Beyza Bulutoglu PhD, the lead author of the manuscript. Usta suggested that such a strategy may help accelerate the search for effective drugs for NAFLD conditions that range from benign fat accumulation to more serious complications including fibrosis, cirrhosis, and liver cancer. "
Science Daily,Engineers Demonstrate Key Step in Robotic Disassembly,Computers & Math,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829115434.htm,"   The research is an important advance for manufacturers looking for more efficient ways to build products from a combination of reused, repaired and new parts. Known as remanufacturing, this process is becoming increasingly commonplace in manufacturing and is attractive because it can use as little as 10 per cent of the energy and raw materials required to build the product from scratch. It can also reduce CO2 emissions by more than 80 per cent. A key part of the process is the ability to disassemble the 'core', the returned product. It's a challenge because of the huge variety within these products, with lots of unknowns in the size, shape and condition of components. The new study, published in Royal Society Open Science, demonstrates a process for removing pins from holes -- components like these are extremely common in a wide variety of machines, such as internal combustion engines. The research is the first to investigate this operation in depth and identify the key parameters required to automate the process. Yongquan Zhang, of the Autonomous Remanufacturing Laboratory at the University of Birmingham is lead author on the paper. ""Processes currently used for automating disassembly are fairly ad hoc,"" he explained. ""We need to be able to design robust systems that can handle the uncertainties that are inherent in disassembly processes -- and to do that, we need a better fundamental understanding of disassembly."" ""The results of this study demonstrate how that fundamental understanding can be used to design robotic systems for reliably performing one common disassembly operation."" "
Science Daily,"Small Changes, Big Gains: Low-Cost Techniques Dramatically Boost Learning in STEM Classes",Computers & Math,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829101052.htm,"   ""Many university STEM classes continue to rely on conventional lectures, despite substantial research that suggests active teaching techniques like peer instruction and group discussion are more effective,"" said UBC researcher Patricia Schulte, senior author of the study, published this week in PLOS ONE. ""But this confirms that group work significantly enhances how well students grasp and retain concepts. And strikingly, having students go through worksheets in groups -- an easily implemented, low cost classroom technique -- resulted in particularly strong improvements in scores."" Increasing class time dedicated to group work just 10 per cent (five minutes in a 50-minute class) correlated with roughly a three per cent improvement in student performance. That equates to almost one letter grade, depending on the institution. Using in-class worksheets -- a wide variety of structured handouts that contain a few questions or tasks related to a concept -- resulted in even more significant increases in student scores. In general, classes had to spend half or more of their time in group-work to see significant boost in learning. The study is the first large-scale, direct observation of classes across a curriculum that examines the impact of different active learning approaches. The researchers observed classroom practices across 31 lecture sections in the biology program at UBC, classifying classes by the degree of group work conducted in each. They administered tests to more than 3,700 students at the beginning and end of term to assess the extent of their learning, independent of regular course exams. Most students were in first and second year. Most of the previous work on active learning is based on instructor self-reports, qualitative surveys, or indirect observations of active learning, rather than direct assessments of teaching practices and independent assessments of learning. "
Science Daily,Biophysics: The Art of Worming Through Tight Spaces,Earth & Climate,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829115423.htm,"   Biological forms of active matter, such as bacterial biofilms or sheets of epithelial cells, are often found in confined microspaces. Working out how such systems colonize their environment and extend their range by invading new territories will enhance our understanding of many of the normal functions and disease states observed in higher organisms. In cooperation with Dr. Amin Doostmohammadi (University of Oxford), LMU physicists Felix Kempf and Professor Erwin Frey have now demonstrated with the aid of computer simulations that cell collectives exhibit a variety of motility patterns as they approach and pass through local constrictions. The authors of the new study go on to show that the pattern adopted depends on the level of active motility that develops at the leading edge of the assemblage. The findings appear in the journal Soft Matter. Several previous publications had suggested that the collective motions of biological matter are influenced by the nature of the terrain in which such systems find themselves. In particular, in-vitro experiments carried out with epithelial and bacterial cells, and with mixtures consisting of isolated intracellular biofilaments and molecular motors, have revealed that spatial boundaries have a significant impact on motility. ""So far, this type of research has concentrated primarily on the interactions between the shape of the obstacle employed and the motile activity of the particles concerned,"" says Kempf, the lead author of the new paper. However, in most of these systems, the number of particles does not remain constant. Under natural conditions, epithelial or bacterial cells divide at regular intervals and, when confined in capillary tubes, they form an advancing invasion front. Therefore, in order to understand how these patterns form and evolve, it is necessary to take the growth dynamics of these systems into account. Kempf and colleagues used computer simulations to explore the effects of this factor. They observed three fundamentally distinct modes of invasion, which can be distinguished on the basis of the overall activity of the growing system and the behavior of the invasion front as it approaches the constriction. If the level of motile activity is low, the invasion front retains its smooth and sharply defined outline as it advances at a constant speed. At higher levels of activity, the leading edge takes on an irregular outline. Finally, once the activity level exceeds a certain threshold, small clusters of cells detach from the advancing front, which can then worm their way through the narrow gap. The simulations also enabled the researchers to characterize the processes that drive the transitions observed as the invasion front evolves, and to quantify their impact on the speed with which the cells advanced into the ever more confined space. ""These findings make a significant contribution to our understanding of active matter, and have several implications that can be tested in future experiments,"" says Kempf. "
Science Daily,"Marathoners, Take Your Marks...and Fluid and Salt!",Earth & Climate,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150712.htm,"   While Pheidippides' demise was more likely brought about by a 300-mile run he reportedly made just prior to his ""marathon,"" today's long-distance runners face a mostly short-term but still serious physical threat known as acute kidney injury, or AKI. Now, results of a new study of marathon runners led by researchers at Johns Hopkins Medicine and Yale University suggest that sweat (fluid) volume and sweat sodium losses, rather than a rise in core body temperature, are the key contributors to post-race AKI. ""We knew from a previous study that a large number of marathoners developed short-term AKI following a race, so we wanted more specifically to pin down the causes,"" says Chirag Parikh, Ph.D., director of the Division of Nephrology at the Johns Hopkins University School of Medicine and senior author of the new paper. ""Our findings suggest that managing fluid volume and salt losses with a personalized regimen during the time period surrounding a marathon may help reduce the number or lessen the severity of AKI incidents afterward."" The researchers say they also found that runners with AKI following a marathon had increased levels of a blood serum protein known as copeptin. If the connection is confirmed with future studies, they say, copeptin could be valuable as a biomarker during training for predicting post-marathon susceptibility to AKI. AKI, as described by the National Kidney Foundation, is a ""sudden episode of kidney failure or kidney damage that happens within a few hours or a few days."" It causes waste products to build up in the blood, making it hard for kidneys to maintain the correct balance of fluids in the body. Symptoms of AKI differ depending on the cause and may include: too little urine leaving the body; swelling in legs, ankles and around the eyes; fatigue; shortness of breath; confusion; nausea; chest pain; and in severe cases, seizures or coma. The disorder is most commonly seen in hospitalized patients whose kidneys are affected by medical and surgical stress and complications. Similarly, a marathon subjects a runner to sustained physical stress, reduced blood flow to the kidneys and significant increases in the metabolic rate. Together, these events severely challenge the body's ability to keep fluid volume, electrolytes and temperature levels -- along with the regulatory responses to changes in all three -- in balance. The result, as seen in 82% of the runners evaluated by the same researchers in a 2017 Yale University study, was AKI that averaged two days in duration. For the latest study, the goal was to better define the risk factors and mechanism for the problem by examining 23 runners, ages 22-63, who competed in the 2017 Hartford Marathon in Connecticut. Participants were volunteers recruited through local running clubs and the marathon's registration process. Divided nearly equally between men and women, they were all experienced runners with a body mass index ranging between 18.5-24.9, and had completed at least four races longer than 20 kilometers (12.4 miles) within the previous three years. Urine and blood samples were collected from the participants at three time points: 24 hours prior to the marathon, within 30 minutes of completing the race and 24 hours after. The researchers evaluated the samples for sodium levels; key biomolecules such as creatine phosphokinase, hemoglobin, urine protein and copeptin; and biomarkers associated with kidney injury such as interleukin-18 and kidney injury molecule-1. Sweat collection patches were placed on the runners prior to the marathon and recovered at the 5-mile mark (because they became too saturated further in the race). Blood pressure, heart rate and weight were measured at all three time points, while a bioharness worn during the marathon continually recorded body temperature. Twelve of the 23 runners (55%) developed AKI after the race, while 17 (74%) tested positive for markers indicating some injury to the renal tubules, the tiny portals in the kidneys where blood is filtered. In the runners with post-race AKI, the researchers observed distinct sodium and fluid volume losses. The median salt loss was 2.3 grams, with some losing as much as 7 grams. Fluid volume loss via sweat had a midpoint level of 2.5 liters (5.2 pints), up to a maximum of 6.8 liters (14.4 pints). For comparison, a 155-pound (70-kilogram) body contains about 42 liters (85 pints) of fluid. Core body temperature, while significantly elevated throughout a marathon, was basically the same for all runners and therefore, was not considered a causal factor for AKI. However, the researchers say that the combination of high-body temperature along with fluid and salt losses may add to the development of kidney injury. ""Putting the sodium and fluid volume loss numbers into perspective, the median salt loss for the AKI runners was about 1 1/4 teaspoons, or the entire daily amount recommended by the American Heart Association,"" Parikh says. ""Their median fluid volume loss was equivalent to sweating out slightly more than a 2-liter soda bottle. Beyond that, we had evidence that runners weren't adequately keeping up with those depletions."" In turn, Parikh says, that failure to balance the sodium and fluid losses during a marathon may account for the new study's other relevant finding: the higher levels of copeptin seen in runners with post-race AKI. Copeptin is a precursor to the release of vasopressin, a hormone secreted by the pituitary gland in response to reduced blood volume. It tells our kidneys and blood vessels to hold on to water, preventing a sudden drop in blood pressure and physical collapse. ""In the runners who developed AKI, we found copeptin levels as much as 20 times higher than those who did not,"" Parikh says. ""This is biological evidence that the AKI sufferers were severely volume down."" Because vasopressin reduces blood flow to the kidneys, and decreases renal filtration and urine output, he adds, it also may induce inflammation and injury to the kidney tissues if secreted for an extended period of time. This may explain why a large number of marathon runners get AKI while those competing at shorter distances do not. Parikh says future studies, using larger samples, will need to evaluate whether optimizing fluid and salt volumes in marathon runners lowers rates or reduces the severity of post-race AKI. Additionally, he says, the researchers would like to follow runners who participate in multiple marathons to look for any cumulative kidney damage. ""The long-term goal will be to document an individual runner's metabolic and sweat profile to develop a fluid and salt replacement regimen just for him or her,"" he says. ""Then, runners could consume this personalized drink during the race to better maintain fluid and salt balance."" "
Science Daily,Biochar: A Better Start to Rain Forest Restoration,Earth & Climate,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830162300.htm,"   A carbon-based soil amendment called biochar is a cheap and effective way to support tree seedling survival during reforestation efforts in the Amazon rain forest, according to new research from Wake Forest University's Center for Amazonian Scientific Innovation (CINCIA). Restoring and recovering rain forests has become increasingly important for combatting climate change, since these wide swaths of trees can absorb billions of tons of carbon dioxide each year. The problem is particularly acute in areas mined for alluvial gold deposits, which devastate not only rain forest trees but also soils. High costs can be a huge barrier to replanting, fertilizing and nurturing trees to replace those lost in the rain forest. The scientists found that using biochar combined with fertilizer significantly improved height and diameter growth of tree seedlings while also increasing the number of leaves the seedlings developed. The experiment, based in a Peruvian Amazon region called Madre de Dios, the heart of illegal gold mining trade in that country, used two tropical tree species: the fast-growing Guazuma crinita and Terminalia amazonia, a late successional tree often used as timber. ""The most difficult period in a tree seedling's life is the first few months after transplanting,"" said Miles Silman, CINCIA associate director of science and Wake Forest's Andrew Sabin Presidential Chair of Conservation Biology. ""But just a little bit of biochar does wonderful things to the soil, and it really shines when you add organic fertilizer."" The CINCIA scientists make biochar out of Brazil nut husks discarded by large-scale processors in Peru. They burn the husks slowly in 55-gallon barrels, a low-tech, inexpensive and easily scalable method for producing biochar. The study, ""Biochar effects on two native tropical tree species and its potential as a tool for reforestation,"" appears online this month in the peer-reviewed journal Forests. Until this study, little was known about whether biochar could benefit tree growth in tropical tree seedlings. ""We show that while both biochar and fertilizer can improve tree seedling growth, combining them makes seedlings thrive beyond either amendment alone,"" said Silman. The native peoples of the Amazon created ""dark earths"" using biochar thousands of years ago, and those soils are still productive today. Biochar's benefits are many: It improves the soil's ability to hold water and makes it less acidic. It provides a welcoming habitat for microbes, which support plant growth. It holds onto fertilizer and releases it over time, decreasing the need for repeat applications of fertilizer, which cuts labor and supply costs.The scientists used soils recovered from the San Jacinto native community, where gold mining has ravaged the land. Silman explained that the dirt that comes from the mining sluice is devoid of the organic matter and microbes that supports plant life. ""These are the kinds of landscapes we have to recover, and we are still trying to determine how to grow plants in them,"" he said. ""This soil is extremely limiting for natural regrowth, but treating them with biochar turns it into something that plants can grow in. That's good for biodiversity and good for the people that have to make a living from the land."" "
Science Daily,Evidence for Past High-Level Sea Rise,Earth & Climate,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150751.htm,"   The scientists, including Professor Yemane Asmerom and Sr. Research Scientist Victor Polyak from The University of New Mexico, the University of South Florida, Universitat de les Illes Balears and Columbia University, published their findings in today's edition of the journal Nature. The analysis of deposits from Artà Cave on the island of Mallorca in the western Mediterranean Sea produced sea levels that serve as a target for future studies of ice sheet stability, ice sheet model calibrations and projections of future sea level rise, the scientists said. Sea level rises as a result of melting ice sheets, such as those that cover Greenland and Antarctica. However, how much and how fast sea level will rise during warming is a question scientists have worked to answer. Reconstructing ice sheet and sea-level changes during past periods when climate was naturally warmer than today, provides an Earth's scale laboratory experiment to study this question according to USF Ph.D. student Oana Dumitru, the lead author, who did much of her dating work at UNM under the guidance of Asmerom and Polyak. ""Constraining models for sea level rise due to increased warming critically depends on actual measurements of past sea level,"" said Polyak. ""This study provides very robust measurements of sea level heights during the Pliocene."" ""We can use knowledge gained from past warm periods to tune ice sheet models that are then used to predict future ice sheet response to current global warming,"" said USF Department of Geosciences Professor Bogdan Onac. The project focused on cave deposits known as phreatic overgrowths on speleothems. The deposits form in coastal caves at the interface between brackish water and cave air each time the ancient caves were flooded by rising sea levels. In Artà Cave, which is located within 100 meters of the coast, the water table is -- and was in the past -- coincident with sea level, says Professor Joan J. Fornós of Universitat de les Illes Balears. The scientists discovered, analyzed, and interpreted six of the geologic formations found at elevations of 22.5 to 32 meters above present sea level. Careful sampling and laboratory analyses of 70 samples resulted in ages ranging from 4.4 to 3.3 million years old BP (Before Present), indicating that the cave deposits formed during the Pliocene epoch. The ages were determined using uranium-lead radiometric dating in UNM's Radiogenic Isotope Laboratory. ""This was a unique convergence between an ideally-suited natural setting worked out by the team of cave scientists and the technical developments we have achieved over the years in our lab at The University of New Mexico,"" said Asmerom. ""Judicious investments in instrumentation and techniques result in these kinds of high-impact dividends."" ""Sea level changes at Artà Cave can be caused by the melting and growing of ice sheets or by uplift or subsidence of the island itself,"" said Columbia University Assistant Professor Jacky Austermann, a member of the research team. She used numerical and statistical models to carefully analyze how much uplift or subsidence might have happened since the Pliocene and subtracted this from the elevation of the formations they investigated. One key interval of particular interest during the Pliocene is the mid Piacenzian Warm Period -- some 3.264 to 3.025 million years ago -- when temperatures were 2 to 3º Celsius higher than pre-industrial levels. ""The interval also marks the last time the Earth's atmospheric CO2 was as high as today, providing important clues about what the future holds in the face of current anthropogenic warming,"" Onac says. This study found that during this period, global mean sea level was as high as 16.2 meters (with an uncertainty range of 5.6 to 19.2 meters) above present. This means that even if atmospheric CO2 stabilizes around current levels, the global mean sea level would still likely rise at least that high, if not higher, the scientists concluded. In fact, it is likely to rise higher because of the increase in the volume of the oceans due to rising temperature. ""Considering the present-day melt patterns, this extent of sea level rise would most likely be caused by a collapse of both Greenland and the West Antarctic ice sheets,"" Dumitru said. The authors also measured sea level at 23.5 meters higher than present about four million years ago during the Pliocene Climatic Optimum, when global mean temperatures were up to 4°C higher than pre-industrial levels. ""This is a possible scenario, if active and aggressive reduction in green house gases into the atmosphere is not undertaken,"" Asmerom said. "
Science Daily,Amazon Deforestation Has a Significant Impact on the Local Climate in Brazil,Earth & Climate,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830112813.htm,"   The UN Environment Programme has said warned that the Amazon wildfires threaten ."" ..this precious natural resource..."" and that the forest helps mitigate the effects of climate change. Insight into the effects of deforestation in the Amazon -- and the way it can intensify climate change, particularly at a local level -- has been published open access in the journal Frontiers. Using satellite data, Jess Baker and Professor Dominick Spracklen from the University of Leeds, evaluated the climatic consequences of deforestation in the Amazon between 2001 and 2013. They found that deforestation causes the local climate to warm -- and that warming intensified as the severity of deforestation increased. Intact forests in the region, with less than 5% canopy loss, had the most climate stability over the ten years, showing only small increases in temperature. Areas that had tree cover reduced to below 70% warmed 0.44°C more than neighbouring intact forests during the study period. The differences between intact and disturbed forests were most pronounced during the driest part of the year, when temperature increases of up to 1.5°C were observed in areas affected by severe deforestation. This increase is additional to global temperature rises driven by climate change. Study co-author Jess Baker from the School of Earth and Environment at Leeds said: ""The Amazon wildfires have reminded us all of the important role that forests play in our global systems. But it cannot be overlooked that intact Amazon forests are also crucially important for Brazil's own local climate. ""A healthy intact Amazon forest helps regulate the local climate and can even act as a buffer to the warming effects of climate change, compared with disturbed forests."" Study co-author Dominick Spracklen, Professor of Biopshere-Atmosphere Interactions at Leeds said: ""Deforestation decreases the amount of water emitted to the atmosphere from the forest through a process called evapotranspiration. ""Evapotranspiration can be thought of as the forest 'sweating'; when the moisture emitted by the forests evaporates it cools the local climate. Deforestation reduces evapotranspiration, taking away this cooling function and causing local temperatures to rise. ""As temperatures rise this increases drought stress and makes forests more susceptible to burning."" "
Science Daily,Defrosting Surfaces in Seconds,Earth & Climate,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830112811.htm,"   A group of researchers at the University of Illinois at Urbana-Champaign and Kyushu University has developed a way to remove ice and frost from surfaces extremely efficiently, using less than 1% of the energy and less than 0.01% of the time needed for traditional defrosting methods. The group describes the method in Applied Physics Letters, from AIP Publishing. Instead of conventional defrosting, which melts all the ice or frost from the top layer down, the researchers established a technique that melts the ice where the surface and the ice meet, so the ice can simply slide off. ""The work was motivated by the large energy efficiency losses of building energy systems and refrigeration systems due to the need to do intermittent defrosting. The systems must be shut down, the working fluid is heated up, then it needs to be cooled down again,"" said author Nenad Miljkovic, at UIUC. ""This eats up a lot of energy when you think of the yearly operational costs of running intermittent defrosting cycles."" According to the authors, the biggest source of inefficiency in conventional systems is that much of the energy used for de-icing goes into heating other components of the system rather than directly heating the frost or ice. This increases energy consumption and system downtime. Instead, the researchers proposed delivering a pulse of very high current where the ice and the surface meet to create a layer of water. To ensure the pulse reaches the intended space rather than melting the exposed ice, the researchers apply a thin coating of indium tin oxide (ITO) -- a conductive film often used for defrosting -- to the surface of the material. Then, they leave the rest to gravity. To test this, the scientists defrosted a small glass surface cooled to minus 15.1 degrees Celsius -- about as cold as the warmest parts of Antarctica -- and to minus 71 degrees Celsius -- colder than the coldest parts of Antarctica. These temperatures were chosen to model heating, ventilation, air conditioning and refrigeration applications and aerospace applications, respectively. In all tests, the ice was removed with a pulse lasting less than one second. In a real, 3D system, gravity would be assisted by air flow. ""At scale, it all depends on the geometry,"" Miljkovic said. ""However, the efficiency of this approach should definitely still be much better than conventional approaches."" The group hasn't studied more complicated surfaces like airplanes yet, but they think it's an obvious future step. ""They are a natural extension as they travel fast, so the shear forces on the ice are large, meaning only a very thin layer at the interface needs to be melted in order to remove the ice,"" Miljkovic said. ""Work would be needed to figure out how we can coat curved components conformally with the ITO and to figure out how much energy we would need."" The researchers hope to work with external companies on scaling up their approach for commercialization. "
Science Daily,This Protein Is How Creatures Sense Cold,Earth & Climate,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150803.htm,"   The findings, scheduled for publication Aug. 29 in the journal Cell, reveal the first known cold-sensing protein to respond to extreme cold. ""Clearly, nerves in the skin can sense cold. But no one has been able to pinpoint exactly how they sense it,"" said Shawn Xu, a faculty member at the University of Michigan Life Sciences Institute and senior author of the study. ""Now, I think we have an answer."" When environmental temperatures drop to uncomfortable, and even dangerous levels, receptor proteins within the sensory nerves in the skin perceive the change, and they relay that information to the brain. This is true for organisms from humans all the way down to the tiny, millimeter-long worms that researchers study in Xu's lab at the Life Sciences Institute: the model system Caenorhabditis elegans. ""When you step outside and you sense it's too cold, you're going to take action to get back to a warmer environment as soon as you can,"" said Xu, who is also a professor in the U-M Medical School's Department of Molecular and Integrative Physiology. ""When the worms sense cold, they also engage in avoidance behavior -- moving away from cold temperatures, just like humans."" But unlike humans or other complex organisms, C. elegans have a simple, well-mapped genome and a short lifespan, making them a valuable model system for studying sensory responses. Previous searches for a cold receptor have been unsuccessful because researchers were focusing on specific groups of genes that are related to sensation, which is a biased approach, Xu said. Capitalizing on the simplicity of C. elegans, he and his colleagues instead took an unbiased approach. They looked across thousands of random genetic variations to determine which affected the worms' responses to cold. The researchers found that worms missing the glutamate receptor gene glr-3 no longer responded when temperatures dipped below 18 degrees Celsius (64 F). This gene is responsible for making the GLR-3 receptor protein. Without this protein, the worms became insensitive to cold temperatures, indicating that the protein is required for the worms to sense cold. What's more, the glr-3 gene is evolutionarily conserved across species, including humans. And it turns out the vertebrate versions of the gene can also function as a cold-sensing receptor. When the researchers added the mammalian version of the gene to mutant worms lacking glr-3 -- and were thus insensitive to cold -- they found that it rescued the worms' cold sensitivity. They also added the worm, zebrafish, mouse and human versions of the genes to cold-insensitive mammalian cells. With all versions of the gene, the cells became sensitive to cold temperatures. The mouse version of the gene, GluK2 (for glutamate ionotropic receptor kainate type subunit 2), is well known for its role in transmitting chemical signals within the brain. The researchers discovered, however, that this gene is also active in a group of mouse sensory neurons that detect environmental stimuli, such as temperature, through sensory endings in the animals' skin. Reducing the expression of GluK2 in mouse sensory neurons suppressed their ability to sense cold, but not cool, temperatures. The findings provide additional evidence that the GluK2 protein serves as a cold receptor in mammals. ""For all these years, attention has been focused on this gene's function in the brain. Now, we've found that it has a role in the peripheral sensory system, as well,"" Xu said. ""It's really exciting. This was one of the few remaining sensory receptors that had not yet been identified in nature."" In addition to Xu, study authors are: Elizabeth Ronan, Wei Cai, Mahar Fatima, Hankyu Lee, Zhaoyu Li, Kevin Pipe and Bo Duan of U-M; Jianke Gong, Jinzhi Liu, Feiteng He and Wenyuan Zhang of Huazhong University of Science and Technology in China and U-M; Jianfeng Liu of Huazhong University of Science and Technology; and Gun-Ho Kim of the Ulsan National Institute of Science and Technology in South Korea. "
Science Daily,"The 'Universal Break-Up Criterion' of Hot, Flowing Lava?",Earth & Climate,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150808.htm,"   Jones, of Rice University, studies the behavior of low-viscosity lava, the runny kind that's found at most volcanoes. About two years ago, he began a series of lab experiments and field observations that provided the raw inputs for a new fluid dynamic model of lava break-up. The work is described in a paper in Nature Communications. Low-viscosity lava is the red-hot, flowing type one might see at Hawaii's famed Kilauea volcano, and Jones said it usually behaves in one of two ways. ""It can bubble or spew out, breaking into chunks that spatter about the vent, or it can flow smoothly, forming lava streams that can rapidly move downhill,"" he said. But that behavior can sometimes change quickly during the course of an eruption, and so can the associated dangers: While spattering eruptions throw hot lava fragments into the air, lava flows can threaten to destroy whole neighborhoods and towns. Jones' model, the first of its kind, allows scientists to calculate when an eruption will transition from a spattering spray to a flowing stream, based upon the liquid properties of the lava itself and the eruption conditions at the vent. Jones said additional work is needed to refine the tool, and he looks forward to doing some of it himself. ""We will validate this by going to an active volcano, taking some high-speed videos and seeing when things break apart and under what conditions,"" he said. ""We also plan to look at the effect of adding bubbles and crystals, because real magmas aren't as simple as the idealized liquid in our mathematical model. Real magmas can also have bubbles and crystals in them. I'm sure those will change things. We want to find out how."" Jones said pairing the new model with real-time information about a lava's liquid properties and eruption conditions could allow emergency officials to predict when an eruption will change style and become a hazard to at-risk communities. ""We want to use this as a forecasting tool for eruption behavior,"" he said. ""By developing a model of what's happening in the subsurface we can then watch for indications that it's about to cross the tipping point and change behavior."" "
Science Daily,Oxygen Depletion in Ancient Oceans Caused Major Mass Extinction,Earth & Climate,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150801.htm,"   For years, scientists struggled to connect a mechanism to this mass extinction, one of the 10 most dramatic ever recorded in Earth's history. Now, researchers from Florida State University have confirmed that this event, referred to by scientists as the Lau/Kozlowskii extinction, was triggered by an all-too-familiar culprit: rapid and widespread depletion of oxygen in the global oceans. Their study, published today in the journal Geology, resolves a longstanding paleoclimate mystery, and raises urgent concerns about the ruinous fate that could befall our modern oceans if well-established trends of deoxygenation persist and accelerate. Unlike other famous mass extinctions that can be tidily linked to discrete, apocalyptic calamities like meteor impacts or volcanic eruptions, there was no known, spectacularly destructive event responsible for the Lau/Kozlowskii extinction. ""This makes it one of the few extinction events that is comparable to the large-scale declines in biodiversity currently happening today, and a valuable window into future climate scenarios,"" said study co-author Seth Young, an assistant professor in the Department of Earth, Ocean and Atmospheric Science. Scientists have long been aware of the Lau/Kozlowskii extinction, as well as a related disruption in Earth's carbon cycle during which the burial of enormous amounts of organic matter caused significant climate and environmental changes. But the link and timing between these two associated events -- the extinction preceded the carbon cycle disruption by more than a hundred thousand years -- remained stubbornly opaque. ""It's never been clearly understood how this timing of events could be linked to a climate perturbation, or whether there was direct evidence linking widespread low-oxygen conditions to the extinction,"" said FSU doctoral student Chelsie Bowman, who led the study. To crack this difficult case, the team employed a pioneering research strategy. Using advanced geochemical methods including thallium isotope, manganese concentration, and sulfur isotope measurements from important sites in Latvia and Sweden, the FSU scientists were able to reconstruct a timeline of ocean deoxygenation with relation to the Lau/Kozlowskii extinction and subsequent changes to the global carbon cycle. The team's new and surprising findings confirmed their original hypothesis that the extinction record might be driven by a decline of ocean oxygenation. Their multiproxy measurements established a clear connection between the steady creep of deoxygenated waters and the step-wise nature of the extinction event -- its start in communities of deep-water organisms and eventual spread to shallow-water organisms. Their investigations also revealed that the extinction was likely driven in part by the proliferation of sulfidic ocean conditions. ""For the first time, this research provides a mechanism to drive the observed step-wise extinction event, which first coincided with ocean deoxygenation and was followed by more severe and toxic ocean conditions with sulfide in the water column,"" Bowman said. With the oxygen-starved oceans of the Lau/Kozlowskii extinction serving as an unnerving precursor to the increasingly deoxygenated waters observed around the world today, study co-author Jeremy Owens, an assistant professor in the Department of Earth, Ocean and Atmospheric Science, said that there are still important lessons to be learned from ecological crises of the distant past. ""This work provides another line of evidence that initial deoxygenation in ancient oceans coincides with the start of extinction events,"" he said. ""This is important as our observations of the modern ocean suggest there is significant widespread deoxygenation which may cause greater stresses on organisms that require oxygen, and may be the initial steps towards another marine mass extinction."" "
Science Daily,Researchers Determine Pollen Abundance and Diversity in Pollinator-Dependent Crops,Earth & Climate,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830150757.htm,"   The study, a collaboration between OSU and Texas A&M University, found that almond, cherry and meadowfoam provide ample pollen to honeybees, but highbush blueberry and hybrid carrot seed crops may not. In addition, California almonds don't provide as much pollen diversity as other crops, according to the findings, published in the Journal of Economic Entomology. The western honeybee is the major pollinator of fruit, nut, vegetable and seed crops that depend on bee pollination for high quality and yield. The findings are important because both pollen abundance and diversity are critical for colony growth and survival of the western honeybee, said study corresponding author Ramesh Sagili, associate professor of apiculture and honeybee Extension specialist in OSU's College of Agricultural Sciences. ""Pollen diversity is important for the growth and development of bees, and low amounts of pollen availability to honeybee colonies can dramatically affect brood rearing,"" Sagili said. ""Beekeepers that employ their colonies for pollination of crops like hybrid carrot seed and highbush blueberry should frequently assess the amount of pollen stores in their colonies and provide protein supplements if pollen stores are low."" Nectar and pollen provide essential nutrients for honeybees. A honeybee colony's protein source is pollen, which has varying amounts of amino acids, lipids, vitamins and minerals. These nutrients obtained from pollen are essential for honeybee larval development. Pollen largely contributes to the growth of fat bodies in larvae and egg development in the queen. Well-nourished individuals in a honeybee colony are able to withstand the effects of other stressors such as parasites and insecticides, in addition to the long-distance transport of colonies known as ""migratory management."" Bees are trucked across the county to pollinate various cropping systems -- more than 1 million hives are transported to California each year just to pollinate almonds. A diet low in pollen diversity hurts a colony's defense system, which consequently increases disease susceptibility and pesticide sensitivity. During critical crop bloom periods, growers rent large numbers of honeybee colonies to pollinate their crops. Approximately 2.5 million commercially managed honeybee colonies are used for crop pollination in the United States every year. Some cropping systems may put bees at risk for temporary nutritional deficiency if the crop plant's pollen is deficient in certain nutrients and bees are unable to find an alternative source of these nutrients, Sagili said. ""It's crucial for beekeepers and crop producers to understand the pollen abundance and diversity that honeybees encounter during crop pollination,"" he said, adding that blueberry and hybrid carrot seed producers can mitigate nutritional deficiencies by providing supplemental food or forage, including commercially available protein supplements for bees. Renting colonies to growers for pollination services is a significant source of income for commercial beekeepers, but it also requires them to repeatedly transport the colonies between crops throughout the growing season. In this study, the research team collaborated with 17 migratory commercial beekeepers for pollen collection from honeybee colonies in five different cropping systems from late February to August of 2012. They installed pollen traps on at least five colonies at each site and collected pollen from the colonies at the height of the blooming season. They found that California's vast almond footprint -- 1 million acres and counting -- provides more than enough pollen for the nearly 2 million honeybees employed to pollinate the orchards, but pollen diversity was low when compared with other crops. ""We think the reason for that is almonds bloom early in the year when there are so few plant species in bloom, so bees have few other forage options and primarily rely on almond pollen,"" Sagili said. ""There are parts of the northern and southern ends of California's San Joaquin Valley where there are no other crops in bloom when almond trees bloom, which may further contribute to poor availability of diverse pollen."" "
Science Daily,What If We Paid Countries to Protect Biodiversity?,Science & Society,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830112815.htm,"   After long negotiations, the international community has agreed to safeguard the global ecosystems and improve on the status of biodiversity. The global conservation goals for 2020, called the Aichi targets, are an ambitious hallmark. Yet, effective implementation is largely lacking. Biodiversity is still dwindling at rates only comparable to the last planetary mass extinction. Additional effort is required to reach the Aichi targets and even more so to halt biodiversity loss. ""Human well-being depends on ecological life support. Yet, we are constantly losing biodiversity and therefore the resilience of ecosystems. At the international level, there are political goals, but the implementation of conservation policies is a national task. There is no global financial mechanism that can help nations to reach their biodiversity targets,"" says lead author Nils Droste from Lund University, Sweden. Brazil has successfully implemented Ecological Fiscal Transfer systems that compensate municipalities for hosting protected areas at a local level since the early 1990's. According to previous findings, such mechanisms help to create additional protected areas. The international research team has therefore set out to scale this idea up to the global level where not municipalities but nations are in charge of designating protected areas. They developed and compared three different design options: An ecocentric model: where only protected area extent per country counts -- the bigger the protected area, the better; A socio-ecological model: where protected areas and Human Development Index count, adding development justice to the previous model; An anthropocentric model: where population density is also considered, as people benefit locally from protected areas. The socio-ecological design was the one that proved to be the most efficient. The model provided the highest marginal incentives -- that is, the most additional money for protecting an additional percent of a country's area -- for countries that are the farthest from reaching the global conservation goals. The result surprised the researchers. ""While we developed the socio-ecological design with a fairness element in mind, believing that developing countries might be more easily convinced by a design that benefits them, we were surprised how well this particular design aligns with the global policy goals,"" says Nils Droste. ""It would most strongly incentivize additional conservation action where the global community is lacking it the most,"" he adds. As the study was aimed at providing options, not prescriptions for policy makers, the study did not detail who should be paying or how large the fund should exactly be. Rather, it provides a yet unexplored option to develop a financial mechanism for biodiversity conservation akin to what the Green Climate Fund is for climate change. ""We know that we need to change land use in order to preserve biodiversity. Protecting land from degradation and providing healthy ecosystems, clean air or clean rivers is a function of the state. Giving a financial reward to governments for such public ecosystem services will ease the provision of corresponding conservation efforts and will help to put this on the agenda,"" concludes Nils Droste. "
Science Daily,Ancient Civilizations Were Already Messing Up the Planet,Science & Society,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829150702.htm,"   ""Through this crowdsourced data, we can see that there was global environmental impact by land use at least 3,000 years ago,"" says Gary Feinman, MacArthur Curator of Anthropology at the Field Museum and one of the study's 250 authors. ""And that means that the idea of seeing human impact on the environment as a newer phenomenon is too focused on the recent past."" Feinman says that to understand our current climate crisis, we need to understand the history of humans altering their environments. The study, led by Lucas Stephens of the University of Pennsylvania, is a part of a larger project called ArchaeoGLOBE, where online surveys are used to gather information from regional experts on how land use has changed over time in 146 different areas around the world. Land use can be anything from hunting and gathering to farming to grazing animals. And as it turns out, many of the ways ancient people used the land weren't as ""leave-no-trace"" as many have imagined. ""About 12,000 years ago, humans were mainly foraging, meaning they didn't interact with their environments as intensively as farmers generally do,"" says Feinman. ""And now we see that 3,000 years ago, we have people doing really invasive farming in many parts of the globe."" Humans in these time periods began clearing out forests to plant food and domesticating plants and animals to make them dependent on human interaction. Early herders also changed their surroundings through land clearance and selective breeding. While these changes were at varying paces, the examples are now known to be widespread and can provide insight on how we came to degrade our relationship with the Earth and its natural resources. ""We saw an accelerated trajectory of environmental impact,"" says Ryan Williams, associate curator and head of anthropology at the Field Museum and co-author of the study. ""While the rate at which the environment is currently changing is much more drastic, we see the effects that human impacts had on the Earth thousands of years ago."" The results, however, are more optimistic than they seem. Now that researchers know the beginnings of environmental impact, they can use this data to study what solutions ancient civilizations used to mitigate the negative effects of deforestation, water scarcity, and more. In addition to pointing out the history behind what most assume is a recent phenomenon, the study is one of the first of its kind to operate on such a large scale. Use of online resources and professional connections helped the project span across the world. The emphasis now, however, is on the parts we often miss. ""We need to invest in these regions that haven't been as intensively studied,"" says Williams. ""If we incentivize and create opportunities for researchers there then you can just imagine what the results of the next study like this could be."" For a long time, war, environment, transportation and colonization prevented researchers from being able to work together and share their findings about certain parts of the world. As a result, today's archaeologists are still adding to and growing the network of expertise in these regions. ""What really got me here was not so much the results, although I think that the results provide a foundation to support what many archeologists suspected,"" says Feinman. ""But I think the most innovative aspect of this was the whole research design. To gather information from 250 scholars and to make sure that the whole world was covered, that's really something."" While today's climate change and environmental destruction are happening more quickly and on a far larger scale than the world has ever seen, Feinman notes that this study helps provide a historical context to today's problems. ""There's such a focus on how the present is different from the past in contemporary science. I think this study provides a check, a counter-weight to that, by showing that yes, there have been more accelerated changes in land use recently, but humans have been doing this for a long time. And the patterns start 3,000 years ago,"" says Feinman. ""It shows that the problems we face today are very deep-rooted, and they are going to take more than simple solutions to solve. They cannot be ignored."" "
Science Daily,Ultra-Fast Bomb Detection Method Could Upgrade Airport Security,Science & Society,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829101051.htm,"   In a comprehensive two-part paper published by the journal Propellants, Explosives, Pyrotechnics and Forensic Science International: Synergy, a team of researchers from Surrey detail how they have built on their previous ground-breaking work on super-fast fingerprint drug testing, to develop a technique that is able to detect key explosives in just 30 seconds. The new method, which uses swabbing material to collect samples of explosives, is able to detect substances such as nitrotoluenes, trinitrotriazine, hexamethylene triperoxide diamine and nitroglycerine. Detection of peroxide-based explosives is key as high-profile terrorist attacks such as the London bombings in 2007 used devices made from these materials. Surrey's swab spray technique is able to achieve higher sensitivity results than previously published works and was also tested on dirty surfaces such as new and used keyboards. Dr Melanie Bailey, co-author of the paper from the University of Surrey, said: ""It's the unfortunate reality that security, especially in our airports, has to stay several steps ahead of those that wish to cause harm and destruction. The current thermal based way of detecting explosive material is becoming outdated and has the propensity of producing false positives. What we demonstrate with our research is an extremely fast, accurate and sensitive detection system that is able to identify a wide range of explosive materials."" Dr Catia Costa, co-author of the paper from the University of Surrey, said: ""The need for fast screening methods with enhanced selectivity and sensitivity to explosives has reached a new boiling point with the recent terrorist activity. The use of paper spray for applications such as these may help reduce false-negative events whilst also allowing simultaneous detection of other substances such as drugs, as previously reported by our group."" Dr Patrick Sears, co-author of the paper from the University of Surrey, said: ""The critical advantage of this system is the ability to uniquely identify the explosive being detected, making it much less likely to create false alarms. The selectivity of this system means that it could also be used to identify a range of other threat materials whilst the sensitivity would allow the detection of invisible traces of explosives."" "
Science Daily,Most-Comprehensive Analysis of Fentanyl Crisis Urges Innovative Action,Science & Society,2019-08-29,-,https://www.sciencedaily.com/releases/2019/08/190829081407.htm,"   ""This crisis is different because the spread of synthetic opioids is largely driven by suppliers' decisions, not by user demand,"" said Bryce Pardo, lead author of the study and an associate policy researcher at RAND, a nonprofit research organization. ""Most people who use opioids are not asking for fentanyl and would prefer to avoid exposure."" While fentanyl had appeared in U.S. illicit drug markets before, production was limited to one or a few capable chemists, and bottlenecks in production and distribution slowed the drug's diffusion. Law enforcement was able to detect and shut down illicit manufacture to contain these outbreaks. RAND researchers found that today's synthetic opioid surge is fueled by multiple sources. Mexican drug trafficking organizations smuggle fentanyl into the U.S., and China's pharmaceutical and chemical industries are inadequately regulated, allowing producers to advertise and ship synthetic opioids to buyers anywhere in the world. While traditional criminal organizations play a role in the spread of fentanyl, the internet also has made it easier to traffic these drugs and to share information about their synthesis. Overdose deaths involving fentanyl and other synthetic opioids have increased from about 3,000 in 2013 to more than 30,000 in 2018. These deaths have remained concentrated in Appalachia, the mid-Atlantic and New England. ""While synthetic opioids have not yet become entrenched in illicit drug markets west of the Mississippi River, authorities must remain vigilant,"" said Jirka Taylor, study co-author and senior policy analyst at RAND. ""Even delaying the onset in these markets by a few years could save thousands of lives."" For U.S. policymakers, nontraditional strategies may be required to address this new challenge. The researchers avoid making specific policy recommendations, but advocate consideration of a broad array of innovative approaches such as supervised consumption sites, creative supply disruption, drug content testing, and increasing access to novel treatments that are available in other countries, such as heroin-assisted treatment. ""Indeed, it might be that the synthetic opioid problem will eventually be resolved with approaches or technologies that do not currently exist or have yet to be tested,"" said Beau Kilmer, study co-author and director of the RAND Drug Policy Research Center. ""Limiting policy responses to existing approaches will likely be insufficient and may condemn many people to early deaths."" RAND researchers say that since the diffusion of fentanyl is driven by suppliers' decisions, it makes sense to consider supply disruption as one piece of a comprehensive response, particularly where that supply is not yet firmly entrenched. But the researchers note there is little reason to believe that tougher sentences, including drug-induced homicide laws for low-level retailers and couriers, will make a difference. Instead, they call for an exploration of innovative disruption efforts that confuse or dissuade online sourcing. The study is the most comprehensive document to be published on the past, present and future of illicit synthetic opioids. RAND researchers analyzed mortality and drug seizure data, reviewed existing literature, and conducted expert interviews and international case studies. RAND researchers examined synthetic opioid markets across the U.S. and in other parts of the world, such as Estonia (where fentanyl first appeared 20 years ago). Canada's experience with synthetic opioids is most similar to that in the United States in terms of its timing, sudden increase in drug-related harms, and regional concentration. ""Problems in parts of Canada are as severe as in the Eastern United States despite substantial differences in drug policy, and the delivery of public health and social services,"" said Jonathan Caulkins, study co-author and Stever University Professor at Carnegie Mellon University. A handful of other countries in Europe also have seen synthetic opioids increasingly displace heroin. Their experience is varied and shows a range of directions some future markets in the United States may take. For instance, Sweden developed an online market with fentanyl analogs sold primarily as nasal sprays. Evidence from abroad suggests synthetic opioids may be here to stay: the study found no instance where fentanyl lost ground to another opioid after attaining a dominant position in drug markets. Funding for the study was provided by RAND Ventures, which is supported by gifts from RAND supporters and income from operations. The study: The Future of Fentanyl and Other Synthetic Opioids. "
Science Daily,Clues to Early Social Structures May Be Found in Ancient Extraordinary Graves,Science & Society,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828143053.htm,"   As early farming communities gave rise to larger, more complex sedentary societies, new social hierarchies arose, presenting opportunities for individual people to achieve positions of importance. The authors cite two archetypal ""pathways to power"" such individuals might follow: one self-aggrandizing and often autocratic, and the other more group-oriented and egalitarian. But how these ""pathways"" were expressed in early cultures remains unclear. This study focused on a single burial in the Ba'ja settlement of southern Jordan, dating between 7,500-6,900 BC, during the Late Pre-Pottery B Period. The elaborate construction of this grave and sophistication of associated symbolic objects suggest the deceased was a person of importance in the ancient society. The authors suggest that the presence of exotic items in the grave indicate a person who achieved individual prestige by access to trade networks, while the proximity of the grave to other less elaborate graves indicates that they were nonetheless considered close in status to the broader community, not neatly fitting either archetype of a powerful individual. The authors propose that this sort of data can provide insights into cultural views toward leadership and social hierarchy in early cultures. They also suggest that further investigations of this body and others in Ba'ja, including ancient DNA analysis to illuminate familial relationships, may combine with grave information to create a more refined picture of early community social structures. The authors add: ""We suggest that leadership can be understood only by studying the social contexts and the pathways to power (not only the burials of extraordinary individuals). In fact, studying rich tombs to interpret social structures has been done before, but our new approach emphasizes the social environments of leadership. The key study of the elaborate burial of the late PPNB site of Ba'ja lets us surmise that access to leadership was possible through corporate leadership-type of primus inter pares than by autocratic coercive power."" "
Science Daily,Some Vaccine Doubters May Be Swayed by Proximity to Disease Outbreak,Science & Society,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828143108.htm,"   In both the US and globally, there is growing vaccine hesitancy, which can manifest itself in increased non-medical exemption rates, decreased vaccination rates and increased outbreaks of vaccine-preventable diseases. The formation of attitudes about vaccination is complex and linked to many factors including media and peer group influence, distrust of science, information access, and socio-economic barriers. In the new study, researchers surveyed 1,006 online respondents across the United States about their political beliefs, vaccination attitudes and demographics. The survey was carried out in January 2017, following local outbreaks of measles in 2016. The respondent pool was generated by a market research firm to be a nationally representative sample of the U.S. voting age population and the final sample matched known population in terms of gender, age, income race and Census region. The researchers found that an individual's proximity to a measles outbreak independent had no independent effect on measles vaccination attitudes (p = 0.43). However, they found that trust in government medical experts is strongly and positively related to vaccination attitudes (p=0.01). Moreover, the study uncovered an interactive relationship between the two variables. People who are skeptical of the CDC and similar institutions and live farther away from a disease outbreak harbor less favorable vaccination views than those who are skeptical but live in close proximity to an outbreak. People who have high levels of trust are not affected by disease proximity. The research therefore suggests that, unlike people who trust government experts, people who are skeptical of the CDC and similar institutions may consider whether or not a given disease occurs nearby when making decisions about vaccination. Justwan adds: ""In this paper, we explore whether people's vaccination attitudes with regards to measles are shaped by how far away they live from a recent outbreak. We find that this is the case -- but only for individuals who also distrust government medical experts. Put differently: citizens who are skeptical of the CDC and similar institutions base their vaccination decision-making to some degree on whether or not a given disease occurs in close vicinity to their community."" "
Science Daily,Nuclear Winter Would Threaten Nearly Everyone on Earth,Science & Society,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828080543.htm,"   Indeed, death by famine would threaten nearly all of the Earth's 7.7 billion people, said co-author Alan Robock, a Distinguished Professor in the Department of Environmental Sciences at Rutgers University-New Brunswick. The study in the Journal of Geophysical Research-Atmospheres provides more evidence to support The Treaty on the Prohibition of Nuclear Weapons passed by the United Nations two years ago, Robock said. Twenty-five nations have ratified the treaty so far, not including the United States, and it would take effect when the number hits 50. Lead author Joshua Coupe, a Rutgers doctoral student, and other scientists used a modern climate model to simulate the climatic effects of an all-out nuclear war between the United States and Russia. Such a war could send 150 million tons of black smoke from fires in cities and industrial areas into the lower and upper atmosphere, where it could linger for months to years and block sunlight. The scientists used a new climate model from the National Center for Atmospheric Research with higher resolution and improved simulations compared with a NASA model used by a Robock-led team 12 years ago. The new model represents the Earth at many more locations and includes simulations of the growth of the smoke particles and ozone destruction from the heating of the atmosphere. Still, the climate response to a nuclear war from the new model was nearly identical to that from the NASA model. ""This means that we have much more confidence in the climate response to a large-scale nuclear war,"" Coupe said. ""There really would be a nuclear winter with catastrophic consequences."" In both the new and old models, a nuclear winter occurs as soot (black carbon) in the upper atmosphere blocks sunlight and causes global average surface temperatures to plummet by more than 15 degrees Fahrenheit. Because a major nuclear war could erupt by accident or as a result of hacking, computer failure or an unstable world leader, the only safe action that the world can take is to eliminate nuclear weapons, said Robock, who works in the School of Environmental and Biological Sciences. "
Science Daily,Would a Carbon Tax Help to Innovate More-Efficient Energy Use?,Science & Society,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827111120.htm,"   Despite advances in solar, wind, and other renewable energy sources, fossil fuels remain the primary source of the climate-change-causing carbon emissions. In order to halt global warming at the 2 degrees Celsius limit set by the Paris Agreement, we must reduce and eventually stop or completely offset carbon released into the atmosphere by burning of oil, coal, and gas. ""It has long been theorized that raising carbon prices would provide an incentive to reduce emissions through energy efficiency improvements,"" explained lead author Rong. ""So, we looked to history to determine how cost increases have affected energy use efficiency in the past."" The researchers developed their own version of the productivity model created by Nobel Prize-winning economist Robert Solow. They found that historically, in various countries, when the cost of energy comprised a larger fraction of the cost of production, those countries found new ways to reduce energy use or to use it more efficiently. Rong and his colleagues asked what would happen if these historical relationships between energy costs and efficiency improvements continued into the future. When this dynamic was continuously in play, according to their model, by 2100 energy usage would be reduced by up to 30 percent relative to simulations where this dynamic was not considered. ""Other studies have examined how taxing carbon emission would drive innovation in renewables,"" explained Caldeira. ""But we show that it would also lead to more-efficient consumption of energy -- not just by getting people to use better existing technology, but also by motivating people to innovate better ways to use energy. This means that solving the climate problem, while still hard, is a little easier than previously believed."" "
Science Daily,Deep Transformations Needed to Achieve Sustainable Development Goals,Science & Society,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112705.htm,"   The UN Sustainable Development Goals (SDGs) focus on time-bound targets for prosperity, people, planet, peace, and partnership -- collectively known as the five Ps. By adopting the 2030 Agenda with its 17 SDGs and the Paris Climate Agreement, UN member states effectively created a framework for national action and global cooperation on sustainable development, while the Paris Agreement committed signatory countries to achieving net-zero greenhouse gas emissions by the middle of the century. SDG 13 on climate change specifically links to the Paris Agreement noting that the UN Framework Convention on Climate Change ""is the primary international, intergovernmental forum for negotiating the global response to climate change."" Despite the interconnectivity and clear aims of these global goals, stakeholders seem to lack a shared understanding of how the 17 SDGs can be operationalized. Building on previous work by The World in 2050 -- a global research initiative established by IIASA -- the authors of the study published in the journal Nature Sustainability propose six transformations to organize SDG interventions through a semi-modular action agenda that can be designed by discrete, yet interacting, parts of government. According to the paper, the proposed framework may be operationalized within the structures of governments while still respecting the strong interdependencies across the 17 SDGs. The authors also outline an action agenda for science to provide the knowledge required for designing, implementing, and monitoring the SDG Transformations. ""The 2030 Agenda and the Paris Agreement have given the world an aspirational narrative and an actionable agenda to achieve a just, safe, and sustainable future for all within planetary boundaries. The six transformations provide an integrated and holistic framework for action that reduces the complexity, yet encompasses the 17 SDGs, their 169 targets, and the Paris Agreement. They provide a new approach to shift from incremental to transformational change; to identify synergies using sustainable development pathways; formulate actionable roadmaps; and a focus on inter-relationships to uncover multiple benefits and synergies,"" explains study co-author Nebojsa Nakicenovic, executive director of The World in 2050 (TWI2050) research initiative at IIASA. In their paper the researchers considered which key interventions would be necessary to achieve the SDG outcomes and how their implementation might be organized into a limited set of six transformations namely education, gender, and inequality; health, wellbeing, and demography; energy decarbonization and sustainable industry; sustainable food, land, water, and oceans; sustainable cities and communities; and digital revolution for sustainable development. To simplify the discussion of interlinkages between interventions and SDGs, the authors further identified intermediate outputs generated by combinations of interventions, which in turn contribute to the achievement of each SDG. Each SDG transformation describes a major change in societal structure (economic, political, technological, and social) to achieve long-term sustainable development, while also each contributing to multiple SDGs. Excluding any of them would make it virtually impossible to achieve the SDGs. Pursuing the six transformations will require deep, deliberate, long-term structural changes in resource use, infrastructure, institutions, technologies, and social relations, which have to happen in a relatively short time window. Previous societal transformations, like industrialization in 19th century Europe, were initiated by technological changes like the steam engine and were largely undirected, while 20th century technologies like semiconductors, the Internet and Global Positioning Systems, were promoted through directed innovation to meet military aims. The authors emphasize that it is crucial that SDG transformations are formally directed in order to meet time-bound, quantitative targets, such as net-zero carbon emissions by mid-century. ""By achieving change in these six key areas, we can save both people and planet. To deliver on both ambitious climate targets and meet all the Sustainable Development Goals, we identify very concrete levers that governments can pull. For instance, investing in agriculture with known technologies and management practices can enable both food security, human health, and climate mitigation. Investing in young children's education is another example. It improves human wellbeing, increases economic development, and stabilizes population growth,"" says study co-author Johan Rockström from the Potsdam Institute for Climate Impact Research in Germany. ""The six transformations in this paper have the ultimate goal of enhancing human prosperity and reducing inequalities. This is of course not easy. In fact, it is the largest human endeavor of all time. Science is here to provide governments with a fact-based framework. If political leadership fails to act, however, we would face unprecedented risks for the stability of societies, and for our Earth system."" "
Science Daily,New Rider Data Shows How Public Transit Reduces Greenhouse Gas and Pollutant Emissions,Science & Society,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826143407.htm,"   In a paper published in Environmental Research Communications, University of Utah researchers Daniel Mendoza, Martin Buchert and John Lin used tap-on tap-off rider data to quantify the emissions saved by buses and commuter rail lines, and also project how much additional emissions could be saved by upgrading the bus and rail fleet. The study was conducted in cooperation with the Utah Transit Authority and the Utah Department of Environmental Quality, Division of Air Quality. High-resolution rider data Mendoza and his colleagues are certainly not the first to ask how much pollution public transit can save. But a couple of recent technological advances have enabled them to answer the question with a level of detail previously unparalleled. The first is the advance of tap-on tap-off farecards that provide anonymized data on where those riders who have electronic passes enter and exit public transit. Approximately half of UTA's passengers use an electronic fare medium. ""Now we can truly quantify trips in both time and space,"" Mendoza says. ""We accounted for all of the 2016 passenger miles by scaling the farecard data, and we know which trips farecard holders make on buses, light rail and commuter rail."" The second is the General Transit Feed Specification system. It's the data source that supplies Google Maps with transit information to help users find the bus or train they need. With that data source, the researchers could track where and how often UTA's buses and trains run. So, with high-resolution data on the movement of both vehicles and passengers, the researchers could paint a nearly comprehensive picture of public transit along the Wasatch Front. Balancing emissions So, with that data, the researchers could quantify the emissions produced and miles traveled of the transit systems (TRAX light rail uses electricity produced outside the Wasatch Front, hence the emissions aren't in Salt Lake's air) and balance that with the miles traveled by passengers and the estimated amount of car travel avoided by riding transit. On weekdays during rush hours, and in densely populated areas, the balance was clearly on the side of reduced emissions. ""That tapers off significantly during the evening hours, on the outskirts of the city, and definitely during the weekends,"" Mendoza says. In those situations, the number of passengers and how far they rode transit did not offset certain criteria pollutant emissions. (Criteria pollutants are six common air pollutants that the EPA sets standards for through the Clean Air Act.) For transit to improve its regional reduction in emissions, particularly PM2.5 and NOx, the following strategies, alone or in combination, could be employed: more daily riders per trip, more clean-fuel buses and train cars and/or fewer low-ridership trips. What-ifs The current study looks at the bus and train fleet as they are now, with some UTA buses around 20 years old and FrontRunner trains whose engines are rated a Tier 0+ on a 0-4 scale of how clean a locomotive's emissions are (Tier 4 is the cleanest; UTA is scheduled to receive funds programmed through the Metropolitan Planning Organizations to upgrade FrontRunner locomotives to Tier 2+). So, Mendoza and his colleagues envisioned the future. ""What if we upgrade all these buses, some of them from 1996 or so?"" Mendoza says. ""They emit a significantly larger amount than the newer buses, which are 2013 and newer."" What if, they asked, UTA upgraded their buses to only 2010 models and newer, fueled by either natural gas or clean diesel? And what if the FrontRunner engines were upgraded to Tier 3? Emissions of some pollutants would drop by 50%, and some by up to 75%, they found. ""Now, with this information, UTA can go to stakeholders and funding agencies and say, 'Look, we've done this analysis,"" Mendoza says. ""This is how much less we can pollute.'"" Mendoza adds that taking transit offers additional benefits besides reducing air pollution. Taking transit gives riders time to read, work or listen while traveling. How does Mendoza know? He's a dedicated transit rider. ""I always get to where I need to go pretty much on time and completely unstressed,"" he says. ""I almost never drive."" "
Science Daily,Deep Transformations Needed to Achieve Sustainable Development Goals,Business & Industry,2019-08-26,-,https://www.sciencedaily.com/releases/2019/08/190826112705.htm,"   The UN Sustainable Development Goals (SDGs) focus on time-bound targets for prosperity, people, planet, peace, and partnership -- collectively known as the five Ps. By adopting the 2030 Agenda with its 17 SDGs and the Paris Climate Agreement, UN member states effectively created a framework for national action and global cooperation on sustainable development, while the Paris Agreement committed signatory countries to achieving net-zero greenhouse gas emissions by the middle of the century. SDG 13 on climate change specifically links to the Paris Agreement noting that the UN Framework Convention on Climate Change ""is the primary international, intergovernmental forum for negotiating the global response to climate change."" Despite the interconnectivity and clear aims of these global goals, stakeholders seem to lack a shared understanding of how the 17 SDGs can be operationalized. Building on previous work by The World in 2050 -- a global research initiative established by IIASA -- the authors of the study published in the journal Nature Sustainability propose six transformations to organize SDG interventions through a semi-modular action agenda that can be designed by discrete, yet interacting, parts of government. According to the paper, the proposed framework may be operationalized within the structures of governments while still respecting the strong interdependencies across the 17 SDGs. The authors also outline an action agenda for science to provide the knowledge required for designing, implementing, and monitoring the SDG Transformations. ""The 2030 Agenda and the Paris Agreement have given the world an aspirational narrative and an actionable agenda to achieve a just, safe, and sustainable future for all within planetary boundaries. The six transformations provide an integrated and holistic framework for action that reduces the complexity, yet encompasses the 17 SDGs, their 169 targets, and the Paris Agreement. They provide a new approach to shift from incremental to transformational change; to identify synergies using sustainable development pathways; formulate actionable roadmaps; and a focus on inter-relationships to uncover multiple benefits and synergies,"" explains study co-author Nebojsa Nakicenovic, executive director of The World in 2050 (TWI2050) research initiative at IIASA. In their paper the researchers considered which key interventions would be necessary to achieve the SDG outcomes and how their implementation might be organized into a limited set of six transformations namely education, gender, and inequality; health, wellbeing, and demography; energy decarbonization and sustainable industry; sustainable food, land, water, and oceans; sustainable cities and communities; and digital revolution for sustainable development. To simplify the discussion of interlinkages between interventions and SDGs, the authors further identified intermediate outputs generated by combinations of interventions, which in turn contribute to the achievement of each SDG. Each SDG transformation describes a major change in societal structure (economic, political, technological, and social) to achieve long-term sustainable development, while also each contributing to multiple SDGs. Excluding any of them would make it virtually impossible to achieve the SDGs. Pursuing the six transformations will require deep, deliberate, long-term structural changes in resource use, infrastructure, institutions, technologies, and social relations, which have to happen in a relatively short time window. Previous societal transformations, like industrialization in 19th century Europe, were initiated by technological changes like the steam engine and were largely undirected, while 20th century technologies like semiconductors, the Internet and Global Positioning Systems, were promoted through directed innovation to meet military aims. The authors emphasize that it is crucial that SDG transformations are formally directed in order to meet time-bound, quantitative targets, such as net-zero carbon emissions by mid-century. ""By achieving change in these six key areas, we can save both people and planet. To deliver on both ambitious climate targets and meet all the Sustainable Development Goals, we identify very concrete levers that governments can pull. For instance, investing in agriculture with known technologies and management practices can enable both food security, human health, and climate mitigation. Investing in young children's education is another example. It improves human wellbeing, increases economic development, and stabilizes population growth,"" says study co-author Johan Rockström from the Potsdam Institute for Climate Impact Research in Germany. ""The six transformations in this paper have the ultimate goal of enhancing human prosperity and reducing inequalities. This is of course not easy. In fact, it is the largest human endeavor of all time. Science is here to provide governments with a fact-based framework. If political leadership fails to act, however, we would face unprecedented risks for the stability of societies, and for our Earth system."" "
Science Daily,Victorian Child Hearing-Loss Databank to Go Global,Education & Learning,2019-08-30,-,https://www.sciencedaily.com/releases/2019/08/190830092107.htm,"   The Victorian Childhood Hearing Impairment Longitudinal Databank, which has collected information for eight years, is featured in the latest International Journal of Epidemiology. Its data shows that language development and speech in hearing-impaired children lags behind other children, despite advancements in earlier detection and intervention in the past decade. The paper's* lead author, Murdoch Children's Research Institute's (MCRI) Dr Valerie Sung, says researchers world-wide can use the databank to answer questions around childhood hearing loss. ""This register can help us understand why some children with a hearing loss do so well, while others experience greater difficulties,"" she says. ""Universal newborn hearing screening is detecting hearing loss earlier than ever before, usually within a few weeks of birth. ""Children with hearing loss have very early access to hearing aids, early intervention services and for some, cochlear implantation. It was expected that hearing-impaired children would quickly come to enjoy the same language and educational outcomes as their hearing peers. ""However, early clinical diagnosis and intervention does not guarantee equality in health outcomes, with language and related outcomes of children with hearing loss remaining on average well below population means and the children's true cognitive potential. ""Demonstrating the reasons for this inequality has been hampered until now by the lack of population based prospective research."" The Victorian Childhood Hearing Impairment Longitudinal Databank (VicCHILD) is a population-based longitudinal databank open to every child with permanent hearing loss in Victoria. VicCHILD started in 2012 and stems from 25 years of work by The Royal Children's Hospital and MCRI. At the end 2018, 807 children were enrolled and provided baseline data. By 2020 more than 1000 children will be taking part, making it the largest hearing databank in the world. VicCHILD collects data at enrolment, two years of age, school entry and late primary /early high school. It involves parent questionnaires, child assessments and taking saliva samples. Dr Sung, who is also a honorary fellow at the University of Melbourne, says about 600 Australian infants each year are diagnosed with congenital hearing loss within weeks of birth. ""As these children grow, they can face challenges in things that come naturally to others like language and learning. This can impact their quality of life,"" she says. ""Hearing loss incurs significant burden and medical costs and impacts adversely on educational attainment and employment opportunities. ""This important bank of information could improve interventions and ultimately the lives of children with hearing loss and their families. It will also act as a platform for research trials to understand the effectiveness of different interventions."" "
Science Daily,"Millennials, Think You're Digitally Better Than Us? Yes, According to Science",Education & Learning,2019-08-28,-,https://www.sciencedaily.com/releases/2019/08/190828092457.htm,"   Legend has it that millennials, specifically the ""Net Generation,"" use many technologies simultaneously, masterfully switching from one to the next. They claim that it's easy and that they can do it much better than older generations. Research, so far, hasn't proven this claim and the consequences of these incessant interruptions on attention and performance. Florida Atlantic University researchers in the Charles E. Schmidt College of Science are one of the first to examine this phenomenon in college-age students. The study provides some of the first results on whether or not ""Net Genners,"" who have grown up with widespread access to technology, are developing greater digital literacy than generations before them, and if this has enriched them with an ability to switch their attention more efficiently. For the study, researchers simulated a typical working environment, complete with IT interruptions, to allow them to track the effects on participants' inhibitory processes. One hundred and seventy-seven mostly college-age participants were divided into three groups: those who received IT interruptions; those who did not, and a control group. Researchers compared the three groups' accuracy and response time on completing tasks, gauging their level of anxiety. Results, published in the journal Applied Neuropsychology: Adult, indicate that there is no need to ""pardon these interruptions,"" at least for this younger generation. Findings show that switching between technologies did not deplete or diminish performance in the group that had the IT interruptions compared to the control group or the group that did not receive IT interruptions. Unexpectedly, however, researchers discovered diminished performance in the participants from the group that did not receive any IT interruptions. All three groups reported low levels of anxiety during the study. Seventy-five percent of two of the groups reported their anxiety as ""not at all"" or ""a little bit,"" and the researchers did not find any significant differences between groups. ""We were really surprised to find impaired performance in the group that did not receive any information technology interruptions. It appears that the Net Generation thrives on switching their attention and they can do it more efficiently because information technology is woven throughout their daily lives,"" said Mónica Rosselli, Ph.D., senior author, professor and assistant chair of psychology in FAU's Charles E. Schmidt College of Science, and a member of the FAU Brain Institute (I-BRAIN), one of the University's four research pillars. ""Because younger generations are so accustomed to using instant messaging, pop-ups like the ones we used for our study, may blend into the background and may not appear surprising or unplanned, and therefore may not produce anxiety."" Prior research in the general population has found that it takes about 25 minutes to return to an original task following an IT interruption and 41 percent of these interruptions result in discontinuing the interrupted task altogether. Emails alone cause about 96 interruptions in an eight-hour day with an added one-and-a-half hours of recovery time per day. Results of the new FAU study sheds light on younger generations who have commonly used instant messaging as a major communication tool and this communication preference may reveal a perception gap between generations. ""How we adapt to technology and leverage it to our advantage by deciding what information we attend to at any given moment has substantial implications on our ability to remain valuable and productive in our respective work and education domains,"" said Deven M. Christopher, co-author and a graduate psychology student at FAU. ""Results from our study may provide a basis for further research, especially because younger generations are developing in a more connected world than preceding generations."" "
Science Daily,Family-School Engagement Has Specific Perks for Young Students,Education & Learning,2019-08-27,-,https://www.sciencedaily.com/releases/2019/08/190827123536.htm,"   After surveying more than 3,170 students and 200 teachers, researchers at the University of Missouri found that families are less engaged with their child's schooling in middle school than they are when their child is in elementary school. However, the researchers also found a silver lining: Both elementary school children and middle school children are less likely to have concentration problems and behavioral issues at the end of a school year if their parents made a greater effort to be engaged with their schooling earlier in the year. ""In addition to being less likely to have emotional or behavioral issues in class, we also found that students with engaged parents ended the year with better social skills and were able to focus on tasks easier,"" said Tyler Smith, a senior research associate in the College of Education. ""This means that when parents are more involved at school, the benefits to their child grow over time."" The researchers said that family-school engagement often drops from elementary to middle school for several reasons, including a change in student-teacher ratio and a desire to respect their child's growing sense of independence. ""Keeping in contact with multiple teachers can be more challenging for parents with children in middle school, but our study shows evidence that parents and teachers should continue to make an effort to connect,"" said Keith Herman, a professor in the College of Education and co-author on the study. ""There are many options for parents to become more involved at both levels without feeling intrusive."" Herman suggests that parents can explore getting involved with their child's schooling in a variety of ways. Options outside of the home include attending school functions, volunteering at events and joining parent groups. However, parents and family members can also take a more active role by helping with homework and keeping in touch with the child's teacher(s). Smith adds that teachers can also do their part in encouraging families to get more involved by providing opportunities for parents to connect with them. ""Teachers have a lot on their hands, obviously, but even small efforts to help build better family-teacher relationships can have big payoffs for everyone involved,"" Smith said. ""Teachers might consider inviting parents to special events or giving students assignments that involve their parents so that the students can help begin to build that relationship naturally."" "
